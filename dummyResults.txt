{"numFound":18,"docs":[{"@type":"ScientificDocument","id":14,"appId":"397f59ab4065226b181bd1597701afb099a8ffae","timeCreated":1455821742570,"timeModified":1461915875742,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.5321749,"uri":"file:///Users/cheny13/Documents/References/SenseCam reminiscence and action recall.pdf","plainTextContent":"This article was downloaded by: [Aalto-yliopiston kirjasto]\rOn: 15 June 2015, At: 00:37\rPublisher: Routledge\rInforma Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK\rClick for updates\rMemory\rPublication details, including instructions for authors and subscription information:\rhttp://www.tandfonline.com/loi/pmem20\rSenseCam reminiscence and action recall in memory-unimpaired people\rJohn G. Seamona, Tacie N. Moskowitza, Ashley E. Swana, Boyuan Zhonga, Amy Golembeskia, Christopher Lionga, Alexa C. Narzikula & Olumide A. Sosana\ra Department of Psychology, Wesleyan University, Middletown, CT, USA Published online: 01 Oct 2013.\rTo cite this article: John G. Seamon, Tacie N. Moskowitz, Ashley E. Swan, Boyuan Zhong, Amy Golembeski, Christopher Liong, Alexa C. Narzikul & Olumide A. Sosan (2014) SenseCam reminiscence and action recall in memory- unimpaired people, Memory, 22:7, 861-866, DOI: 10.1080/09658211.2013.839711\rTo link to this article: http://dx.doi.org/10.1080/09658211.2013.839711\rPLEASE SCROLL DOWN FOR ARTICLE\rTaylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability\rfor any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of\rthe Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.\rThis article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. Terms & Conditions of access and use can be found at http://www.tandfonline.com/page/terms-and-conditions\r\nMemory, 2014\rVol. 22, No. 7, 861–866, http://dx.doi.org/10.1080/09658211.2013.839711\rSenseCam reminiscence and action recall in memory- unimpaired people\rJohn G. Seamon*, Tacie N. Moskowitz, Ashley E. Swan, Boyuan Zhong,\rAmy Golembeski, Christopher Liong, Alexa C. Narzikul, and Olumide A. Sosan\rDepartment of Psychology, Wesleyan University, Middletown, CT, USA\r(Received 23 May 2013; accepted 26 August 2013)\rCase studies of memory-impaired individuals consistently show that reminiscing with SenseCam images enhances event recall. This exploratory study examined whether a similar benefit would occur for the consolidation of memories in memory-unimpaired people. We tested delayed recall for atypical actions observed on a lengthy walk. Participants used SenseCam, a diary, or no external memory aid while walking, followed by reminiscence with SenseCam images, diary entries, or no aid, either alone (self- reminiscence) or with the experimenter (social reminiscence). One week later, when tested without SenseCam images or diary entries, prior social reminiscence produced greater recall than self-reminiscence, but there were no differences between memory aid conditions for action free recall or action order recall. When methodological variables were controlled, there was no recall advantage for SenseCam reminiscence with memory-unimpaired participants. The case studies and present study differ in multiple ways, making direct comparisons problematic. SenseCam is a valuable aid to the memory impaired, but its mnemonic value for non-clinical populations remains to be determined.\rKeywords: SenseCam reminiscence; Memory for actions; Memory aids.\rMiniature automatic cameras offer the promise of effortlessly recording life experiences into e-mem- ories for later reminiscence. Originally developed by Microsoft, SenseCam is a small, lightweight camera that is worn around the neck by a lanyard. It has a large memory and takes fish-eye, wide- angle pictures of the environment, either manually or automatically, every 30 seconds, from the perspective of the wearer. At the end of the day the camera images can be uploaded into a com- puter and used for reminiscence viewing. Research employing this camera has shown beneficial effects on recall for select individuals with organic mem- ory impairments.\rBerry et al. (2007), for example, used Sense- Cam in their study of a 63-year-old woman named\rMrs B who demonstrated mild to moderate retro- grade amnesia and marked anterograde amnesia due to limbic encephalitis. Following her brain infection she had difficulty remembering past experiences, including recent experiences shared with her husband. When Mrs B’s husband kept a written diary of shared events for nightly reminis- cence with his wife, little change was observed in Mrs B’s recall. Later, Mrs B wore a SenseCam for several weeks, and her husband uploaded the images into their computer. Together they remi- nisced nightly about their shared experiences while viewing the SenseCam images. Mrs B was able to recall approximately 80% of the recent images, and her episodic recall was maintained 3 months after viewing the pictures. Berry et al.\r*Address correspondence to: John Seamon, Department of Psychology, Wesleyan University Middletown, CT 06459, USA. E-mail: jseamon@wesleyan.edu\r© 2013 Taylor & Francis\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015\r\n862 SEAMON ET AL.\radded that Mrs B was not simply recalling the images; she provided details of past events that were not captured in those pictures.\rSimilar memory enhancing effects with Sense- Cam have been observed with individuals suffering from anterograde amnesia—including a 47-year- old woman with herpes simplex viral encephalitis (Loveday & Conway, 2011), a 21-year-old man with acquired brain injury (Brindley, Bateman, & Gracey, 2011), a 13-year-old boy with a brain tumor (Pauly-Takacs, Moulin, & Estlin, 2011), and a 55-year-old woman with mild cognitive impair- ment (Browne et al., 2011). In each case, remin- iscence with SenseCam images produced greater recall and a greater sense of wellbeing than reminiscence with verbal diaries. Loveday and Conway (2011) stated that SenseCam images can cue a “Proustian moment” in which individuals experience intense recollections of past events, even accessing memories of formerly inaccessible events not captured in the SenseCam pictures, a finding also reported by others (Berry et al., 2007; Brindley et al., 2011; Browne et al., 2011).\rAlthough these case studies show that SenseCam aids episodic recall, they were conducted under conditions where experimental controls were neces- sarily less stringent than those typically observed in laboratory testing. In Berry et al.’s (2007) study of Mrs B, for example, her husband determined which activities to record in a daily diary, he wrote the diary entries after they occurred, he conducted the remin- iscence sessions, and he graded his wife’s memory performance. This study and other SenseCam case studies have typically included a small number of recorded events that were confounded with the type of reminiscence used for each event (e.g., one event had SenseCam reminiscence; a different event had diary reminiscence). These methodological caveats in no way diminish the importance of the SenseCam case studies for demonstrating the benefits of a new technology with memory-impaired individuals. In fact the similarity of the findings from the case studies suggests that Mr B, for one, performed in a highly effective fashion. Still, the limited control of poten- tially important methodological variables in these studies leaves open the issue of what specifically led to enhanced episodic recall following SenseCam reminiscence.\rAdditionally, only a few studies have explored the use of SenseCam with memory-unimpaired people to determine if it has wider application. St. Jacques, Conway, and Cabezza (2011) measured brain activation patterns from SenseCam images or verbal cues with fMRI; Silva, Pinho, Macedo,\rand Moulin (2013) observed better performance on a neurological test battery for people who pre- viously reviewed their SenseCam images of every- day events than their diary entries; and Finley, Brewer, and Benjamin (2011) found better recog- nition of SenseCam images and enhanced recall of their depicted activities for previously reviewed than not-reviewed images. The present research extended these studies by testing whether Sense- Cam offered any mnemonic advantage for mem- ory-unimpaired people in recalling previously seen actions in a natural setting.\rSpecifically, we examined how well memory- unimpaired participants recalled actions they previously saw performed 1 week earlier on a structured walk following reminiscence with SenseCam images, a written diary, or no external memory aid. Control of potentially important methodological variables was achieved in several ways. First, all participants observed the same sequence of actions performed in the same loca- tions on a campus walk so that experiences were not confounded with type of reminiscence. Second, immediately after the walk participants reminisced with either their SenseCam images taken during the walk, their diary entries written during the walk, or what they remembered from the walk on their own. All participants engaged in either social or self-reminiscence. Social reminiscence was designed to approximate the reminiscence discussions reported in the case studies between a memory-impaired person and spouse, whereas self-reminiscence had participants reminisce alone for the same length of time.\rIf a memory camera can aid memory-unimpaired people, the recall of previously observed actions should be better for participants who used Sense- Cam during reminiscence than those who used their diary or had no external memory aid. Additionally, social reminiscence should produce greater delayed recall than self-reminiscence based on studies of collaborative recall that show that accurate information from another person enhances memory (e.g., Roediger, Meade, & Bergman, 2001; Wright, Self, & Justice, 2000) and studies of the testing effect (e.g., Roediger & Karpicke, 2006) where retesting enhances reten- tion. In the present context if immediate reminis- cence after a walk constitutes a practice recall test, experimenter-based cueing with corrective feedback provides a relearning opportunity and should produce better recall than self-based cueing with no feedback.\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015\r\nParticipants\rMETHOD\rparticipants for the three memory aid conditions, group sizes that should provide an adequate test of any meaningful memory aid effect.\rThe individually tested participants were told that this two-session experiment involved memory for actions to be observed on a campus walk. They were informed that the walk would take approxi- mately 1 hour, with stops at 43 locations where the experimenter would state an action and perform it. All participants were instructed to try to remember those actions for an unspecified, delayed memory test. They could talk with the experimenter during the walk, but they could not engage in any activities other than those that were part of the study.\rParticipants in the two SenseCam groups wore the camera and it took pictures automatically, approximately once every 30 seconds. At each location where an action was stated and per- formed, the experimenter maintained that action until the camera recorded it. Participants in the two diary groups wrote the name of the location and the action performed during each stop in a diary booklet that contained one page for each location–action pair. Similar to the memory cam- era, the diary ensured that all location–action pairs were recorded in their correct sequential order. Finally participants in the two no aid groups observed the same locations and actions for the same length of time, approximately 20–30 seconds per stop, but they had to rely solely on their memory for retention.\rUpon completing the walk all participants received a 5-minute distraction task of finding Waldo characters in a Finding Waldo book. This task gave the experimenter sufficient time to upload the images from a SenseCam participant into the computer.\rFollowing the distraction task, all participants engaged in social or self-reminiscence for 25 minutes. One group of participants from the SenseCam, Diary, and No Aid Conditions (Groups 1, 3, & 5) engaged in social reminiscence. For the SenseCam participants the experimenter sequen- tially reviewed the images on the computer by skimming quickly through the irrelevant images, stopping and focusing on the action images, and asking the participant to recall each action as it was displayed. For the diary participants the experi- menter sequentially reviewed their diary entries, asking the participant to state each written action. For the no aid participants the experimenter asked them to recall the actions from the walk in sequence. In each condition the experimenter sat with the participant and engaged in an open but\rThe participants were 144 Wesleyan University students, between 17 and 23 years of age, who received introductory psychology credit or served as paid volunteers. None had taken part in any related memory research.\rMaterials\rThe SenseCam memory camera used in this study was sold commercially as the Vicon Revue. The walk and accompanying actions were similar to those used by Seamon et al. (2009; Seamon, Philbin, & Harrison, 2006). We chose 43 distinct, physical locations involving five campus buildings in close proximity. All locations were indoors to permit testing in varied weather conditions (e.g., an elevator door in a hallway; a Pepsi machine in a snack area). Because SenseCam is deemed best suited for recording non-mundane events, we used an atypical action at each location (e.g., Press the elevator button with one elbow) to minimise pre- experimental associations between locations and actions (the action–location list is available from the first author). No location or action was used more than once.\rProcedure\rWe employed a 3 × 2 design with Memory Aid during the walk and reminiscence period (Sense- Cam vs Diary vs No Aid) and Reminiscence Type after the walk (social vs self-reminiscence) as between-participants variables. The participants were randomly assigned to one of six groups. Two groups used the memory camera, two groups wrote in a diary, and two groups had no external memory aid. After the walk one group within each memory aid condition socially reminisced with the experimenter, whereas the other group reminisced alone. The six groups were SenseCam/Social (n = 25), SenseCam/Self (n = 24), Diary/Social (n = 23), Diary/Self (n = 25), No Aid/Social (n = 24), and No Aid/Self (n = 27). Because our research was novel and different from the case studies in participants and procedures, no estimate of effect size was possible. The sizes of our six groups are typical of memory research, and pooling over the two reminiscence conditions yielded 49, 48, and 51\rSENSECAM REMINISCENCE 863\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015\r\n864 SEAMON ET AL.\rstructured conversation, helping to guide the parti- cipant’s oral recall of the action sequence. Mon- itoring the participant’s recall, the experimenter provided corrective feedback for missing actions, incorrect actions, or action sequencing errors. Dur- ing the reminiscence period all participants dis- cussed and reviewed all actions at least twice at their own pace with the experimenter.\rThe other participants in the SenseCam, Diary, and No Aid Conditions (Groups 2, 4, & 6) engaged in self-reminiscence for the same length of time. Participants who used the camera were presented with their images on the computer and shown how to toggle through the images sequentially, partici- pants who wrote in a diary were given their diaries and instructed to review their diary entries, and participants in the no aid condition were asked to remember the walk on their own over the interval. All participants were informed that the reminis- cence period was important for long-term retention and that they should review the walk’s locations and actions several times at their own rate. Observed from a distance, the participants com- plied with this instruction. Following reminiscence all participants were scheduled for a follow-up session 1 week later, plus or minus 1 day.\rTwo memory tests were given in the second session. The first test was a written free recall test of the 43 actions observed previously during the walk. The participants were given 12 minutes to recall as many actions as they could remember, without regard to order. Next the participants were given a test of action order in which they were given 5 note cards, each with the name of a campus building used on the walk, and a shuffled deck of 43 cards, each containing a previously observed action. After sorting the actions by their campus buildings, the participants were asked to place them in their correct sequential order on a display board that was numbered from 1 to 43. They had 15 minutes to sort the actions by building and put them in their sequential order, guessing if they were unsure.\rRESULTS\rWe scored the free recall data in two ways: first by a strict criterion in which the recalled action had to be essentially the same as the action that was performed during the walk; second by a lenient criterion in which the gist of action was recalled with no inaccuracy. For example, for the action Rub the cheek of Olin’s bust in the Olin Library Rotunda, a correct recall by the strict criterion\rcould be Rubbed the cheek of the statue in Olin, whereas a correct lenient recall could be Touched the statue in Olin. The same statistical analyses were applied to all recall data from the strict and lenient criteria, and these analyses yielded ident- ical outcomes in each instance. To conserve space we will provide only those analyses based on the lenient scoring criterion.\rFree recall\rTable 1 shows that participants produced greater delayed free recall following social reminiscence than self-reminiscence (.78 vs .64) F(1, 142) = 13.26, MSE = .70, ηp2= .09, p < .001, but their recall was unaffected by the memory aid condi- tions. Action recall was not reliably different for the SenseCam (.74), diary (.69), and no aid (.68) conditions, F < 1, and there was no interaction between memory aid and reminiscence type, F < 1. Because eight people scored low on the free recall test (.30 or less), we also applied an arc sine transformation to the data, but this transformation did not change the outcome. There remained a main effect of reminiscence type, F(1, 142) = 14.79, MSE = 1.27, ηp2= .09, p < .001, but no effect of memory aid, F < 1, and no memory aid by reminiscence type interaction, F < 1. Because these eight outliers were not evenly distributed across conditions (one from SenseCam/Social, two from Diary/Self, and five from No Aid/Self), we did not remove them from the analyses.\rOrder recall\rWe calculated a Spearman rank-order correlation based on each participant’s recalled action order and the actual order from the walk, and performed the same two-factor analysis of variance on the correlations that we used with the free recall data. These results (absent the correlations from six participants that were lost) are also shown in Table 1. Participants in all conditions demon- strated strong action order memory by producing high positive correlations. There was no difference between the three memory aid conditions, F < 1, the two reminiscence types, F(1, 126) = 1.81, MSE = .06, η2= .01, p > .10, and no interaction of these variables, F(2, 126) = 1.13, MSE = .04, η2= .02, p > .10. Our participants’ general familiarity with the campus buildings may have contributed partly to their strong action order memory.\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015\r\nTABLE 1\rDelayed action free recall and action order recall\rcognitive resources were generally sufficient for the present task. Although the free recall results did not exhibit a ceiling effect in any condition, more demanding tasks might show a SenseCam advantage over the diary and no aid conditions.\rEqually surprising was our finding that partici- pants in the no aid condition recalled as much as participants in the two aid conditions. It might be that the no aid participants, knowing that the only source of the actions for a later test was their memory, focused harder on remembering the actions during the walk than participants with the memory camera or diary that provided a record of those actions. Sparrow, Lui, and Wegner (2011) found that people tend to forget information that they believe is available, but remember informa- tion they think is unavailable. A similar effect might have operated in the present study, mini- mising potential differences between our memory aid conditions.\rDirect comparisons between the case studies that have consistently shown beneficial effects of SenseCam reminiscence and the present study remain problematic for several reasons. First, memory-impaired individuals may benefit more from SenseCam reminiscence than memory- unimpaired people because the camera, in aug- menting diminished cognitive function, is a more useful prosthetic device for the memory impaired. Second, the data-scoring procedures for the case studies and the present experiment were different. In the case studies recall was scored in terms of how well an event was subsequently recalled from either the memory-impaired person’s immediate recall (Brindley et al., 2011; Loveday & Conway, 2011) or the recall of the memory-impaired person’s spouse (Berry et al., 2007; Browne et al., 2011). In either instance the number of details could vary with each event. In our study recall was scored in terms of how many actions the partici- pants in each condition recalled from the same campus walk. Third, the memory-impaired indivi- duals in the case studies wore the memory camera for special events, and they often reminisced multiple times with the images in different sessions (e.g., Berry et al., 2007; Browne et al., 2011; Pauly- Takacs et al., 2011). In our study the actions from the walk were reminisced several times in a single session, shortly after the actions were observed. Differences in massed vs distributed reminiscence and the amount of reminiscence (cf., Cepeda, Pashler, Vul, Wixted, & Rohrer, 2006) might influence SenseCam’s effectiveness.\rSENSECAM REMINISCENCE 865\rMemory aid and type of reminiscence\rSenseCam\rDiary\r.75 (.20) .64 (.23)\r.71 (.22) .59 (.24)\r.85 (.19) .86 (.20)\rRecalls are mean proportions correct;\rrank-order correlations. Standard deviations in parentheses.\rIn summary, social reminiscence was better than self-reminiscence for delayed action recall, but reminiscing with SenseCam images or diary entries was not better than unaided reminiscence. Although the camera images captured all actions visually and the diary entries, scored by the strict (.95) or lenient (.99) recall criteria, were virtually verbatim copies of all stated actions, we found that faithful representa- tions of the actions for reminiscence, whether by SenseCam images or diary entries, did not enhance action recall relative no external aid.\rGENERAL DISCUSSION\rOur comparison of reminiscence conditions showed no memory consolidation benefit from prior reminiscence with SenseCam images or diary entries for the recall of unrelated, atypical actions, observed 1 week earlier by memory-unimpaired people. Participants with unaided memory remem- bered the actions just as well as those previously armed with a memory camera or diary. Social reminiscence immediately after the walk did lead to better recall than self-reminiscence, likely reflecting a testing effect with experimenter-based cueing and corrective feedback better than self- based cueing and no feedback.\rGiven that SenseCam has consistently enhanced recall in memory-impaired individuals, our finding of no difference between SenseCam, Diary, and No Aid reminiscence was surprising. Our use of memory-unimpaired participants is likely a contributing factor, as the camera was designed to assist memory-impaired people. It may be that our memory-unimpaired participants revealed no SenseCam benefit because their\rAction recall—lenient criterion Social reminiscence\rSelf reminiscence\rAction recall—strict criterion Social reminiscence\rSelf reminiscence\rAction order recall—correlations\rSocial reminiscence .92 (.15) Self reminiscence .87 (.18)\rNo Aid\r.78 (.18) .59 (.30)\r.75 (.19) .56 (.31)\r.93 (.15) .84 (.19)\r.79 (.22) .68 (.21)\r.75 (.26)\r.62 (.21)\rorder recalls are mean\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015\r\n866 SEAMON ET AL.\rIn addition our use of unrelated actions on the campus walk made it difficult for SenseCam images to trigger memories of other actions because our atypical actions were arbitrary and independent. The actions on the walk lacked a coherent schema that linked them into a larger, meaningful episode. Unlike the present study, the SenseCam images in the case studies were obtained from recording meaningful events—such as a visit to a museum or the beach—where recalling one aspect of an event could trigger related aspects of the event that the memory-impaired person previously experienced (see Hodges, Berry, & Wood, 2011; Loveday & Conway, 2011). Sense- Cam memory enhancement might be sensitive to the coherence of an event.\rFinally, unlike the present study, the case studies have provided SenseCam images to individuals at test to trigger memories of events that are captured in the images as well as aspects of those events not explicitly shown. Memory-impaired people may or may not recall the events depicted in the images at test. The same procedure cannot be used with memory-unimpaired people. If we had given our memory-unimpaired participants either their Sen- seCam images or diary entries at test, we would not know whether they were recalling the actions from those aids or their memory. Most likely they would merely state what was shown in the image or diary entry. Consequently we offered no aid to our participants on the delayed memory test to deter- mine how well our study and reminiscence condi- tions influenced their long-term recall. Future testing might consider providing partial information via SenseCam images or diary entries to determine the effectiveness of each aid for cueing the remain- ing uncued material.\rIn closing, when methodological variables were controlled across memory aid conditions, we found that SenseCam provided memory-unimpaired peo- ple no consolidation benefit in recalling atypical actions observed on a campus walk. SenseCam is a valuable aid to the memory impaired, but its value for non-clinical populations remains to be determined.\rREFERENCES\rBerry, E., Kapur, N., Williams, L., Hodges, S., Watson, P., Smyth, G., ... Wood, K. (2007). The use of a wearable camera, SenseCam, as a pictorial diary to improve autobiographical memory in a patient with\rlimbic encephalitis: A preliminary report. Neuropsy-\rchological Rehabilitation, 17, 582–601.\rBrindley, R., Bateman, A., & Gracey, F. (2011).\rExploration of the use of SenseCam to support autobiographical memory retrieval within a cognit- ive-behavioural therapeutic intervention following acquired brain injury. Memory, 19, 745–757.\rBrowne, G., Berry, E., Kapur, N., Hodges, S., Smyth, G., Watson, P., & Wood, K. (2011). SenseCam improves memory for recent events and quality of life in a patient with memory retrieval difficulties. Memory, 19, 713–722.\rCepeda, N. J., Pashler, H., Vul, E., Wixted, J., & Rohrer, D. (2006). Distributed practice in verbal recall tasks: A review and quantitative synthesis. Psychological Bulletin, 132, 354–380.\rFinley, J. R., Brewer, W. F., & Benjamin, A. S. (2011). The effects of end-of-day picture review and a sensor-based picture capture procedure on autobio- graphical memory using SenseCam. Memory, 19, 796–807.\rHodges, S., Berry, E., & Wood, K. (2011). SenseCam: A wearable camera that stimulates and rehabilitates autobiographical memory. Memory, 19, 685–696.\rLoveday, C., & Conway, M. A. (2011). Using SenseCam with an amnesic patient: Accessing inaccessible everyday memories. Memory, 19, 697–704.\rPauly-Takacs, K., Moulin, C. J. A., & Estlin, E. J. (2011). SenseCam as a rehabilitation tool in a child with anterograde amnesia. Memory, 19, 705–712.\rRoediger, H. L. III, & Karpicke, J. D. (2006). The power of testing memory: Basic research and impli- cations for educational practice. Perspectives on Psychological Science, 1, 181–210.\rRoediger, H. L., III, Meade, M. L., & Bergman, E. T. (2001). Social contagion of memory. Psychonomic Bulletin & Review, 8, 365–371.\rSeamon, J. G., Blumenson, C. N., Karp, S. R., Perl, J. J., Rindlaub, L. A., & Speisman, B. B. (2009). Did we see someone shake hands with a fire hydrant?: Collaborative recall affects false recollections from a campus walk. American Journal of Psychology, 122, 235–247.\rSeamon, J. G., Philbin, M. M., & Harrison, L. G. (2006). Do you remember proposing marriage to the Pepsi machine? False recollections from a campus walk. Psychonomic Bulletin & Review, 13, 752–756.\rSilva, A. R., Pinho, S., Macedo, L. M., & Moulin, C. J. (2013). Benefits of SenseCam review on neuropsy- chological test performance. American Journal of Preventive Medicine, 44, 302–307.\rSparrow, B., Lui, J., & Wegner, D. M. (2011). Google effects on memory: Cognitive consequences of having information at our fingertips. Science, 333, 776–778.\rSt. Jacques, P. L., Conway, M. A., & Cabeza, R. (2011). Gender differences in autobiographical memory for everyday events: Retrieval elicited by SenseCam images versus verbal cues. Memory, 19, 723–732.\rWright, D. B., Self, G., & Justice, C. (2000). Memory conformity: Exploring misinformation effects when presented by another person. British Journal of Psychology, 91, 189–202.\rDownloaded by [Aalto-yliopiston kirjasto] at 00:37 15 June 2015","title":"SenseCam reminiscence and action recall in memory-unimpaired people","eventTime":1450258801000,"mimeType":"application/pdf","authors":[],"keywords":["SenseCam reminiscence","Memory for actions","Memory aids"],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"diari","time":1461915875706,"auto":true,"weight":1.0},{"@type":"Tag","text":"memori","time":1461915875726,"auto":true,"weight":1.0},{"@type":"Tag","text":"sensecam","time":1461915875701,"auto":true,"weight":1.0},{"@type":"Tag","text":"action","time":1461915875716,"auto":true,"weight":1.0},{"@type":"Tag","text":"2011","time":1461915875738,"auto":true,"weight":1.0},{"@type":"Tag","text":"reminisc","time":1461915875697,"auto":true,"weight":1.0},{"@type":"Tag","text":"aid","time":1461915875733,"auto":true,"weight":1.0},{"@type":"Tag","text":"walk","time":1461915875711,"auto":true,"weight":1.0},{"@type":"Tag","text":"recal","time":1461915875721,"auto":true,"weight":1.0},{"@type":"Tag","text":"unimpair","time":1461915875742,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":9,"appId":"0e317291f240436f4ed73345ab884f77dec76fb3","timeCreated":1455056110222,"timeModified":1461915877056,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.5048654,"uri":"file:///Users/cheny13/Documents/References/experience to memory.pdf","plainTextContent":"17 Spelke, E.S. (1994) Initial knowledge: six suggestions. Cognition 50, 431–445\r18 Flavell, J.H. (1963) The Developmental\rPsychology of Jean Piaget, Nostrand\r19 Baillargeon, R. (1995) A model of physical reasoning\rin infancy. In Advances in Infancy Research (Rovee-\rCollier, C. and Lipsitt, L.P., eds), pp. 305–371, Ablex\r20 Baillargeon, R. et al. (1995) The acquisition of\rphysical knowledge in infancy. In Causal Cognition: A Multidisciplinary Debate (Sperber, D. et al., eds), pp. 79–116, Clarendon Press\r21 Baillargeon, R. (1998) Infants’ understanding of the physical world. In Advances in Psychological Science (Sabourin, M. et al., eds), pp. 503–529, Psychology Press\r22 Hespos, S.J. and Baillargeon, R. (2001) Infants’ knowledge about occlusion and containment events: a surprising discrepancy. Psychol. Sci. 12, 140–147\r23 Baillargeon, R. and DeVos, J. (1991) Object permanence in 3.5- and 4.5-month-old infants: further evidence. Child Dev. 62, 1227–1246\r24 Baillargeon, R. and Graber, M. (1987) Where’s the rabbit? 5.5-month-old infants’ representation of the height of a hidden object. Cogn. Dev. 2, 375–392\r25 Baillargeon, R. The acquisition of physical knowledge in infancy: A summary in eight lessons. In Handbook of Childhood Cognitive Development (Goswami, U., ed.), Blackwell (in press)\r26 Spelke, E.S. et al. (1995) Infants’ knowledge of object motion and human action. In Causal Cognition: A Multidisciplinary Debate (Sperber, D. et al., eds), pp. 44–78, Clarendon Press\r27 Aguiar, A. and Baillargeon, R. Developments in young infants’ reasoning about occluded objects. Cogn.Psychol.(inpress)\r28 Baillargeon, R. Infants’ physical knowledge: of acquired expectations and core principles. In Language, Brain, and Cognitive Development: Essays in Honor of Jacques Mehler (Dupoux, E., ed.), MIT Press (in press)\r29 Aguiar, A. and Baillargeon, R. (2000) Perseveration and problem solving in infancy. In Advances in Child Development and Behavior (Reese, H.W., ed.), pp. 135–180, Academic Press\r30 Sitskoorn, S.M. and Smitsman, A.W. (1995) Infants’ perception of dynamic relations between objects: passing through or support? Dev. Psychol. 31, 437–447\r31 Spelke, E.S. et al. (1995) Spatio-temporal continuity, smoothness of motion, and object identity in infancy. Br. J. Dev. Psychol. 13, 113–142\r32 Wilcox, T. and Baillargeon, R. (1998) Object individuation in infancy: the use of featural information in reasoning about occlusion events. Cogn. Psychol. 17, 97–155\r33 Xu, F. and Carey, S. (1996) Infants’ metaphysics: the case of numerical identity. Cogn. Psychol. 30, 111–153\r34 Van de Walle, G. et al. (2001) Bases for object individuation in infancy: evidence from manual search. J. Cogn. Dev. 1, 249–280\r35 Wilcox, T. and Chapa, C. Infants’ reasoning about opaque and transparent occluders in an individuation task. Cognition (in press)\r36 Wilcox T. et al. Object individuation in infancy. In Progress in Infancy Research (Fagan, F. and Hayne, H., eds), Erlbaum (in press)\r37 Santos, L.R. et al. Object individuation using property/kind information in rhesus macaques (macacamulatta).Cognition(inpress)\r38 Uller, C. et al. (1997) Is language needed for constructing sortal concepts? A study with nonhuman primates. In Proceedings of the 21st Annual Boston University Conference on Language Development (Hughes, E., ed.),\rpp. 665–677, Oxford University Press\r39 Wilcox, T. (1999) Object individuation: infants’ use of\rshape, size, pattern, and color. Cognition 72, 125–166 40 Wilcox, T. and Baillargeon, R. (1998) Object\rindividuation in young infants: further evidence\rwith an event-monitoring task. Dev. Sci. 1, 127–142 41 Wilcox, T. and Schweinle, A. Object individuation\rand event mapping: developmental changes in infants’ use of featural information. Dev. Sci. (in press)\r42 Needham, A. and Baillargeon, R. (2000) Infants’ use of featural and experiential information in segregating and individuating objects: a reply to Xu, Carey, and Welch. Cognition 74, 255–284\r43 Baldwin, D.A. et al. (1993) Infants’ ability to draw inferences about non-obvious object properties: evidence from exploratory play. Child Dev.\r64, 711–728\r44 Mandler, J.M. (2000) Perceptual and conceptual processes in infancy. J. Cogn. Dev. 1, 3–36\r45 Needham, A. and Modi, A. (2000) Infants’ use of prior experiences with objects in object segregation: Implications for object recognition in early infancy. In Advances in Child Development and Behavior (Reese, H.W., ed.), pp. 99–133, Academic Press\r46 Quinn, P.C. and Eimas, P.D. (1996) Perceptual organization and categorization in young infants. In Advances in Infancy Research (Rovee-Collier, C. andLipsitt,L.P.,eds),pp.1–36,Ablex\rReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 93\rObserving the transformation of experience into memory\rKen A. Paller and Anthony D. Wagner\rThe ability to remember one’s past depends on neural processing set in motion at the moment each event is experienced. Memory formation can be observed by segregating neural responses according to whether or not each event is recalled or recognized on a subsequent memory test. Subsequent memory analyses have been performed with various neural measures, including brain potentials extracted from intracranial and extracranial electroencephalographic recordings, and hemodynamic responses from functional magnetic resonance imaging. Neural responses can predict which events, and which aspects of those events, will be subsequently remembered or forgotten, thereby elucidating the neurocognitive processes that establish durable episodic memories.\rSome of life’s episodes are remembered so well that we can accurately bring back to mind or recollect tremendous detail, even after considerable time has elapsed. Other events are seemingly experienced in an identical way, and yet are irretrievably lost from memory, even moments later. A fundamental\rchallenge for memory theorists is to specify the neurocognitive processes that impact the mnemonic fate of our experiences, influencing whether they will be remembered or forgotten. A significant step towards meeting this challenge is to delineate encoding operations and their impact on subsequent memorability. Although multiple factors influence our ability to remember, one factor that must be critical for remembering is whether the experience coincides with the effective laying down of an engram in the brain. Insights into effective memory formation can be gained by monitoring brain activity during an experience and relating these neural measures to behavioral evidence that a memory was formed. In this review, we consider how correlations between neural activation and subsequent remembering have informed our theories of how experiences are transformed into memories.\rhttp://tics.trends.com 1364-6613/02/$ – see front matter © 2002 Elsevier Science Ltd. All rights reserved. PII: S1364-6613(00)01845-3\r\n94\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\r(a)\rTime 1 (encoding)\rTime 2 (retrieval)\rNeural responses to events measured at Time 1\rEvents at Time 1 classified; responses extracted\rVisual events\rMUSTARD GIRAFFE MOON...\rRemembered\rMemory test for events\rForgotten\rRemembered\rForgotten\rClasssification based on behavioral measures of memory\rRemembered events\rForgotten events\r(b)\rLeft inferior prefrontal cortex\rLeft medial temporal lobe\rTRENDS in Cognitive Sciences\r4 3 2 1 0\r–1\r0 4 8 12\rTime (s)\r4 3 2 1 0\r–1\r0 4 8 12\rTime (s)\rKen A. Paller\rDept of Psychology and Institute for Neuroscience, Northwestern University, 2029 Sheridan Road, Evanston, IL 60208-2710, USA.\re-mail: kap@northwestern.edu\rAnthony D. Wagner\rDept of Brain and Cognitive Sciences, and Center for Learning and Memory, Massachusetts Institute of Technology, Cambridge, MA 02139 and the Martinos Center for Biomedical Imaging, MGH/MIT/HMS, Charlestown MA 02129, USA.\re-mail: awagner@psyche.mit.edu\rFig. 1. The subsequent memory paradigm. (a) Neural responses are acquired during event processing (in this example, visual word presentations). Subsequently, memory is probed and events are classified. (b) Neural responses are analyzed based on subsequent memory, revealing neural correlates of encoding in various brain regions. Graphs (redrawn from Ref. 8) show that remembered events (blue) elicited greater responses than forgotten events (green).\rThe first steps to remembering: witnessing the creation of memories\rEpisodic encoding refers to the initial information processing steps whereby a memory trace is created such that it can subsequently support the conscious recollection of one’s past [1]. Encoding depends on at least two components:\r(1) The initial component mediates the transformation of sensory input into internal representations that are interpreted or comprehended. This often entails the retrieval of associated knowledge relevant to current goals.\r(2) The second component binds the internal representations into an enduring trace such that the resultant representation ultimately permits the experience to be brought back to mind. The representation may include perceptual and conceptual fragments, self-generated thoughts, and contextual details, and critically links such fragments together to form an integrated engram.\rThese two components of encoding are clearly illustrated by patterns of spared and impaired performance in individuals with anterograde amnesia due to bilateral medial temporal or diencephalic insult. Patients with amnesia can carry on a normal conversation, repeat back information when queried immediately after the information is provided, and show intact working memory [2].\rThus, they can generate and work with internal representations, but have difficulty remembering experiences once active representations have dropped from consciousness. Contemporary explanations for this pattern typically posit a deficit at the second stage of encoding, whereby durable relational, configural, or declarative traces are formed and/or consolidated [3–7].\rThe two components of intact episodic encoding – generating an internal representation of an experience and storing a bound trace of those representations – typically interact during learning. We describe below how these interactions may reflect the convergence of (1) prefrontal and posterior neocortical computations responsible for the representation and goal-directed processing of events with (2) medial temporal computations that guide the storage of durable episodic traces whereby the elements of these representations are linked together.\rTo learn how experiences are transformed into memories, no single methodology will suffice. Cognitive neuroscientists strive to obtain precise observations of both the behavior that arises from cognitive processes and the neural computations that support cognition. A particularly powerful approach to the study of memory encoding is the ‘subsequent memory paradigm’ (see Fig. 1), which provides measures of neural activity correlated with later remembering. In general, neural responses to distinct stimulus events are recorded and then classified based on testing the subject’s memory for the stimuli at a later time. The key contrast is typically between neural responses to stimuli later remembered and to those later forgotten. Differential neural activity based on memory, sometimes referred to as Dm,\rhttp://tics.trends.com\r\nReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 95\rGSR\rEEG Depth EEG ERPs units\rfMRI\r1970 1975 1980 1985 1990 1995 2000\rTRENDS in Cognitive Sciences\rFig. 2. Timeline of Dm analyses of episodic encoding. The current blossoming of interest in physiological correlates of memory encoding stems from earlier work using skin conductance measures (GSR). Measures of brain activity were subsequently used, including EEG measures in the frequency domain, event-related potentials recorded from scalp and intracranial electrodes, single unit activity, and functional magnetic resonance imaging.\rprovides an index of neural computations at the time of encoding that are predictive of accurate recall or recognition (Fig. 2). Subsequent memory effects, which presumably index pivotal operations at the time of learning that influence what will be remembered, constitute some of the tightest correlations between neural function and\rencoding. Here, we consider how the subsequent memory paradigm has shed new light on the complexity and variety of encoding processes, providing leverage on delineating how episodic memories are created.\rEncoding circuits: evidence from stimulus effects\rThe cerebral cortex consists of multiple processing modules that represent and perform computations on specific stimulus dimensions or features, such as visual attributes, spatial configuration, and domains of meaning. High-level outputs from neocortical processing modules are directed to the medial aspect of the temporal lobe (MTL), which binds representations together in the service of episodic memory formation. Different combinations of neocortical circuits can thus potentially interact with the MTL; which specific circuits are recruited in any given instance of learning is a function of factors such as the nature of the event being encoded.\rAccordingly, material-sensitive subsequent memory effects have been observed using ERPs and fMRI. For example, different brain potentials were correlated with later memory depending on whether subjects learned words or faces [9]. The two initial fMRI studies of subsequent memory effects [8,10] also revealed activation patterns that varied depending on the stimuli. Wagner and colleagues [8], as well as others [11–17], reported that subsequent recognition memory for incidentally learned words was correlated with greater encoding activity primarily in left inferior prefrontal cortex (PFC), fusiform cortex, and MTL. Topographic patterns of subsequent memory effects in scalp ERPs derived from current source density analyses have likewise been interpreted as reflections of left inferior PFC activity [18].\rBy contrast, Brewer and colleagues [10] observed that activity levels in bilateral MTL and right inferior PFC predicted subsequent remembering for incidentally learned complex visual scenes. Thus, verbal encoding was associated with left inferior PFC and MTL computations, and pictorial encoding with right inferior PFC and bilateral MTL processing.\rIn a study that directly compared word and visual scene encoding, contrasts between novel and highly familiar stimuli were juxtaposed to subsequent memory effects [12]. Scene novelty preferentially activated right inferior PFC and bilateral posterior fusiform, consistent with their putative role in processing visuo-spatial/visuo-object information [19,20]. Word novelty preferentially activated the posterior extent of left inferior PFC and left anterior fusiform, perhaps reflecting phonological and/or lexical computations. Word novelty also selectively activated the anterior extent of left inferior PFC and left middle/inferior temporal cortex, regions involved in the representation and controlled retrieval of semantic knowledge [21]. Speculatively, PFC regions may influence encoding by modulating posterior neocortical processing and by regulating input to MTL. Importantly, activity levels in almost all of these regions – as well as in MTL structures – were positively correlated with subsequent recognition memory for the novel stimuli. Thus, some cortical regions that are sensitive to stimulus novelty also support episodic encoding.\rEmotional intensity is another stimulus-bound dimension that impacts encoding. Consistent with the hypothesis that the amygdala modulates episodic memory for emotional experiences [22,23], several groups observed between-subject correlations between subsequent episodic memory and levels of amygdala activation during encoding of blocks of negative and, in some studies, positive stimuli [24–26]. Within-subject fMRI analyses revealed that amygdala activation during the viewing of scenes was predictive of subsequent memory only for emotionally arousing scenes [27]. This approach illustrates the usefulness of combining behavioral measures (judgments of emotional arousal and subsequent memory) with indices of neural computation.\rProcessing goals influence encoding\rThe configuration of neocortical modules that mediate encoding varies not only with the nature\rof the stimulus but also with how attention is allocated to different stimulus features and types of processing. A central theoretical focus in memory research has concerned the influence of goal- directed attentional orientation on encoding, as can be manipulated by instructions to process stimuli for meaning, phonology, or structural form [28,29]. Initial PET and fMRI studies of encoding using blocked designs exploited the superiority of meaning-based versus non-semantic orientation to\rhttp://tics.trends.com\r\n96\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rshow, for example, that specific left PFC networks are involved in semantic encoding (for a review see Ref. [30]).\rThe influence of attentional orientation on Dm has also been explored with event-related designs. In one study, for example, ERPs were examined for words studied under four different attentional orienting conditions, using two structural and two semantic tasks [31]. Dm effects – enhanced posterior positivity at a latency of 400–800 ms for subsequently remembered words – were greater during semantic than during non-semantic orienting. These findings, along with other ERP evidence [32–37], suggest that some Dm effects reflect processing variations that come into play chiefly when subjects maintain a meaning-based attentional orientation, and that different meaning- based goals can differentially influence encoding.\rAs a general rule, Dm can emerge when the type or degree of processing varies from trial to trial so as to produce subsets of strongly and weakly encoded items, and when that processing is sufficiently time-locked to stimulus presentation.\rIn recent fMRI studies, incidental learning paradigms were used to determine whether different encoding networks were engaged during semantic versus non-semantic processing. For words processed in a semantic manner, activation based on subsequent recognition was observed in left inferior PFC, fusiform, MTL, and right inferior PFC [13,14,16]. For words processed in a structural manner, in two studies the subsequent memory effects were observed in a subset of these regions; namely, left inferior PFC and anterior MTL [13] or left PFC, fusiform, and right PFC regions [14].\rIn both studies, no additional regions differentially predicted subsequent memory following non-semantic orienting (but see Ref. [36] for divergent ERP results), suggesting that the frontal-temporal encoding circuit recruited during non-semantic orienting may be a subset of the circuit recruited during semantic orienting.\rFailure to observe Dm effects that are greater following non-semantic, relative to semantic, orienting may be partially attributable to low power due to poor later remembering following structural encoding [16,31] as well as to the obligatory nature of phonological and structural processing during semantic orienting. However, prior observations of crossover semantic/phonological interactions within left PFC, posterior temporal and parietal cortices [38], raise the possibility that semantic and non-semantic Dm effects sometimes diverge. Indeed, new evidence along these lines was recently obtained by comparing semantic to phonological conditions [16]. Whereas activation in left and medial PFC regions predicted recognition following semantic orienting, activation in bilateral intraparietal, fusiform, right PFC, and left occipital regions predicted recognition following phonological orienting. The specific PFC-posterior\rneocortical circuits that subserve encoding partially depend on the event features attended during learning, either due to differences in the stimulus- bound features or in the individual’s attentional goals. Encoding emerges as a byproduct of such goal-directed event processing [8,16,28].\rThe allocation of attention can be directed not only to a single stimulus but also to associative processes whereby two or more stimulus items are processed in relation to one another (i.e. inter-item rather than intra-item processing [39]). In one ERP study, recognition was superior for associatively compared to nonassociatively encoded word pairs, and Dm was observed only for the associatively encoded pairs [40] (see also [41] for a contrast between different types of associations). By contrast, an fMRI study of intra-item (rote) and inter-item (elaborative) rehearsal of three simultaneously presented words revealed subsequent memory effects primarily following rote rehearsal, although a Dm effect in left hippocampus was specifically associated with item recognition following elaborative encoding [17]. Associative or elaborative inter-item processing, which probably demands strategic rehearsal and manipulations of actively maintained representations [42], may likewise underlie frontal Dm findings in ERP studies when subjects processed relations between individual items [43,44].\rFractionating episodic memory\rThe experience of an episode does not yield a single, undifferentiated memory trace. Rather, multiple forms of learning simultaneously occur during event processing. Subsequently, those various traces can support qualitatively different memory phenomena. For instance, behavioral and neuropsychological evidence suggests that memory for the prior occurrence of a stimulus is distinct from memory for a conglomeration of specific details about that prior experience. In the latter case, the retrieval of multiple associations between a stimulus and contextual cues tends to coincide with recollection, the subjective experience of remembering. Item- context associations can pinpoint the source of remembered information, and thus drive the full-blown recollection of an episode [45]. On the other hand, a stimulus can also be recognized in the absence of recollection. In this case, retrieval may support a phenomenon known as ‘familiarity without recollection’ [39,46].\rThe specific neurocognitive processes that support these two manifestations of episodic memory, recollection and familiarity, have been investigated intensely but remain under active debate [47–49]. One hypothesis is that parahippocampal/perirhinal computations contribute mainly to memory for the occurrence of an item, which can underlie subsequent recognition based on familiarity [50,51]. The hippocampus, in contrast, may mediate or participate\rhttp://tics.trends.com\r\nReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002 97\rin the encoding of relations or conjunctions between an item and its context as well as conjunctions between items, which are crucial for subsequent recall and recollection-based recognition [3,7,52,53].\rMemory testing with the ‘R/K procedure’ has been used to distinguish between recollection and familiarity-without-recollection by requiring people to indicate their basis for recognition, either ‘remembered’ (R) or ‘known’ (K) [54,55]. Several ERP studies have combined R/K judgments with Dm analyses [35,44,56]. For example, larger positive responses were observed for subsequently recollected items than either known items or items not recognized, with no ERP differences between the latter two conditions [35]. These ERPs associated with subsequent recollection may fall into the broader category of the parietal-maximum Dm observed by many investigators and generally more robust with recall than with recognition [37,57,58]. On the other hand, in one study an ERP correlate of encoding supporting familiarity was ascribed to an N340 potential thought to be associated with attention- dependent conceptual processing in left temporal and/or inferior frontal neocortex [44].\rThe reliability of Dm analyses can generally be enhanced when recollective experience or decision confidence is taken into account. Subjects with a preponderance of correct guesses, with lenient criteria for judging items to be old, or with criteria that vary erratically over the course on an experiment, may not show Dm effects. Consistent with this suggestion, reliable Dm effects can be apparent in fMRI studies particularly when confidence or R/K judgments were required during recognition. These procedures restrict analyses to the memory extremes, consequently minimizing the influence of guessing. Accordingly, fMRI Dm magnitudes tend to be larger for events later recognized with high relative to with low confidence [8,13] and also larger for events later ‘remembered’ than during those later ‘known’ [10,11].\rTo date, fMRI studies have not provided clear evidence for a qualitative difference between encoding that yields recollection versus familiarity alone. Brewer and colleagues [10] observed graded encoding activation that declined across subsequent remembering to knowing to forgetting, suggesting merely quantitative differences. Henson and colleagues [11] did not run conventional subsequent memory analyses because of low levels of forgetting, but did compare encoding activations as a function of whether recognition was associated with an R or K response; activation in left inferior and middle PFC and left precuneus was predictive of ‘remembering,’ whereas activation in right parahippocampal gyrus and precuneus was predictive of ‘knowing.’ Additional evidence from studies that explore the relation between Dm for subsequent source recollection and for item memory without recollection, relative to forgotten trials, may\rprove informative in clarifying whether different encoding processes promote memory with and without recollection.\rAn intriguing speculation regarding the relation between ERP and fMRI results, made by Friedman and Johnson [18], is that parietal scalp ERP subsequent memory effects could derive from left precuneus computations associated with encoding [11]. Of course, combining ERP and fMRI data to achieve high temporal and high spatial resolution will ultimately require additional evidence that the same neural activity is responsible for both effects. Nevertheless, this possible multimodal imaging integration highlights the potential fruitfulness of such efforts. Relative timing data that show when encoding-related processes are set into motion can be used together with localization data from neuroimaging to provide new insights into the cognitive functions of specific neural computations.\rAlthough familiarity entails episodic memory restricted to a single item, it must be distinguished from another sort of item-specific long-term memory known as priming. Is this non-conscious facilitation or biasing of stimulus processing due to recent experience supported by the same encoding events that support episodic encoding? In two attempts to address this question, ERPs were found to predict subsequent recall and recognition but not priming [59,60]. However, it has recently been suggested [35] that when intentional encoding is avoided, ERPs can predict subsequent priming [35,61]. A critical issue for future investigation is thus to characterize the relation between processes supporting priming and those yielding effective episodic memory.\rIndeed, priming can act to hinder episodic encoding under some circumstances. Wagner and colleagues [62] found that the magnitude of priming during word processing was inversely related to episodic learning, as indexed by subsequent recognition. Priming was manipulated by interposing either a long or short lag between initial and repeat trials. Neural priming in left inferior PFC was greater with the short lag, as was behavioral priming. Subsequent recognition was superior with the long lag, the condition yielding less priming. Between- subject negative correlations were also observed between priming and subsequent recognition, even when lag was held constant. Priming might promote a stereotyped or sparse re-encoding experience, thus producing a less effective episodic memory [62].\rMedial temporal contributions to encoding\rNeocortical and medial temporal regions make different contributions to episodic memory. Even with a severe amnesia, the active representation of the multidimensional features of moment-to- moment experiences can still be supported by neocortical mechanisms. By contrast, storing neocortical memory fragments as coherent episodic representations is characteristically problematic in\rhttp://tics.trends.com\r\n98\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\ramnesia. The hippocampus and associated MTL structures (perirhinal, entorhinal and parahippocampal cortices) might play an essential role in linking up the multiple representations, dispersed across distinct neocortical regions, that constitute an experience [3,6]. Consistent with this idea, MTL activation has been observed in blocked- design PET and fMRI studies of episodic encoding with item-based, associative, and novelty- assessment paradigms (for reviews see Refs [63,64]). But many questions remain regarding exactly how MTL-neocortical interactions support learning, when MTL computations are engaged, and whether distinct anatomical subregions within the MTL subserve distinct mnemonic functions.\rERP recordings from intracranial electrodes suggest that a posterior hippocampal contribution might begin within the first 300–900 ms after stimulus onset [65]. Color patterns that patients were required to remember in a delayed matching-to- sample test elicited larger hippocampal potentials than did similar stimuli presented during the delay or when recognition decisions were made. These potentials could reflect the recruitment of hippocampal circuitry during intentional encoding, although the small number of recognition errors prohibited an analysis to determine whether these potentials predicted subsequent remembering.\rIn other experiments using intracranial ERPs, subsequent memory analyses were used to implicate specific MTL structures in encoding. Within-subject Dm effects were found for visual words based on recall after a filled 30-s delay [66,67]. Potentials that reached a negative peak 400–500 ms after word onset were larger for subsequently recalled words. These potentials, putatively generated in rhinal cortex\r(i.e. entorhinal and perirhinal cortex, the site of principal connections between hippocampus and neocortex), were thought to reflect the richness of semantic analysis [67]. By contrast, hippocampal Dm effects, which arose only after the rhinal potential peak, were thought to reflect trial-by-trial differences in associative learning processes that can begin only after initial semantic analyses.\rSingle-unit recordings from the human hippocampus further indicate that the rate of neural firing in this structure varies with encoding [68]. In response to visual presentations of to-be-remembered word pairs, firing rates of\rsome neurons demonstrated a positive\rcorrelation and others a negative correlation with later cued recall.\rMTL correlates of subsequent memory have also been demonstrated in several event-related fMRI studies. Encoding of words subsequently recognized was associated with activation in left posterior parahippocampal cortex [8,12] and left hippocampus [12,13,17]. Recall performance combined for groups of five words revealed correlated activity in posterior hippocampus [69] and tonic state effects in entorhinal\rcortex [70]. Subsequent recognition of complex visual scenes was correlated with bilateral posterior parahippocampal activation [10,12] and bilateral hippocampal activation [12].\rThe MTL regions implicated in these fMRI studies – typically posterior parahippocampal cortices and hippocampus – were not always the same as those observed in intracranial ERP experiments. Such discrepancies could potentially result from: (1) differences in the cognitive paradigms implemented; (2) the fact that ERPs and fMRI may detect different subsets of neural activity; (3) limits on fMRI sensitivity in anterior MTL;\r(4) limited intracranial ERP sampling from MTL regions; or (5) pathology or medication effects in patients with intracranial electrodes. Importantly, Strange and colleagues [71] reported subsequent memory effects in perirhinal, hippocampal, and posterior parahippocampal regions when they adopted the same cognitive paradigm as in prior intracranial ERP experiments [66] and optimized the fMRI scanning protocol for sensitivity to anterior MTL signals. A complex pattern of functional dissociations was observed across MTL subregions based on serial position within the learning list [71]. Further clarification of possible functional subdivisions within the MTL might be gained through subsequent memory analyses for contextual information or for associations between the constituent elements of an episode.\rIn a novel investigation of MTL regions and their interactions during encoding, the subsequent memory methodology was applied to single-trial EEG in the gamma frequency range (32–48 Hz) [72]. An increase in phase synchronization between rhinal and hippocampal regions (100–300 ms and 500–600 ms) followed by a decrease in synchronization (after\r1000 ms) suggested that an initially enhanced interaction between processing in these two regions may reflect encoding computations that facilitate later recall. Changes in synchronization could emerge directly from the dynamics of the MTL circuit or as the result of an external signal that entrains the MTL circuit [72]. Given the repeated observation of PFC correlates of subsequent memory, one speculative source of top-down gating of the MTL might be PFC control processes [73]. Other EEG analyses in the frequency domain suggest that successful episodic encoding is associated with a scalp-recorded increase in theta power (as defined on an individualized basis, generally in the 4.0–7.5 Hz range) [74,75]. Klimesch speculated that this theta activity is induced in neocortical regions via cortico- hippocampal feedback loops [76]. Scalp-recorded theta synchronization also has been used to implicate cooperative activity across neocortical regions during encoding [77].\rThe existing data suggest that episodic encoding processes are dependent upon MTL networks in conjunction with other neocortical networks. Yet,\rhttp://tics.trends.com\r\nReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002 99\rBox 1. How the brain creates false memories\r(a)\r(b)\rERPs to objects (c)\rRemembered Forgotten\r400−800ms\r–0.2 3.6 + μV\r4 μV\rERPs to words\rMisremembered\rNot misremembered\r–0.1 1.8\rTRENDS in Cognitive Sciences\r600−900ms\r0 300 600 900 1200 ms\rMemories are not always accurate. Encoding processes can sometimes support the subsequent recollection of a false memory. One type of false memory occurs when someone recalls an episode that was imagined and mistakenly believes that it had actually occurred. Gonsalves and Paller set up a laboratory experiment in which both accurate memories and false memories of this sort were produced [a]. Furthermore, different brain potentials at encoding were associated with accurate versus false memories. The accurate memories were for photos of objects viewed on a video monitor. ERPs to objects presented during the study phase differed as a function of whether the object was later remembered or forgotten on a subsequent recognition test, in which the names of those objects were spoken (Fig. Ia).\rVisual words were also presented during the study phase, and people were instructed to mentally generate a visual image in response to each word. ERPs to those words differed according to whether or not people subsequently claimed to have seen the corresponding object as a photo on the video monitor. (This contrasts with results from an experiment on another sort of false memory, memory conjunction errors,\rin which ERPs at encoding were not predictive of whether errors would occur at test [b].) As shown in Fig. Ib, ERPs from 600–900 ms were more positive for later false memories than when people later responded correctly that they had not seen that object. Similar ERPs recorded in a prior experiment had been associated with generating visual images in response to words [c].\rThe results from these two experiments taken together [a,c] support the following explanation for this type of false memory. The more vivid, detailed, or robust the visual imagery generated in response to a single word, the more likely\rFig. I. Brain potentials associated with the formation of true and false memories. (a) ERPs to objects were averaged according to whether those objects were subsequently remembered. (b) ERPs to words were averaged according to whether people later mistakenly thought they had seen the corresponding objects. Recordings were from the midline occipital scalp location. (c) Topographic maps were interpolated based on differences between pairs of waveforms at each electrode location (shown by small circles). (Adapted from Ref. a.)\rthe memory for that imagery will be mistakenly attributed to a memory resulting from actually viewing the corresponding object. Strong visual imagery at encoding promotes the retrieval of perceptual detail at test, which is generally a diagnostic sign of an episodic memory [d]. In some cases, however, such perceptual detail can be misleading and lead to a false memory, as when people claim to remember an event that was imagined but that never actually happened. The topographic maps (Fig. Ic) show that ERP differences associated with subsequent accurate memories were widespread across the scalp, whereas ERP differences associated with subsequent false memories were restricted to posterior scalp locations.\rA recent fMRI study using the same paradigm showed that activation in a large set of brain regions, including prefrontal cortex, fusiform gyrus, and hippocampus/parahippocampal gyrus, predicted later accurate memories for objects, whereas activation in the precuneus, anterior cingulate and inferior\roccipital gyrus predicted later false memories [e]. An intriguing possibility is that the misleading visual imagery responsible for false memories in this paradigm was produced by neural networks in the precuneus, a region hypothesized to play a role in representing internal visual images [f].\rReferences\ra Gonsalves,B.andPaller,K.A.(2000)Neural events that underlie remembering something that never happened. Nat. Neurosci.\r3, 1316–1321\rb Rubin,S.R.etal.(1999)Memoryconjunction errors in younger and older adults: event- related potential and neuropsychological data. Cogn. Neuropsychol. 16, 459–488\rc Gonsalves, B. and Paller, K.A. (2000)\rBrain potentials associated with recollective processing of spoken words. Mem. Cogn.\r28, 321–330\rd Johnson,M.K.(1992)MEM:Mechanismsof recollection. J. Cogn. Neurosci. 4, 268–280\re Gonsalves, B. et al. (2001) Event-related fMRI reveals brain activity at encoding that predicts true and false memory for visual objects.\rSoc. Neurosci. Abstr. 27, 239.6\rf Fletcher,P.C.etal.(1995)Themind’seye: precuneus activation in memory-related visual imagery. NeuroImage 2, 195–200\rconsiderable controversy remains regarding the distinctive contributions that specific MTL subregions make. Moreover, the posited importance of PFC–MTL interactions remains to be empirically tested. Resolution of these fundamental questions should prove central to unraveling the mysteries of MTL contributions to memory formation.\rReverse engineering cognitive architecture\rTo what extent can episodic encoding be influenced by working memory maintenance processes? A prevalent view is that ‘ “mindless” rote rehearsal per se is insufficient to create durable memories’ ([78] p. 23). This view is based on the lack of a correlation between rehearsal duration and later recall [79,80], and the robust effects of levels-of-processing on episodic\rhttp://tics.trends.com\r\n100\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rBox 2. Observing mind and brain to track episodic encoding\r(1) Brain imaging during direct manipulations of encoding\r• Acquisitionversusretrieval\r• Levelsofprocessing(e.g.deepvsshallow)\r• Intentiontoremember • Novelty\r(2) Post-hoc item-by-item, state, and between- subject analyses\r• Comparisonsbasedonsubsequentremembering\ror forgetting of items (Dm effects)\r• Subsequent memory analyses emphasizing\rspecific aspects of episodic memories (e.g. source,\rfamiliarity)\r• Analyses of state changes that influence encoding\r(e.g. slow and sustained changes in attention or arousal)\r• Between-subjectcorrelationsbetweenmemory performance and the magnitude of neural responses\r(3) Lesion and disruption approaches\r• Analyses of encoding in patients with memory disorders\r• Temporary neural disruption (e.g. electrical or magnetic stimulation)\r(4) Integrated methods\r• Juxtaposing subsequent memory analyses with encoding manipulations\r• Testing the causal role of subsequent memory effects via neural disruption\r• Integrating electromagnetic and hemodynamic signals\rmemory. However, behavioral evidence that rote rehearsal can facilitate subsequent recognition [81–83] has been substantiated by fMRI evidence that activation levels during rote rehearsal, in neural regions associated with phonological working memory, correlate with later recognition [17]. Given the minimal impact of rote rehearsal on free recall,\rit may be that phonological maintenance can support subsequent item recognition but not recollection.\rTheorists have also examined the impact of retrieval processes on encoding. Retrieval is a particularly powerful incidental learning experience [84,85]. Encoded experiences that are later retrieved are subsequently better remembered compared to experiences not receiving retrieval practice [86]. Moreover, the magnitude of fMRI activation during recognition judgments on foils (unstudied words) was predictive of whether the foils were later recognized or forgotten [15]. The relation between engagement of retrieval circuits when accessing memories of studied items and later memory for those items remains to be determined.\rEncoding/retrieval correlations point to a related question: How can certain processing produce forgetting? Retrieval of some memories results in the suppression [87] or blocking [88] of other, non- retrieved traces. Neuroimaging predictors of retrieval-induced forgetting could prove invaluable for understanding suppression and blocking. More generally, neural computations correlated with forgetting are beginning to garner attention: across four fMRI studies, forgetting was correlated with greater activation in dorsolateral and medial PFC, posterior cingulate, and parietal structures [8,89,90]. These ‘subsequent forgetting effects’ could reflect either (a) a diversion of neurocognitive resources away from processes that yield effective encoding, or (b) encoding processes that yield undifferentiated traces, leading to more interference.\rFinally, it is worth noting that subsequent memory analyses can shed light on diverse cognitive and social-cognitive phenomena. For example, studies have explored the relations between encoding and later false remembering (Box 1) and between perceptual expertise and the phenomenon that people remember faces of their own race better than those of other races [91].\rConclusion\rThe ability to remember an episode is a function of multiple processes, some of which are engaged at encoding, some at retrieval, some in-between, and some emerging as an outcome of how encoding and retrieval processes interrelate. Intermediate processes constituting additional encoding or consolidation may be particularly critical for the stability of episodic memories over time [92–94]. One limitation of current implementations of the subsequent memory paradigm is that such intermediate processes are seldom indexed. Here, we focused on the initial processing of event information to illustrate the wealth of evidence available, and potentially obtainable, based on analyses of neural correlates of encoding.\rAlthough subsequent memory analyses by themselves do not conclusively demonstrate causality with respect to encoding, transcranial magnetic stimulation when applied to fMRI-identified structures could serve as a critical test of the necessity of specific neural computations for learning. Whereas subsequent memory analyses can be central to investigations of episodic encoding, this approach is most informative when combined with a full range of complementary approaches (see Box 2). Identifying the relevant neural events in this way does not merely reveal where encoding happens, but rather serves to clarify how multiple processes optimally coalesce such that we can successfully remember the past.\rhttp://tics.trends.com\r\nReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 101\rQuestions for future research\r• Aretheneuralcomputationscorrelatedwith subsequent memory necessary for effective memory formation?Cantheybeselectivelydisruptedvia transcranial magnetic stimulation such that forgetting results?\r• Howdoglobalchangesincognitivesetor attentional state, which remain undetected by typical subsequent memory analyses conducted at theitem-level,impactencoding?Howdothe effects of global state interact with event-related encoding processes?\r• Howdoprefrontalandposteriorneocortical networks represent the diverse aspects of experience that form the building blocks of episodicmemories?Howdointeractionsbetween these brain networks and medial temporal networks support memory storage that can last a lifetime?\r• Whataretheexperientialprerequisitesforepisodic encoding? Can unconscious perception give rise to\r•\r•\r•\repisodicmemoryforaneventthatwasnot consciously experienced? Howdoprefrontalcontrolprocesseshelpto initiate the cascade of neural events that modulate encoding efficacy? Does this cascade begin with top-downprefrontalmodulations,bottom-up posterior neocortical processes, or interactions of the two? Howdotheinteractionsthatbeginatencoding evolve over time, both across the course of an event as well as when consolidation and interveningretrievaleventscomeintoplay?\rHow do those intervening events regulate episodic forgetting? Howdoesencodingdifferforepisodesversusother types of information such as facts, skills, and implicitly conditioned associations? Do the neocortical encoding events that putatively support primingconstituteasubsetofthosesupporting episodic learning or are they distinct?\rAcknowledgements.\rThis article represents a collaborative effort based on equal contributions from the two authors.\rWe thank R. Poldrack for insightful comments on an earlier draft, and gratefully acknowledge research support from NIDCD (DC04466), NIMH (MH60941), NINDS (NS34639), Ellison Medical Foundation, McKnight Endowment Fund for Neuroscience, and P. Newton.\rReferences\r1 Tulving, E. (1983) Elements of Episodic Memory, Cambridge University Press\r2 Cave, C.B. and Squire, L.R. (1992) Intact verbal and nonverbal short-term memory following damage to the human hippocampus. Hippocampus 2, 151–163\r3 Squire, L.R. (1992) Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans. Psychol. Rev. 99, 195–231\r4 Cohen, N.J. and Eichenbaum, H.E. (1993) Memory, Amnesia, and the Hippocampal System, MIT Press\r5 Mayes, A.R. and Downes, J.J. (1997) Theories of Organic Amnesia, Psychology Press\r6 Paller, K.A. (1997) Consolidating dispersed neocortical memories: the missing link in amnesia. Memory 5, 73–88\r7 O’Reilly, R.C. and Rudy, J.W. (2001) Conjunctive representations in learning and memory: principles of cortical and hippocampal function. Psychol. Rev. 108, 311–345\r8 Wagner, A.D. et al. (1998) Building memories: remembering and forgetting of verbal experiences as predicted by brain activity. Science\r281, 1188–1191\r9 Sommer, W. et al. (1991) Human brain potential correlates of face encoding into memory. Electroencephalogr. Clin. Neurophysiol.\r79, 457–463\r10 Brewer, J.B. et al. (1998) Making memories: brain activity that predicts how well visual experience will be remembered. Science 281, 1185–1187\r11 Henson, R.N.A. et al. (1999) Recollection and familiarity in recognition memory: an event-related functional magnetic resonance imaging study. J. Neurosci. 19, 3962–3972\r12 Kirchhoff, B.A. et al. (2000) Prefrontal–temporal circuitry for novelty encoding and subsequent memory. J. Neurosci. 20, 6173–6180\r13 Otten, L.J. et al. (2001) Depth of processing effects on neural correlates of memory encoding: relationship between findings from across- and within-task comparisons. Brain 124, 399–412\r14 Baker, J.T. et al. (2001) Neural correlates of verbal memory encoding during semantic and structural processing tasks. NeuroReport\r12, 1251–1256\r15 Buckner, R.L. et al. (2001) Encoding processes during retrieval tasks. J. Cogn. Neurosci.\r13, 406–415\r16 Otten, L.J. and Rugg, M.D. (2001) Task- dependency of the neural correlates of episodic encoding as measured by fMRI. Cereb. Cortex 11, 1150–1160\r17 Davachi, L. et al. (2001) When keeping in mind supports later bringing to mind: neural markers of phonological rehearsal predict subsequent remembering. J. Cogn. Neurosci.\r13, 1059–1070\r18 Friedman, D. and Johnson, R., Jr (2000)\rEvent-related potential (ERP) studies of memory encoding and retrieval: a selective review. Microsc. Res. Tech. 51, 6–28\r19 Kelley, W.M. et al. (1998) Hemispheric specialization in human dorsal frontal cortex and medial temporal lobe for verbal and nonverbal memory encoding. Neuron\r20, 927–936\r20 Wagner, A.D. et al. (1998) Material-specific lateralization of prefrontal activation during episodic encoding and retrieval. NeuroReport 9, 3711–3717\r21 Wagner, A.D. et al. (2001) Recovering meaning: left prefrontal cortex guides controlled semantic retrieval. Neuron 31, 329–338\r22 Cahill, L. and McGaugh, J.L. (1998) Mechanisms of emotional arousal and lasting declarative memory. Trends Neurosci. 21, 294–299\r23 Hamann, S. (2001) Cognitive and neural mechanisms of emotional memory. Trends Cogn. Sci. 5, 394–400\r24 Cahill, L. et al. (1996) Amygdala activity at encoding correlated with long-term, free recall of emotional information. Proc. Natl. Acad. Sci.\rU. S. A. 93, 8016–8021\r25 Canli, T. et al. (1999) fMRI identifies a network of structures correlated with retention of positive and negative emotional memory. Psychobiology 27, 441–452\r26 Hamann, S.B. et al. (1999) Amygdala activity related to enhanced memory for pleasant and aversive stimuli. Nat. Neurosci. 2, 289–293\r27 Canli, T. et al. (2000) Event-related activation in the human amygdala associates with later memory for individual emotional experience.\rJ. Neurosci. 20, RC99\r28 Craik, F.I.M. and Lockhart, R.S. (1972) Levels of processing: a framework for memory research. J. Verbal Learn. Verbal Behav.\r11, 671–684\r29 Craik, F.I.M. and Tulving, E. (1975) Depth of processing and the retention of words in episodic memory. J. Exp. Psych. Gen.\r104, 268–294\r30 Wagner, A.D. et al. (1999) When encoding yields remembering: insights from event-related neuroimaging. Phil. Trans. R. Soc. London Ser. B. (Biology) 354, 1307–1324\r31 Paller, K.A. et al. (1987) Neural correlates of encoding in an incidental learning paradigm. Electroencephalog. Clin. Neurophysiol.\r67, 360–371\r32 Neville, H.J. et al. (1986) Event-related brain potentials during initial encoding and recognition memory of congruous and incongruous words.\rJ. Mem. Lang. 25, 75–92\r33 Friedman, D. (1990) ERPs during continuous recognition memory for words. Biol. Psychol. 30, 61–87\r34 Cycowicz, Y.M. and Friedman, D. (1999)\rThe effect of intention to learn novel, environmental sounds on the novelty P3 and old/new recognition memory. Biol. Psychol. 50, 35–60\rhttp://tics.trends.com\r\n102 Review\r35 Friedman, D. and Trott, C. (2000)\rAn event-related potential study of encoding in young and older adults. Neuropsychologia\r38, 542–557\r36 Otten, L.J. and Rugg, M.D. (2001) Electrophysiological correlates of memory encoding are task-dependent. Cogn. Brain Res. 12, 11–18\r37 Johnson, R.J. (1995) Event-related potential insights into the neurobiology of memory systems. In The Handbook of Neuropsychology (Vol. 10) (Boller, F. and Grafman, J., eds), pp. 135–164, Elsevier Science Publishers\r38 Price, C.J. et al. (1997) Segregating semantic from phonological processes during reading. J. Cogn. Neurosci. 9, 727–733\r39 Mandler, G. (1980) Recognizing: the judgment of previous occurrence. Psychol. Rev.\r87, 252–271\r40 Weyerts, H. et al. (1997) ERPs to encoding and recognition in two different inter-item association tasks. NeuroReport 8, 1583–1588\r41 Kounios, J. et al. (2001) Cognitive association formation in human memory revealed by spatiotemporal brain imaging. Neuron\r29, 297–306.\r42 Wagner, A.D. et al. (2001) Prefrontal contributions to executive control: fMRI evidence for functional distinctions within lateral prefrontal cortex. NeuroImage 14, 1337–1347\r43 Fabiani, M. et al. (1990) Effects of mnemonic strategy manipulation in a Von Restorff paradigm. Electroencephalographic and Clinical Neurophysiology 75, 22–35\r44 Mangels, J.A. et al. (2001) Attention and successful episodic encoding: an event-related potential study. Cogn. Brain Res. 11, 77–95\r45 Johnson, M.K. (1992) MEM: mechanisms of recollection. J. Cogn. Neurosci. 4, 268–280\r46 Jacoby, L.L. (1991) A process dissociation framework: separating automatic from intentional uses of memory. J. Mem. Lang. 30, 513–541\r47 Tulving, E. and Markowitsch, H.J. (1998) Episodic and declarative memory: role of the hippocampus. Hippocampus 8, 198–204\r48 Squire, L.R. and Zola, S.M. (1998) Episodic memory, semantic memory, and amnesia. Hippocampus 8, 205–211\r49 Mishkin, M. et al. (1998) Amnesia and the organization of the hippocampal system. Hippocampus 8, 212–216\r50 Vargha-Khadem, F. et al. (1997) Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380\r51 Brown, M.W. and Aggleton, J.P. (2001) Recognition memory: what are the roles of the perirhinal cortex and hippocampus? Nat. Rev. Neurosci. 2, 51–61\r52 Sutherland, R.J. and Rudy, J.W. (1989) Configural association theory: the role of the hippocampal formation in learning, memory, and amnesia. Psychobiology 17, 129–144\r53 Holdstock, J.S. et al. Under what conditions is recognition spared relative to recall following selective hippocampal damage in humans? Hippocampus (in press)\r54 Tulving, E. (1985) Memory and consciousness. Can. Psychol. 26, 1–12\r55 Gardiner, J.M. (1988) Functional aspects of recollective experience. Mem. Cogn.\r16, 309–313\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rhttp://tics.trends.com\r56 Smith, M.E. (1993) Neurophysiological manifestations of recollective experience during recognition memory judgments. J. Cogn. Neurosci. 5, 1–13\r57 Münte, T.F. et al. (1988) Effects of a cholinergic nootropic (WEB 1881 FU) on event-related potentials recorded in incidental and intentional memory tasks. Neuropsychobiology 19, 158–168\r58 Paller, K.A. et al. (1988) ERPs predictive of subsequent recall and recognition performance. Biological Psychology 26, 269–276.\r59 Paller, K.A. (1990) Recall and stem-completion priming have different electrophysiological correlates and are modified differentially by directed forgetting. J. Exp. Psychol. Learn. Mem. Cogn. 16, 1021–1032\r60 Paller, K.A. and Kutas, M. (1992) Brain potentials during memory retrieval provide neurophysiological support for the distinction between conscious recollection and priming.\rJ. Cogn. Neurosci. 4, 375–391\r61 Paller, K.A. et al. (1987) Brain responses to\rconcrete and abstract words reflect processes that correlate with later performance on a test of stem- completion priming. Electroencephalog. Clin. Neurophysiol. Suppl. 40, 360–365\r62 Wagner, A.D. et al. (2000) Interactions between forms of memory: when priming hinders new episodic learning. J. Cogn. Neurosci. 12, 52–60\r63 Lepage, M. et al. (1998) Hippocampal PET activations of memory encoding and retrieval: the HIPER model. Hippocampus 8, 313–322\r64 Schacter, D.L. and Wagner, A.D. (1999) Medial temporal lobe activations in fMRI and PET studies of episodic encoding and retrieval. Hippocampus 9, 7–24\r65 Paller, K.A. et al. (1990) Potentials recorded from the scalp and from the hippocampus in humans performing a visual recognition memory test.\rJ. Clin. Exp. Neuropsychol. 12, 401\r66 Fernández, G. et al. (1999) Real-time tracking of memory formation in the human rhinal cortex and hippocampus. Science 285, 1582–1585\r67 Fernández, G. et al. Human declarative memory formation: Segregating rhinal and hippocampal contributions. Hippocampus (in press)\r68 Cameron, K.A. et al. (2001) Human hippocampal neurons predict how well word pairs will be remembered. Neuron 30, 289–298\r69 Fernández, G. et al. (1998) Successful verbal encoding into episodic memory engages the posterior hippocampus: a parametrically analyzed functional magnetic resonance imaging study. J. Neurosci. 18, 1841–1847\r70 Fernández, G. et al. (1999) Level of sustained entorhinal activity at study correlates with subsequent cued-recall performance:\ra functional magnetic resonance imaging study with high acquisition rate. Hippocampus\r9, 35–44\r71 Strange, B.A. et al. Dissociable human perirhinal,\rhippocampal, and parahippocampal roles during\rverbal encoding. J. Neurosci. (in press)\r72 Fell, J. et al. (2001) Human memory formation is\raccompanied by rhinal-hippocampal coupling and\rdecoupling. Nat. Neurosci. 4, 1259–1264\r73 Wagner, A.D. (2001) Synchronicity: when you’re\rgone I’m lost without a trace. Nat. Neurosci.\r4, 1159–1160\r74 Klimesch, W. et al. (1996) Theta band power in the\rhuman scalp EEG and the encoding of new information. NeuroReport 7, 1235–1240\r75 Klimesch, W. et al. (1997) Theta synchronization and alpha desynchronization in a memory task. Psychophysiology 34, 169–176\r76 Klimesch, W. (1999) EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis. Brain Res. Rev. 29, 169–195\r77 Weiss, S. et al. (2000) Theta synchronization predicts efficient memory encoding of concrete and abstract nouns. NeuroReport 11, 2357–2361\r78 Bower, G.H. (2000) A brief history of memory research. In The Oxford Handbook of Memory (Tulving, E. and Craik, F.I.M., eds), pp. 3–32, Oxford University Press\r79 Atkinson, R.C. and Shiffrin, R.M. (1968) Human memory: a proposed system and its control processes. In The Psychology of Learning and Motivation (Vol. 2) (Spence, K. and Spence, J., eds.), pp. 89–195, Academic Press\r80 Craik, F.I.M. and Watkins, M.J. (1973) The role of rehearsal in short-term memory. J. Verbal Learn. Verbal Behav. 12, 599–607\r81 Woodward, A.E. et al. (1973) Recall and recognition as a function of primary rehearsal. J. Verbal Learn. Verbal Behav. 12, 608–617\r82 Greene, R.L. (1987) Effects of maintenance rehearsal on human memory. Psychol. Bull. 102, 403–413\r83 Naveh-Benjamin, M. and Jonides, J. (1984) Maintenance rehearsal: a two-component analysis. J. Exp. Psychol. Learn. Mem. Cogn. 10, 369–385\r84 Whitten, W.B. and Bjork, R.A. (1977) Learning from tests: Effects of spacing. J. Verbal Learn. Verbal Behav. 16, 465–478\r85 Roediger, H.L. and Payne, D.G. (1982) Hypermnesia: the role of repeated testing. J. Exp. Psychol. Learn. Mem. Cogn. 8, 66–72\r86 Anderson, M.C. et al. (1994) Remembering can cause forgetting: retrieval dynamics in long-term memory. J. Exp. Psychol. Learn. Mem. Cogn.\r20, 1063–1087\r87 Anderson, M.C. and Spellman, B.A. (1995) On the status of inhibitory mechanisms in cognition: memory retrieval as a model case. Psychol. Rev. 102, 68–100\r88 Mensink, G-J. and Raaijmakers, J.G.W. (1988) A model for interference and forgetting. Psychol. Rev. 95, 434–455\r89 Otten, L.J. and Rugg, M.D. (2001) When more means less: neural activity related to unsuccessful memory encoding. Curr. Biol. 11, 1528–1530\r90 Wagner, A.D. and Davachi, L. (2001) Cognitive neuroscience: forgetting of things past. Curr. Biol. 11, R964–967\r91 Golby, A.J. et al. (2001) Differential responses in the fusiform region to same-race and other-race faces. Nat. Neurosci. 4, 845–850\r92 Squire, L.R. et al. (1984) The medial temporal region and memory consolidation: a new hypothesis. In Memory Consolidation (Weingartner, H. and Parder, E., eds),\rpp. 185–210, Erlbaum\r93 Nadel, L. et al. (2000) Multiple trace theory of\rhuman memory: computational, neuroimaging, and neuropsychological results. Hippocampus 10, 352–368\r94 Paller, K.A. (2002) Cross-cortical consolidation as the core defect in amnesia: prospects for hypothesis-testing with neuropsychology and neuroimaging. In Neuropsychology of Memory, (Vol. 3) (Squire, L.R. and Schacter, D.L., eds),\rpp. 73–87, Guilford Press","title":"TCS Feb 2002, final pages.qxd","eventTime":1450257026000,"mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"erp","time":1461915876971,"auto":true,"weight":1.0},{"@type":"Tag","text":"memori","time":1461915876979,"auto":true,"weight":1.0},{"@type":"Tag","text":"mtl","time":1461915877013,"auto":true,"weight":1.0},{"@type":"Tag","text":"neurosci","time":1461915877056,"auto":true,"weight":1.0},{"@type":"Tag","text":"subsequ","time":1461915876994,"auto":true,"weight":1.0},{"@type":"Tag","text":"hippocampu","time":1461915877001,"auto":true,"weight":1.0},{"@type":"Tag","text":"dm","time":1461915877046,"auto":true,"weight":1.0},{"@type":"Tag","text":"encod","time":1461915876958,"auto":true,"weight":1.0},{"@type":"Tag","text":"cogn","time":1461915876986,"auto":true,"weight":1.0},{"@type":"Tag","text":"episod","time":1461915877025,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":15,"appId":"deabdd180e0545021e7b706bdb6e6cadf5c5fdcf","timeCreated":1455812302781,"timeModified":1461915875602,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.45313057,"uri":"file:///Users/cheny13/Documents/References/SenseCam-MEMORY-article.pdf","plainTextContent":"Memory, in press Running Head: SenseCam Reminscence\rSenseCam Reminiscence and Action Recall in Memory-Unimpaired People\rJohn G. Seamon, Tacie N. Moskowitz, Ashley E. Swan, Boyuan Zhong, Amy Golembeski, Christopher Liong, Alexa C. Narzikul, and Olumide A. Sosan\rWesleyan University\rMailing Address:\rJohn Seamon\rDepartment of Psychology Wesleyan University Middletown, CT 06459 (USA) (860) 685-2868 jseamon@wesleyan.edu\r(Word count: 3,429 excluding abstract and references)\rMemory Camera 1\r\nAbstract\rCase studies of memory-impaired individuals consistently show that reminiscing with\rSenseCam images enhances event recall. This exploratory study examined whether a similar benefit would occur for the consolidation of memories in memory-unimpaired people. We tested delayed recall for atypical actions observed on a lengthy walk. Participants used SenseCam, a diary, or no external memory aid while walking, followed by reminiscence with SenseCam images, diary entries, or no aid, either alone (self-reminiscence) or with the experimenter (social reminiscence). One week later, when tested without SenseCam images or diary entries, prior social reminiscence produced greater recall than self-reminiscence, but there were no differences between memory aid conditions for action free recall or action order recall. When methodological variables were controlled, there was no recall advantage for SenseCam reminiscence with memory-unimpaired participants. The case studies and present study differ in multiple ways, making direct comparisons problematic. SenseCam is a valuable aid to the memory impaired, but its mnemonic value for non-clinical populations remains to be determined.\rKey Words: SenseCam Reminiscence, Memory for Actions, Memory Aids\rMemory Camera 2\r\nSenseCam Reminiscence and Action Recall in Memory-Unimpaired People Miniature automatic cameras offer the promise of effortlessly recording life experiences\rinto e-memories for later reminiscence. Originally developed by Microsoft, SenseCam is a small, lightweight camera that is worn around the neck by a lanyard. It has a large memory and takes fish-eye, wide-angle pictures of the environment, either manually or automatically every 30 seconds, from the perspective of the wearer. At the end of the day, the camera images can be uploaded into a computer and used for reminiscence viewing. Research employing this camera has shown beneficial effects on recall for select individuals with organic memory impairments.\rBerry et al. (2007), for example, used SenseCam in their study of a 63-year-old woman named Mrs. B who demonstrated mild to moderate retrograde amnesia and marked anterograde amnesia due to limbic encephalitis. Following her brain infection, she had difficulty remembering past experiences, including recent experiences shared with her husband. When Mrs. B’s husband kept a written diary of shared events for nightly reminiscence with his wife, little change was observed in Mrs. B’s recall. Later, Mrs. B wore a SenseCam for several weeks, and her husband uploaded the images into their computer. Together, they reminisced nightly about their shared experiences while viewing the SenseCam images. Mrs. B was able to recall approximately 80 percent of the recent images, and her episodic recall was maintained three months after viewing the pictures. Berry et al. added that Mrs. B was not simply recalling the images; she provided details of past events that were not captured in those pictures.\rSimilar memory enhancing effects with SenseCam have been observed with individuals suffering from anterograde amnesia - including a 47-year-old woman with herpes simplex viral encephalitis (Loveday & Conway, 2011), a 21-year-old man with acquired brain injury (Brindley, Bateman, & Gracey, 2011), a 13-year-old boy with a brain tumor (Pauly-Takas,\rMemory Camera 3\r\nMoulin, & Estlin, 2011), and a 55-year-old woman with mild cognitive impairment (Browne et al., 2011). In each case, reminiscence with SenseCam images produced greater recall and a greater sense of well being than reminiscence with verbal diaries. Loveday and Conway (2011) stated that SenseCam images can cue a “Proustian moment” in which individuals experience intense recollections of past events, even accessing memories of formerly inaccessible events not captured in the SenseCam pictures, a finding also reported by others (Berry et al., 2007; Brindley et al., 2011; Browne et al., 2011).\rAlthough these case studies show that SenseCam aids episodic recall, they were conducted under conditions where experimental controls were necessarily less stringent than those typically observed in laboratory testing. In Berry et al.’s (2007) study of Mrs. B, for example, her husband determined which activities to record in a daily diary, he wrote the diary entries after they occurred, he conducted the reminiscence sessions, and he graded his wife’s memory performance. This study and other SenseCam case studies have typically included a small number of recorded events that were confounded with the type of reminiscence used for each event (e.g., one event had SenseCam reminiscence; a different event had diary reminiscence). These methodological caveats in no way diminish the importance of the SenseCam case studies for demonstrating the benefits of a new technology with memory- impaired individuals. In fact, the similarity of the findings from the case studies suggests that Mr. B, for one, performed in a highly effective fashion. Still, the limited control of potentially important methodological variables in these studies leaves open the issue of what specifically led to enhanced episodic recall following SenseCam reminiscence.\rAdditionally, only a few studies have explored the use of SenseCam with memory- unimpaired people to determine if it has wider application. St. Jacques, Conway, and Cabezza (2011) measured brain activation patterns from SenseCam images or verbal cues with fMRI,\rMemory Camera 4\r\nSilva, Pinho, Macedo, and Moulin (2013) observed better performance on a neurological test battery for people who previously reviewed their SenseCam images of everyday events than their diary entries, and Finley, Brewer, and Benjamin (2011) found better recognition of SenseCam images and enhanced recall of their depicted activities for previously reviewed than not reviewed images. The present research extended these studies by testing whether SenseCam offered any mnemonic advantage for memory-unimpaired people in recalling previously seen actions in a natural setting.\rSpecifically, we examined how well memory-unimpaired participants recalled actions they previously saw performed one week earlier on a structured walk following reminiscence with SenseCam images, a written diary, or no external memory aid. Control of potentially important methodological variables was achieved in several ways. First, all participants observed the same sequence of actions performed in the same locations on a campus walk so that experiences were not confounded with type of reminiscence. Second, immediately after the walk, participants reminisced with either their SenseCam images taken during the walk, their diary entries written during the walk, or what they remembered from the walk on their own. All participants engaged in either social or self-reminiscence. Social reminiscence was designed to approximate the reminiscence discussions reported in the case studies between a memory-impaired person and spouse, whereas self-reminiscence had participants reminisce alone for the same length of time.\rIf a memory camera can aid memory-unimpaired people, the recall of previously observed actions should be better for participants who used SenseCam during reminiscence than those who used their diary or had no external memory aid. Additionally, social reminiscence should produce greater delayed recall than self-reminiscence based on studies of collaborative recall that show that accurate information from another person enhances memory\rMemory Camera 5\r\n(e.g., Roediger, Meade, & Bergman, 2001; Wright, Self, & Justice, 2000) and studies of the testing effect (e.g., Roediger & Karpicke, 2006) where retesting enhances retention. In the present context, if immediate reminiscence after a walk constitutes a practice recall test, experimenter-based cueing with corrective feedback provides a relearning opportunity and should produce better recall than self-based cueing with no feedback.\rMethod\rParticipants. The participants were 144 Wesleyan University students, between 17 and\r23 years of age, who received introductory psychology credit or served as paid volunteers. None had taken part in any related memory research.\rMaterials. The SenseCam memory camera used in this study was sold commercially as the Vicon Revue. The walk and accompanying actions were similar to those used by Seamon et al. (2006; 2009). We chose 43 distinct, physical locations involving five campus buildings in close proximity. All locations were indoors to permit testing in varied weather conditions (e.g., an elevator door in a hallway; a Pepsi machine in a snack area). Because SenseCam is deemed best suited for recording non-mundane events, we used an atypical action at each location (e.g., Press the elevator button with one elbow) to minimize pre-experimental associations between locations and actions (The action-location list is available from the first author). No location or action was used more than once.\rProcedure. We employed a 3 x 2 design with Memory Aid during the walk and reminiscence period (SenseCam vs. Diary vs. No Aid) and Reminiscence Type after the walk (social vs. self-reminiscence) as between-participants variables. The participants were randomly assigned to one of six groups. Two groups used the memory camera, two groups wrote in a diary, and two groups had no external memory aid. After the walk, one group within each memory aid condition socially reminisced with the experimenter, whereas the other group\rMemory Camera 6\r\nreminisced alone. The six groups were SenseCam/Social (n = 25), SenseCam/Self (n = 24), Diary/Social (n = 23), Diary/Self (n = 25), No Aid/Social (n = 24), and No Aid/Self (n = 27). Because our research was novel and different from the case studies in participants and procedures, no estimate of effect size was possible. The sizes of our six groups are typical of memory research, and pooling over the two reminiscence conditions yielded 49, 48, and 51 participants for the three memory aid conditions, group sizes that should provide an adequate test of any meaningful memory aid effect.\rThe individually tested participants were told that this two-session experiment involved memory for actions to be observed on a campus walk. They were informed that the walk would take approximately 1 hour, with stops at 43 locations where the experimenter would state an action and perform it. All participants were instructed to try to remember those actions for an unspecified, delayed memory test. They could talk with the experimenter during the walk, but they could not engage in any activities other than those that were part of the study.\rParticipants in the two SenseCam groups wore the camera and it took pictures automatically, approximately once every 30 seconds. At each location where an action was stated and performed, the experimenter maintained that action until the camera recorded it. Participants in the two diary groups wrote the name of the location and the action performed during each stop in a diary booklet that contained one page for each location-action pair. Similar to the memory camera, the diary ensured that all location-action pairs were recorded in their correct sequential order. Finally, participants in the two no aid groups observed the same locations and actions for the same length of time, approximately 20 - 30 seconds per stop, but they had to rely solely on their memory for retention.\rMemory Camera 7\r\nUpon completing the walk, all participants received a 5-minute distraction task of finding Waldo characters in a Finding Waldo book. This task gave the experimenter sufficient time to upload the images from a SenseCam participant into the computer.\rFollowing the distraction task, all participants engaged in social or self-reminiscence for 25 minutes. One group of participants from the SenseCam, Diary, and No Aid Conditions (Groups 1, 3, & 5) engaged in social reminiscence. For the SenseCam participants, the experimenter sequentially reviewed the images on the computer by skimming quickly through the irrelevant images, stopping and focusing on the action images, and asking the participant to recall each action as it was displayed. For the diary participants, the experimenter sequentially reviewed their diary entries, asking the participant to state each written action. For the no aid participants, the experimenter asked them to recall the actions from the walk in sequence. In each condition, the experimenter sat with the participant and engaged in an open, but structured conversation, helping to guide the participant’s oral recall of the action sequence. Monitoring the participant’s recall, the experimenter provided corrective feedback for missing actions, incorrect actions, or action sequencing errors. During the reminiscence period, all participants discussed and reviewed all actions at least twice at their own pace with the experimenter.\rThe other participants in the SenseCam, Diary, and No Aid Conditions (Groups 2, 4, & 6) engaged in self-reminiscence for the same length of time. Participants who used the camera were presented with their images on the computer and shown how to toggle through the images sequentially, participants who wrote in a diary were given their diaries and instructed to review their diary entries, and participants in the no aid condition were asked to remember the walk on their own over the interval. All participants were informed that the reminiscence period was important for long-term retention and that they should review the walk’s locations and actions several times at their own rate. Observed from a distance, the participants complied with this\rMemory Camera 8\r\ninstruction. Following reminiscence, all participants were scheduled for a follow-up session one week later, plus or minus one day.\rTwo memory tests were given in the second session. The first test was a written free recall test of the 43 actions observed previously during the walk. The participants were given 12 minutes to recall as many actions as they could remember, without regard to order. Next, the participants were given a test of action order in which they were given 5 note cards, each with the name of a campus building used on the walk, and a shuffled deck of 43 cards, each containing a previously observed action. After sorting the actions by their campus buildings, the participants were asked to place them in their correct sequential order on a display board that was numbered from 1 to 43. They had 15 minutes to sort the actions by building and put them in their sequential order, guessing if they were unsure.\rResults\rWe scored the free recall data in two ways: first by a strict criterion in which the recalled action had to be essentially the same as the action that was performed during the walk; second by a lenient criterion in which the gist of action was recalled with no inaccuracy. For example, for the action Rub the cheek of Olin’s bust in the Olin Library Rotunda, a correct recall by the strict criterion could be Rubbed the cheek of the statue in Olin, whereas a correct lenient recall could be Touched the statue in Olin. The same statistical analyses were applied to all recall data from the strict and lenient criteria, and these analyses yielded identical outcomes in each instance. To conserve space, we will provide only those analyses based on the lenient scoring criterion.\rFree Recall. Table 1 shows that participants produced greater delayed free recall following social reminiscence than self-reminiscence (.78 vs. .64) F (1, 142) = 13.26, MSE = .70, ηp2 = .09, p < .001, but their recall was unaffected by the memory aid conditions. Action\rMemory Camera 9\r\nrecall was not reliably different for the SenseCam (.74), diary (.69), and no aid (.68) conditions, F < 1, and there was no interaction between memory aid and reminiscence type, F < 1. Because eight people scored low on the free recall test (.30 or less), we also applied an arc sine transformation to the data but this transformation did not change the outcome. There remained a main effect of reminiscence type, F (1, 142) = 14.79, MSE = 1.27, ηp2 = .09, p < .001, but no effect of memory aid, F < 1, and no memory aid by reminiscence type interaction, F < 1. Because these eight outliers were not evenly distributed across conditions (1 from SenseCam/Social, 2 from Diary/Self, and 5 from No Aid/Self), we did not remove them from the analyses.\rInsert Table 1 about here\rOrder Recall. We calculated a Spearman rank-order correlation based on each\rparticipant’s recalled action order and the actual order from the walk and performed the same two-factor analysis of variance on the correlations that we used with the free recall data. These results (absent the correlations from six participants that were lost) are also shown in Table 1. Participants in all conditions demonstrated strong action order memory by producing high positive correlations. There was no difference between the three memory aid conditions, F < 1, the two reminiscence types, F (1, 126) = 1.81, MSE = .06, η2 = .01, p > .10, and no interaction of these variables, F (2, 126) = 1.13, MSE = .04, η2 = .02, p > .10. Our participants’ general familiarity with the campus buildings may have contributed partly to their strong action order memory.\rIn summary, social reminiscence was better that self-reminiscence for delayed action recall, but reminiscing with SenseCam images or diary entries was not better than unaided reminiscence. Although the camera images captured all actions visually and the diary entries, scored by the strict (.95) or lenient (.99) recall criteria, were virtually verbatim copies of all\rMemory Camera 10\r\nstated actions, we found that faithful representations of the actions for reminiscence, whether by SenseCam images or diary entries, did not enhance action recall relative no external aid.\rGeneral Discussion\rOur comparison of reminiscence conditions showed no memory consolidation benefit\rfrom prior reminiscence with SenseCam images or diary entries for the recall of unrelated, atypical actions, observed one-week earlier by memory-unimpaired people. Participants with unaided memory remembered the actions just as well as those previously armed with a memory camera or diary. Social reminiscence immediately after the walk did lead to better recall than self-reminiscence, likely reflecting a testing effect with experimenter-based cueing and corrective feedback better than self-based cueing and no feedback.\rGiven that SenseCam has consistently enhanced recall in memory-impaired individuals, our finding of no difference between SenseCam, Diary, and No Aid reminiscence was surprising. Our use of memory-unimpaired participants is likely a contributing factor, as the camera was designed to assist memory-impaired people. It may be that our memory- unimpaired participants revealed no SenseCam benefit because their cognitive resources were generally sufficient for the present task. Although the free recall results did not exhibit a ceiling effect in any condition, more demanding tasks might show a SenseCam advantage over the diary and no aid conditions.\rEqually surprising was our finding that participants in the no aid condition recalled as much as participants in the two aid conditions. It may be that the no aid participants, knowing that the only source of the actions for a later test was their memory, focused harder on remembering the actions during the walk than participants with the memory camera or diary that provided a record of those actions. Sparrow, Lui, and Wegner (2011) found that people tend to forget information that they believe is available, but remember information they think is\rMemory Camera 11\r\nunavailable. A similar effect might have operated in the present study minimizing potential differences between our memory aid conditions.\rDirect comparisons between the case studies that have consistently shown beneficial effects of SenseCam reminiscence and the present study remain problematic for several reasons. First, memory-impaired individuals may benefit more from SenseCam reminiscence than memory-unimpaired people because the camera, in augmenting diminished cognitive function, is a more useful prosthetic device for the memory impaired. Second, the data scoring procedures for the case studies and the present experiment were different. In the case studies, recall was scored in terms of how well an event was subsequently recalled from either the memory-impaired person’s immediate recall (Brindley et al., 2011; Loveday & Conway, 2011) or the recall of the memory-impaired person’s spouse (Berry et al., 2007; Browne et al., 2011). In either instance, the number of details could vary with each event. In our study, recall was scored in terms of how many actions the participants in each condition recalled from the same campus walk. Third, the memory-impaired individuals in the case studies wore the memory camera for special events, and they often reminisced multiple times with the images in different sessions (e.g., Berry et al., 2007; Browne et al., 2011; Pauly-Takacs et al., 2011). In our study, the actions from the walk were reminisced several times in a single session, shortly after the actions were observed. Differences in massed vs. distributed reminiscence and the amount of reminiscence (cf., Cepeda, Pashler, Wixted, & Rohrer, 2006) may influence SenseCam’s effectiveness.\rIn addition, our use of unrelated actions on the campus walk made it difficult for SenseCam images to trigger memories of other actions because our atypical actions were arbitrary and independent. The actions on the walk lacked a coherent schema that linked them into a larger, meaningful episode. Unlike the present study, the SenseCam images in the case\rMemory Camera 12\r\nstudies were obtained from recording meaningful events - such as a visit to a museum or the beach – where recalling one aspect of an event could trigger related aspects of the event that the memory-impaired person previously experienced (see Hodges, Berry, & Wood, 2011; Loveday & Conway, 2011). SenseCam memory enhancement may be sensitive to the coherence of an event.\rFinally, unlike the present study, the case studies have provided SenseCam images to individuals at test to trigger memories of events that are captured in the images as well as aspects of those events not explicitly shown. Memory-impaired people may or may not recall the events depicted in the images at test. The same procedure cannot be used with memory- unimpaired people. If we had given our memory-unimpaired participants either their SenseCam images or diary entries at test, we would not know whether they were recalling the actions from those aids or their memory. Most likely, they would merely state what was shown in the image or diary entry. Consequently, we offered no aid to our participants on the delayed memory test to determine how well our study and reminiscence conditions influenced their long-term recall. Future testing might consider providing partial information via SenseCam images or diary entries to determine the effectiveness of each aid for cueing the remaining uncued material.\rIn closing, when methodological variables were controlled across memory aid conditions, we found that SenseCam provided memory-unimpaired people no consolidation benefit in recalling atypical actions observed by on a campus walk. SenseCam is a valuable aid to the memory impaired, but its value for non-clinical populations remains to be determined.\rMemory Camera 13\r\nReferences\rBerry, E., Kapur, N., Williams, L., Hodges, S., Watson, P., Smyth, G., Srinivasan, J., Smith,\rR., Wilson, B., & Wood, K. (2007). The use of a wearable camera, SenseCam, as a pictorial diary to improce autobiographical memory in a patient with limbic encephalitis: A preliminary report. Neuropsychological Rehabilitation, 17, 582-601.\rBrindley, R., Bateman, A., & Gracey, F. (2011). Exploration of the use of SenseCam to support autobiographical memory retrieval within a cognitive-behavioural therapeutic intervention following acquired brain injury. Memory, 19, 745-757.\rBrowne, G., Berry, E., Kapur, N., Hodges, S., Smyth, G., Watson, P., & Wood, K. (2011). SenseCam improves memory for recent events and quality of life in a patient with memory retrieval difficulties. Memory, 19, 713-722.\rCepeda, N. J., Pashler, H., Vul, E., Wixted, J., & Rohrer, D. (2006). Distributed practice in verbal recall tasks: A review and quantitative synthesis. Psychological Bulletin, 132, 354- 380.\rFinley, J. R., Brewer, W. F., & Benjamin, A. S. (2011). The effects of end-of-day picture review and a sensor-based picture capture procedure on autobiographical memory using SenseCam. Memory, 19, 796-807.\rHodges, S., Berry, E., & Wood, K. (2011). SenseCam: A wearable camera that stimulates and rehabilitates autobiographical memory. Memory, 19, 685-696.\rLoveday, C., & Conway, M. A. (2011). Using SenseCam with an amnesic patient: Accessing inaccessible everyday memories. Memory, 19, 697-704.\rPauly-Takacs, K., Moulin, C. J. A., & Estlin, E. J. (2011). SenseCam as a rehabilitation tool in a child with anterograde amnesia. Memory, 19, 705-712.\rMemory Camera 14\r\nRoediger, H. L. III, & Karpicke, J. D. (2006). The power of testing memory: Basic research and implications for educational practice. Perspectives on Psychological Science, 1, 181- 210.\rRoediger, H. L., III, Meade, M. L., & Bergman, E. T. (2001). Social contagion of memory. Psychonomic Bulletin & Rreview, 8, 365-371.\rSeamon, J. G., Blumenson, C. N., Karp, S. R., Perl, J. J., Rindlaub, L. A., & Speisman, B. B. (2009) Did we see someone shake hands with a fire hydrant?: Collaborative recall affects false recollections from a campus walk. American Journal of Psychology, 122, 235-247.\rSeamon, J. G., Philbin, M. M., & Harrison, L. G. (2006). Do you remember proposing marriage to the Pepi machine? False recollections from a campus walk. Psychonomic Bulletin & Review, 13, 752-756.\rSilva, A. R., Pinho, S., Macedo, L. M., & Moulin, C. J. (2013). Benefits of SenseCam review on neuropsychological test performance. American Journal of Preventive Medicine, 44, 302-307.\rSt. Jacques, P. L., Conway, M. A., & Cabeza, R. (2011). Gender differences in autobiographical memory for everyday events: Retrieval elicited by SenseCam images versus verbal cues. Memory, 19, 723-732.\rSparrow, B., Lui, J., & Wegner, D. M. (2011). Google effects on memory: Cognitive consequences of having information at our fingertips. Science, 333, 776-778.\rWright, D. B., Self, G., & Justice, C. (2000). Memory conformity: Exploring misinformation effects when presented by another person. British Journal of Psychology, 91, 189-202.\rMemory Camera 15\r\nNote.\rAddress correspondence to John Seamon, Psychology Department, Wesleyan University, Middletown, CT 06459, USA (jseamon@wesleyan.edu).\rMemory Camera 16\r\nTable 1: Delayed Action Free Recall and Action Order Recall Memory Aid and Type of Reminiscence SenseCam Diary No Aid\r_______________________________________________________________ Action Recall - Lenient Criterion\rSocial Reminiscence Self Reminiscence\rAction Recall - Strict Criterion Social Reminiscence\rSelf Reminiscence\r.79 (.22) .68 (.21)\r.75 (.26) .62 (.21)\r.75 (.20) .64 (.23)\r.71 (.22) .59 (.24)\r.85 (.19)\r.78 (.18) .59 (.30)\r.75 (.19) .56 (.31)\r.93 (.15)\rAction Order Recall - Correlations Social Reminiscence .92 (.15) Self Reminiscence .87 (.18)\r.86 (.20) _______________________________________________________________\rNotes. Recalls are mean proportions correct; order recalls are mean rank-order correlations. Standard deviations in parentheses.\r.84 (.19)\rMemory Camera 17","title":"SenseCam MEM SR 13 95.R1 copy","mimeType":"application/pdf","authors":[{"id":20,"firstName":"John","lastName":"Seamon","middleNames":[]}],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"diari","time":1461915875566,"auto":true,"weight":1.0},{"@type":"Tag","text":"memori","time":1461915875586,"auto":true,"weight":1.0},{"@type":"Tag","text":"sensecam","time":1461915875556,"auto":true,"weight":1.0},{"@type":"Tag","text":"action","time":1461915875582,"auto":true,"weight":1.0},{"@type":"Tag","text":"2011","time":1461915875602,"auto":true,"weight":1.0},{"@type":"Tag","text":"reminisc","time":1461915875549,"auto":true,"weight":1.0},{"@type":"Tag","text":"camera","time":1461915875572,"auto":true,"weight":1.0},{"@type":"Tag","text":"aid","time":1461915875596,"auto":true,"weight":1.0},{"@type":"Tag","text":"walk","time":1461915875578,"auto":true,"weight":1.0},{"@type":"Tag","text":"recal","time":1461915875591,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":30,"appId":"PeyeDF_0e317291f240436f4ed73345ab884f77dec76fb3","timeCreated":1456693254472,"timeModified":1461915877391,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.4197998,"uri":"file:///Users/cheny13/Documents/References/experience to memory.pdf","plainTextContent":"17 Spelke, E.S. (1994) Initial knowledge: six suggestions. Cognition 50, 431–445\r18 Flavell, J.H. (1963) The Developmental\rPsychology of Jean Piaget, Nostrand\r19 Baillargeon, R. (1995) A model of physical reasoning\rin infancy. In Advances in Infancy Research (Rovee-\rCollier, C. and Lipsitt, L.P., eds), pp. 305–371, Ablex\r20 Baillargeon, R. et al. (1995) The acquisition of\rphysical knowledge in infancy. In Causal Cognition: A Multidisciplinary Debate (Sperber, D. et al., eds), pp. 79–116, Clarendon Press\r21 Baillargeon, R. (1998) Infants’ understanding of the physical world. In Advances in Psychological Science (Sabourin, M. et al., eds), pp. 503–529, Psychology Press\r22 Hespos, S.J. and Baillargeon, R. (2001) Infants’ knowledge about occlusion and containment events: a surprising discrepancy. Psychol. Sci. 12, 140–147\r23 Baillargeon, R. and DeVos, J. (1991) Object permanence in 3.5- and 4.5-month-old infants: further evidence. Child Dev. 62, 1227–1246\r24 Baillargeon, R. and Graber, M. (1987) Where’s the rabbit? 5.5-month-old infants’ representation of the height of a hidden object. Cogn. Dev. 2, 375–392\r25 Baillargeon, R. The acquisition of physical knowledge in infancy: A summary in eight lessons. In Handbook of Childhood Cognitive Development (Goswami, U., ed.), Blackwell (in press)\r26 Spelke, E.S. et al. (1995) Infants’ knowledge of object motion and human action. In Causal Cognition: A Multidisciplinary Debate (Sperber, D. et al., eds), pp. 44–78, Clarendon Press\r27 Aguiar, A. and Baillargeon, R. Developments in young infants’ reasoning about occluded objects. Cogn.Psychol.(inpress)\r28 Baillargeon, R. Infants’ physical knowledge: of acquired expectations and core principles. In Language, Brain, and Cognitive Development: Essays in Honor of Jacques Mehler (Dupoux, E., ed.), MIT Press (in press)\r29 Aguiar, A. and Baillargeon, R. (2000) Perseveration and problem solving in infancy. In Advances in Child Development and Behavior (Reese, H.W., ed.), pp. 135–180, Academic Press\r30 Sitskoorn, S.M. and Smitsman, A.W. (1995) Infants’ perception of dynamic relations between objects: passing through or support? Dev. Psychol. 31, 437–447\r31 Spelke, E.S. et al. (1995) Spatio-temporal continuity, smoothness of motion, and object identity in infancy. Br. J. Dev. Psychol. 13, 113–142\r32 Wilcox, T. and Baillargeon, R. (1998) Object individuation in infancy: the use of featural information in reasoning about occlusion events. Cogn. Psychol. 17, 97–155\r33 Xu, F. and Carey, S. (1996) Infants’ metaphysics: the case of numerical identity. Cogn. Psychol. 30, 111–153\r34 Van de Walle, G. et al. (2001) Bases for object individuation in infancy: evidence from manual search. J. Cogn. Dev. 1, 249–280\r35 Wilcox, T. and Chapa, C. Infants’ reasoning about opaque and transparent occluders in an individuation task. Cognition (in press)\r36 Wilcox T. et al. Object individuation in infancy. In Progress in Infancy Research (Fagan, F. and Hayne, H., eds), Erlbaum (in press)\r37 Santos, L.R. et al. Object individuation using property/kind information in rhesus macaques (macacamulatta).Cognition(inpress)\r38 Uller, C. et al. (1997) Is language needed for constructing sortal concepts? A study with nonhuman primates. In Proceedings of the 21st Annual Boston University Conference on Language Development (Hughes, E., ed.),\rpp. 665–677, Oxford University Press\r39 Wilcox, T. (1999) Object individuation: infants’ use of\rshape, size, pattern, and color. Cognition 72, 125–166 40 Wilcox, T. and Baillargeon, R. (1998) Object\rindividuation in young infants: further evidence\rwith an event-monitoring task. Dev. Sci. 1, 127–142 41 Wilcox, T. and Schweinle, A. Object individuation\rand event mapping: developmental changes in infants’ use of featural information. Dev. Sci. (in press)\r42 Needham, A. and Baillargeon, R. (2000) Infants’ use of featural and experiential information in segregating and individuating objects: a reply to Xu, Carey, and Welch. Cognition 74, 255–284\r43 Baldwin, D.A. et al. (1993) Infants’ ability to draw inferences about non-obvious object properties: evidence from exploratory play. Child Dev.\r64, 711–728\r44 Mandler, J.M. (2000) Perceptual and conceptual processes in infancy. J. Cogn. Dev. 1, 3–36\r45 Needham, A. and Modi, A. (2000) Infants’ use of prior experiences with objects in object segregation: Implications for object recognition in early infancy. In Advances in Child Development and Behavior (Reese, H.W., ed.), pp. 99–133, Academic Press\r46 Quinn, P.C. and Eimas, P.D. (1996) Perceptual organization and categorization in young infants. In Advances in Infancy Research (Rovee-Collier, C. andLipsitt,L.P.,eds),pp.1–36,Ablex\rReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 93\rObserving the transformation of experience into memory\rKen A. Paller and Anthony D. Wagner\rThe ability to remember one’s past depends on neural processing set in motion at the moment each event is experienced. Memory formation can be observed by segregating neural responses according to whether or not each event is recalled or recognized on a subsequent memory test. Subsequent memory analyses have been performed with various neural measures, including brain potentials extracted from intracranial and extracranial electroencephalographic recordings, and hemodynamic responses from functional magnetic resonance imaging. Neural responses can predict which events, and which aspects of those events, will be subsequently remembered or forgotten, thereby elucidating the neurocognitive processes that establish durable episodic memories.\rSome of life’s episodes are remembered so well that we can accurately bring back to mind or recollect tremendous detail, even after considerable time has elapsed. Other events are seemingly experienced in an identical way, and yet are irretrievably lost from memory, even moments later. A fundamental\rchallenge for memory theorists is to specify the neurocognitive processes that impact the mnemonic fate of our experiences, influencing whether they will be remembered or forgotten. A significant step towards meeting this challenge is to delineate encoding operations and their impact on subsequent memorability. Although multiple factors influence our ability to remember, one factor that must be critical for remembering is whether the experience coincides with the effective laying down of an engram in the brain. Insights into effective memory formation can be gained by monitoring brain activity during an experience and relating these neural measures to behavioral evidence that a memory was formed. In this review, we consider how correlations between neural activation and subsequent remembering have informed our theories of how experiences are transformed into memories.\rhttp://tics.trends.com 1364-6613/02/$ – see front matter © 2002 Elsevier Science Ltd. All rights reserved. PII: S1364-6613(00)01845-3\r\n94\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\r(a)\rTime 1 (encoding)\rTime 2 (retrieval)\rNeural responses to events measured at Time 1\rEvents at Time 1 classified; responses extracted\rVisual events\rMUSTARD GIRAFFE MOON...\rRemembered\rMemory test for events\rForgotten\rRemembered\rForgotten\rClasssification based on behavioral measures of memory\rRemembered events\rForgotten events\r(b)\rLeft inferior prefrontal cortex\rLeft medial temporal lobe\rTRENDS in Cognitive Sciences\r4 3 2 1 0\r–1\r0 4 8 12\rTime (s)\r4 3 2 1 0\r–1\r0 4 8 12\rTime (s)\rKen A. Paller\rDept of Psychology and Institute for Neuroscience, Northwestern University, 2029 Sheridan Road, Evanston, IL 60208-2710, USA.\re-mail: kap@northwestern.edu\rAnthony D. Wagner\rDept of Brain and Cognitive Sciences, and Center for Learning and Memory, Massachusetts Institute of Technology, Cambridge, MA 02139 and the Martinos Center for Biomedical Imaging, MGH/MIT/HMS, Charlestown MA 02129, USA.\re-mail: awagner@psyche.mit.edu\rFig. 1. The subsequent memory paradigm. (a) Neural responses are acquired during event processing (in this example, visual word presentations). Subsequently, memory is probed and events are classified. (b) Neural responses are analyzed based on subsequent memory, revealing neural correlates of encoding in various brain regions. Graphs (redrawn from Ref. 8) show that remembered events (blue) elicited greater responses than forgotten events (green).\rThe first steps to remembering: witnessing the creation of memories\rEpisodic encoding refers to the initial information processing steps whereby a memory trace is created such that it can subsequently support the conscious recollection of one’s past [1]. Encoding depends on at least two components:\r(1) The initial component mediates the transformation of sensory input into internal representations that are interpreted or comprehended. This often entails the retrieval of associated knowledge relevant to current goals.\r(2) The second component binds the internal representations into an enduring trace such that the resultant representation ultimately permits the experience to be brought back to mind. The representation may include perceptual and conceptual fragments, self-generated thoughts, and contextual details, and critically links such fragments together to form an integrated engram.\rThese two components of encoding are clearly illustrated by patterns of spared and impaired performance in individuals with anterograde amnesia due to bilateral medial temporal or diencephalic insult. Patients with amnesia can carry on a normal conversation, repeat back information when queried immediately after the information is provided, and show intact working memory [2].\rThus, they can generate and work with internal representations, but have difficulty remembering experiences once active representations have dropped from consciousness. Contemporary explanations for this pattern typically posit a deficit at the second stage of encoding, whereby durable relational, configural, or declarative traces are formed and/or consolidated [3–7].\rThe two components of intact episodic encoding – generating an internal representation of an experience and storing a bound trace of those representations – typically interact during learning. We describe below how these interactions may reflect the convergence of (1) prefrontal and posterior neocortical computations responsible for the representation and goal-directed processing of events with (2) medial temporal computations that guide the storage of durable episodic traces whereby the elements of these representations are linked together.\rTo learn how experiences are transformed into memories, no single methodology will suffice. Cognitive neuroscientists strive to obtain precise observations of both the behavior that arises from cognitive processes and the neural computations that support cognition. A particularly powerful approach to the study of memory encoding is the ‘subsequent memory paradigm’ (see Fig. 1), which provides measures of neural activity correlated with later remembering. In general, neural responses to distinct stimulus events are recorded and then classified based on testing the subject’s memory for the stimuli at a later time. The key contrast is typically between neural responses to stimuli later remembered and to those later forgotten. Differential neural activity based on memory, sometimes referred to as Dm,\rhttp://tics.trends.com\r\nReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 95\rGSR\rEEG Depth EEG ERPs units\rfMRI\r1970 1975 1980 1985 1990 1995 2000\rTRENDS in Cognitive Sciences\rFig. 2. Timeline of Dm analyses of episodic encoding. The current blossoming of interest in physiological correlates of memory encoding stems from earlier work using skin conductance measures (GSR). Measures of brain activity were subsequently used, including EEG measures in the frequency domain, event-related potentials recorded from scalp and intracranial electrodes, single unit activity, and functional magnetic resonance imaging.\rprovides an index of neural computations at the time of encoding that are predictive of accurate recall or recognition (Fig. 2). Subsequent memory effects, which presumably index pivotal operations at the time of learning that influence what will be remembered, constitute some of the tightest correlations between neural function and\rencoding. Here, we consider how the subsequent memory paradigm has shed new light on the complexity and variety of encoding processes, providing leverage on delineating how episodic memories are created.\rEncoding circuits: evidence from stimulus effects\rThe cerebral cortex consists of multiple processing modules that represent and perform computations on specific stimulus dimensions or features, such as visual attributes, spatial configuration, and domains of meaning. High-level outputs from neocortical processing modules are directed to the medial aspect of the temporal lobe (MTL), which binds representations together in the service of episodic memory formation. Different combinations of neocortical circuits can thus potentially interact with the MTL; which specific circuits are recruited in any given instance of learning is a function of factors such as the nature of the event being encoded.\rAccordingly, material-sensitive subsequent memory effects have been observed using ERPs and fMRI. For example, different brain potentials were correlated with later memory depending on whether subjects learned words or faces [9]. The two initial fMRI studies of subsequent memory effects [8,10] also revealed activation patterns that varied depending on the stimuli. Wagner and colleagues [8], as well as others [11–17], reported that subsequent recognition memory for incidentally learned words was correlated with greater encoding activity primarily in left inferior prefrontal cortex (PFC), fusiform cortex, and MTL. Topographic patterns of subsequent memory effects in scalp ERPs derived from current source density analyses have likewise been interpreted as reflections of left inferior PFC activity [18].\rBy contrast, Brewer and colleagues [10] observed that activity levels in bilateral MTL and right inferior PFC predicted subsequent remembering for incidentally learned complex visual scenes. Thus, verbal encoding was associated with left inferior PFC and MTL computations, and pictorial encoding with right inferior PFC and bilateral MTL processing.\rIn a study that directly compared word and visual scene encoding, contrasts between novel and highly familiar stimuli were juxtaposed to subsequent memory effects [12]. Scene novelty preferentially activated right inferior PFC and bilateral posterior fusiform, consistent with their putative role in processing visuo-spatial/visuo-object information [19,20]. Word novelty preferentially activated the posterior extent of left inferior PFC and left anterior fusiform, perhaps reflecting phonological and/or lexical computations. Word novelty also selectively activated the anterior extent of left inferior PFC and left middle/inferior temporal cortex, regions involved in the representation and controlled retrieval of semantic knowledge [21]. Speculatively, PFC regions may influence encoding by modulating posterior neocortical processing and by regulating input to MTL. Importantly, activity levels in almost all of these regions – as well as in MTL structures – were positively correlated with subsequent recognition memory for the novel stimuli. Thus, some cortical regions that are sensitive to stimulus novelty also support episodic encoding.\rEmotional intensity is another stimulus-bound dimension that impacts encoding. Consistent with the hypothesis that the amygdala modulates episodic memory for emotional experiences [22,23], several groups observed between-subject correlations between subsequent episodic memory and levels of amygdala activation during encoding of blocks of negative and, in some studies, positive stimuli [24–26]. Within-subject fMRI analyses revealed that amygdala activation during the viewing of scenes was predictive of subsequent memory only for emotionally arousing scenes [27]. This approach illustrates the usefulness of combining behavioral measures (judgments of emotional arousal and subsequent memory) with indices of neural computation.\rProcessing goals influence encoding\rThe configuration of neocortical modules that mediate encoding varies not only with the nature\rof the stimulus but also with how attention is allocated to different stimulus features and types of processing. A central theoretical focus in memory research has concerned the influence of goal- directed attentional orientation on encoding, as can be manipulated by instructions to process stimuli for meaning, phonology, or structural form [28,29]. Initial PET and fMRI studies of encoding using blocked designs exploited the superiority of meaning-based versus non-semantic orientation to\rhttp://tics.trends.com\r\n96\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rshow, for example, that specific left PFC networks are involved in semantic encoding (for a review see Ref. [30]).\rThe influence of attentional orientation on Dm has also been explored with event-related designs. In one study, for example, ERPs were examined for words studied under four different attentional orienting conditions, using two structural and two semantic tasks [31]. Dm effects – enhanced posterior positivity at a latency of 400–800 ms for subsequently remembered words – were greater during semantic than during non-semantic orienting. These findings, along with other ERP evidence [32–37], suggest that some Dm effects reflect processing variations that come into play chiefly when subjects maintain a meaning-based attentional orientation, and that different meaning- based goals can differentially influence encoding.\rAs a general rule, Dm can emerge when the type or degree of processing varies from trial to trial so as to produce subsets of strongly and weakly encoded items, and when that processing is sufficiently time-locked to stimulus presentation.\rIn recent fMRI studies, incidental learning paradigms were used to determine whether different encoding networks were engaged during semantic versus non-semantic processing. For words processed in a semantic manner, activation based on subsequent recognition was observed in left inferior PFC, fusiform, MTL, and right inferior PFC [13,14,16]. For words processed in a structural manner, in two studies the subsequent memory effects were observed in a subset of these regions; namely, left inferior PFC and anterior MTL [13] or left PFC, fusiform, and right PFC regions [14].\rIn both studies, no additional regions differentially predicted subsequent memory following non-semantic orienting (but see Ref. [36] for divergent ERP results), suggesting that the frontal-temporal encoding circuit recruited during non-semantic orienting may be a subset of the circuit recruited during semantic orienting.\rFailure to observe Dm effects that are greater following non-semantic, relative to semantic, orienting may be partially attributable to low power due to poor later remembering following structural encoding [16,31] as well as to the obligatory nature of phonological and structural processing during semantic orienting. However, prior observations of crossover semantic/phonological interactions within left PFC, posterior temporal and parietal cortices [38], raise the possibility that semantic and non-semantic Dm effects sometimes diverge. Indeed, new evidence along these lines was recently obtained by comparing semantic to phonological conditions [16]. Whereas activation in left and medial PFC regions predicted recognition following semantic orienting, activation in bilateral intraparietal, fusiform, right PFC, and left occipital regions predicted recognition following phonological orienting. The specific PFC-posterior\rneocortical circuits that subserve encoding partially depend on the event features attended during learning, either due to differences in the stimulus- bound features or in the individual’s attentional goals. Encoding emerges as a byproduct of such goal-directed event processing [8,16,28].\rThe allocation of attention can be directed not only to a single stimulus but also to associative processes whereby two or more stimulus items are processed in relation to one another (i.e. inter-item rather than intra-item processing [39]). In one ERP study, recognition was superior for associatively compared to nonassociatively encoded word pairs, and Dm was observed only for the associatively encoded pairs [40] (see also [41] for a contrast between different types of associations). By contrast, an fMRI study of intra-item (rote) and inter-item (elaborative) rehearsal of three simultaneously presented words revealed subsequent memory effects primarily following rote rehearsal, although a Dm effect in left hippocampus was specifically associated with item recognition following elaborative encoding [17]. Associative or elaborative inter-item processing, which probably demands strategic rehearsal and manipulations of actively maintained representations [42], may likewise underlie frontal Dm findings in ERP studies when subjects processed relations between individual items [43,44].\rFractionating episodic memory\rThe experience of an episode does not yield a single, undifferentiated memory trace. Rather, multiple forms of learning simultaneously occur during event processing. Subsequently, those various traces can support qualitatively different memory phenomena. For instance, behavioral and neuropsychological evidence suggests that memory for the prior occurrence of a stimulus is distinct from memory for a conglomeration of specific details about that prior experience. In the latter case, the retrieval of multiple associations between a stimulus and contextual cues tends to coincide with recollection, the subjective experience of remembering. Item- context associations can pinpoint the source of remembered information, and thus drive the full-blown recollection of an episode [45]. On the other hand, a stimulus can also be recognized in the absence of recollection. In this case, retrieval may support a phenomenon known as ‘familiarity without recollection’ [39,46].\rThe specific neurocognitive processes that support these two manifestations of episodic memory, recollection and familiarity, have been investigated intensely but remain under active debate [47–49]. One hypothesis is that parahippocampal/perirhinal computations contribute mainly to memory for the occurrence of an item, which can underlie subsequent recognition based on familiarity [50,51]. The hippocampus, in contrast, may mediate or participate\rhttp://tics.trends.com\r\nReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002 97\rin the encoding of relations or conjunctions between an item and its context as well as conjunctions between items, which are crucial for subsequent recall and recollection-based recognition [3,7,52,53].\rMemory testing with the ‘R/K procedure’ has been used to distinguish between recollection and familiarity-without-recollection by requiring people to indicate their basis for recognition, either ‘remembered’ (R) or ‘known’ (K) [54,55]. Several ERP studies have combined R/K judgments with Dm analyses [35,44,56]. For example, larger positive responses were observed for subsequently recollected items than either known items or items not recognized, with no ERP differences between the latter two conditions [35]. These ERPs associated with subsequent recollection may fall into the broader category of the parietal-maximum Dm observed by many investigators and generally more robust with recall than with recognition [37,57,58]. On the other hand, in one study an ERP correlate of encoding supporting familiarity was ascribed to an N340 potential thought to be associated with attention- dependent conceptual processing in left temporal and/or inferior frontal neocortex [44].\rThe reliability of Dm analyses can generally be enhanced when recollective experience or decision confidence is taken into account. Subjects with a preponderance of correct guesses, with lenient criteria for judging items to be old, or with criteria that vary erratically over the course on an experiment, may not show Dm effects. Consistent with this suggestion, reliable Dm effects can be apparent in fMRI studies particularly when confidence or R/K judgments were required during recognition. These procedures restrict analyses to the memory extremes, consequently minimizing the influence of guessing. Accordingly, fMRI Dm magnitudes tend to be larger for events later recognized with high relative to with low confidence [8,13] and also larger for events later ‘remembered’ than during those later ‘known’ [10,11].\rTo date, fMRI studies have not provided clear evidence for a qualitative difference between encoding that yields recollection versus familiarity alone. Brewer and colleagues [10] observed graded encoding activation that declined across subsequent remembering to knowing to forgetting, suggesting merely quantitative differences. Henson and colleagues [11] did not run conventional subsequent memory analyses because of low levels of forgetting, but did compare encoding activations as a function of whether recognition was associated with an R or K response; activation in left inferior and middle PFC and left precuneus was predictive of ‘remembering,’ whereas activation in right parahippocampal gyrus and precuneus was predictive of ‘knowing.’ Additional evidence from studies that explore the relation between Dm for subsequent source recollection and for item memory without recollection, relative to forgotten trials, may\rprove informative in clarifying whether different encoding processes promote memory with and without recollection.\rAn intriguing speculation regarding the relation between ERP and fMRI results, made by Friedman and Johnson [18], is that parietal scalp ERP subsequent memory effects could derive from left precuneus computations associated with encoding [11]. Of course, combining ERP and fMRI data to achieve high temporal and high spatial resolution will ultimately require additional evidence that the same neural activity is responsible for both effects. Nevertheless, this possible multimodal imaging integration highlights the potential fruitfulness of such efforts. Relative timing data that show when encoding-related processes are set into motion can be used together with localization data from neuroimaging to provide new insights into the cognitive functions of specific neural computations.\rAlthough familiarity entails episodic memory restricted to a single item, it must be distinguished from another sort of item-specific long-term memory known as priming. Is this non-conscious facilitation or biasing of stimulus processing due to recent experience supported by the same encoding events that support episodic encoding? In two attempts to address this question, ERPs were found to predict subsequent recall and recognition but not priming [59,60]. However, it has recently been suggested [35] that when intentional encoding is avoided, ERPs can predict subsequent priming [35,61]. A critical issue for future investigation is thus to characterize the relation between processes supporting priming and those yielding effective episodic memory.\rIndeed, priming can act to hinder episodic encoding under some circumstances. Wagner and colleagues [62] found that the magnitude of priming during word processing was inversely related to episodic learning, as indexed by subsequent recognition. Priming was manipulated by interposing either a long or short lag between initial and repeat trials. Neural priming in left inferior PFC was greater with the short lag, as was behavioral priming. Subsequent recognition was superior with the long lag, the condition yielding less priming. Between- subject negative correlations were also observed between priming and subsequent recognition, even when lag was held constant. Priming might promote a stereotyped or sparse re-encoding experience, thus producing a less effective episodic memory [62].\rMedial temporal contributions to encoding\rNeocortical and medial temporal regions make different contributions to episodic memory. Even with a severe amnesia, the active representation of the multidimensional features of moment-to- moment experiences can still be supported by neocortical mechanisms. By contrast, storing neocortical memory fragments as coherent episodic representations is characteristically problematic in\rhttp://tics.trends.com\r\n98\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\ramnesia. The hippocampus and associated MTL structures (perirhinal, entorhinal and parahippocampal cortices) might play an essential role in linking up the multiple representations, dispersed across distinct neocortical regions, that constitute an experience [3,6]. Consistent with this idea, MTL activation has been observed in blocked- design PET and fMRI studies of episodic encoding with item-based, associative, and novelty- assessment paradigms (for reviews see Refs [63,64]). But many questions remain regarding exactly how MTL-neocortical interactions support learning, when MTL computations are engaged, and whether distinct anatomical subregions within the MTL subserve distinct mnemonic functions.\rERP recordings from intracranial electrodes suggest that a posterior hippocampal contribution might begin within the first 300–900 ms after stimulus onset [65]. Color patterns that patients were required to remember in a delayed matching-to- sample test elicited larger hippocampal potentials than did similar stimuli presented during the delay or when recognition decisions were made. These potentials could reflect the recruitment of hippocampal circuitry during intentional encoding, although the small number of recognition errors prohibited an analysis to determine whether these potentials predicted subsequent remembering.\rIn other experiments using intracranial ERPs, subsequent memory analyses were used to implicate specific MTL structures in encoding. Within-subject Dm effects were found for visual words based on recall after a filled 30-s delay [66,67]. Potentials that reached a negative peak 400–500 ms after word onset were larger for subsequently recalled words. These potentials, putatively generated in rhinal cortex\r(i.e. entorhinal and perirhinal cortex, the site of principal connections between hippocampus and neocortex), were thought to reflect the richness of semantic analysis [67]. By contrast, hippocampal Dm effects, which arose only after the rhinal potential peak, were thought to reflect trial-by-trial differences in associative learning processes that can begin only after initial semantic analyses.\rSingle-unit recordings from the human hippocampus further indicate that the rate of neural firing in this structure varies with encoding [68]. In response to visual presentations of to-be-remembered word pairs, firing rates of\rsome neurons demonstrated a positive\rcorrelation and others a negative correlation with later cued recall.\rMTL correlates of subsequent memory have also been demonstrated in several event-related fMRI studies. Encoding of words subsequently recognized was associated with activation in left posterior parahippocampal cortex [8,12] and left hippocampus [12,13,17]. Recall performance combined for groups of five words revealed correlated activity in posterior hippocampus [69] and tonic state effects in entorhinal\rcortex [70]. Subsequent recognition of complex visual scenes was correlated with bilateral posterior parahippocampal activation [10,12] and bilateral hippocampal activation [12].\rThe MTL regions implicated in these fMRI studies – typically posterior parahippocampal cortices and hippocampus – were not always the same as those observed in intracranial ERP experiments. Such discrepancies could potentially result from: (1) differences in the cognitive paradigms implemented; (2) the fact that ERPs and fMRI may detect different subsets of neural activity; (3) limits on fMRI sensitivity in anterior MTL;\r(4) limited intracranial ERP sampling from MTL regions; or (5) pathology or medication effects in patients with intracranial electrodes. Importantly, Strange and colleagues [71] reported subsequent memory effects in perirhinal, hippocampal, and posterior parahippocampal regions when they adopted the same cognitive paradigm as in prior intracranial ERP experiments [66] and optimized the fMRI scanning protocol for sensitivity to anterior MTL signals. A complex pattern of functional dissociations was observed across MTL subregions based on serial position within the learning list [71]. Further clarification of possible functional subdivisions within the MTL might be gained through subsequent memory analyses for contextual information or for associations between the constituent elements of an episode.\rIn a novel investigation of MTL regions and their interactions during encoding, the subsequent memory methodology was applied to single-trial EEG in the gamma frequency range (32–48 Hz) [72]. An increase in phase synchronization between rhinal and hippocampal regions (100–300 ms and 500–600 ms) followed by a decrease in synchronization (after\r1000 ms) suggested that an initially enhanced interaction between processing in these two regions may reflect encoding computations that facilitate later recall. Changes in synchronization could emerge directly from the dynamics of the MTL circuit or as the result of an external signal that entrains the MTL circuit [72]. Given the repeated observation of PFC correlates of subsequent memory, one speculative source of top-down gating of the MTL might be PFC control processes [73]. Other EEG analyses in the frequency domain suggest that successful episodic encoding is associated with a scalp-recorded increase in theta power (as defined on an individualized basis, generally in the 4.0–7.5 Hz range) [74,75]. Klimesch speculated that this theta activity is induced in neocortical regions via cortico- hippocampal feedback loops [76]. Scalp-recorded theta synchronization also has been used to implicate cooperative activity across neocortical regions during encoding [77].\rThe existing data suggest that episodic encoding processes are dependent upon MTL networks in conjunction with other neocortical networks. Yet,\rhttp://tics.trends.com\r\nReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002 99\rBox 1. How the brain creates false memories\r(a)\r(b)\rERPs to objects (c)\rRemembered Forgotten\r400−800ms\r–0.2 3.6 + μV\r4 μV\rERPs to words\rMisremembered\rNot misremembered\r–0.1 1.8\rTRENDS in Cognitive Sciences\r600−900ms\r0 300 600 900 1200 ms\rMemories are not always accurate. Encoding processes can sometimes support the subsequent recollection of a false memory. One type of false memory occurs when someone recalls an episode that was imagined and mistakenly believes that it had actually occurred. Gonsalves and Paller set up a laboratory experiment in which both accurate memories and false memories of this sort were produced [a]. Furthermore, different brain potentials at encoding were associated with accurate versus false memories. The accurate memories were for photos of objects viewed on a video monitor. ERPs to objects presented during the study phase differed as a function of whether the object was later remembered or forgotten on a subsequent recognition test, in which the names of those objects were spoken (Fig. Ia).\rVisual words were also presented during the study phase, and people were instructed to mentally generate a visual image in response to each word. ERPs to those words differed according to whether or not people subsequently claimed to have seen the corresponding object as a photo on the video monitor. (This contrasts with results from an experiment on another sort of false memory, memory conjunction errors,\rin which ERPs at encoding were not predictive of whether errors would occur at test [b].) As shown in Fig. Ib, ERPs from 600–900 ms were more positive for later false memories than when people later responded correctly that they had not seen that object. Similar ERPs recorded in a prior experiment had been associated with generating visual images in response to words [c].\rThe results from these two experiments taken together [a,c] support the following explanation for this type of false memory. The more vivid, detailed, or robust the visual imagery generated in response to a single word, the more likely\rFig. I. Brain potentials associated with the formation of true and false memories. (a) ERPs to objects were averaged according to whether those objects were subsequently remembered. (b) ERPs to words were averaged according to whether people later mistakenly thought they had seen the corresponding objects. Recordings were from the midline occipital scalp location. (c) Topographic maps were interpolated based on differences between pairs of waveforms at each electrode location (shown by small circles). (Adapted from Ref. a.)\rthe memory for that imagery will be mistakenly attributed to a memory resulting from actually viewing the corresponding object. Strong visual imagery at encoding promotes the retrieval of perceptual detail at test, which is generally a diagnostic sign of an episodic memory [d]. In some cases, however, such perceptual detail can be misleading and lead to a false memory, as when people claim to remember an event that was imagined but that never actually happened. The topographic maps (Fig. Ic) show that ERP differences associated with subsequent accurate memories were widespread across the scalp, whereas ERP differences associated with subsequent false memories were restricted to posterior scalp locations.\rA recent fMRI study using the same paradigm showed that activation in a large set of brain regions, including prefrontal cortex, fusiform gyrus, and hippocampus/parahippocampal gyrus, predicted later accurate memories for objects, whereas activation in the precuneus, anterior cingulate and inferior\roccipital gyrus predicted later false memories [e]. An intriguing possibility is that the misleading visual imagery responsible for false memories in this paradigm was produced by neural networks in the precuneus, a region hypothesized to play a role in representing internal visual images [f].\rReferences\ra Gonsalves,B.andPaller,K.A.(2000)Neural events that underlie remembering something that never happened. Nat. Neurosci.\r3, 1316–1321\rb Rubin,S.R.etal.(1999)Memoryconjunction errors in younger and older adults: event- related potential and neuropsychological data. Cogn. Neuropsychol. 16, 459–488\rc Gonsalves, B. and Paller, K.A. (2000)\rBrain potentials associated with recollective processing of spoken words. Mem. Cogn.\r28, 321–330\rd Johnson,M.K.(1992)MEM:Mechanismsof recollection. J. Cogn. Neurosci. 4, 268–280\re Gonsalves, B. et al. (2001) Event-related fMRI reveals brain activity at encoding that predicts true and false memory for visual objects.\rSoc. Neurosci. Abstr. 27, 239.6\rf Fletcher,P.C.etal.(1995)Themind’seye: precuneus activation in memory-related visual imagery. NeuroImage 2, 195–200\rconsiderable controversy remains regarding the distinctive contributions that specific MTL subregions make. Moreover, the posited importance of PFC–MTL interactions remains to be empirically tested. Resolution of these fundamental questions should prove central to unraveling the mysteries of MTL contributions to memory formation.\rReverse engineering cognitive architecture\rTo what extent can episodic encoding be influenced by working memory maintenance processes? A prevalent view is that ‘ “mindless” rote rehearsal per se is insufficient to create durable memories’ ([78] p. 23). This view is based on the lack of a correlation between rehearsal duration and later recall [79,80], and the robust effects of levels-of-processing on episodic\rhttp://tics.trends.com\r\n100\rReview TRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rBox 2. Observing mind and brain to track episodic encoding\r(1) Brain imaging during direct manipulations of encoding\r• Acquisitionversusretrieval\r• Levelsofprocessing(e.g.deepvsshallow)\r• Intentiontoremember • Novelty\r(2) Post-hoc item-by-item, state, and between- subject analyses\r• Comparisonsbasedonsubsequentremembering\ror forgetting of items (Dm effects)\r• Subsequent memory analyses emphasizing\rspecific aspects of episodic memories (e.g. source,\rfamiliarity)\r• Analyses of state changes that influence encoding\r(e.g. slow and sustained changes in attention or arousal)\r• Between-subjectcorrelationsbetweenmemory performance and the magnitude of neural responses\r(3) Lesion and disruption approaches\r• Analyses of encoding in patients with memory disorders\r• Temporary neural disruption (e.g. electrical or magnetic stimulation)\r(4) Integrated methods\r• Juxtaposing subsequent memory analyses with encoding manipulations\r• Testing the causal role of subsequent memory effects via neural disruption\r• Integrating electromagnetic and hemodynamic signals\rmemory. However, behavioral evidence that rote rehearsal can facilitate subsequent recognition [81–83] has been substantiated by fMRI evidence that activation levels during rote rehearsal, in neural regions associated with phonological working memory, correlate with later recognition [17]. Given the minimal impact of rote rehearsal on free recall,\rit may be that phonological maintenance can support subsequent item recognition but not recollection.\rTheorists have also examined the impact of retrieval processes on encoding. Retrieval is a particularly powerful incidental learning experience [84,85]. Encoded experiences that are later retrieved are subsequently better remembered compared to experiences not receiving retrieval practice [86]. Moreover, the magnitude of fMRI activation during recognition judgments on foils (unstudied words) was predictive of whether the foils were later recognized or forgotten [15]. The relation between engagement of retrieval circuits when accessing memories of studied items and later memory for those items remains to be determined.\rEncoding/retrieval correlations point to a related question: How can certain processing produce forgetting? Retrieval of some memories results in the suppression [87] or blocking [88] of other, non- retrieved traces. Neuroimaging predictors of retrieval-induced forgetting could prove invaluable for understanding suppression and blocking. More generally, neural computations correlated with forgetting are beginning to garner attention: across four fMRI studies, forgetting was correlated with greater activation in dorsolateral and medial PFC, posterior cingulate, and parietal structures [8,89,90]. These ‘subsequent forgetting effects’ could reflect either (a) a diversion of neurocognitive resources away from processes that yield effective encoding, or (b) encoding processes that yield undifferentiated traces, leading to more interference.\rFinally, it is worth noting that subsequent memory analyses can shed light on diverse cognitive and social-cognitive phenomena. For example, studies have explored the relations between encoding and later false remembering (Box 1) and between perceptual expertise and the phenomenon that people remember faces of their own race better than those of other races [91].\rConclusion\rThe ability to remember an episode is a function of multiple processes, some of which are engaged at encoding, some at retrieval, some in-between, and some emerging as an outcome of how encoding and retrieval processes interrelate. Intermediate processes constituting additional encoding or consolidation may be particularly critical for the stability of episodic memories over time [92–94]. One limitation of current implementations of the subsequent memory paradigm is that such intermediate processes are seldom indexed. Here, we focused on the initial processing of event information to illustrate the wealth of evidence available, and potentially obtainable, based on analyses of neural correlates of encoding.\rAlthough subsequent memory analyses by themselves do not conclusively demonstrate causality with respect to encoding, transcranial magnetic stimulation when applied to fMRI-identified structures could serve as a critical test of the necessity of specific neural computations for learning. Whereas subsequent memory analyses can be central to investigations of episodic encoding, this approach is most informative when combined with a full range of complementary approaches (see Box 2). Identifying the relevant neural events in this way does not merely reveal where encoding happens, but rather serves to clarify how multiple processes optimally coalesce such that we can successfully remember the past.\rhttp://tics.trends.com\r\nReview\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002 101\rQuestions for future research\r• Aretheneuralcomputationscorrelatedwith subsequent memory necessary for effective memory formation?Cantheybeselectivelydisruptedvia transcranial magnetic stimulation such that forgetting results?\r• Howdoglobalchangesincognitivesetor attentional state, which remain undetected by typical subsequent memory analyses conducted at theitem-level,impactencoding?Howdothe effects of global state interact with event-related encoding processes?\r• Howdoprefrontalandposteriorneocortical networks represent the diverse aspects of experience that form the building blocks of episodicmemories?Howdointeractionsbetween these brain networks and medial temporal networks support memory storage that can last a lifetime?\r• Whataretheexperientialprerequisitesforepisodic encoding? Can unconscious perception give rise to\r•\r•\r•\repisodicmemoryforaneventthatwasnot consciously experienced? Howdoprefrontalcontrolprocesseshelpto initiate the cascade of neural events that modulate encoding efficacy? Does this cascade begin with top-downprefrontalmodulations,bottom-up posterior neocortical processes, or interactions of the two? Howdotheinteractionsthatbeginatencoding evolve over time, both across the course of an event as well as when consolidation and interveningretrievaleventscomeintoplay?\rHow do those intervening events regulate episodic forgetting? Howdoesencodingdifferforepisodesversusother types of information such as facts, skills, and implicitly conditioned associations? Do the neocortical encoding events that putatively support primingconstituteasubsetofthosesupporting episodic learning or are they distinct?\rAcknowledgements.\rThis article represents a collaborative effort based on equal contributions from the two authors.\rWe thank R. Poldrack for insightful comments on an earlier draft, and gratefully acknowledge research support from NIDCD (DC04466), NIMH (MH60941), NINDS (NS34639), Ellison Medical Foundation, McKnight Endowment Fund for Neuroscience, and P. Newton.\rReferences\r1 Tulving, E. (1983) Elements of Episodic Memory, Cambridge University Press\r2 Cave, C.B. and Squire, L.R. (1992) Intact verbal and nonverbal short-term memory following damage to the human hippocampus. Hippocampus 2, 151–163\r3 Squire, L.R. (1992) Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans. Psychol. Rev. 99, 195–231\r4 Cohen, N.J. and Eichenbaum, H.E. (1993) Memory, Amnesia, and the Hippocampal System, MIT Press\r5 Mayes, A.R. and Downes, J.J. (1997) Theories of Organic Amnesia, Psychology Press\r6 Paller, K.A. (1997) Consolidating dispersed neocortical memories: the missing link in amnesia. Memory 5, 73–88\r7 O’Reilly, R.C. and Rudy, J.W. (2001) Conjunctive representations in learning and memory: principles of cortical and hippocampal function. Psychol. Rev. 108, 311–345\r8 Wagner, A.D. et al. (1998) Building memories: remembering and forgetting of verbal experiences as predicted by brain activity. Science\r281, 1188–1191\r9 Sommer, W. et al. (1991) Human brain potential correlates of face encoding into memory. Electroencephalogr. Clin. Neurophysiol.\r79, 457–463\r10 Brewer, J.B. et al. (1998) Making memories: brain activity that predicts how well visual experience will be remembered. Science 281, 1185–1187\r11 Henson, R.N.A. et al. (1999) Recollection and familiarity in recognition memory: an event-related functional magnetic resonance imaging study. J. Neurosci. 19, 3962–3972\r12 Kirchhoff, B.A. et al. (2000) Prefrontal–temporal circuitry for novelty encoding and subsequent memory. J. Neurosci. 20, 6173–6180\r13 Otten, L.J. et al. (2001) Depth of processing effects on neural correlates of memory encoding: relationship between findings from across- and within-task comparisons. Brain 124, 399–412\r14 Baker, J.T. et al. (2001) Neural correlates of verbal memory encoding during semantic and structural processing tasks. NeuroReport\r12, 1251–1256\r15 Buckner, R.L. et al. (2001) Encoding processes during retrieval tasks. J. Cogn. Neurosci.\r13, 406–415\r16 Otten, L.J. and Rugg, M.D. (2001) Task- dependency of the neural correlates of episodic encoding as measured by fMRI. Cereb. Cortex 11, 1150–1160\r17 Davachi, L. et al. (2001) When keeping in mind supports later bringing to mind: neural markers of phonological rehearsal predict subsequent remembering. J. Cogn. Neurosci.\r13, 1059–1070\r18 Friedman, D. and Johnson, R., Jr (2000)\rEvent-related potential (ERP) studies of memory encoding and retrieval: a selective review. Microsc. Res. Tech. 51, 6–28\r19 Kelley, W.M. et al. (1998) Hemispheric specialization in human dorsal frontal cortex and medial temporal lobe for verbal and nonverbal memory encoding. Neuron\r20, 927–936\r20 Wagner, A.D. et al. (1998) Material-specific lateralization of prefrontal activation during episodic encoding and retrieval. NeuroReport 9, 3711–3717\r21 Wagner, A.D. et al. (2001) Recovering meaning: left prefrontal cortex guides controlled semantic retrieval. Neuron 31, 329–338\r22 Cahill, L. and McGaugh, J.L. (1998) Mechanisms of emotional arousal and lasting declarative memory. Trends Neurosci. 21, 294–299\r23 Hamann, S. (2001) Cognitive and neural mechanisms of emotional memory. Trends Cogn. Sci. 5, 394–400\r24 Cahill, L. et al. (1996) Amygdala activity at encoding correlated with long-term, free recall of emotional information. Proc. Natl. Acad. Sci.\rU. S. A. 93, 8016–8021\r25 Canli, T. et al. (1999) fMRI identifies a network of structures correlated with retention of positive and negative emotional memory. Psychobiology 27, 441–452\r26 Hamann, S.B. et al. (1999) Amygdala activity related to enhanced memory for pleasant and aversive stimuli. Nat. Neurosci. 2, 289–293\r27 Canli, T. et al. (2000) Event-related activation in the human amygdala associates with later memory for individual emotional experience.\rJ. Neurosci. 20, RC99\r28 Craik, F.I.M. and Lockhart, R.S. (1972) Levels of processing: a framework for memory research. J. Verbal Learn. Verbal Behav.\r11, 671–684\r29 Craik, F.I.M. and Tulving, E. (1975) Depth of processing and the retention of words in episodic memory. J. Exp. Psych. Gen.\r104, 268–294\r30 Wagner, A.D. et al. (1999) When encoding yields remembering: insights from event-related neuroimaging. Phil. Trans. R. Soc. London Ser. B. (Biology) 354, 1307–1324\r31 Paller, K.A. et al. (1987) Neural correlates of encoding in an incidental learning paradigm. Electroencephalog. Clin. Neurophysiol.\r67, 360–371\r32 Neville, H.J. et al. (1986) Event-related brain potentials during initial encoding and recognition memory of congruous and incongruous words.\rJ. Mem. Lang. 25, 75–92\r33 Friedman, D. (1990) ERPs during continuous recognition memory for words. Biol. Psychol. 30, 61–87\r34 Cycowicz, Y.M. and Friedman, D. (1999)\rThe effect of intention to learn novel, environmental sounds on the novelty P3 and old/new recognition memory. Biol. Psychol. 50, 35–60\rhttp://tics.trends.com\r\n102 Review\r35 Friedman, D. and Trott, C. (2000)\rAn event-related potential study of encoding in young and older adults. Neuropsychologia\r38, 542–557\r36 Otten, L.J. and Rugg, M.D. (2001) Electrophysiological correlates of memory encoding are task-dependent. Cogn. Brain Res. 12, 11–18\r37 Johnson, R.J. (1995) Event-related potential insights into the neurobiology of memory systems. In The Handbook of Neuropsychology (Vol. 10) (Boller, F. and Grafman, J., eds), pp. 135–164, Elsevier Science Publishers\r38 Price, C.J. et al. (1997) Segregating semantic from phonological processes during reading. J. Cogn. Neurosci. 9, 727–733\r39 Mandler, G. (1980) Recognizing: the judgment of previous occurrence. Psychol. Rev.\r87, 252–271\r40 Weyerts, H. et al. (1997) ERPs to encoding and recognition in two different inter-item association tasks. NeuroReport 8, 1583–1588\r41 Kounios, J. et al. (2001) Cognitive association formation in human memory revealed by spatiotemporal brain imaging. Neuron\r29, 297–306.\r42 Wagner, A.D. et al. (2001) Prefrontal contributions to executive control: fMRI evidence for functional distinctions within lateral prefrontal cortex. NeuroImage 14, 1337–1347\r43 Fabiani, M. et al. (1990) Effects of mnemonic strategy manipulation in a Von Restorff paradigm. Electroencephalographic and Clinical Neurophysiology 75, 22–35\r44 Mangels, J.A. et al. (2001) Attention and successful episodic encoding: an event-related potential study. Cogn. Brain Res. 11, 77–95\r45 Johnson, M.K. (1992) MEM: mechanisms of recollection. J. Cogn. Neurosci. 4, 268–280\r46 Jacoby, L.L. (1991) A process dissociation framework: separating automatic from intentional uses of memory. J. Mem. Lang. 30, 513–541\r47 Tulving, E. and Markowitsch, H.J. (1998) Episodic and declarative memory: role of the hippocampus. Hippocampus 8, 198–204\r48 Squire, L.R. and Zola, S.M. (1998) Episodic memory, semantic memory, and amnesia. Hippocampus 8, 205–211\r49 Mishkin, M. et al. (1998) Amnesia and the organization of the hippocampal system. Hippocampus 8, 212–216\r50 Vargha-Khadem, F. et al. (1997) Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380\r51 Brown, M.W. and Aggleton, J.P. (2001) Recognition memory: what are the roles of the perirhinal cortex and hippocampus? Nat. Rev. Neurosci. 2, 51–61\r52 Sutherland, R.J. and Rudy, J.W. (1989) Configural association theory: the role of the hippocampal formation in learning, memory, and amnesia. Psychobiology 17, 129–144\r53 Holdstock, J.S. et al. Under what conditions is recognition spared relative to recall following selective hippocampal damage in humans? Hippocampus (in press)\r54 Tulving, E. (1985) Memory and consciousness. Can. Psychol. 26, 1–12\r55 Gardiner, J.M. (1988) Functional aspects of recollective experience. Mem. Cogn.\r16, 309–313\rTRENDS in Cognitive Sciences Vol.6 No.2 February 2002\rhttp://tics.trends.com\r56 Smith, M.E. (1993) Neurophysiological manifestations of recollective experience during recognition memory judgments. J. Cogn. Neurosci. 5, 1–13\r57 Münte, T.F. et al. (1988) Effects of a cholinergic nootropic (WEB 1881 FU) on event-related potentials recorded in incidental and intentional memory tasks. Neuropsychobiology 19, 158–168\r58 Paller, K.A. et al. (1988) ERPs predictive of subsequent recall and recognition performance. Biological Psychology 26, 269–276.\r59 Paller, K.A. (1990) Recall and stem-completion priming have different electrophysiological correlates and are modified differentially by directed forgetting. J. Exp. Psychol. Learn. Mem. Cogn. 16, 1021–1032\r60 Paller, K.A. and Kutas, M. (1992) Brain potentials during memory retrieval provide neurophysiological support for the distinction between conscious recollection and priming.\rJ. Cogn. Neurosci. 4, 375–391\r61 Paller, K.A. et al. (1987) Brain responses to\rconcrete and abstract words reflect processes that correlate with later performance on a test of stem- completion priming. Electroencephalog. Clin. Neurophysiol. Suppl. 40, 360–365\r62 Wagner, A.D. et al. (2000) Interactions between forms of memory: when priming hinders new episodic learning. J. Cogn. Neurosci. 12, 52–60\r63 Lepage, M. et al. (1998) Hippocampal PET activations of memory encoding and retrieval: the HIPER model. Hippocampus 8, 313–322\r64 Schacter, D.L. and Wagner, A.D. (1999) Medial temporal lobe activations in fMRI and PET studies of episodic encoding and retrieval. Hippocampus 9, 7–24\r65 Paller, K.A. et al. (1990) Potentials recorded from the scalp and from the hippocampus in humans performing a visual recognition memory test.\rJ. Clin. Exp. Neuropsychol. 12, 401\r66 Fernández, G. et al. (1999) Real-time tracking of memory formation in the human rhinal cortex and hippocampus. Science 285, 1582–1585\r67 Fernández, G. et al. Human declarative memory formation: Segregating rhinal and hippocampal contributions. Hippocampus (in press)\r68 Cameron, K.A. et al. (2001) Human hippocampal neurons predict how well word pairs will be remembered. Neuron 30, 289–298\r69 Fernández, G. et al. (1998) Successful verbal encoding into episodic memory engages the posterior hippocampus: a parametrically analyzed functional magnetic resonance imaging study. J. Neurosci. 18, 1841–1847\r70 Fernández, G. et al. (1999) Level of sustained entorhinal activity at study correlates with subsequent cued-recall performance:\ra functional magnetic resonance imaging study with high acquisition rate. Hippocampus\r9, 35–44\r71 Strange, B.A. et al. Dissociable human perirhinal,\rhippocampal, and parahippocampal roles during\rverbal encoding. J. Neurosci. (in press)\r72 Fell, J. et al. (2001) Human memory formation is\raccompanied by rhinal-hippocampal coupling and\rdecoupling. Nat. Neurosci. 4, 1259–1264\r73 Wagner, A.D. (2001) Synchronicity: when you’re\rgone I’m lost without a trace. Nat. Neurosci.\r4, 1159–1160\r74 Klimesch, W. et al. (1996) Theta band power in the\rhuman scalp EEG and the encoding of new information. NeuroReport 7, 1235–1240\r75 Klimesch, W. et al. (1997) Theta synchronization and alpha desynchronization in a memory task. Psychophysiology 34, 169–176\r76 Klimesch, W. (1999) EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis. Brain Res. Rev. 29, 169–195\r77 Weiss, S. et al. (2000) Theta synchronization predicts efficient memory encoding of concrete and abstract nouns. NeuroReport 11, 2357–2361\r78 Bower, G.H. (2000) A brief history of memory research. In The Oxford Handbook of Memory (Tulving, E. and Craik, F.I.M., eds), pp. 3–32, Oxford University Press\r79 Atkinson, R.C. and Shiffrin, R.M. (1968) Human memory: a proposed system and its control processes. In The Psychology of Learning and Motivation (Vol. 2) (Spence, K. and Spence, J., eds.), pp. 89–195, Academic Press\r80 Craik, F.I.M. and Watkins, M.J. (1973) The role of rehearsal in short-term memory. J. Verbal Learn. Verbal Behav. 12, 599–607\r81 Woodward, A.E. et al. (1973) Recall and recognition as a function of primary rehearsal. J. Verbal Learn. Verbal Behav. 12, 608–617\r82 Greene, R.L. (1987) Effects of maintenance rehearsal on human memory. Psychol. Bull. 102, 403–413\r83 Naveh-Benjamin, M. and Jonides, J. (1984) Maintenance rehearsal: a two-component analysis. J. Exp. Psychol. Learn. Mem. Cogn. 10, 369–385\r84 Whitten, W.B. and Bjork, R.A. (1977) Learning from tests: Effects of spacing. J. Verbal Learn. Verbal Behav. 16, 465–478\r85 Roediger, H.L. and Payne, D.G. (1982) Hypermnesia: the role of repeated testing. J. Exp. Psychol. Learn. Mem. Cogn. 8, 66–72\r86 Anderson, M.C. et al. (1994) Remembering can cause forgetting: retrieval dynamics in long-term memory. J. Exp. Psychol. Learn. Mem. Cogn.\r20, 1063–1087\r87 Anderson, M.C. and Spellman, B.A. (1995) On the status of inhibitory mechanisms in cognition: memory retrieval as a model case. Psychol. Rev. 102, 68–100\r88 Mensink, G-J. and Raaijmakers, J.G.W. (1988) A model for interference and forgetting. Psychol. Rev. 95, 434–455\r89 Otten, L.J. and Rugg, M.D. (2001) When more means less: neural activity related to unsuccessful memory encoding. Curr. Biol. 11, 1528–1530\r90 Wagner, A.D. and Davachi, L. (2001) Cognitive neuroscience: forgetting of things past. Curr. Biol. 11, R964–967\r91 Golby, A.J. et al. (2001) Differential responses in the fusiform region to same-race and other-race faces. Nat. Neurosci. 4, 845–850\r92 Squire, L.R. et al. (1984) The medial temporal region and memory consolidation: a new hypothesis. In Memory Consolidation (Weingartner, H. and Parder, E., eds),\rpp. 185–210, Erlbaum\r93 Nadel, L. et al. (2000) Multiple trace theory of\rhuman memory: computational, neuroimaging, and neuropsychological results. Hippocampus 10, 352–368\r94 Paller, K.A. (2002) Cross-cortical consolidation as the core defect in amnesia: prospects for hypothesis-testing with neuropsychology and neuroimaging. In Neuropsychology of Memory, (Vol. 3) (Squire, L.R. and Schacter, D.L., eds),\rpp. 73–87, Guilford Press","title":"Observing the transformation of experience into memory\r","isStoredAs":"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo/#LocalFileDataObject","contentHash":"0e317291f240436f4ed73345ab884f77dec76fb3","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"erp","time":1461915877325,"auto":true,"weight":1.0},{"@type":"Tag","text":"memori","time":1461915877333,"auto":true,"weight":1.0},{"@type":"Tag","text":"mtl","time":1461915877366,"auto":true,"weight":1.0},{"@type":"Tag","text":"neurosci","time":1461915877390,"auto":true,"weight":1.0},{"@type":"Tag","text":"subsequ","time":1461915877349,"auto":true,"weight":1.0},{"@type":"Tag","text":"hippocampu","time":1461915877358,"auto":true,"weight":1.0},{"@type":"Tag","text":"dm","time":1461915877382,"auto":true,"weight":1.0},{"@type":"Tag","text":"encod","time":1461915877316,"auto":true,"weight":1.0},{"@type":"Tag","text":"cogn","time":1461915877340,"auto":true,"weight":1.0},{"@type":"Tag","text":"episod","time":1461915877374,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":12,"appId":"a75b7ce73e3bce0771b5d2c966b82083747407d1","timeCreated":1461915874887,"timeModified":1461915874954,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.37923294,"uri":"file:///Users/cheny13/Documents/References/JohnMedina_Recognition Memory_June08.pdf","plainTextContent":"MOLECULES OF THE MIND\rJUNE 2008\rThe Biology of Recognition Memory\rPSYCHIATRIC TIMES 13 www.psychiatrictimes.com\rby John J. Medina, PhD\rattend to the nov- el object, indicat- ing a memory of the previous ob- ject, the test is useful for assess- ing recognition.\rIn laboratory\ranimals, hippo-\rcampal lesion studies showed that the region was critically involved in the ability to detect both new and famil- iar stimuli—at least most of the time. The problem was that some studies showed no impairment in behavior, even with well-characterized lesions. This has been explained several ways, from variability in lesion size to indi- vidual behavioral differences among animals.\rWhen larger groups of patients with hippocampal damage were ex- amined using recognition tests, the individual assignments originally proposed for recognition memory (recollection and familiarity) were not observed. Findings from a study that involved a group of 6 patients indicate that the hippocampus was involved in both recollection and familiarity. Findings from another study that involved 56 patients show the same thing. Because they all had larger sample sizes, these data were in obvious contradiction to the previous case studies. What, then, were the researchers who advocated for this dichotomy actually studying?\rClose analysis of data derived from these tests eventually led some researchers to become skeptical of the hippocampal-perirhinal recognition assignments. They concluded that researchers who were attempting to isolate the recollection/familiarity substrate dichotomy were actually just measuring memory strength.\rThis conclusion was based on an- other test—described below—which is based on a notion called signal- detection theory. This theory com- bines old items called the target with new items called the foils in the context of memory strength. Memory strength is a probabilistic judgment on the part of the learner that an item did or did not appear on a list pre- viously given to the learner. It is mea- suring the degree of certainty the learner is exhibiting about an item. In general, items with greater memory strength (usually above some numer- ical threshold) are judged to be old,\r(Please see Recognition Memory, page 14)\rLast month I examined emotions and their surpris- ing impact on the formation of memories. Nor- epinephrine arriving at certain memory-related tissues near the moment of learning turned out to play a critical role. There has been great progress in at- tempting to explain the interaction between emotions and memory formation at the molecular level.\rsearchers use in attempting to under- stand recognition memory. As you shall see, the sum total may paint a very different, much more complex picture of interactions within the medial temporal lobe.\rTesting recognition\rThe first set of tests is called remem- ber-know assessments, which ap- peared to confirm the hypothesis of recollection events occurring in the hippocampus. Learners who undergo these tests are presented with a col- lection of objects that, ideally, their brains will record. They are then asked to review another collection of objects that contain previously en- countered objects (“old” objects) and objects not previously encountered in the initial exposure (“new” objects). As the learners are being exposed to this second collection of items they are asked 2 questions. Question A is “Do you remember this particular item?” Question B is “Do you simply remember that the item had been pre- sented before?” Question A (the re- member part) attempts to measure recall. Question B (the know part) at- tempts to measure familiarity.\rWhen remember-know tests were administered to learners with hippo- campal lesions, an extraordinary re- sult was obtained. Patients appeared to be deficient in their ability to rec- ollect objects but had less trouble (and in some cases, no trouble) with familiarity. Research just like this caused some investigators to con- clude that this was due to a division of labor within the brain.\rThe second set of tests is often called delayed nonmatching-to-sam- ple tests. These examinations involve the presentation of an object followed byadelaythatcanlastfrom1to2 seconds to several minutes. After that period the learner is simultaneously reexposed to the test object and an ob- ject he or she has never before ob- served. In the case of animal studies, the choice of the novel, previously un- exposed object is rewarded.\rThe third set of tests is called nov- el object recognition tests. In these protocols the learner is exposed to a pair of identical objects. This is followed by a delay that can range from a few seconds to a few hours. The learner is then exposed to anoth- er pair of objects consisting of a pre- viously seen object and a novel ob- ject. Because humans and animals\rThere was an issue I left out of that article, however, that is appropriate to address in this column. If you did not read my text closely, you might have been left with the impression that memory was a unitarily described, monolithic cognitive talent upon whose various underlying substrates everyone agrees. Nothing could be further from the truth. In fact, I was only describing a single category of memory in the text, a type usual- ly called declarative memory (ie, something you can declare, such as “Thomas Jefferson was the third president of the United States”)—the star player of medial temporal lobe– mediated learning.\rIt turns out that there are many types of memory, categories of which are actively debated at this very mo- ment, and many involving far-flung regions beyond the medial temporal lobe. Currently, we have very little idea how even the major categories relate to each other, let alone how their originating neurological sub- strates produce observed behaviors.\rIn this article, I will illustrate how fluid things can be in this line of work by investigating one subtype of de- clarative memory, a category usually called recognition memory. I will start with some basic definitions of recognition memory, review a few older ways of describing such mem- ory formation at the tissue level, and then show how newer data cast some suspicion on these older descriptions. Patient recognition and recall is ex- traordinarily important in the field of psychiatry and, given its centrality, you may be surprised at how little is known about it.\rSome basics\rAs mentioned, a subcategory of de- clarative memory is called recogni- tion memory. Classically, recognition memory has been defined as the abil- ity to assess accurately that a stimulus has been encountered before. There can be direct recall, which is the abil- ity to remember a stimulus in the ab-\rsence of that stimulus. There can al- so be discrimination components in which the learner may be able to dis- tinguish between a stimulus that had been previously presented and a new stimulus, without any further knowl- edge of either one.\rMany cognitive neuroscientists believe that these characteristics split recognition memory into 2 interlock- ing components that have been as- signed to unique neurological sub- strates. One is formally termed recollection. This involves remem- bering discrete details about an expe- rience to which the learner has been previously exposed. Several years ago a group of researchers proposed that these behaviors were centered in the hippocampus. Their ideas were largely based on neuroimaging case studies involving individual patients with amnesia. Such assignments be- came the investigational womb from which a great deal of research—and subsequent data—emerged.\rThe second component, formally termed familiarity, is a stripped-down version of the first. This involves the conscious awareness that some object has been encountered before, but without an ability to recall anything further about it. The group of re- searchers mentioned previously pro- posed that these behaviors originated in the perirhinal cortex, a region next to the hippocampus. Both regions are part of the medial temporal lobe memory system, whose full comple- ment includes the parahippocampal gyrus; carrying pararhinal, entorhi- nal, and perirhinal cortices, as well as the hippocampus, subiculum, and dentate gyrus. All are involved in var- ious aspects of declarative memory formation.\rLife in the brain, however, is hard- ly this clear-cut or this simple, and the ideas that recollection stems from the hippocampus and that familiarity stems from the perirhinal cortex have been challenged. To understand the details of this turbulence, I will need to turn to some of the tests that re-\r\n14 PSYCHIATRIC TIMES www.psychiatrictimes.com\rRecognition Memory\rContinued from page 13\ror targets. Any items with weaker memory strength (usually below that threshold) are judged to be new, or foils.\rCurves that are generated from these tasks are called receiver oper- ating characteristic (ROC) curves. In general, an ROC curve contrasts the rate at which the learner really does identify old items correctly with the rate at which the learner is inaccurate. By inaccurate I mean “false alarm” rates, which are the rates at which foils are incorrectly tagged as targets.\rMOLECULES OF THE MIND\rJUNE 2008\rThese analyses tend to fit better with the real-world situations of the brain, according to some researchers. In this model, recollection and familiarity are both continuous processes that are capable of combining to allow the researcher to examine the memory strength of an item on a test list. This way of looking at retrieval makes some sense. Many factors can influ- ence memory strength, including prior experience and the events that occur at the time of encoding. It would be hard to make an argument that these factors could not influence recollection and familiarity.\rSome researchers studying data\rfrom ROC curves smelled a rat and actively faulted the remember-know findings by challenging the assump- tion that remember and know mean recollection and familiarity in any ob- jective sense and that such distinc- tions actually reflect a division of la- bor in the brain.\rIn their view, the data seem to point to a much less complex inter- pretation—that what is being mea- sured is simple memory strength; scientists who are trying to study rec- ollection versus familiarity are actu- ally studying the differences between strong memories and weak memo- ries. They believe that the dichotomy,\rif taken at face value, leads to a near contradiction (Figure).\rEven the brain lesion experiments that purport to show a division of labor have been questioned. Most stroke patients do not have neatly bor- dered areas of damage that affect on- ly one well-defined neuroanatomical structure and not another. Indeed, it is often not possible in real-world re- search laboratories to show the exact- to-the-cell extent of the damage even when there are clear behavioral deficits—and remember, the hip- pocampus and perirhinal cortex lie adjacent to each other in the brain. Maybe there is no such thing as rec- ollection and familiarity. Maybe there are only strong memories and weak memories.\rObviously, the neuroanatomical issues could be resolved with direct recording of neuronal activity, nonin- vasive imaging experiments, or both. The most robust results would be obtained using such technologies in a direct comparison of recollection with recognition assays in patients who have damaged hippocampi. Functional assignments could be achieved even in patients in whom the lesions are “messy” or ill-defined.\rWhen such rigor was brought to bear on the question of the roles of the hippocampus and perirhinal cortex in recognition memory, a clearer, more nuanced picture emerged. While it is beyond the scope of this article to ful- ly explain all these data, 2 very spe- cific findings emerged.\rFirst, single-unit studies of recol- lection showed that both the hippo- campus and perirhinal cortex were in- volved in recollection and familiarity. Single-unit recordings are technolo- gies that can measure the activity of individual neural cells in conscious animals. It can provide both temporal and spatial information about the functions of a remarkably small num- ber of neurons. The use of this tech- nology directly refuted the notion that recollection and familiarity oc- curred in separable neuroanatomical substrates.\rSecond, more finely tuned resolu- tion showed that while the distinction in recognition memory was blurred, the 2 regions were hardly identical functionally. Neurons that lie in the perirhinal cortex are more reactive to novel stimuli, at least for a short while, but quickly revert to baseline levels upon subsequent reexposure. Neurons in the hippocampus were found to be more reactive to previ- ously exposed stimuli and showed ac- tive depression or excitation above baseline for a period.\rAnother difference between the\rFigure\rReceiver operating characteristic (ROC) graphs\rA\rB\rC\rShown below are typical graphs that illustrate ROCs in memory recognition tasks. The graphs show an analysis of signal-detection ideas such as in the context of remember-know experiments (see text). Some researchers believe that the simplest interpretation of data like these involves the formation of strong vs weak memories as opposed to separable “recollection” vs “familiarity” memory subsystems in the medial temporal lobe.\rKR\rMemory strength\rKR\rMemory strength\rKR\rMemory strength\rInterpreting this graph\r“Remember” events (R) are said to occur when scores exceed a high memory-strength threshold. “Know” assessments (K) are said to occur when scores fall below the R value but above the K value. Scores that fall below the K value are judged to be new events, never previously encountered.\rStrong memory condition\rShown here are typical profiles obtained when memory conditions are said to be “strong.” The red curve represents the “target,” which involves objects the learner has encountered before. The green curve represents the “foil,” which involves objects the learner has not encountered before. The “remember” hit rate area (which could be called the “recollection score”) is the proportion of the targets that exceed the R threshold. The “know” hit rate area (which could be called the “familiarity score”) is the proportion of the targets between the K and R.\r“Know” hit rate area\r“Remember” hit rate area\rWeak memory condition\rShown here are typical profiles obtained when memory conditions are said to be “weak.” Note that both K and R criteria are left-shifted. The consequence here is that the “remember” hit rate goes down and the “remember” false-alarm rate goes up. Recollection and familiarity scores are often assessed from graphs such as these, which can lead to an odd conclusion: recollection is greatly reduced under weak memory conditions, the same conditions that leave familiarity relatively unaffected. Some researchers suggest that a simpler explanation is that memory is generally weaker under these conditions.\r“Know” hit rate area\r“Remember” hit rate area\r(Please see Recognition Memory, page 16)\rNumber of presented objects Number of presented objects Number of presented objects\r\n16 PSYCHIATRIC TIMES www.psychiatrictimes.com\rRecognition Memory\rContinued from page 14\r2 regions emerged as studies pro- gressed. Perirhinal neurons turned out to be quite finicky about the type of stimulus to which they would re- spond, requiring very specific inputs before reacting. Hippocampal cells turned out to be much less particular. They were more likely to alter their firing rates if a stimulus had been previously encountered, regardless of the type of input supplied.\rConclusion\rThese conclusions seem to point to a different, less sharply defined role for the hippocampus and perirhinal cor- tex in recognition memory. The dis- tinctions between recollection and fa- miliarity may actually have more to do with historical attempts to orga- nize memory behavior than with de- scribing neuroanatomical reality. The brain’s recognition features seemed more distributed among a variety of medial temporal lobe structures act- ing in complementary, cooperative ways and are not easily pigeonholed into 1 or 2 categories.\rAnd that is the whole point of this article. The understanding of many of the human behaviors important to psychiatrists—even ones as simple as “remember that?”—is currently in a near-constant state of flux in the research laboratory. As the physical tools of neuroscience become ever more sophisticated, older ways of thinking about cognition give rise to new ones. Generally, the view gets more complex. Occasionally, the ground shifts from underneath one’s feet and, in this example, may actual- ly be disappearing altogether!\rDr Medina is a developmental molecular biol- ogist and private consultant, with research interests in the genetics of psychiatric disorders.  \rMOLECULES OF THE MIND\rJUNE 2008\r(Please see XXXXXXXXX, page <None>)","eventTime":1450258132000,"mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"recollect","time":1461915874905,"auto":true,"weight":1.0},{"@type":"Tag","text":"memori","time":1461915874898,"auto":true,"weight":1.0},{"@type":"Tag","text":"strength","time":1461915874948,"auto":true,"weight":1.0},{"@type":"Tag","text":"recognit","time":1461915874912,"auto":true,"weight":1.0},{"@type":"Tag","text":"roc","time":1461915874930,"auto":true,"weight":1.0},{"@type":"Tag","text":"learner","time":1461915874937,"auto":true,"weight":1.0},{"@type":"Tag","text":"perirhin","time":1461915874887,"auto":true,"weight":1.0},{"@type":"Tag","text":"hippocampu","time":1461915874918,"auto":true,"weight":1.0},{"@type":"Tag","text":"neuroanatom","time":1461915874954,"auto":true,"weight":1.0},{"@type":"Tag","text":"familiar","time":1461915874924,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":11,"appId":"4a5aaf093602e6a34dbed7391b485bc6db4d67ab","timeCreated":1455812339924,"timeModified":1461915876673,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.33657694,"uri":"file:///Users/cheny13/Documents/References/Gillund-Shiffrin-1984.pdf","plainTextContent":"Psychological Review VOLUME 91 NUMBER 1 JANUARY 1984\rA Retrieval Model for Both Recognition and Recall\rGary Gillund and Richard M. Shiffrin Indiana University\rThe Search of Associative Memory (SAM) model for recall (Raaijmakers & Shiffrin, 1981b) is extended by assuming that a familiarity process is used for recognition. The recall model posits cue-dependent probabilistic sampling and recovery from an associative network. Our recognition model is closely related to the recall model because the total episodic activation due to the context and item cues is used in recall as a basis for sampling and in recognition to make a decision. The model, formalized in a computer simulation program, correctly predicts a number of findings in the literature as well as the results from a new experiment on the word-\rfrequency effect.\r\"A critical problem of long standing in psy- chological study of memory is concerned with the relation between recall and recognition. In what sense are they the same, and in what sense are they different?\" (Tulving & Watkins,\r1973, p. 739).\rThis article is a preliminary attempt to for-\rmulate a theory that describes in detail the relationship between recall and recognition. The relationship is realized in a computer simulation model, and the model's predictions are checked against existing data as well as data generated in our laboratory. The model for recognition is related mathematically and logically to the Search of Associative Memory (SAM) theory of memory retrieval that has\rThis work was supported by Public Health Service Grant MH12717 and the Indiana University Waterman Research Award to Richard Shiftrin.\rWe would like to thank Lloyd Peterson, Margaret Intons- Peterson, Linda Smith, Maynard Thompson, Jim Neely, Michael Humphreys, Jeroen Raaijmakers, Roger Ratcliff, and Steven Smith for their thoughtful comments on earlier drafts.\rShort annotated versions of the SAM simulation pro- grams for recall and recognition, in PASCAL or FORTRAN, are available from the authors on request.\rRequests for reprints should be sent to Richard M. Shiffrin, Psychology Department, Indiana University, Bloomington, Indiana 47405. Gary Gillund is now at the Center for Research in Human Learning, 204 Elliott Hall, University of Minnesota, Minneapolis, Minnesota 55455.\rbeen used previously to fit recall data (Gillund & Shiffrin, 1981; Raaijmakers & Shiffrin,\r1980, 1981a, 1981b).\rWe begin with a brief overview of theoretical\rtreatments in the area of recognition (addi- tional models are discussed in Section 5). Sev- eral studies carried out to test one class of models are then reported and evaluated. The introduction ends with a listing of some of the major findings in recall and recognition that the model is intended to explicate.\rA rough classification of models for recall and recognition is given in Table 1. The com- ponent processes assumed to operate in recall and recognition are indicated by the symbols S, Ds, Dc, and /. S indicates an extended search, with the basic characteristic that it op- erates slowly and uncertainly. The speed and certainty of the search are defined empirically by results from tasks like free recall in which several seconds often elapse between successive recalls and in which many more than half the presented items may not be recalled even after many minutes of testing. Models with this property often assume serial sampling of im- ages from memory, with or without resam- pling. D indicates direct access in which all the relevant information in memory is ob- tained in one step. Ds is a simplified form of direct access in which only the image of the tested item is contacted, so that searchlike ef-\rCopyright 1984 by the American Psychological Association, Inc.\r\nTable 1\rModels for Recall and Recognition\r& Koopmans, 1969), test and lag effects (Mur- dock, 1974; Murdock & Anderson, 1975), repetition effects (Atkinson & Juola, 1973, 1974), and the list-length effect (e.g., Strong,\r1912).\rPerhaps the simplest alternative to Model\r1 is a model proposing common mechanisms for recall and recognition. One such approach is seen in the work of Tulving (e.g., 1976). He proposed that recognition and recall both in- volve cue-dependent retrieval processes (Tulv- ing, 1974), in which performance is dependent on the degree to which the cues available at the time of test (including the test itself in recognition) match the cues during presen- tation (Tulving, 1968,1976; Tulving & Thom- son, 1973).\rA particular type of cue-dependency theory holds that recognition can be explained by the same search mechanisms typically used to model recall (e.g., the SAM model of Raaij- makers & Shiffrin, 198Ib). This is indicated by Model 2 in Table 1. However, these models have a serious problem: Subjects can quickly, accurately, and with high confidence classify at least some distractors correctly (Atkinson & Juola, 1974; Fischler & Juola, 1971; Herrmann, Frisina, &Conti, 1978; Murdock,\r1974; Murdock & Anderson, 1975;see also Glucksberg & McCloskey, 1981). Even a lengthy search may miss targets present in memory; therefore only a weak inference can be made that a nonlocated item is not in memory. Thus search models may possibly be able to explain accurate negative recognition responses to distractors, but only if a long and difficult search fails; conversely, fast negative recognition responses can be generated by search models but only at a considerable cost in accuracy.\rIt is of course possible to posit search models for recall, but assume that recognition operates differently, with direct and certain access to the image of the tested item. Model 1 had this property but had problems because the simple recognition component could not produce searchlike effects. One way out of this difficulty involves retaining a simple direct-access com- ponent for recognition but adding a search component as well, as indicated by Model 3 in Table 1. In two-phase models like Model\rModel\rRecall\rS-\\D5 SI-(J)\rSH-(/) S f (/) s -i- (/)\rRecognition\rA\r,V + ( / )\rS + Ds\r»c\rS + D,\rNote. S — extended search; Ds — simple direct access process; /)c = complex direct access process; / -- implicit recognitionlike process.\rfects, such as list length and fan effects, cannot be predicted. Dc indicates a complex direct access process in which many memory images are contacted together, leaving open the pos- sibility of predicting searchlike effects. / is a recognitionlike process that allows the subject to decide that a name recovered from asam- pled image is appropriate to recall. / is placed in parentheses because it is often an implicit, rather than an explicit, component of a recall model.\rModel 1 may be termed an embedded two- process model because the recognition mech- anism is embedded in the recall mechanism. Examples of such models include Kintsch (1970, 1974) and J. R. Anderson and Bower (1972, 1974). In typical models of this type, recall is a function of two sequential stages. The first is a generate stage, whereby long- term store (LTS) is searched and possible word candidates are output to the recognize stage. In the recognize stage, a word's familiarity (Kintsch, 1970) or contextual associations (J. R. Anderson & Bower, 1972) are evaluated to determine if the generated word had been presented on the study list. During a recog- nition test, only the second stage is employed, and it is applied to the test word only. Direct access is presumably gained to the appropriate stored memory representation (possibly an episodic image), and the decision process then proceeds just as it does for each word generated in the recall process.\rGARY OH,LUND AND\rRICHARD M. SHIFFRIN\rEmbedded two-process models have diffi-\rculty explaining why recognition exhibits\rmany of the same properties as recall, prop-\rerties that seem to imply that recognition in-\rvolves a memory search. Examples include\rwork on organizational effects (Mandler, 1972; 3, recognition can occur either on the basis of Mandler & Boeck, 1974; Mandler, Pearlstone, a simplefa miliarity process (i.e., £>s) or on the\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL\rbasis of a search (a slower process, on the av- erage, than familiarity). These phases can oc- cur sequentially or in parallel.\rTwo phase models have been proposed by Juola, Fischler, Wood, and Atkinson (1971; see also Atkinson & Juola, 1973, 1974; At- kinson, Herrmann, & Wescourt, 1974; Atkin- son & Wescourt, 1975) and Mandler (1972,\r1980; see also Mandler & Boeck, 1974; Man- dler et al., 1969). The Atkinson and Juola (1973, 1974) models are illustrated in Figure 1. A direct-access familiarity process gives rise to a familiarity value that is used for judgment: If the value is quite high (above a yes criterion) the subject responds with a positive, rapid, recognition response. If the value is quite low (below a no criterion) the subject responds with a negative, rapid, recognition response. If the value is intermediate (between the criteria),\rthen a search is carried out. Mandler (1972, 1980) has proposed a model roughly similar in general outline. Both the Mandler (1980) and the Atkinson and Juola (1973, 1974) ap- proaches posit rapid familiarity decisions and slower search processes, on the average. It is this general approach to recognition that we set out to evaluate in a preliminary series of\rstudies.\rIf it is assumed that the familiarity process,\rat least on the average, is faster than the search process (and almost entirely responsible for rapid, accurate, high confidence, negative re- sponses), then it should be possible to separate the familiarity and search components exper- imentally. We reasoned that if subjects are forced to respond rapidly (producing, say, av- erage response times of 500 ms) the responses would be based primarily on familiarity. How-\r( START J\rUSE TEST ITEM AND CONTEXT AS CUES\rLET F=FAMILIARITY VALUE\rr\\\rFigure 1. Retrieval from long-term store for recognition: a flow chart of a two-process recognition model. (The recognition response is based on either a familiarity judgment or a search process.)\r\n.a\r<: -8 OC\r5 .7 EC\r''^^\rA r^S L O W\rLLJ\r6 -\r•? .4 - UJ\r<\rGARY GILLUND AND RICHARD M. SHIFFRIN\r>/\r^'•G - p-'\" FAST\rSHORT LIST\rLONG LIST\rI I I .a .8 .7 / SLOW .6 W.5d' -y.5\r< '3-\rb -2- -\r/ DC d\r-\r.2 1IIIiii1\rNUMBER OF PRESENTATIONS\rfigure 2. Recognition performance as a function of list length (36 or 96), number of presentations, and slow or fast recognition tests.\rever, if subjects are encouraged to take their time and are not even allowed to respond be- fore I (or 1.5) s, then a larger proportion of responses should be based on search processes.\rThe experiments involved visual, sequential, presentations of words in lists. Following each list a yes-no recognition test was given: The list words were mixed with an equal number of distractor words and were tested one at a time. In fast tests, the items were presented every second, and subjects were required to respond to each item in under 900 ms. In slow tests, responses were not allowed before I (or\r1.5) s had elapsed, and 3 (or 4) s were given between tests. In the fast test, subjects' latencies averaged about 500 ms, and in the slow test, the latencies averaged about 2.5 to 3 s (which compares with average latencies in uncon- strained settings of about 800 to 1500 ms). Different lists were tested with slow and fast test techniques, with subjects practiced on the respective techniques and warned in advance of each test (but not each study) concerning the upcoming test speed. The results of three experiments that employed this procedure can be seen in Figures 2, 3, and 4.\rThe first experiment varied list length and number of presentations. The second exper- iment varied depth of encoding (pleasantness vs. pronounceability ratings) and number of presentations. In the third experiment, the type\rof distractor was manipulated.' In all situa- tions, the variables had effects that were ex- pected based on prior findings in the literature: Subjects were better on repeated items than on items presented only once (e.g., Atkinson & Juola, 1973; Ratcliff & Murdock, 1976). Performance was better on words rated for pleasantness than on words rated for pro- nounceability, although this factor did not quite reach statistical significance,F(l, 71) = 3.21, p = .078 (e.g., Jacoby & Dallas, 1981). Accuracy was higher for words from the short list than for words from the long list (e.g., At- kinson & Juola, 1973; Ratcliff & Murdock,\r1976).\rIn the third experiment, the distractors on\reach list were of four types. Control distractors were not related in any systematic fashion to the list items. Other distractors were synonyms of particular list items, were similar graphe- mically to particular list items, or were similar both graphemically and phonemically to par- ticular list items. (Also, the fast vs. slow test variable was a between-groups measure in this experiment.) Subjects made more errors (false alarms) on distractors that were related gra-\r1\rThe experiment on different types of distractors was completed with Tim Feustel. We gratefully acknowledge his contribution in all stages of the experiment.\r/ /\rO -—-0 .4 FAST\r.3\r\ncc .9\r3\rLU\rLU\rDC\rX\rSLOW\rx^^\r0\"\"~~~FAST\rSLOW .9 .8\rtS^\r--o .7\rO--\"~\" FAST\r-- .6\r.8\r.7 .6\r•5 -\r-\r.5\rRETRIEV AL MODEL FOR RECOGNITION AND RECALL\rPLEASANTNESS PRONOUNCEABILITY I.U\\1 iiI.U\rAiiiiA 13 13\rNUMBER OF PRESENTATIONS\rFigure 3. Recognition as a function of orienting task (pleasantness or pronounceability ratings), number of presentations, and slow or fast recognition tests.\rphemically and phonemically to target items than to control items or synonyms. Such re- sults have been found before (e.g., Cramer &\rModel 5. It is our plan in this article to explore a version of Model 4 because we do not wish to propose a more complex model unless the simpler version can be shown to fail. Fur- thermore, even if Model 4 can be shown to fail, its study could prove highly instructive.\rNote that the direct-access component (Dc or D$ in Table 1) is often denoted familiarity.\r2\rEagle, 1972; see also Eagle & Ortof, 1967; 2\rNelson & Davis, 1972).\rIn all three studies, performance was higher\rfor slow tests than for fast tests—a result that is in accord with much work on speed-ac- curacy trade-offs—and research using the sig- nal-to-respond technique (e.g., Dosher, 1981; Pachella, 1974; Reed, 1973; Wickelgren & Corbett, 1977).\rMost important for present purposes, there were no significant interactions between the type of test (fast vs. slow) and any of the other\r3\rvariables in any of the three experiments. Thus these variables responded similarly in the fast and in the slow test conditions. If search had occurred more often in the slow test con- ditions, and if search and familiarity respond\raction would have been expected. At the least, doubt is cast on Model 3 of Table 1. The find- ings therefore suggest either that search is not being utilized in these studies (Model 4 of Ta- ble 1) or that both search and familiarity pro- cesses are being utilized but that they respond\rto these types of variables in similar ways 5\r(Model 5 of Table I). Note that Model 4 is\rcompletely embedded in Model 5 and there- fore cannot predict the data better than does\rIt may appear surprising that no more false alarms\rdifferently to these variables, then an inter- 4\rwere made to synonyms than to control words in view of\rthe large number of studies that have found higher false\ralarms for synonyms than for control words (for examples,\rsee Anisfeld & Knapp, 1968; Grossman & Eagle, 1970).\rHowever, we have replicated this finding several times in\rour laboratory (see also Cramer & Eagle, 1972). One pos-\rsible explanationis that the types of errors reflect the bias\rin encoding, and our subjects were emphasizinglow-level\rfeatures (acoustic or physical) rather than semantic features\rof the words. Some support for this notion can be found\rin the literature (e.g., Cermak, Schnorr, Buschke, & At-\rkinson, 1970; Davies & Cubbage, 1976; Elias & Perfetti,\r1973). Other explanations are discussed later in this article.\r3\rAlthough the larger advantage for physicalsimilarity under fast testing in Figure 4 did not reach significance, the result, if reliable, may suggest that fast tests are more sensitive to low-level similarityfeatures.\r4 The use of different methods of measuring performance\rin our studies, such as utilization of d' measures, did not\rchange our findings or our conclusions.\r5\rThere is a possibility that no interactions occurred because familiarity is not faster than search; this is an unattractive alternative because the puzzle of rapid, ac- curate, negative recognition responses is difficult to explain.\r\nThis terminology seems appropriate for most laboratory recognition studies because the tasks are episodic and the decision is usually one of discriminating a recently presented item from preexperimentally known items. We use the terms familiarity and direct access inter- changeably in this article.\rWe have assumed and defined the famil- iarity component to be faster on the average than the search process, because the search process must predict interresponse times in free recall that are many seconds in length, whereas the familiarity process must allow ac- curate negative recognition responses to be made in under a second (on many trials). Of course it is possible that the Dc or Ds process is a sequential one, similar to the search pro- cess but far more rapid (and efficient). The difficulty of distinguishing serial and parallel processes in general is well known (see Town- send, 1971), and we shall not try to do so here. For present purposes, it suffices to note that Dc (or Ds) cannot be a search process with temporal characteristics close to those needed for search in free recall.\rWe conclude this introduction by indicating the scope of the present article and the types of data we try to explain. Our present version of the SAM model is meant to deal with all of the recall phenomena mentioned in our\rearlier papers; these phenomena lie largely in the domain of episodic memory (see Raaij- makers & Shiffrin, 1980). The extension of the model to recognition is meant to apply primarily to situations in which long-term memory is being accessed (i.e., in which no contamination of recognition from short-term store [STS] occurs) and to situations in which subjects are asked to make episodic judgments that some item has occurred recently (when the distractors have not been presented re- cently and hence are unfamiliar).\rThe phenomena we wish our model to ex- plain include the following:(a) the large num- ber of factors that influence recall and rec- ognition performance similarly, such as the amount of time given for study (Ratcliff & Murdock, 1976; Roberts, 1972), list length (e.g., Murdock, 1962; Roberts, 1972; Strong,\r1912; see also Figure 2), test delay (e.g., Shep- ard, 1967; Strong, 1913; Underwood, 1957), and test position effects (e.g., RatclifF& Mur- dock, 1976; Raaijmakers & Shiffrin, 1981a); (b) the factors that influence recall and rec- ognition differently, such as word frequency (e.g., Gregg, 1976), types of encoding (e.g., Tversky, 1973), types of rehearsal (e.g., Wood- ward, Bjork, & Jongeward, 1973), and the ef- fects of context shifts (e.g., Smith, Glenberg, & Bjork, 1978); and (c) the types of data that\r.40\rg.30\rDC\r5\rDC\r< .20 LU 2-°\r.00\rSLOW\rCONTROL SYNONYM GRAPHEME PHONEME\rDISTRACTOR TYPE\r1\rGARY GILLUND AND RICHARD M. SHIFFRIN\rFigure 4. False alarm rate as a function of distractor similarity: control = not related; synonym = semantically related; grapheme = one letter in word altered, no rhyme; phoneme = one letter in word altered, and rhyme.\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL\rhave been thought to indicate searchlike pro- cesses in recognition (e.g., Murdock, 1974; Tulving & Thomson, 1973).\r1. A Model for Recall and Recognition\rIn Section 1, we present a model to predict the phenomena of both recall and recognition: We include (a) general features of the model that apply to both recognition and recall, (b) the recognition part of the model, (c) a sum- mary of the model as it applies to recall (the recall model and applications have been ex- plicated in detail in previous publications: Gillund & Shiffrin, 1981; Raaijmakers & Shif- frin, 1980, 1981a, 1981b), (d) a simple nu- merical example illustrating the quantitative features of the approach, and (e) certain im- portant assumptions regarding storage and variability.\rGeneral Features of the SAM Model\rLong-term store (LTS) is assumed to be composed of closely interconnected, relatively unitized, permanent sets of features called im- ages. Episodic images contain many types of information, including the following: (a) con- textual elements that can be used to identify the contextual-temporal setting in which an item occurs, (b) item information that can be used to name and identify the item encoded in the image, and (c) interitem information that links one image to others.\rThe process of retrieval is assumed to be cue dependent (Tulving, 1974). Access to LTS occurs when probe cues called a probe set are assembled in STS and are used to activate an associated set of information in LTS. The ac- tivated set consists of a large set of images activated to differing degrees. In recognition, some sort of integration across the entire ac- tivated set is used to generate a decision. In recall, one image is sampled from the activated set during each cycle of the memory search.\rThe quantitative details of the retrieval pro- cess are based on a retrieval structure (see the left-hand matrix in Figure 8). The retrieval structure is a matrix of retrieval strengths re- lating all possible cues to all possible images. All of these strengths are greater than zero, although many are of negligible magnitude. In theory, all cues and all images are contained\rin the structure, but in practice only the non- negligible cues and images are considered. The retrieval strengths represent the relative ten- dency for a given cue to activate a given image and are related to the strength of the associative connections between that cue and that image.\rIn an episodic task, the entries in the re- trieval structure are determined by two main factors. The first factor is the coding and re- hearsal process used at the time of study (and any additional learning that occurs during the course of retrieval). The second factor is the match of the cue used at test to the nominally identical cue used at study: These might differ due to encoding variability and other factors. Particularly important cues in an episodic task are (a) context cues, which through similarity to the storage context give the subject access to recently presented items; (b) item cues, which have high strengths to the image con- taining the cue item itself and to the images containing items rehearsed with the cue item; (c) category cues, which have high (preexper- imental) strengths to the images containing items from the cue category.\rFor computational simplicity, the retrieval structure is assumed to contain images of all presented items (rather than all images in memory). The context cue has strengths to each image in proportion to the time spent rehearsing that item. Each list item, as well as new items that are not from the list, can be a cue. If the cue item has not been rehearsed with the item in a given image, then the strength from item cue to image is set to a small residual value based on preexperimental factors; otherwise the strength is proportional to the time that both items had been rehearsed together. A category cue is given a high strength to images of items from that category and a smaller residual strength to images of items from other categories.\rThe SAM Model for Recognition\rAs indicated in the introduction, we are in- terested in considering Models 4 and 5 of Table 1. We present a version of the simpler model, Model 4, in spite of the fact that the basic logic of the SAM model requires that subjects should be able to carry out a memory search during recognition testing, if they choose to do so. Thus, for the purposes of the devel-\r\nopment, we adopt the working assumption that the subjects choose not to carry out a memory search during recognition.\rModel 4 of Table 1assumes that recognition is determined by a direct-access familiarity process, a process complex enough to produce characteristics that normally are the result of a search process. The model we propose does this in a very simple way. It is assumed that in a yes-no recognition test the subject probes memory with two cues: the context cue and the tested item. The total activation of LTS in response to this probe set is taken to be the value of familiarity on which a response is based. Thus when a target is tested, the acti- vation is based not only on the activation of the image of that target, but also on the ac- tivations of all other images in memory. For this reason, we refer to the approach as a global model of familiarity. Searchlike effects are predicted because images of items other than\rthe tested item are included in the activation on which a decision is based. The simplicity of the model is illustrated in Figure 5. In re- sponse to a test item, a value of familiarity is generated, and the subject responds yes or no depending on whether the familiarity value is above or below a criterion value chosen by the subject.\rThe quantitative details are captured in Equation 1. This equation gives the general expression for the total LTS activation when cues Qi, Q2, .. ., QM are used together to probe memory and when there are N images in LTS. The W/ are weighting factors associated with each cue, possibly reflecting the amount of attention allotted to each cue. (In previous work with the SAM model these weights had always been set to 1.0.) The term represented by the product gives the activation contributed by any given image, I/c. Thus the activation of a given image isjust the product of the separate\rGARY GTLLUND AND RICHARD M. SHIFFRIN\rf START J\rUSE TEST ITEM AND CONTEXT AS CUES\rLET F=DENOMINATOR OF SAMPLING RULE\rFigure 5. A flow chart for the basic SAM recognition process.\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL\ractivations of that image by each of the cues, the individual activation being the strength in the retrieval structure, S(Q;, y. The total ac- tivation is just the sum of the activations for each of the images.\rand distractors. As indicated in Figure 5, a decision is made only after a criterion is chosen by the subject. Positive recognition judgments are made when the familiarity value is higher than the criterion. Therefore the recognition model is fully specified by Equations 1, 2, and\rNM\rP(Qi,Ch,..•,QA/)= 2 E S(Q;,yw'.(1) 3,andbyasetofassumptionsgoverningthe\rk=\\ ;=i\rIn a yes-no recognition test for the presence of single items on a recently presented list, we assume that just two cues are used to probe memory: the context cue, C, and the tested item, lj. In this case, Equation 1 simplifies to Equation 2, and when the weights are set to 1.0, simplifies even further, as indicated in Equation 3. (For a numerical illustration of Equation 3, see the first three matrices along the top of Figure 8.)\r*\\c,ij)=2 s(c,yw<sd;,ywj. &) F(c, ij) = 2 s(c, ysa,, y. (3)\rfc-i\rThe familiarity values indicated in Equa- tions 1,2, or 3 can produce higher than chance recognition judgments because a few com- ponents of the sum making up the total ac- tivation will generally be higher for targets than distractors. In particular, the strength of a tar- get to its own image, 8(1,, I;), will usually be higher than the strength of a distractor to that same image, S(L/, I,-). Furthermore, the inter- item strengthsare sometimes higher for targets, S(lj, y , k + j, than for distractors, S(Id, y , d + k; this will usually be the case when the test item, lj, had been rehearsed during list presentation with item lk.\rIn applications to episodic tasks, the fa- miliarity value is calculated under the sim- plifying assumption that only the strengths for list items contribute nonnegligible activations (i.e., the sum in Equations 1, 2, or 3 is taken only over the N list items). This assumption leads to what may seem an unusual feature of the model: When a distractor is tested, its own image does not contribute to its famil- iarity; a distractor is familiar only to the degree that it activates the list items.\rEquations 1, 2, and 3 specify the values of familiarity that are produced by tests of targets\rchoice of criterion (see Sections 2 and 5). The SAM Model for Recall\rThe SAM recall model is quite a bit more complicated than the model for recognition, because recall is assumed to require a memory search. The search consists of a sequence of sampling and recovery operations, each cycle in the sequence containing a sample of an image from LTS and an evaluation of the in- formation recovered from the sampled image.\rWe begin our discussion of the recall model by considering the basis for sampling an image from LTS. During a given cycle of the search of memory, assume that cues, Qls Q2, .. ., QM are used together as a probe set. Then Equation 4 gives the probability of sampling image I,:\rM\rS(Q;\r2 l S(Q,,\r. (4)\rThe denominator of this equation is of course just the total activation of LTS, in re- sponse to the cues, that was the value of fa- miliarity used to make a recognition decision. The terms are defined similarly. Equation 4 stipulates that sampling obeys a ratio rule (Luce, 1959) in which an image's chance of being sampled is proportional to its net strength to the probe cues. That is, the nu- merator is just the activation of a given image, whereas the denominator is the summed ac- tivation across all images.\rThe justification for the product rule by which individual cue strengths are combined into a single activation for a given image is easy to see in the sampling equation. The use of a product rule ensures that the sampled image tends to be an item strongly connected to all cues rather than to just one. In other\r\n10 GARY GILLUND AND RICHARD M. SHIFFRIN\rwords, the sampled image tends to come from the most dense region of the intersection of the associative fields of the separate cues. Thus the product rule allows the subject to use mul- tiple cues to focus the search.\rEquation 4 is somewhat easier to follow in the case that has been used in simulations of simple free-recall in previous work (Raaij- makers & Shiffrin, 1980, 1981b). In these cases the W,- were all set to 1.0, and only two types of probe sets were allowed: the context cue alone or the context cue plus one item cue. If the context cue alone is used, Equation 4 be- comes Equation 5 (see the top row of the sec- ond, third, and fourth matrices along the top of Figure 8):\rWhen context and an item I/ are cues, we get Equation 9 (see rows 2 through 7 in the lower matrices of Figure 8):\r/mlC, I,)\r= 1 - exp{-S(C, I,) - SdJ} I,)}. (9)\rEquations 7, 8, and 9 are not quite as ar- bitrary as they look at first glance. Let 1 - exp{-W,S(Qj, I;)} be the probability of cor- rectly identifying the item in the sampled im- age, for some particular cue, Q ; , used alone. Then Equations 7, 8, and 9 give the general expressions for correct recovery under the as- sumption that each different cue has an in- dependent chance of producing such recovery.\rEquations 4-9 describe what happens on a given cycle of the search. The whole course of retrieval consists of a series of such cycles. Figure 6 depicts the course of the retrieval process. On each cycle the subject chooses probe cues according to a retrieval plan that takes into account both prior knowledge and the outcome of the retrieval process to date. The overall plan may involve strategies such as alphabetic search or temporally ordered search or instead may involve a strategy based on the momentary outcome at each stage of search. After probe cues are selected, the rel- atively automatic sampling and recovery phases occur. Then the subject evaluates the outcome and decides whether to terminate or to continue search. In free-recall tasks, the termination decision is presumably based on the number or proportion of search cycles that ate failures (these are cycles on which no new item is successfully recovered). If the search continues, then the cycle begins anew with the choice of probe cues.\rTo predict certain changes that take place during the course of retrieval, it is necessary to assume that some learning occurs during retrieval (such learning was termed incre- menting in previous publications, in defiance of grammatical rules). In free recall, for ex- ample, each successful recovery is assumed to be followed by an increase in the strengths relating the cues in that probe to the image sampled (see Rundus, 1973).\rOur brief review of the SAM model for recall may be completed by pointing out that sep-\r/ W/ I Q =\rS(C, I,) 2 s(c,\r( 5 )\rIf the context cue and an item I, are used as cues then Equation 4 becomes Equation 6 (see rows 2 through 7 in the second, third, and fourth matrices in Figure 8):\rDn \\r< T ^-\r5l\rSIT Tisn 11\rOnce an item has been sampled, the subject attempts to recover the information contained therein and utilize it to succeed at the given task. In the case when recall is required, then the name encoded in the image is desired; in this case the probability of recovering the en- coded name has been specified in earlierwork and is given in Equation 7:\rM\rj, i.)}.\rIn this equation, the weights do not nec- essarily have to be identical to those used in Equation 4. In applications to free recall, how- ever, the weights were set to 1.0 also. When context alone is the cue, we get Equation 8 (see the top row of the lower matrices in Figure 8):\rFR(I,|C) = 1 - exp{-S(C, I,)}. (8)\r^ ' t)^(\\j, I,) s(C,\r(6)\r\n(NO)\rEVALUATION\r[CONTEXT COMPARISONS] [ ITEM DECISIONS j\rOUTPUT DECISION\rTERMINATION DECISION (YES)\r(YES)\r\"L\rRESPONSE PRODUCTION\rRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 11\rarate recovery attempts that involve the same image and at least one common cue are not independent. In free-recall tasks, it is assumed\rthat a new and independent recovery chance for an image (that has not yet had its item recovered) occurs if and only if at least one\rI o tr <\rQUESTION\ri RETRIEVAL\rPLAN\rASSEMBLE PROBE CUES IN STS\rit\rSEARCH - S E T SELECTION\r[RESTRICTION TO RELEVANT! [ L T S REGION J\rLU\rin <s> LU o o o: O.\rcc.\rLU SAMPLING* OF ITEM a:\r[ / a . OF AGGREGATE OF FEATURES] o\rLU\rCO t\rRECOVERY\r[OF CONTEXT AND ITEM FEA TURES]\rFigure 6. A generalized depiction of the course of retrieval from long-term store during recall (taken from Raaijmakers & Shiffrin, 1981b).\r\n12 GARY GILLUND AND RICHARD M. SHIFFRIN\rnew cue (that has not been tried yet for the 6\rsented list. This is the process denoted / in Table 1.\rA Numerical Example\rTo help readers who are unfamiliar with the SAM model understand the implications of the equations and assumptions, we give a nu- merical example. The example is illustrated in Figure 8. The retrieval structure is indicated in the left-hand matrix. This gives the retrieval strengths linking cues to memory images. In this example it is assumed that four items are presented to the subject for study, resulting in an interconnected storage network containing four images corresponding to the four pre- sented items. The images are denoted by I* and the possible cues that might be used to probe memory are given along the left-hand margin of the matrix. The cue labeled C is the context cue (which effectively restricts the search to the just-presented list). The items of the list are possible cues and are denoted I,. The items as cues are distinguished from the memory images of those same items because these are never identical (and sometimes can be quite different if, say, the encoding context shifts between study and test). The remaining cues denoted D, are items that were not on the study list (and might be distractors in a recognition test).\rThe retrieval strengths in the matrix are assumed to result from the operation of re- hearsal and coding processes during the study of the list. Note that all of the cues are con- nected by nonzero strengths to all of the im- ages, even if they were not rehearsed together (as must have been the case for the D, cues). The high strength connecting the context cue to I* probably means that \\* had been re- hearsed extensively, and the high strength re- lating I3 to I* (the self-strength) probably re-\r6\rBecause our recovery rule is consistent with an hy- pothesis that each cue has an independent chance of leading to recovery, it is perhaps more reasonable to assume that the recovery probability is based only on the strengths associated with the new cues and not on the strengths associated with the old cues that failed earlier. However, we have utilized the old assumption in the present article to be consistent with the earlier SAM papers. We doubt that important differences would result for the predictions in the present article if the other assumption is adopted.\rpresent image) is in the probe set.\rThe manner in which the process of retrieval '\ris implemented for a particular task is illus- trated by the case of free recall. The flow chart for retrieval used by Raaijmakers and Shiffrin (1980) for this task is depicted in Figure 7. The retrieval structure is first filled according to a buffer-rehearsal process. Retrieval begins with the output of all items currently in STS (unless an arithmetic or other task is used to empty the words from STS before recall be- gins).\rThe course of LTS retrieval is indicated in the flow chart. Recall begins with the context cue alone. This cue continues to be used on successive search cycles until a word is recalled. Then an increment is given and the next probe set consists of the context cue plus the item just recalled. This probe set is used either until a new item is recalled or until Lmax failures in a row occur; in the case of Lmax failures, the item cue is dropped and the subject reverts to the context cue alone. Whenever a new item is recalled the probe cues to image strengths are given an increment, and the next probe set consists of context plus the just recalled item. Recall terminates whenever a total of Kmax failures accumulate. Although strategies are surely idiosyncratic and variable, the free- recall strategy outlined here (and in Figure 7) seems to us a reasonable approximation that is fairly rational and simple to apply.\rNote that cued-recall tasks are also easy to handle within the same framework. If an item is presented as a cue, and some other item is required as a response, it is natural to assume that the subject uses as a probe set the context cue along with the presented item. This probe set can be repeated until the response is found or the search is terminated.\rFinally, note that the SAM recall model in- corporates several processes in the recovery phase, implicitly. For example, in cued recall the recovery probability incorporates a deci- sion process by which the subject decides a sampled item that can be named is in fact the response to the stimulus. More important for present purposes, the recovery process in free recall incorporates an implicit decision process by which the subject decides that a nameable sampled item was in fact on the recently pre-\r\nYES\ruse item j as new cue\rT\rincrease association to context and to item i\r( START J\rK=0\rsample item i using context-cue\r( STOP )\rJfES\rNO\rincrease association to context\rsample item j using item i and context as cues\rFigure 7. A flow chart of the SAM retrieval process for free-recall (taken from Raaijmakers & Shiffrin, 19801.\rYES\rpi\rO D w\r370 O\rH O\r> z a\rr\r\nRETRIEVAL STRUCTURE\rMemory Images\r**** IiI2IsIi»\rC .5.3.8.4 Ii .3.3 .4 .1 12 .3.4.1.1\rSTRENGTH TO PROBE SET\rSUM OF STRENGTHS\rPROBABILITY OF SAMPLING AN\rIMAGE WITH THE PROBE SET\r.4 .2 .7 .3 3u.1 .1 .2 .4 Di .1 .03 .1 .1 D2 .2 .1 .3 .1\rIiI2 laU c.5 .3 .8 .4 01 ! .15 .09 .32 .04 OI2 .15 .12 .08 .04 01 3.20.06.56.12 .05 .03 .16 .16 OD, .05 .013 .08 .04 OD2 .10.03 .24 .04\rSum of Strengths\rProbeSet (2.0)\r.60 .39 .94 .40\r.185 .41\rRecovery Probabilities\rI2 I3\rIi I2 Is\r.25 .15 .40\r.25 .15 .53 o\r.39 .31 .20 I o\r.21 .06 .60 F .13 .08 .40 za .27 .08 .43\r.24 .07 .59\rn\rO s\r2\rz\ruu\r\"\r03\rca 0-\rFamiliarity\rof\r§c.**** ***\rc .5 0 1 ! .&\rOI2 .8\rOI3 .9 .6\r.3\r.6\r.7 .9 .5 1.5 .4 1.0\r.8 .4 1.2 .5 .5 .7 .8 .9 .5 OB2 .7 .4 1.1 .5\r.39 .26 .55 .45 .55 .50 .59 .39 .45 .33 .45 .30 .50 .33\r.55 .70 .59 .78 .63 .59 .67\r.33 .39 .39 .50 .55 .39 .39\r.6 .35\r\nRETRIEV AL MODEL FOR RECOGNITION AND RECALL 15\rfleets the same factor. The interitem strengths are not very high, but the value of 0.4 for I3 to If may reflect some extra joint rehearsal of these two items. The residual values con- necting the distractor D) to the list-item images are quite low, as is usually the case. However, the residual values for item D2 are somewhat higher, possibly indicating a higher similarity of this distractor to (some of) the list items than was true for item D,. Finally, note that even in cases where rehearsal and coding are not factors (e.g., the rows of strengths for the distractor cues), the strengths are somewhat variable. This variability is quite important; as we see, without variability, recognition per- formance would generally be perfect.\rAccording to the SAM model, the context cue is always used to probe memory in episodic tasks but sometimes is combined with other cues, in this case the item cues. The possible probe sets size of 1 or 2 are given on the left- hand margin of the second matrix. The nu- merator of Equation 6 specifies that the strength when cues are combined is the prod- uct of the individual cue strengths. These products are given in the second matrix(except for the first row, for the context cue alone, which is determined by the numerator of Equation 5). Thus the strength connecting probe set (C + Ij) to I* is given by the product of the strength connecting C to If (i.e., 0.5) and the strength connecting I] to If (i.e., 0.3) for a result of 0.15.\rFor a given probe set the sum of the strengths to all the images is the familiarity value of Equations 1, 2, or 3 and also the denominator of Equations 4, 5, or 6. We treat this value as the total LTS activation engen- dered by the probe set, and the results are given in the column vector of Figure 8. If the subject were carrying out a recognition test with targets ^-Lt and distractors DI and D2, a criterion would be chosen and responses made accordingly. Because the familiarity value of D2 is higher than that of the two list items, I2 and I4, errors occur wherever the criterion is placed. Thus if the criterion were\rset at 0.402, afalse alarm would be given to D2 and a miss would be given to I2 and L».\rEquations 4, 5, or 6 determine the sampling probabilities for each image for a given probe set, for each cycle of the search process. The probabilities are simply the numbers in the rows of the second matrix in Figure 8 divided by the row sums given in the column vector. The resulting sampling probabilities are given in the right-upper matrix of Figure 8. Note that the absolute values of the strengths in the first two matrices along any row are not im- portant for sampling; sampling is based only on the relative strengths along any row of the second matrix. If learning takes place during retrieval, then some of the strengths in the retrieval structure increase and, consequently, some of the sampling probabilities change during the course of retrieval.\rOnce an image is sampled, there is no cer- tainty that a recovery of the information in that image will be complete enough that the name of the item embedded in that image will be recallable. The probability of recovery is given by Equations 7, 8, or 9 and is based on an exponential function of the sum of strengths for the cues in the probe set. The sums of strengths are given in the lower left-hand ma- trix of Figure 8, and the corresponding re- covery probabilities from Equations 8 and 9 are given in the lower right-hand matrix. These probabilities apply whenever an image is sam- pled by a probe set that has not previously been used to sample that image.\rStorage and Variability Assumptions\rThe values in the retrieval structure are the result of (a) rehearsal and coding processes that take place during the study of the list, (b) preexperimental associations, and (c) the match of the cue encodings at study and test. Storage assumptions based on a rehearsal buffer have been used in previous applications of SAM to free and cued recall. However, these previous storage assumptions cannot be used without modification in applications to rec-\rI'igure 8. A numerical illustration of a possible retrieval structure for a four-item list with context, item, and distractor cues. (The remaining matrices give quantities that are used during the application of SAM to recognition and recall.)\r\n16 GARY GILLUND AND RICHARD M. SHIFFRIN\rognition. The previous storage assumptions allowed variability in the strengths placed in the retrieval structure only to the degree that the amount of rehearsal (or coding) varied. Thus the residual values placed in the structure (when two items were not rehearsed together) were set to a fixed value. Because the residual value was low, and less than the self-strength for any list item, every list item would have a higher familiarity value than every distractor. This is evident from inspection of Equation 3: Both targets and distractors share the values of the context strengths, S(C, lk); for the dis- tractors, each interitem strength, 8(1,, lk), would attain a constant, small, residual value; for list items, some of the interitem strengths would be higher than the residual value due to rehearsal of an item I,. The result of these assumptions is the prediction that recognition performance would be perfect.\rObviously what is needed for realistic rec- ognition predictions is variability of the strengths placed in the retrieval structure. Fa- miliarity distributions for targets and distrac- tors would then have the possibility of over- lapping. Because items obviously differ (as do many other factors besides rehearsal time), variability is a very reasonable assumption. Variability was not assumed in previous pub- lications concerning free recall because it was not needed, but now that we introduce it for recognition, we also use the assumption for recall, and generate predictions accordingly.\rIntroduction of variability opens many de- grees of freedom for the model. We decided to adopt a very simple approach, at least at the outset: After the retrieval structure isfilled in the usual fashion, each entry is replaced by a random sample from a 3-point distribution. In particular, if the starting value is x, then the final stored value, y, is chosen according to Equation 10.\rproaching a normal distribution for the dis- tractor tests).\rAlthough the choice of Equation 10 is fairly arbitrary, it has a property that we regard as desirable: The standard deviation associated with a value in the retrieval structure is a fixed multiple (uV2/3) of that value. Such an as- sumption is not compatible with a hypothesis that the variance associated with a strength based on a storage episode of time 2t is the sum of variances associated with the two suc- cessive storage episodes of time t for the same item (because storage is linear with time in our model). This independence assumption would lead to a linear growth of variance, rather than standard deviation, with strength. Our present variability assumption is com- patible with a dependence hypothesis in which the random deviation associated with the strength for any subinterval of time for an item is in the same direction as the deviation associated with any other subinterval. This variability assumption has what we regard as another desirable property: Multiplication of all the values in the retrieval matrix by any constant (before noise is added) leaves un-\ry=\r(1- v)x, P = 1/3\rx, p= 1/3. (10)\rSummary\rOur model for recognition and recall is of the type labeled Model 4 in Table 1. Recall is carried out with a search process (containing an implicit episodic identification process embedded in the recovery phase of the search cycle). Recognition is carried out with a direct- access familiarity process that is of the global type because familiarity is a sum of familiarity values for the images of each list item. Recall and recognition are linked quantitatively be-\r7 Because overall level of strength has an effect on re- covery but not on sampling (see Equations 4 and 7), and because recognition performance is not dependent on overall level of strength, the strength level may be used to adjust the relative performance level of recall versus rec- ognition. For this reason, it is possible to choose a value ofu fairly arbitrarily, within certain limits, and adjust the other parameters to fit the combined recognition-recall data.\r(1 +v)x, p= 1/3\rThis discrete distribution is sufficient for our\rpurposes because the familiarity value results from a sum across the list items; the law of large numbersassures that the distribution of the sum approaches a continuous distribution as the list length becomes large (actually ap-\rchanged the shapes and overlap of the famil- 7\riarity distributions for targets and distractors. Other assumptions concerning variability are obviously possible but are not explored in this article.\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 17\rcause the familiarity value used in recognition for item and context cues is the denominator of the sampling ratio used during search when that item and context cue make up the probe set.\r2. The Simulation Model and Qualitative Predictions\rIn Section 2 we first describe a particularly simple simulation model, its parameters, and the choices of parameter values. The model is then used to produce qualitative predictions for a number of findings in the recall and rec- ognition domains.\rThe Simulation Model\rThe model's predictions are all generated with a computer simulation because analytic derivations are in most cases not feasible. The basic situation to be modeled assumes pre- sentation of a list of items followed by a rec- ognition or a recall test. The storage and re- trieval assumptions follow.\rStorage\rA buffer rehearsal process is assumed (At- kinson & Shiffrin, 1968). In this version a par- ticularly simple buffer is assumed. The buffer contains a maximum of r items. When the buffer is full, each new item replaces the oldest item in the buffer.8 Based on earlier work, r was set equal to 4 (see Raaijmakers & Shiffrin, 1980).\rWe assume that the items in the buffer share rehearsal time—when n items are present, each gets \\/n of the available rehearsal time. (Thus primacy is predicted because the first item is alone and several items are needed to fill the buffer.) In particular, a mean of a* units of strength are stored between an item and the context cue for each second of rehearsal; a mean of c* units of strength are stored between an item when used as a cue and its own mem- ory image, for each second of rehearsal; a mean of b* units of strength are stored between two different items, for each second of individual rehearsal for one of the items that takes place during the period that both items are rehearsed together; and a mean of d units of strength are present in the retrieval structure between\rany two items that have never been rehearsed together. For studies using random word lists, the value of d is assumed to be small relative to b* and represents preexperimental associ- ations between the items. Note that a distractor as a cue is given mean interitem strength values of d, like a list-item cue that has not been rehearsed with any other list items.\rRetrieval During Free Recall\rThe flow chart shown in Figure 7 is used to govern retrieval, but for simplicity no in- crements are utilized (that is, the increment parameters are set to 0). The two retrieval parameters that govern cessation of search are chosen fairly arbitrarily, although with pre- vious fits in mind, and are as follows: KmaK = 30,Lmm =8.\rRetrieval During Recognition\rThe value of familiarity is calculated using Equation 8. A criterion, Cr, is chosen. If the familiarity value is higher than the criterion, then a yes recognition response is given, oth- erwise a no response is given. Sensible criterion choices are generally very near the point where the distributions of familiarity for targets and distractors cross. An example is given in Fig- ure 9.\rHow the subject learns where to place the criterion is an interesting question that is dis- cussed later. For present purposes it is simply assumed that the criterion is fixed in value from the start of the recognition test until the end. In every simulation in this section we assume that the criterion is placed at a point 0.455 times the mean value of the distractor distribution plus the mean value of the target distribution; that is, Cr = 0.455 (Md + Mt). This criterion choice was used because for the present parameters it places the criterion\r8 An assumption of oldest item deletion predicts a re- cency effect of size r, which surely doesn't match the known data when r = 4. However, because in this article we predict only long-term performance (say, in situations where STS is cleared before test), our buffer assumption makes very little difference, and we use oldest item deletion for ease of calculation.\r\n18 GARY GILLUND AND RICHARD M. SHIFFRIN\rsomewhere close to the crossing points of the target and distractor distributions.\rStrengths in the Retrieval Structure\rA given strength in the retrieval structure between a cue arid an image depends not only on coding processes that take place at the time of study but also on the match of the encoding given to the cue at test to the encoding given that cue during storage. F'or example, increas- ing the delay of test usually causes the context cue at test to differ more from the context cue used at study. As another example, a change in the item preceding the test item could cause different meanings of the item to be chosen at test and study.\rWe can capture this factor in the model by setting the strength in the retrieval structure equal to the originally stored strength times a factor indicating the degree of match between the two encodings of the cue: S(Q,, 1^) = m(Q,)S*(Q;, I;), where S* represents the orig- inally stored strength and m(Q/) represents the degree of match of the encodings of the cue at test and study. Our storage assumption lets us produce the strength values by setting a -= m(C)<z*, b = m(\\)b*, and c = m(l)c*, andlet-\rting the retrieval strengths equal a, b, and c times the appropriate number of seconds of rehearsal (C is the context cue and I is an item cue).\rIt is convenient to couch the predictions of the model in terms of a, h, and c. When ex- perimental manipulations are used that should shift the degree of match (e.g., delay of test or biased encoding), these can be described as shifts in a, b, or c rather than in the m pa- rameters. This approach keeps the present treatment identical to that used in earlier pa- pers on the SAM model.\rSimulation Procedures\rFor the set of qualitative simulations, an arbitrary parameter set was chosen, roughly in accord with previous simulations of free- recall data. The values were a =• 0.25,b =• 0.2, c = 0.15, and d = 0.075. In addition, the variability parameter, v, was set equal to0.5. The patterns of predictions that result are not dependent on the exact values chosen ( a l - though, of course, quantitative performance levels do depend on these values).\rFor a given application, simulated data is generated for 500 lists for both recall and rec-\r.200 .175\r3-125\rO iu\r£.100\r.000\r«-RESPOND\"NO-\rDISTRACTORS\r•*\rRESPOND \"YES\"\r* -\rCRITERION\rTARGETS\r1.0\rFAMILIARITY VALUE\rFigure 9. A target and distractor distribution. (If the value is greater [to the right of the criterion] a yes response is made. If the value is to the left of the criterion, a no response is made. The distributions are generated from the model applied to a 20-item list at 2 s per item. A buffer size of 4 is assumed, with oldest item deletion. Parameter values are a = 0.25, b = 0.20, c=0.l5,cl= 0.075, and v = 0.5.)\r1.5 2.0\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 19\rognition (using different randomizations each time). These pseudodata are averaged and the results are plotted as the predictions of the model. Because we are not trying to produce quantitative predictions, we do not plot actual data. Instead the literature is reviewed and the qualitative patterns that are observed in the literature are described. Recall predictions are given in terms of probability of correct recall and recognition predictions in terms of d'? (Additional information is provided in Ap- pendix A.)\rRoles of the Parameters\rThe roles played by the various parameters become clear in the following parts of this ar- ticle, where applications to various paradigms are discussed. It is useful nevertheless to sum- marize briefly the main functions of the pa- rameters. To demonstrate the actions of the parameters most clearly, predictions were de- rived for a standard set of parameter values, and then each value was varied in turn, while holding the other values constant. The results arc given in Figure 10 for recognition (d'), and in Figure 11 for free recall.\rParameter v. For recognition paradigms, decreases in the variability parameter, v, de- crease the variability of the entries in the re- trieval structure and hence decrease the vari- abilities of the target and distractor distribu- tions, without changing the mean difference. Performance therefore improves, as indicated in the right-hand panel of Figure 10.\rIn free recall, changes in the v parameter have an effect on the recovery process. Because recovery probability is an exponential function of strength, moving from a smaller to a larger variance (increases in v) produces a lowering of the average recovery probability. The results are shown in the right-hand panel of Fig- ure 11.\rParameter a. The context parameter, a, acts as a scale factor in recognition and does not affect performance. (For example, mul- tiplying the value of a by a factor k causes all the entries in the top row of the retrieval struc- ture of Figure 8 to be multiplied by k. In turn, this causes all entries in the adjacent matrix in Figure 8 to be multiplied by k and, hence, all the familiarity values to rise by a factor of\rk. Obviously the ordering of magnitudes of targets and distractors are not affected by mul- tiplication by a constant.) The results are shown in the left-hand panel of Figure 10.\rIn recall tasks, increases in the a parameter cause increases in recovery probabilities (and no effects on sampling probabilities) and, hence, produce increases in recall, as shown in the first panel of Figure 11.\rParameter b. Increases in the interitem pa- rameter, b, improve recognition performance by raising familiarity of the target distribution, but not the distractor distribution. Thus only the rows labeled / in the retrieval structure of Figure 8 are altered by changes in b. The results are shown in the second panel of Figure 10.\rIncreases in b also improve free recall by improving recovery and by altering sampling in subtle ways (e.g., decreasing self-sampling when an item cue is used—see next para- graph). The results are indicated in the second panel of Figure 11.\rParameter c. Increases in the self-coding parameter, c, improve recognition for similar reasons as in the case of the /; parameter: Only the target distribution is raised (one of the entries in each of the rows labeled / in the retrieval structure of Figure 8 are raised). On the other hand, increases in the c parameter harm free recall by increasing self-sampling (self-sampling is the tendency for an item used as a cue to sample its own image). The op- posing results are shown in the third panel of Figures 10 and 11.\rParameter d. Increases of the residual pa- rameter, J, reduce recognition by increasing the familiarity of the distractors much more than the familiarity of targets (e.g., all the en- tries in the lower two rows of the retrieval structure of Figure 8 are increased, but only a few entries in the rows labeled / are in- creased) and by increasing the variance of both distributions. Increases in the d parameter im-\r9 Because the theoretical target and distractor familiarity distributions arc of unequal variance, and because the target distribution is skewed, it is technically incorrect to analyze the data in terms of d'. Nevertheless, because most actual data are analyzed this way, we have converted the hit and the false alarm rates into d' scores. The patterns of predictions are unaltered by alternative measures of discriminability.\r\n6.0\r5.0\r4.0\rDC\rO LL\rLU O\rDC 3.0 LU\r0.\rQ 2.0\ro cj LU oc\r0.0\rO\rI O\rC z D\r> O\rO\r>\r73\rD\ren\r1.0\r.20 .25 .30\r.10 .20 .30\r.10 .15 .20\r.050 .075.100\r.25 .50 .75\rabCd\rFigure 10. Changes in predicted recognition performance (d') for a 20-item list caused by changes in the a, b, c, d, and v parameters. (Unless otherwise indicated, t = 2s;r = 4;a = 0.25; b = 0.20; c = 0.15; d = 0.075; v =0.5.)\r\n.65\r< .60 O\rLU\roc\rLU -55\rLLI\rCC\rfe .50\r>\r5 .45 OQ\r2B\roa\rpo td\r8o\rim .40 o>z Q. a\rf abCd\rFigure 11. Changes in predicted recall performance for a 20-item list caused by changes in the a, b, c, d, and v parameters. (Unless otherwise indicated, t = 2 s; r = 4, a = 0.25, b = 0.20, c = 0.15, d = 0.075, «;=0.5;K^ =30,L^ =8.)\r.20 .25 .30 .10 .20 .30 .10 .15 .20\r.050 .075 .100 .25 .50 .75\r\n22 GARY GILLUND AND RICHARD M. SHIFFRIN\rprove free recall mainly by increasing recovery probabilities and also through subtle effects on sampling (e.g., decreasing self-sampling). The predictions are shown in the fourth panels of Figures 10 and 11.\rKmax and Lmax. These parameters apply only to free recall, and only Km.M has large effects: Increases in Kmm are equivalent to ex- tending the length of search and produce better free recall. We do not vary Km.M in any of the applications in this article.\rApplications of the Recall and Recognition Model\rWe illustrate the workings of the new model by applying it to a variety of results from the literature. In most of the cases we contrast the predictions of the model for a recall task with the predictions of the model with the same parameters and assumptions for a comparable recognition task. We are not interested in exact quantitative fits of data; rather we show that the model (usually with a standard parameter set) fits the qualitative patterns that are ob- served in the data. (Detailed quantitative fits are carried out in Section 3 of this article.) The figures plot recognition performance in terms of d'. The means of the target and dis- tractor distributions and the hit and false alarm rates for each point are given in Appendix A.\rThe List-Length Effect\rIt is well known that the probability of re- calling or recognizing a particular item from a list decreases as the list length increases. This is true for recall (e.g., Murdock, 1962; Roberts,\r1972) as well as for recognition (e.g., Strong, 1912; see also Figure 2).\rFigure 12 gives the results of the model's predictions for lists of words presented for 2 s each. Recall and recognition are plotted to- gether, but we do not wish to imply that d' and the probability of recall are in any way directly comparable. We show the results in this fashion simply to demonstrate that per- formance decreases for both measures.\rThe list-length effect occurs in free recall as a result of the sampling rules. The probability of sampling a given item on a given search decreases as the list length increases. Even\rthough more total samples are made for longer lists before the stopping criterion is reached (because the probability of resampling a pre- viously sampled item goes down with list length), the sampling effect is the more pow- erful and recall per item decreases with list length.\rIn recognition, quite a different mechanism is responsible for the list-length effect. The mean difference in familiarity for targets and distractors remains constant as list length in- creases. This is so because only a few items are jointly rehearsed with any target item, re- gardless of list length (beyond some mini- mum). Therefore, adding list items plays the same role for target and distractor tests—in each case images are added to memory that are only residually associated to the test item. Why then does performance decrease? The answer is simple: The variance of the distri- butions increases with increasing list length (because each additional item in the list adds an independent variance component). As a result, there is more overlap of the target and distractor distributions for the longer lists and d' is decreased.\rPresentation Time\rIncreasing the presentation time per item improves both recall and recognition perfor- mance (e.g., Ratcliff & Murdock, 1976; Rob- erts, 1972;Shiffrin, 1970). In SAM, increasing presentation time increases the strengths of associations built up during rehearsal: Context, interitem, and self-strengths all get larger. In recall these increases improve the probability of recovery (the ratio rule for sampling does not change with presentation time).\rIn recognition, the increase in d' arises be- cause the self-strength and interitem strengths rise with presentation time. These increases cause only the target distribution to increase (the rise in mean far outweighs the concom- itant rise in variance), thereby improving per- formance. Note that the increase in context strength does not affect d'—the increase in context strength acts like a scale factor in- creasing both mean familiarity and variances so that no net change in d' results (see the discussion of the a parameter). The results of the model's predictions for presentation time\r\no\rUJ DC\rLU LU CC\r3.5\r.50\rcc .40\r.30\r4.0\rLL\r3.0 e\rCO\r< .20 CD\rRETRIEVAL MODEL FOR RECOGNITION AND RECALL 23\r=! .40 CQ\rCO\r3.0 UJ Q.\r2.5 1\rCD\r.70\r.60\r.50\r—i 1\rQ RECOGNITION\rRECALL\rr\r73\r4.0 LU O\roO\rcc 10 20 30 40 O\rQ.\rLU CC\rRECALL\r2.0\r1.0\rLIST LENGTH\rFigure 12. The model's predictions for recall and recognition as a function of list length, (t = 2 s; r = 4, a=0.25,i=0.20,c=0.15,d=0.075,«=0.5;*„,„„- 30,Lmax=8.)\rfor a list of 30 words can be seen in Fig- ure 13.\rEncoding Effects\rA large literature exists on the effects of different types of encoding operations on recall and recognition performance. Consider first\r.60\rthe effects of item-specific, or maintenance, rehearsal. Woodward, Bjork, and Jongeward (1973) presented a series of words, each of which was followed by 0, 4, or 12 s of delay. After the delay there was a cue to remember or to forget the word. It was assumed that the delay would be used for maintenance rehearsal until the cue was given. At that time subjects\r5.0\r< LU OO\rC5 RECOGNITION Oo\rCC o\rQ.\rLU CC\r123\rPRESENT A TION T IM E (SEC/ITEM)\rfigure 13. The model's predictions for increasing amounts of presentation time per word. (List length = 30; r = 4, a = 0.25, b - 0.20, c = 0.15, af = 0.075, v = 0.5; Kmm = 30, Lmax = 8.)\rCC\rO LI- CC\rT3\rCC\rrr\rLU\ra.\r\n24 GARY GILLUND AND RICHARD M. SHIFFRIN\rcould either quit processing the word or give it more elaborate processing. Subjects were then given a test for the to-be-remembered words only. In a surprise end-of-session free- recall or recognition test for all words, Wood- ward et al. (1973) found no advantage of extra maintenance rehearsal for recall performance but a significant advantage for recognition (sec Glenberg & Adams, 1978; Glenberg, Smith, & Green, 1977).\rWe assume that the effect of maintenance rehearsal is to cause repetitions of the features related to the physical aspects of the item, so self-coding, c, should increase with the amount of rehearsal. We also assume that extra context and interitem coding due to increased mainte- nance rehearsal are reduced to a minimum. These assumptions are quantified bybeginning with the standard parameter set of the pre- ceding figures and then modeling changes in maintenance rehearsal by changes in the c pa- rameter. The results are depicted in the third panel of Figures 10 and 11, for the parameter values c = 0.10, 0.15, and 0.20, from left to right, respectively.\rFigure 11 shows recall to fall with increases in c because self-sampling is increased: When an item is recalled and used as a cue it tends\rto sample its own image, reducing the chances of sampling something as yet unrecalled. Per- haps context coding does increase slightly with maintenance rehearsal, canceling this drop. The predictions when the c values are 0.10, 0.15, and 0.20, as before, but a = 0.23, 0.25, and 0.27, from left to right, are given in Fig- ure 14.\rAn interesting result by Nairne (1983) seems superficially at odds with the predictions just given. He found in several conditions of his study that increasing amounts of maintenance rehearsal of pairs of items did not improve recognition performance on a later surprise test. The answer lies in the nature of the dis- tractors used: The distractors consisted of two list items, one from each of two different stud- ied pairs. In such a case, it turns out that the b parameter, rather than the c parameter, is the crucial determinant of performance. Be- cause the b parameter is not assumed to rise in this type of study, little effect is predicted. The reasons underlying the predictions for these special types of stimuli and distractors are covered in greater detail in Section 4.\rConsider next the experiments on inten- tionality of encoding and test expectancy. These paradigms may produce variations in\r2\rUJ\roO\rUJ UJ DC\r.55\r.50\r.45\rRECOGNITION\rRECALL\r3.5\r3.0\r2.5\rDC\rO u. CC UJ 0.\rm oO\rDC O\rLOW MEDIUM HIGH\rAMOUNT OF MAINTENANCE REHEARSAL\rFigure 14. Predictions from the model for increasing amounts of maintenance rehearsal. (Increasing mainte- nance rehearsal is assumed to increase slightly context coding [a increases from 0.23 to 0.25 to 0.27, from left to right] and to increase substantially self-coding [c increases from 0.10 to 0.15 to 0.20, from left to right], whereas interitem encoding remains constant [r = 4, b = 0.2, d = 0.075, v - 0.5; Kmm = 30, Lmax = 8; 20-word list at ?. s/item].)\r40 -\rz: 2.0 (3\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL\r25\r.60\r.55\rUJ\rLU RECOGNITION a: .50 O-\r.45\r< .40 CD\rO RECALL (£.\rQ.\r4.0\r3.5\r3.0\r2.5\r2.0\rO\rDC\rOLU QC LU Q.\r2\rC3 O O LU tr\rO\rLLJ\rtr\rCO\rthe amount of interitem rehearsal, (a) Increas- ing interitem processing by instructional set results in increased recall performance but lit- tle change in recognition performance (e.g., Estes & DaPolito, 1967). (b) Changing from incidental to intentional instructions increases recall performance but may slightly lower rec- ognition performance (e.g., Eagle & Leiter, 1964). (c) Varying the test expectancy from recognition to recall generally increases both recall and recognition performance (e.g., Bal- ota & Neely, 1980; Neely & Balota, 1981; Hall, Grossman, & Elwood, 1976).\rWe suggest that increased intentionality, or increased expectancy of recall, leads to in- creased interitem coding, possibly at the ex- pense of decreased self-coding, that is, in-\rI0\rto right. In this case recall increases but rec- ognition remains relatively constant because subjects arc trading interitem coding for self- coding.\rWhy does the model predict the patterns seen in Figure 15? For recall, predictions are clearcut: Both increases in the b parameter and decreases in the c parameter improve re- call. Increases in b improve both sampling and recovery and decreases in c reduce self-sam- pling. For recognition, the situation is more complicated: Increases in b improve recog- nition performance (due to increased associ- ations to items rehearsed with the test item) but decreases in c harm recognition (due to lower associations of the image of the test item\r10\rNeely and Balota (1981) came to the opposite con- clusion, that interitem coding was not affected, because the test-expectancy effect (better performance for recall expectancy) did not interact with the semantic organization effect (better performance for related words). Their ar- gument was predicated on an assumption derived from the theory of J. R. Anderson (1972) and of J. R. Anderson and Bower (1972) that interitem coding should not only be higher under recall expectancy but should be differ- entially higher for related words. We see no basis for such a prediction in our theory and, hence, we do not agree with the conclusion. Furthermore, we think our theory could handle the Neely and Balota (1981) results, although we have not yet attempted to fit the findings.\rcreased b and possibly decreased c.\rond panel of Figures 10 and 11 show the predictions of the model when the b parameter takes on the values 0.10, 0.20, and 0.30, and the other parameters are unchanged. In this case, both recall and recognition performance are predicted to rise as a result of increased interitem coding.\rFigure 15 shows predictions of the model when the b parameter increases from 0.10 to 0.15 to 0.20, but in addition, the c parameter decreases: c = 0.51, 0.33, and 0.15, from left\rThe sec-\rMEDIUM\rHIGH\rLOW\rRATIO OF INTERITEM TO SELF CODING\rFigure 15. Predicted recall and recognition performance as interitem coding increases (b increases) and self-coding decreases (c decreases). (From left to right, b = 0.1, 0.15, and 0.2; and c = 0.51, 0.33, and 0.15. r=4,a=0.25,d=0.075,v=0.5;Km,=30;LmM =8;20-wordlistat2s/item.)\r\n26 GARY GILLUND AND RICHARD M. SHIFFRIN\ritself). In this case, the precise nature of the trade-off in rehearsal is crucial to the predicted outcome. Just such a sensitivity to trade-offs may explain the variability of observed results.\rMatch Between Encodings at Study and Test\rThere is a well-tested principle stating that memory improves to the degree that codings at storage and test match (e.g., the encoding- specificity principle; Tulving & Thomson, 1973). For example, a number of studies show that words recognized in one semantic context may not be recognized in another. Light and Cartcr-Sobell (1970) presented subjects with homographic nouns to study. Each was pre- sented with an adjective to bias a specific meaning of the word (e.g., traffic-jam). Subjects were then tested for recognition of the ho- mographic nouns in the presence of the same or a different biasing adjective (e.g., strawberry-\rjam). Recognition performance was higher in the same-adjective tests (see also Tulving & Thomson, 1971; Winograd & Conn, 1971).\rThe present model handles such coding ef- fects through the match parameters, m,which determine the relationship between encoding of a cue at study and test. Changes in m are captured in our present simulation by changes in a, b, and c as appropriate.\" This encoding variability approach makes it easy to predict a variety of semantic context effects. The rea- soning is similar to that put forth by Tulving and Thomson (1973) and Reder, Anderson, and Bjork (1974), among others. By the same reasoning, when a word has one meaning(e.g., rhinoceros) effects of semantic context should be lowered (Reder et al., 1974; Muter, in press).\rIt may be asked, how does the biasing item or context affect the encoding of a study or test item? Because a variety of subject con- trolled coding and rehearsal processes occur during most study settings, the effects of se- mantic context are easy to incorporate. On the other hand, the initial encoding of a test item is generally thought to be a relatively automatic process (e.g., Shiffrin & Schneider,\r1977). However, even though the initial en- coding may be automatic, it is determined by the general stimulus environment, not just by the nominal word itself. Therefore, context determines encoding at both study and test, and the possible different encodings then pro- duce the cited effects of semantic context.\rThere is a variety of ways to demonstrate encoding match effects in our present simu- lation. Perhaps simplest is the assumption that the match of encoding at test to that at storage is lowered, thereby lowering the values of b and c. The resulting predictions (for recog- nition only) are given in Figure 16. Decreases in the b and c parameters each cause perfor- mance to drop as shown.\rAlthough changes in meaning are undoubt- edly responsible for many semantic context effects, another factor may also contribute. When the test consists of both a critical word and a word used to produce a bias, both words may be used in the probe set, along withcon- text. As we discuss in Section 4 the biasing word affects the resulting familiarity value. These changes in familiarity may produce se- mantic context effects similar to those ob- served, even if the coding of the critical word is assumed to be the same in all contexts. For example, if biasing items at study and test are different, recognition of the critical item is harmed: An identical bias item is more strongly linked to both the test item (due to rehearsal) and itself (due to identity) than is a shifted bias item, so the familiarity of the pair of items is higher in the identical case. If the bias item is present only at test, it produces more familiarity when it matches the domi- nant meaning, because the dominant meaning was more likely coded at study and the residual association to the image of the test item is higher in such a case. These examples make it clear that the SAM model has two comple- mentary mechanisms that can help explain context effects in recognition, one based on the coding of the test item and the other based on the familiarity of a test pair.\rRelated results concerning encoding matches have been found in a slightly different paradigm. The paradigm consists of three basic phases: (a) the study of A-B pairs of words, of which the B members are the critical words; (b) a recognition test for the B members. The recognition test is given in the absence of the\r\" An alternative treatment consistent with the SAM model would involve treating each new and different en- coding of a nominally identical item (whether presented at study or at test) as a new item with appropriate interitem strengths. This approach would probably be difficult to distinguish from the approach suggested in the text.\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 27\rA members and, in addition, is usually given in a situation where a different meaning or sense of the word is indicated by the sur- rounding test items; and (c) a cued-recall test in which the A members are presented as cues for recall of the B members. The standard result is that there are a number of words that are recalled but not recognized. Tulving and Thomson (1973) were the first researchers to utilize the procedure, and they termed the re- sult \"recognition failure of recallable words\" (p. 39). Since that study, the results and the implications of the results have been a topic of much debate and experimentation (e.g., Antognini, 1975; Rabinowitz, Mandler, & Barsalou, 1977; Reder et al., 1974; Salzberg, 1976; Tulving & Watkins, 1975; Watkins & Tulving, 1975; Wiseman & Tulving, 1975).\rThe finding that cues can sometimes lead to recall of items that cannot be recognized is quite consistent with the SAM model. The fact that recall operates as a stochastic search process guarantees that some searches succeed even though the familiarity value for the re- called items may not reach criterion. However, if recall occurs first, recall of an item would probably produce learning (an increment in context strength) that would raise the famil- iarity value for that item. Thus a later rec- ognition test of that item would tend to be\rabove threshold. Most recognition failures in this case (and many other cases, as well), then, are probably due to a biasing item or to other context changes that induce a meaningduring the recognition test that is different from the meaning accessed during the recall test. For similar reasons, it is predicted that cued recall can fail when free recall succeeds, or vice versa. Such results are expected whenever differing test and study contexts induce different mean- ings of a word (and may even occur when the same meaning is involved due to the random nature of the search process). All such effects can be expected to decrease when the target words have only one meaning or encoding (e.g., Muter, in press).\rContext Shifts and Test Delays\rIt is well known that interposing a delay or new items between presentation and test de- creases both recall and recognition perfor- mance (e.g., Shepard, 1967; Strong, 1913; Un- derwood, 1957). In the SAM model two factors are mainly responsible for such decreases in performance. One factor is the storage in memory of new information that occurs dur- ing the delay. Indeed, the context cue used at test may well be more closely associated to the intervening material (items, stimuli, external\r5.0\r0\r4.0\rcc\r2 cc LU 0_\r2.0 O\r1.0\rO\rO\rLU\rDC 0.0\r3.0\rHIGH\ra,\rMEDIUM LOW\rMATCH BETWEEN ENCODINGS AT STUDY AND TEST\rFigure 16. Predictions for recognition as the match between the studied items' encodings at study and test decreases. (From left to right, b = 0.20, 0.16, and 0.12; and c = 0.15, 0.12, and 0.09. r = 4, a = 0.25, d = 0.075, v = 0.5, KmM = 30, Lmax = 8; 20-word list at 2 s/item.)\r\n28 GARY GILLUND AND RICHARD M. SHIFFRIN\revents, thoughts, etc.) than to the list items themselves, because of the closer temporal proximity of the test setting to the intervening material than to the list. On the other hand, the context cue surely includes some sort of list cue that tends to access only the list items, thereby offsetting the factor of temporal prox- imity. Regardless of the balance of effects, there is some tendency at test to sample information stored during the delay, and the tendency in- creases with delay duration. The effect of this factor is much the same as that of increased list length-—performance drops for both recall and recognition for the reasons discussed in connection with the list-length effect.\rThe second factor contributing to the delay effect is the change in context from storage to test (see Bower, 1972). Normally the test con- text is increasingly changed from the storage context, as delay increases. Remember that the a parameter combines both a storage pa- rameter and a degree of cue matching; thus it represents the degree of context similarity between study and test. As a result, the effect of context shift can be captured simply by lowering the value of a. Lowering a lowers recall directly, because recovery probabilities are less. However, lowering a has no effect on recognition, because a acts as a scale factor.\rIn an experiment varying delay, these two factors operate jointly, and respective effects are difficult to disentangle. However, some studies have altered context while holding delay constant. For example, Smith et al., (1978) presented words for study in a particular room (context). Subjects were tested for the words by recall or recognition in either the same con- text or in another room that was very different in setting than the first. The results showed decrements in recall with context change but no change in recognition performance (see also Godden & Baddeley, 1975, 1980). Similar findings have sometimes been been reported in the state-dependent learning literature (see Eich, 1980; but see also Birnbaum & Parker,\r1977). In a typical state-dependent learning task, subjects learn some material under the influence of a drug (e.g., caffeine, alcohol) and then are tested later either under the same or different drug state. It is sometimes found that performance decreases with state changes when measured by recall but remains relatively constant when measured by recognition tests.\rWe model the context-shift paradigm (with fixed delay) by letting the a parameter decrease with context shift. The results for no delay, given in Figure 17, show the expected pat- tern—recall drops but recognition is unaf- fected.\rWe model the delay situation by retaining the assumption that the a parameter drops with delay but by adding the assumption that new information is added to memory. In par- ticular, the retrieval structure is increased in size through the addition of several columns in the matrix that are meant to represent new information that has been stored. We refer to this new information as junk. Item-to-junk strengths are assigned a value ofj\\ and context- to-junk strengths are assigned a value jc. In- creases in delay are modeled by increases in the number of such junk items (i.e., columns in the matrix).\rFigure 18 shows predictions derived by let- ting a = 0.25, 0.20, and 0.15 and letting the number ofjunk columns be 0, 10, or 20, from left to right, respectively. The junk parameters werej'c = 0.1125 and j{ = 0.375. Both the decrease in the a parameter and the increase in the number of junk columns cause recall to decrease. Only the increase in the number of junk columns causes recognition to de- crease.\rEach of the patterns in Figures 17 and 18 is of course consistent with the results in the literature. The notion of junk information is actually worth a deeper analysis. In all appli- cations of SAM thus far, it has been assumed that the context cue is sufficiently precise to limit search to the list just presented (or at least it has been assumed that the contributions of nonlist information is negligibly small). For recall, such assumptions do not seem to be crucial, failing mainly because intrusions of nonlist items are not predicted. We have ex- amined the standard recall predictions (in previous papers as well as in this article) with the added assumption that several junk col- umns exist in the retrieval matrix (possibly representing events that occurred prior to the list). Recall is always lowered by this alteration, but the patterns of recall are unchanged. In fact, an adjustment of parameter values can be made to reinstate the original predictions with little change.\rFor recognition, the presence of junk col-\r\n.60\ro\rUJ .50 DC\rRECALL\ro- RECOGNITION\r5.0 r- UJ\rO\rCC\r2 DC\ro o LU cc\rLU LJJ DC LU\rLL O\rCO\r<\rCQ\rO cc CL\r.40\r.30\r.20\ro -\r4.0\r3.0\r2.0\r1.0\rRETRIEVAL MODEL FOR RECOGNITION AND RECALL 29\rumns (whether due to information occurring during a study-test delay or to information preceding the list) can make an important dif- ference. For example, the lack of effect of con- text shift shown in Figure 17 (in the absence of substantial delay) no longer obtains if junk\rcolumns are assumed; this is shown in Figure 19. Figure 19 indicates that the absence of an effect (due to context shift) occurs only when no junk is present; as the junk increases, the effect appears and gets larger. By this logic, it can be weakly argued in the context of the\r5.0\r4.0\r.60\r.50\rLU\r£ .40\r2\rLU O\rcc\r< (J\rRECALL\ra.\rRECOGNITION\rSMALL\rCONTEXT SHIFT AND DELAY\rCO\r<\rCO\rO cc o_\r2.0\r1.0\r.30\r.20\r3.0 2\rcc\rLU Q_\rO O LU CC\rSMALL\rCONTEXT SHIFT\rMEDIUM\rLARGE\rFigure 17. Predictions for context shifts between presentation and test, with delay held small and constant. (Fromlefttoright,a=0.25,0.20,and0.15.r=4,b=0.2,c=0.15,d=0.075,v=0.5;Kmm =30,Lmax = 8; 20-item list at 2 s/item.)\rMEDIUM\rLARGE\rFigure 18. Predictions for increasing amounts of delay in conjunction with increasing amounts of context shift. (From left to right, a = 0.25, 0.2, and 0.15; and number of junk items = 0, 10, and 20. r = 4, b = 0.2, c=0.\\5,d= 0.075, v = 0.5;jc = 0.1125,;, = 0.375; 20-word list at 2 s/item.)\r\n30 GARY GILLUND AND RICHARD M. SHIFFRIN\rPANEL A PANEL B\rI1i a .ii TJ '°\r1 CONTEXT SHIFT\r<£ UJ\rO C0\rltju\rSMALL A_^^ MEDIUM ( X ^ LARGE D -^\r^\"---\r<J\r^ 4.0\rLU\rCC ^^-^^\r1\rCONTEXT SHIFT\rUJ\rLU\rK .40\r^^^\r\"\"~~--TV ~\"~~O-^_\r~-—-\r~ --^^\rS\rEC\r0 3.0 e\rO\r> . 3 0\rh-\r_J\rCO O < .20 -\rCO\rOD C\r'-'-LARGE\rLARGE\r~~\"\"I U\rSMALL MEDIUM LARGE\rAMOUNT OF EXTRA-LIST INFORMATION (DELAY!\ri SMALL\ri MEDIUM\r~^^~—\r\" ^\r*>ss?== ^\\^^-y\\\r~~°\r^ ^ ^ ^ ^ ~ ~ - A - SMALL •^~~--^^p»IVlEDIUM -\rFigure 19, The model's predictions for different amounts of context shift as a function of the amount of extra-list information (delay) present at test. Panel A shows the results for recall. Panel B shows the results for recognition. (All parameters in each case are identical to Figure 18, except number of junk items = 0,\r10, and 20 from left to right; and a = 0.25, 0.20, and 0.15 from top to bottom.)\rpresent model that only a small amount of junk existsin certain types of listexperiments, because the observed effect of context shift on recognition is so small (where small amount refers either to fewjunk items or to weak junk\rstrengths).\rTest Position Effects\rIn previous recall applications we have as- sumed that learning can take place during the testing procedure. Specifically, we have as- sumed that any time a probe set successfully recovers an item from memory, the strength of association from the cues in the probe set to the recovered item is given an increment. The result of strengthening is to reduce the probability of recall of a new item as the num- ber of items previously recalled increases. This occurs because self-sampling is increased: If any of the cues that have been given an in- crement are used again, they are more to likely sample an old (previously recovered) item, and thus fewer new items are recalled (see Raaij- makers & Shiffrin, 198la, 1981b, for a more detailed discussion of learning during recall).\rThe justification for learning during recall is straightforward: Learning occurs when items are together in STS, and is especially strong if those items are attended, coded, or re- hearsed. If an image is recovered in the pres- ence of a set of cues, then that image and those\rcues are together in STS, probably under at- tended conditions. The situation in recognition is quite different. In this case we have assumed that a specific image is not recovered; instead, access is gained to the accumulated activation of all images. It might be possible to imagine that the cue strengths to all images are slightly increased, but we think that this is an im- plausible assumption and is likely to have neg- ligible effects on performance in any event.\rInstead, we propose that learning inrec- ognition testing takes a slightly different form—each item tested produces a new image in the retrieval structure with some new self- strength and some new context strength. (It is possible that these strengths could differ for items given positive and negative responses, because they might receive differential atten- tion; see Rabinowitz, Mandler, & Patterson, 1977. However, as an initial simplification the strength for positives and negatives is assumed equal.) The effect of this assumption is quite simple: Recognition testing produces an in- crease in the effective list length that applies for subsequent test items. The effect is much the same as that seen when list length increases (Figure 12), or when junk items are added to a list due to delay of test (Figure 18). Perfor- mance is predicted to decrease across test po- sitions.\rSimulation predictions are shown in Figure 20. Self-strength is not relevant when items\rDC UJ\r^ 2.0\rg 1 . 0 - o\rLU\rDELAY\r(AMOUNT OF EXTRA-LIST INFORMATION)\r\nLU O 2\rCC\r3.5\ro\rDC 1234 Q.\rTEST POSITION BY BLOCK\rFigure 21. Probability of correct rejections and hits for 24-item (short) lists and 240-item (long) lists, averaged across quarters of the 48- or 480-item test lists. (Arithmetic intervened between presentation and test.)\rThe probability of hits, and of correct rejec- tions, averaged over quarters of the test list, is shown in Figure 21, for both long (240 items) and short (24 items) lists.\rObviously the effects are not very large, ap- pearing only in the case of hits for the long list, and showing only about a 9% falloff in that case. (Other partitions of test positions gave rise to similar results.) It seems likely that an effect of this magnitude could be handled by assuming that a small amount of learning takes place during testing, primarily for pos- itive responses.12\rDistractor Type and Similarity\rIt is well known that the choice of distractor affects recognition performance (e.g., Anisfeld\r12\rTodres and Watkins (1981) found a small decrement in recognition performance for categorized items when subjects were given additional exposure to some of the presented or nonpresented items before test (as compared to subjects not receiving such experience). Neely, Schmidt, and Roediger (1983) showed that six successive test items from a category slowed reaction time to another test item from that category (compared to two successive test items before the critical item). In both of these cases, learning during retrieval or during an intervening task could account for the findings, although a quantitative demonstration of this claim would require a simulation of the particular procedures in these studies.\r£ 3.0\rRETRIEV AL MODEL FOR\rRECOGNITION\rAND\rRECALL\r31\rare not retested. Therefore only one parameter is needed; The context strength for the new images of tested items was set to 0.5. The changes in d' across quadrants of the test list (all 20 targets plus 20 distractors, mixed) is calculated by comparing hit rates and false alarm rates for items in similar test positions, with the results shown in the Figure 20.\rDo such predictions correspond to the ob- served data? It is difficult to answer this ques- tion on the basis of previous research. Many studies report decreasing performance with test position (usually confounded with lag effects for targets), but the situations are ones in which short-term memory contributions to perfor- mance are not controlled or eliminated (e.g., Murdock & Anderson, 1975; Ratcliff& Mur- dock, 1976). It seems sensible that an item still in STS when tested will give rise to an extremely rapid and accurate yes response. Because the probability of this event decreases with test position, exaggerated position effects are to be expected. (Furthermore, the common practice of restricting consideration to high confidence responses only accentuates this problem.) We obtained test position effects from an experiment in which intervening arithmetic was used to clear STS before testing.\rLU\rCO\r•z. 1.00 o\rQ.\rCO\rLU\rOC .90\ru\rLU\rQC .80 OC\rO O\rLL\ro\r.60\r00\r<\r00\r^SHORT CR SHORT HIT\rLONG CR\rHIT\rCC LU Q.\r2.5\rO O LU CC\rFigure 20. Predictions for recognition performance in a 20-item list (40-item test list) as a function of test position (blocked in groups of 10 items) when each test causes storage of a new image with total context strength (incre- ment) = 0.5, and residual interitem strengths (d) of 0.075. (v = 0.5, r = 4, a = 0.25, b = 0.20, c = 0.15, d = 0.075; 2 s/item.)\rOUTPUT POSITION BY BLOCKS\r.70\r\n32 GARY GILLUND AND RICHARD M. SHIFFRIN\r& Knapp, 1968; Hall, 1979; Underwood & Freund, 1968; Walter, 1977). The point seems clear: If a distractor is highly confusable with the list items, discriminations arc difficult. An interesting question concerns the nature of the relationship necessary to produce such an ef- fect: Must the distractor be similar to many (or all) of the list items, or may the distractor be similar to just one list item? Our model suggests that a very large decrement occurs when the distractor is similar to a large pro- portion of the list items, because familiarity is a sum of familiarity values over all of the list items. For example, if a study list consisted of flower names and the test list consisted of all flower names (targets mixed with distrac- tors), surely performance would be lower than in the case where all of the distractors were animal names. Evidence that this actually oc- curs is difficult to find, perhaps because ex- perimenters believe the outcome is so obvious that experiments have not been done. There is evidence from reaction time studies that noncategory distractors are rejected faster than category distractors (distractors from the same category as the presented items; Herrmann, McLaughlin, & Nelson, 1975; Homa, 1973; Okada & Burrows, 1973).\rOf greater interest for some purposes is the effect of familiarity of a distractor to a single, matched list item (see Anisfeld & Knapp 1968; Underwood & Freund, 1968). Our model pre- dicts an effect of distractor similarity on the following basis: The residual interitem strength between the similar distractor and the image of the matched list item will be higher than the usual residual value. If this value is called d&, then we suggest that ds is greater than d to the degree that the similarity between the items is high. Let us suppose, moreover, that the similarity of a distractor to a list item does not cause that distractor to share in the as- sociative linkages that had been formed during study between the similar list item and other\rjointly rehearsed list items. That is, ds for the distractor would apply only to the single, sim- ilar list item. The predictions under these as- sumptions are depicted in Figure 22.\rThe lower curve in Figure 22 shows a sharp performance drop when similarity of the dis- tractors to all of the list items is increased. In this case the value of d is increased from left to right (d = 0.075, 0.1, and 0.125), but oth-\rerwise the parameters remain the same in the previous model applications. The upper curve shows a much smaller decrease when distractor similarity to a single list item is increased. In this case, all parameters remain the same as for previous applications, but for just a single list item, the value of ds is increased from left to right (d, = 0.75, 0.1, and 0.125). For these predictions, d' is calculated from hit rates for the list item with the similar distractor, and false alarm rates for the similar distractor.\rData concerning similarity effects are somewhat mixed. Some studies report that similarity to a matched list item reduces per- formance (e.g., Anisfeld & Knapp, 1968; Un- derwood & Freund, 1968), but others do not (at least not for some related distractors or under some encoding instructions—Buschke & Lenon, 1969; Cramer & Eagle, 1972; Elias &Perfetti, 1973; Underwood & Freund, 1968). Furthermore, it is often the case that physical similarity (phonemic or graphemic) produces more false alarms (and lower performance) than does semantic similarity (Cramer & Ea- gle, 1972; see also Davies & Cubbage, 1976; Eagle & Ortof, 1967; Juola et al., 1971). We have carried out several studies of our own on this subject, with mixed results. Although one study showed associative similarity to increase false alarms, the others did not. A typical result is shown in Figure 4: Synonyms did not lead to increased false alarms, although phonemic and graphemic similarity did. A similar result was obtained in another study for associative relatedness that was not based on synonyms.\rThese results are generally consistent with the model's predictions that distractor simi- larity to a single list item should produce small performance decrements. Nevertheless, it is not easy for the model to predict that semantic similarity produces smaller effects than phys- ical similarity or that semantic similarity sometimes produces no increase in false alarms at all. A possible explanation for such findings is discussed in Section 5.\r3. Word-Frequency Effects\rThe predictions in Section 2 make it clear that our model for recognition and recall has the potential to predict many of the observed findingsin the literature that involve accuracy of performance. In the present section we ad-\r\nRETRIEV AL\rMODEL FOR\rRECOGNITION\rAND RECALL\r3 3\r5.0\rO\rZ 4.0 <\rDC O\rDC HI CL\r2.0\r1.0\r8\rLJU DC\r_ -—o\rINDIVIDUAL -\rOVERALL\r3.0\rLOW\rDEGREE OF SIMILARITY\rMEDIUM\rLARGE\rFigure 22. Predictions for recognition when the similarity between distractors and targets is varied. (The top curve shows the predictions for the case when a distractor is similar to only a single-list item [d, = 0.075, 0.1, and 0.125 from left to right, all other d - 0.075]. The lower curve shows the predictions when distractors are related to all items, [d = 0.075, 0.1, and 0.125 from left to right; r = 4, a ~ 0.25, b = 0.2, c = 0.15, v = 0.5; 20-item list at 2 s/item].)\rdress the issue of word-frequency effects in some detail. We present the main findings on word-frequency effects and show how our model predicts the observed patterns of results. We then present new experiments on the word- frequency effect and give a quantitative fit of the model to the findings. Finally, we discuss some additional findings on word-frequency effects and related topics.\rThe Word-Frequency Effect\rWord frequency, or more precisely, natural-\rlanguage word frequency refers to the relative\rfrequency of occurrence of words in normal 13\rusage. Words like boy, chair, and blue are words that occur with high frequency (HF) in the language whereas words like tandem, oce- lot, and pagoda, occur with low frequency (LF). The interesting property of HF and of LF words for our present discussion is that they influence recall and recognition quite dif- ferently (see Gregg, 1976, for a review). The most commonly cited finding with regard to frequency effects is that subjects recall HF words better than LF words (e.g., Deese, 1960; Hall, 1954; Sumby, 1963) but recognize LF words better than HF words (e.g., Gorman, 1961; McCormack & Swenson, 1972; Schul- man & Lovelace, 1970). This effect is found\rwhen pure lists are studied. A pure list consists of either all HF or all LF words. The advantage of LF words in recognition is largely unaffected by mixing HF and LF words in a single study list (Allen & Carton, 1968; Carton & Allen, 1968; Gregg, 1976;Schulman, 1967;Shepard, 1967). However, when mixing is used, the standard finding that HF words are better re- called no longer obtains: Although the results are somewhat variable, recall is close to equal for the two types of words. In most studies, LF words are recalled as well as, or even slightly better than, HF words (Duncan, 1974; Gregg, 1976; May & Tryk, 1970), but Balota and Neely (1980) found an advantage for HF words. Gregg, Montgomery, and Castano (1980) extended these recall findings by study- ing both pure and mixed lists (in different groups); to the standard conditions they added two conditions in which a distracting task was used during presentation to eliminate inter- item coding. The standard findings in the nor- mal pure and mixed lists were replicated. With distraction, recall levels went down, and in\r13\rWe hope that there is no confusion between the word- frequency effects discussed here and the frequency-dis- crimination paradigm in which episodic frequency of pre- sentation in a given list is manipulated.\r\n34 GARY GILLUND AND RICHARD M. SHIFFRIN\rpure lists the advantage of HF words was re- duced.\rWhen considering the word-frequency effect for recognition, we note that subjects show a frequency effect for distractors as well as for targets. The correct-rejection rate for LF dis- tractors is higher than the correct-rejection rate for HF distractors (McCormack & Swcnson,\r1972). Even when distractors are not used, however, frequency effects are found. Wallace, Sawyer, and Robertson (1978) presented sub-\rjects with cither HF or LF words and then gave a recognition test that contained no dis- tractors (only the target items were tested). Subjects were informed of this fact and were directed to respond positively to words that they actually remembered seeing on the list. Subjects showed better recognition for LF words than for HF words.\rthe time for episodic coding and rehearsal, and self-coding is based on time for rehearsal of interitem features, whereas interitem and residual interitem strengths are based, at least in part, on extraexperimental semantic factors that are sensitive to word frequency.\rWe assume that two criteria are selected by the subject, one for HF words and one for LF words. This assumption is sensible for pure lists, but we assume it holds for mixed lists and mixed tests, as well. Such an assumption could be justified on the basis that subjects can ascertain the word frequency of the tested word and can use this knowledge to set a cri- terion.\rThe predictions of the simulation model are\rshown in Figures 23 and 24. The bn and dH\rparameters were set equal to the b and d pa-\rPredictions for the Word-Frequency\rEffect\rThe model we use to deal with frequency effects is identical with that presented in Sec- tions 1 and 2, with one slight addition to take word frequency into account. We assume that the tendency for HF items to access images in memory is larger than that for LF items. That is, the retrieval strength between a HF cue and any image is higher than that between a LF cue and any image. This assumption is instantiated in two ways: First, the residual association between any two items not re- hearsed together, or between any distractor and any list item, is made frequency dependent. Thus when a HF word is a cue, the value of the d parameter is set equal to dH; when a LF item is a cue, d = dL; du > dL. (The frequency of the image in memory is not taken into ac- count in this approach.) Similarly, for words that are rehearsed together, it is assumed that HF cues are stronger than LF cues. Thus we setb- buforaHFwordandb=bL fora LF word; £H > bL. In short, HF words are assumed to be stronger cues than are LF words.\rIn our approach to word frequency, we make the assumption that the a and c parameters are not frequency dependent. Thus the strengths to context and to a word's self- strength (tendency to sample a word's own image) are assumed equal for HF and LF words. These assumptions can be justified by the reasoning that context coding is based on\r0-2; 0.075). For LF words we set bL equal to 0.1 and t/L equal to 0.035. The remaining pa-\rrameters, a, c, r, and v are assumed to be constant for both HF and LF words and were set equal to the same values used in earlier simulations (a = 0.25; c = 0.15; r = 4; v = 0.5). The HF criterion was set to 1.3 and the LF criterion to 0.66. Figure 23 shows the model's predictions for uniform frequency lists. Recognition performance is higher for LF words, whereas recall performance is higher for HF words. Figure 24 shows the results for mixed lists. It is assumed that the test list con- tains both HF and LF targets and HF and LF distractors. We follow the usual procedure in such cases by calculating d' from a comparison of HF hits to HF false alarms or from a com- parison of LF hits to LF false alarms. Rec- ognition performance shows the same pattern seen in the pure lists: LF words show better recognition than do HF words. Note that recall in mixed lists is predicted to be equal for LF and HF words.\rThe model predicts a pure-list HF advantage for recall for two reasons. First, a given word (of any frequency) has a higher probability of recovery if it is sampled by a HF word as a cue than if it is sampled by a LF word, due, to the higher strength values. In a pure HF list, only HF words are used as cues, of ne- cessity, so recall is superior to that in a pure LF list, where all cues are LF items, of ne- cessity. Second, equal c values for HF and LF words cause more self-sampling for pure lists\rrameters used in earlier simulations (6H\rt/n\r=\r=\r\nUJ\rCC\rSE -35\r> .30\r5 < CO o CC DL\r3.0 o RECALL LL CC LU\ra.\rO h-\rO O\rCO\ro\rDC Q.\r.25\rRETRIEVAL MODEL FOR RECOGNITION AND RECALL\r35\r.45\rLLJ oO\r.40\rP\rRECOGNITION - 3.5\r.25\r2.5\r2.0\rHIGH W O R D\rLOW FREQUENCY\rFigure 23. The model's predictions for recall and recognition as a function of word frequency for uniform frequency lists (30 words total), (r = 4, a = 0.25, b,, = 0.2, />L = 0.1, c = 0.15, dH = 0.075, rfL = 0.035, v = 5;/Cm,*=30,Lmm - 8;2s/item.)\rof LF words because the interitem strengths are lower for LF cues. Self-sampling reduces recall because sampling the cue itself, which has just been recalled, is of no use and reduces the opportunities for recovering a new word.\rIn mixed lists, however, a HF word is just as likely to sample a LF word as a HF word and then either type is equally likely to be\ro\r-40\rUJ\rUJ\roc ,K U_ .35\rO\r> .30\rrecovered. Similarly, when a LF word is used as a cue, it is equally likely to sample either a HF or a LF word, and they have the same probability of recovery (although this is lower than the probability of recovery when a HF word is used as a cue). As a result HF and LF words are sampled and recovered equally often and, hence, used as cues equally often, pro-\r4.0\rRECOGNITION - 3.5 RECALL\r3.0\r2.5\r2.0\rHIGH\rWORD FREQUENCY\rFigure 24. The model's predictions for recall and recognition as a function of word frequency, for mixed- frequency lists (15 high frequency and 15 low-frequency words), (r = 4, a = 0.25, blt = 0.2, bi = 0.1, c = 0.15, da - 0.075, rfL = 0.035, v = 0.5; Kmm = 30, Z.max = 8; 2 s/item).\rLOW\r\n36\rGARY GILLUND AND RICHARD M. SHIFFRIN\r.ou—\r.48\r.46\r.44—\r.42\rAC\\\ri\rAA\rA 19\rA 20\r# HF WORDS ii\rWORD FREQUENCY\r- \"' |\r20 A\r1CA\r19 A\rO A A A_\rFigure 25. Predictions for free recall of a 20-item list as the composition of the list changes from being one of all high-frequency words to one of all low-frequency words. (Parameters are equal to those in Figures 23 and 24).\rducing equal recall. (Due to self-sampling, more failures take place whenever LF words are cues, but this does not cause any differ- ential recall of HF and LF items).\rThese somewhat mysterious recall predic- tions may be clarified by considering how recall probability changes with the proportion of HF words in a list. Figure 25 gives the relevant predictions, as the number of HF items in a 20-item list is varied from 0 to 20. In any list, recall probability is equal for HF and LF words, but the proportion of HF words de- termines the level of recall for both types.\rThe model predicts recognition superiority for LF words because the self-strength param- eter, c, is equal for HF and LF items. Because the residual interitem parameters are lower for LF items, the increase from distractor to target distributions is larger for LF items, rel- ative to the corresponding variances of the distributions. (Higher values of/JH than bL ac- tually help the HF items more than the LF items, but for most reasonable combinations of parameter values, the net effect of the b\rthat cause LF superiority.) Note that the pre- dictions are identical for mixed lists and for pure lists, because the distribution shapes and placements are determined by the frequency of the test item rather than the frequency com- position of the list.\rAn Experiment on Word Frequency and Test Type\rMost of the experiments we have discussed on word frequency have examined recall or recognition, but not both (although see, e.g., Balota & Neely, 1980; Neely & Balota, 1981). It is thus possible that some of the differences that exist between recall performance and rec- ognition performance are due to encoding dif- ferences for recall and recognition. It may be that subjects who study for recall tests tend to interrelate items and that subjects who study for recognition attend to structural features of the word as Tversky (1973) and Mandler (1980), among others, have suggested. To draw proper comparisons between recall and rec- ognition, therefore, it seems important to hold back knowledge concerning whether a recall or a recognition test is given until after list presentation. This is especially important in cases where the data are to be fit by a quan-\r14\rAll of the predictions in this paper concerning word frequency would be obtained qualitatively if only the d parameter (and not b) varied with frequency. When we fit such a model quantitatively, we found that in order to predict a large enough pure list recall effect, so large a difference between dv and rfL was needed that recognition predictions were seriously in error (and vice versa). How- ever, we are reluctant to conclude that the difference be- tween bH and bi. is essential, without additional analysis and research.\rparameters is outweighed by the other factors 14\r# LF WORDS\rPROBABILITY OF FREE RECALL\r\nRETRIEV AL MODEL FOR RECOGNITION AND RECALL 37\rtitative model. In the present experiment sub- jects received several different types of tests (both recall and recognition) but were not in- formed prior to the test which type they would receive. Presumably, subjects studied all lists in the same fashion. This presumption has the useful property that all encoding parameters in our simulations may be held equal for recall and recognition tests. This provides a good test for our retrieval assumptions and for the model's ability to handle recall and recognition together.\rIn the present experiment, we wished to extend the word-frequency findings to different types of presentation modes and to different types of tests than are normally used in fre- quency experiments. Specifically, in each list, we presented subjects with 16 pairs of words to study. The pairs consisted of either HF- HF, HF-LF, LF-HF, or LF-LF word pairs. Subjects were told to associate the words of a pair together. We hoped that such a procedure and instructions would give us better control of the encoding operations used by the subjects. Each list was followed by an arithmetic task to clear short-term memory.\rFive different types of memory tests were given: (a) a single-recognition test where one item of each pair was tested as a target along with an equal number of never-presented dis- tractors, (b) a pair-recognition test in which targets consisted of both members of a pre- sented pair (tested in the order they were pre- sented) and in which distractors consisted of two items never presented, (c) a cued-recog- nition test where one member of each pair was presented as a cue (and so indicated) along with either its original member or a never- presented distractor, (d) a free-recall test, and (e) a cued-recall test where subjects were given one member of each pair as a cue and were asked to recall the corresponding members.\rWe hoped that such a rich array of test types carried out within subjects would provide a powerful-enough data base to allow us to fit our model quantitatively and to provide a fair test of the model's ability to handle both recall and recognition. The details of the experi- mental procedures are given in Appendix B.\rResults and Discussion\rThere was no difference in performance for items presented as the first member of a pair\ror for items presented as the second item of a pair. This is usually the case when neither word is designated as the cue during study and when, in cued tests, either member may serve as the cue (see Raaijmaker & Shiffrin, 198 la). Therefore, all analyses were completed by summing over position within pair.\rThe primary results can be seen in Figure 26. The probability of correct recall is plotted for the cued data and for the free-recall data. The hit rate minus the false alarm rate is plot- ted for the recognition tests. This measure was used because it takes into account the hit rate and the false alarm rate simultaneously, much like d'. However, d' is sensitive to the as- sumptions of normality and homogeneity of variances (and cannot be computed when ei- ther the hit rate is 1 or the false alarm rate is 0). We have analyzed the recognition data for hit rates minus false alarm rates, for hit rates and false alarm rates separately, and for d'. The patterns of the data are the same for all methods of analysis. In fact the model is fit to the hit rates and to the false alarm rates separately, as can be seen in the upcoming figures.\rThe main findings are as follows. Cued-re- call performance is higher than free-recall per- formance, but in neither case is there a sizeable effect of frequency on performance. In the rec- ognition results the most obvious findings show that performance is higher for LF than for HF words and that paired recognition is superior to single recognition, which is superior to cued recognition. The details of the statistical anal- yses are given in Appendix C.\rIn general, the results accord well with the existing data. Recall performance showed little effect of frequency, which is consistent with the finding that in mixed lists the advantage of HF words over LF words is greatly reduced or eliminated (Duncan, 1974; Gregg et al.,\r1980; May & Tryk, 1970). The findings of LF advantages in recognition are certainly con- sistent with the literature, as is discussed at the start of Section 3.\rA Quantitative Model for Frequency for Different Test Types\rEffects\rThe model, with a few exceptions, is iden- tical to the model outlined before in this sec- tion. However, some special assumptions are made for the different presentation lists and\r\n38\rGARY GILLUND AND RICHARD M. SHIFFRIN\r|\r<.35 O\rUJ\r/'x\r'' SINGLE - .70 S\rRECALL\r.HU, ! ! , ,\rRECOGNITION\r/TC fi.' / /\"^\r.ou\rLU .75 <\r°- .30 f-\rh- /iDC\r0\rLU O/Ks tr- QC\r. . A\r1i ^\"yn\r.6050 ^\r<5>\\ CUE U1i_l\r8.20\r^^v\\\r\"o^-o^-o _\rLL\rO .15\r< .05 - _, CQ\r- H // ^\"\\-<5/\r\\ /\r.55 \"-\rLU .50 H\rDC\r.45 , 1—\r£\rH -10\rX ^--^^-^X\rFREE m-b\r0 iiii iiiiI QC .00 .40\rH-H H-L L-H L-L H-H H-L L-H L-L FREQUENCY OF WORDS IN PAIRS\rFigure 26. Data from the five test conditions of the experiment reported in Section 3. (The left panel gives the recall results and the right panel gives the recognition results, both as a function of word frequency. In mixed pairs, the underlined letter designates whether performance is being measured for the high-frequency (H) or the low-frequency (L) member.)\rthe different types of tests. Specifically, we as- sume that subjects only rehearse items that are presented as pairs at any one time—they never rehearse items from different pairs to- gether. Thus the buffer-storage assumption is gone (as is the r parameter). We simply assume that each presentation of a pair produces a total context strength a, for each item in the pair and a total self-strength c, for each item in the pair. We assume that the strength of association of a HF word to its paired member (whetheritbeaHFwordoraLFword)isbH and from a LF word to its paired member is &L. We assume that the residual associations between items that are not presented together as pairs follow the usual assumption, giving parameters dw and d^. Finally, we assume that the context and self-strengths are not only equal to each other but are equal for HF and LF words (a = c, equal for both HF and LF words).\rFree recall is modeled in the same manner as was discussed earlier. The subject is assumed to begin search with context alone as a cue, and if an item is recovered then the item and context cues are combined and used until a new item is recovered or until Lmax failures accrue. The entire process continues until Kmm total failures are counted. Kmm was set to 30 as it has been in all simulations and in many\rof the previous applications (Raaij makers & Shiffrin, 1980; 1981b).\rFor cued recall the subject is assumed always to use both context and the cue given by the experimenter to probe memory. The subject is assumed to use the cue until Afmax failures accumulate. There are no other stopping rules in the cued-recall model. (Once the paired member is recovered, it is assumed that the subject correctly realizes that that item had been paired with the cue. Furthermore, in our simple model, no provision for intrusions is made. Natural and very minor extensions of the model could be made that would deal with intrusions, but we prefer to keep the assump- tions as simple as possible.)\rThe recognition models we propose are very similar to those used to generate predictions earlier in this section. However, for the paired- and cued-recognition conditions some as- sumptions must be made concerning how the two words available at test are combined to form a familiarity value. One might assume that the two words at test are independent and thus both could be used with context as a three-cue probe. Unfortunately, for any pa- rameter values that fit the other conditions, such a model results in predicted paired-rec- ognition performance that is far higher than that actually observed. The problem lies in\r/\rA\r// CUE\r// LU /' .60U3\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 39\rthe fact that two items that make up a pair have a strong interitem strength, b, as well as strong self-strengths, c, and context strengths, a. When the two items of the pair are used together as cues, the cross product terms be- tween the two items increase familiarity dis- proportionately.\rThe solution we suggest to this problem in- volves fractional weights assigned to multiple cues. In the present case, we decided to pre- serve a special role for the context cue and assign it always a weight of 1.0. We then de- cided to limit the total weights, and attentional capacity, by letting all the remaining cues share a weight of 1.0. That is, the weights assigned to all cues other than context were made to sum to 1.0.\rUnder these assumptions, the modified rec- ognition model is straightforward. Single-item recognition is handled just as in the simulation. Paired recognition is handled by assuming that the subject probes memory with three cues, context with a weight of 1.0, and the two items, each with weights of 0.5. Cued recognition is handled by assuming that the subject weights the test item very heavily and the cue item less heavily. In particular, the cue weight is represented by a parameter, Wc, and then the item weight is 1.0 —Wc.\rNote that the recall and single-recognition models need not be modified at all by these weighting assumptions. Because at most two cues are used in any probe during free or cued recall or during single recognition, the weights will still be 1.0 even under the new assump- tions.\rThe model was fit to the hit rates and the false alarm rates in the various recognition conditions, and to the recall probabilities, by choosing parameter values that minimized the sum of squared deviations between the ob- served and predicted values. Actually, the time needed to produce predictions by Monte Carlo methods was too large to carry out a complete search of the parameter space, so certain sim- plifications were adopted. First, we arbitrarily set v = 0.5, to match the value used in the previous simulations. (Actually, the model seems fairly insensitive to the value of the v parameter over small ranges because fits about as good were obtained with v = 0.25 and v = 0.75, although other parameter values, of course, had to be adjusted accordingly.) The\rrecall-stopping parameters were set to values estimated in previous research (Raaijmakers & Shiffrin, 1980, 1981): KmM = 30; Lmax = 3. Also, following previous work, we set a = c. The remaining parameters were estimated, withthefollowingresults:a=0.16,ba =0.27, bL = 0.19, dH = 0.053, dL = 0.0262; Mmax = 10; and Wc = 0.1. The recognition criteria were set individually for each condition and the values are given in the figure legends.\rThe number of parameters may seem ex- cessive, even considering the different tasks and variety of conditions that are being pre- dicted, but we do not feel that this is a proper assessment. First of all, the parameters used here are essentially the parameters already used by Raaijmakers and Shiffrin (1980) to predict a wide variety of recall data. A few additional parameters are needed to handle the different tasks and conditions, but the additions are quite natural: high and low values of the b and d parameters to take word frequency into account; Mmax to replace KmeM and Lmax for the stopping rule for cued recall; and Wc to govern the weighting of the cue in cued rec- ognition. Second, the values of the estimated parameters are close to the estimates obtained in previous fits to recall data and are about what one might expect given the new stimuli and tasks in the present study.\rThe predictions of the model for the pa- rameter values indicated above are given along with the observed data in Figures 27, 28, 29, and 30. A few words of explanation are needed to explain the genesis of the predicted effects.\rFor free recall, for reasons similar to those discussed earlier, not much effect of frequency is predicted—this is typical of predictions for mixed lists. Actually, there are some slight pre- dicted differences that result from trade-offs between sampling and recovery probabilities when one member of a pair has been recovered and is being used as a cue. In such a case, slight differences in recallability of the paired member occurs depending on the frequency of the cue.\rFor cued recall, the frequency differences are somewhat magnified because all samples from memory are made with the cue of spec- ified frequency. Even here, however, the dif- ferences are small. Thus for mixed lists of pairs of items varying in frequency composition, only very small differences are predicted in\r\n40\rGARY GILLUND AND RICHARD M. SHIFFRIN\rFREE - RECALL _j.40 11r1\rCUED - RECALL —i—i—i—;\rO .35 LU\rCC\r,_ .3 0\r-O OBSERVED --K PREDICTED\r_J\r.35 o LU CC\r.30 |_\rOO LJJ\rCC.25: : .25DC CC CC\rO .20 \" K -*L Li-\rO \".20 O\rLU . .15 °\rh- .10 IJ\r0 .15-\r^ ^ ^ S\r13 .10\r_ v^S^*^* _\r_\rm CD\rCO .05 .05 CO\r0O cc DC\rnnn I ' l l nnn FREQUENCY OF ITEMS IN PAIRS\rFigure 27. Predictions and data for cued and free recall from the experiment of Section 3. (a = 0.16, b,, 0.27,b,.=0.19,c=0.16,du =0.053,d,.=0.0262,v=0.5;Kmm =30,Lmm =3,MmaK =10.)\rboth free and cued recall. Such predictions conform quite well to the findings.\rFor single recognition, substantial effects of frequency are predicted. It is interesting that the frequency of the tested item, rather than the frequency of the paired member, deter- mines performance. This is not surprising when one realizes that the tested item is the cue used to probe memory, so that itsfre- quency is the determining factor according to\rthe model. LF items give superior performance for essentially the reasons mentioned earlier in this section: The self-strength is higher rel- ative to the residuals for LF items, because the c parameter does not change with f r e - quency, but dH > dL (this effect being much larger than an opposing factor; because bH > Z?L, a HF test item gains some strength due to its extra interitem strength to its paired member).\r1.0\rLU\rSINGLE ITEM RECOGNITION\r—O OBSERVED\rP R E D I C T E D\r< 0.9\rCC - - K\r^ 0.8 0.7\rLU\r< 0.3 CC\r0.2 CC\r0.0 LL\rH-H H-L L-H L-L\rFREQUENCY OF ITEMS\rIN P AIRS\rFigure 28. Predictions for the single-item recognition test. (Parameters are equal to those in Figure 27. The criterion for high-frequency (HF) items is 0.295 and for low-frequency (LF) items is 0.150.)\r\nP 0.9 GC\rH 0.8 I\r0.7 -\rLLJ\r„\r3\r<°'\rCC\r;=» 0.2 -\r—O OBSERVED\r~-X PREDICTED „\r^**~\"^ 5t-~ ^^^ ^-^\rx-'\"\rQ — — ^ \" ^\r-V-*^^\" ^\rCC\r1 f\\«\rt i=\r—; U.1 111\rw~^=—__~ Q\rRETRIEV AL MODEL FOR RECOGNITION AND RECALL 4 1\rPAIR RECOGNITION |W=.5I I.U 1 1\r^ ^ ^ ^•*~*\rrnnn 1 1 1 1\rLL FREQUENCY OF ITEMS IN PAIRS\rFigure 29. Predictions for the paired-recognition test using a shared-weight model for multiple cues. (The weights for the two test items are W —0.5. Other parameters are equal to those in Figure 27. The criterion for HF-HF pairs is 0.285, for HF-LF or LF-HF pairs is 0.205, and for LF-LF pairs is 0.146. HF = high frequency; LF = low frequency.)\rFor pair recognition, the predicted effects are slightly different, because the strengths for each cue member, at their respective frequen- cies, are combined multiplicatively and then are summed. In effect, this produces an average of two different frequency effects in mixed- frequency pairs. Paired performance is pre-\rdicted to be somewhat higher than single per- formance because the product rule for the two cues increases the signal-to-noise ratio, due to the interitem strength between members of a studied pair. In fact, the predicted improve- ment would be far larger were it not for the weights of 0.5 assigned to the two item cues.\r1.0\r£ 0.9\r,_ 0.8\rX\r0.7\rLU\rh- 0.3 <\rCC\r.5 0.2 CC\r^°-1\rLU\r(/) 0.0\r—O OBSERVED --X PREDICTED\rCUED RECOGNITION (WCUE =.1, \\A/TEST =.9\rH-H H-L\rL-H L-L\rFREQUENCY OF ITEMS IN PAIRS\rFigure 30. Predictions for the cued-recognition test, using a shared-weight model for multiple cues. (The weights are Wcue =0.1, WKS{ = 0.9. The other parameters are equal to those in Figure 27. The criterion for the pairs as indicated from left to right are 0.292, 0.270, 0.160, and 0.150.)\r\n42 GARY GILLUND AND RICHARD M. SHIFFRIN\rIt should be noted that quite different predic- tions would result if the distractor pairs con- tained either one or two items that had been presented on the list. Then performance would be lowered. This can be seen, in part, by ex- amining the predictions for the cued-recog- nition case.\rFor cued recognition, the cued member is always from the list, whether a distractor or a target is tested. Therefore, its cue effect is com- mon to both targets and distractors, so that discrimination of targets from distractors is not helped. In fact, the effect is to add noise to both target and distractor distributions, thereby reducing cued-recognition perfor- mance relative to the single-recognition case.\rOne might ask why subjects would assign any weight at all to the cue if this would lower performance. There are three plausible an- swers. First, the subjects are instructed that the cue item is present as a potential aid. Sec- ond, they may not realize that assigning any attention to the cue will harm performance. Third, the process by which the cue is assigned attention (and weight) may be automatic, oc- curring without conscious choice by the sub- ject, simply as a byproduct of the subject's attempt to utilize the cue in some fashion.\rNote that the predictions are not as close to the cued and paired data as to the single- recognition data. It may be that our method of weighting cues does not correspond exactly to the manner in which subjects assign their attention.\rDiscussion\rCue Weights\rThe paired-recognition condition provides the first evidence pointing to the need for shar- ing of cue weights. If weights are set equal to\r1.0, then the use of context plus both test items as cues produces predictions of performance levels that are much too high. In particular, for parameter values that allow the single-rec- ognition data to be predicted, paired-recog- nition predictions are essentiallyat ceiling.We are not arguing that the present results provide definitive empirical evidence in favor of shared cue weights. It is clear that models exist that can predict the paired- and cued-recognition data even if weights are all 1.0. For example,\rwe have been able to show this to be the case for a model in which separate familiarity val- ues are calculated for each of the test items and then are added together before a decision is made. This model and others like it, however, do not really resolve the basic problem: If cue weights are 1.0 then subjects can always im- prove their paired-recognition performance to ceiling by utilizing both test items as cues in the probe set. Because this option would always be available according to the logic of the SAM model, and because the expected results are not found, it seems reasonable to conclude that the model is wrong and that shared weights are used for multiple item cues.\rSuch reasoning is made even more plausible when generalized for multiple cues in recog- nition. Imagine a list of single items followed by a typical yes-no test. The model assumes that the subject probes memory with the test item and context as cues. Suppose, however, that the subject adds extra cues to the probe set, cues that are closely related to the test item. For example, if \"table\" is the test item, the subject might choose a probe set consisting of \"context,\" \"table,\" \"food,\" \"eating,\" \"fur- niture,\" \"chair,\" and so forth. Each of these related cues is only residually associated to list items other than \"table,\" but is more strongly associated to \"table\" than to these other items. Thus the product rule for multiple cues insures that the discriminability increases with the number of these added cues. In effect, the fa- miliarity assigned to \"table\" by the product rule, assuming \"table\" is on the list, becomes increasingly independent of the other items on the list, and performance continues to increase to ceiling, as the number of related cues in- creases.\rA general solution to problems of this sort involves limits on cue usage that are produced by appropriate choices of weights. One pos- sibility is a limit on the number of cues that can be utilized (perhaps due to a capacity limit on short-term memory). In this case, when the limit is exceeded, all cue weights for ad- ditional cues would be zero. A second possi- bility is to assign cue weights to new cues in accord with the new information they provide that is not already encapsulated in previous cues. In this case, the weights assigned to new cues continues to decrease as related cues are added. A final possibility is an attention limit\r\nRETRIEV AL MODEL FOR RECOGNITION AND RECALL 4 3\ron cue utilization. In this approach, all cue weights sum to no more than some constant (1.0 in a pure attention sharing approach). If the cue weights sum to 1.0, then adding cues generally does not help and will hurt if the added cues are less strongly associated to the target image than the target name itself, which is usually the case. In fact, in a model of this type, it is best to use just the cues that are most strongly associated to the target and least strongly linked to other list items. Usually the best cue is the test item itself.\rThis line of reasoning and the results of the present experiment convince us that some ap- propriate cue-weighting scheme is needed. Note that none of the qualitative predictions shown in Section 2 are altered by the use of fractional cue weights. Thus, although the use of fractional weights seems highly desirable, it was not necessary to adopt such an approach until the experimental results of the present section were encountered. Finally, although it seems clear that a shared-weight assumption is needed to handle the experimental results, it is probably the case that a variety of sharing rules could handle the present findings quite well, so that additional research is needed be- fore firm conclusions can be reached.\rFrequency-Dependent Cue Strengths\rThe SAM model explicates many results concerning word frequency by positing higher cue strengths to other LTS images for HF words than for LF words; in other words, the b and d parameters are higher for HF words than for LF words, although the a and c pa- rameters do not vary with word frequency. This approach to word-frequency effects is far from novel; similar approaches, albeit stated in somewhat different terms, have been sug- gested or mentioned by Deese (1960), Gregg (1976), Sumby (1963), and Earhard (1982), to name just a few.\rThe Earhard (1982) results are particularly interesting. He set out to test the notion that HF words have more meanings available at study and test, so the chances of identical en- coding at these two times would be lower; without identical encoding, performance would be worse. Although this hypothesis seems compelling, in the sense that different encodings would surely reduce performance,\rand in the sense that HF words surely have more meanings, the degree to which different encodings actually occur may be fairly small, unless specific attempts are made to change the context and bias different meanings (see Section 2). In fact, when Earhard insured identical encodings at study and test, the word- frequency effect was not reduced in size, which suggests that the encodings at study and test are normally similar.\rIn a final experiment, Earhard tested the hypothesis that both HF and LF items cause implicit rehearsal of (largely) HF items during study. If so, false alarms to these rehearsed HF items would tend to occur during test, lowering performance. Indeed, a study phase in which subjects were forced to generate three free-associates to each study item did produce a sizeable word-frequency effect, and errors on the forced-choice test often consisted of choices of previously generated words (mostly HF words). Unfortunately,the word-frequency effect was, if anything, even larger for the con- trol condition in which the study word was\rjust repeated three times. Although subjects may have implicitly generated even more as- sociates in the repeat condition than they were instructed to produce overtly in the experi- mental condition, it seems to us more likely that a different but related explanation is called for.\rIn our model, it is not necessary that an implicit or an explicit HF associate be gen- erated at the time of study. After all, the result of such a generation in Earhard's (1982) model is to make the generated item more familiar at test. In our model, such items would be more familiar when tested anyway, because they have higher residual strengths to the study items. These higher residuals not only produce the extra familiarity of the generated items (as needed in Earhard's model), but also the higher familiarity accruing to HF distractors when these are not rehearsed at study time (thus explaining why the repetition condition also produces a large word-frequency effect).\rAlthough our explanation of the word-fre- quency effect is quite powerful, there is un- doubtedly more to the story, considering the many ways that HF and LF items differ (other than frequency). Furthermore, subjects are generally capable of assessing the frequency of presented items and may vary their rehearsal\r\n44 GARY GILLUND AND RICHARD M. SHIFFRIN\rand coding operations accordingly (see May & Tryk, 1970).\rDirectional Retrieval Strengths\rPrevious articles on the SAM model allowed for the possibility of asymmetrical retrieval strengths between two items (one a cue, one an image), but in practice always adopted a symmetry assumption. It certainly seems plausible that asymmetrical strengths should exist. Examples like air-port and port-air pro- vide illustrations. Within the frequency do- main, illustrations are not so obvious and sometimes seem to go in the direction opposite to our frequency assumption: for example, stirrup-horse and horse-stirrup.\rThis example and others like it are some- what misleading for the following reasons. First, one's intuitions about strengths are probably based on free association results— undoubtedly horse is a more likely response to stirrup than the reverse. However there are probably only a limited number of such in- stances. In general, there are at most a few high likelihood HF responses to a given LF cue, and these responses are unlikely to have been included on the presentation list or test list. In typical studies, virtually all the items on the list are unrelated to a given LF item, in which case it seems quite plausible that the residual strengths should have the relationship bH >bL.\rThe second reason why a higher free-as- sociation probability for a HF response to a LF cue should not cause problems for our model involves the absolute levels of strength. If one extends the SAM model to the semantic domain and substitutes a semantic cue for a context cue, the response probabilities do not directly reflect absolute levels of strength. They reflect relative strength values as well, because these determine sampling. Thus stirrup may cause sampling of horse, not because that con- nection is strong but because the other com- peting associations (like stirrup-table) are so weak. Horse could have a stronger connection to stirrup than the reverse, yet produce sam- pling of stirrup less often because horse has strong connections to so many other words as well.\rIn the SAM model for episodic recall, Equations 7, 8, and 9 give the recovery prob-\rabilities that follow sampling; these do depend on the absolute levels of strength. If applied directly to the semantic situation, these equa- tions predict the counterintuitive result that a LF response to a HF cue, once sampled, would be recovered with a higher probability than the reverse. It is possible that these equa- tions do not apply in their present form to the semantic recall case or in any situation involving asymmetrical retrieval strengths. Perhaps the retrieval strength from the re- sponse to the cue should determine recovery in such cases, or perhaps recovery should de- pend on other factors not yet incorporated in Equations 7, 8, and 9.\rVery Low Frequency Words and Nonwords\rPronounceable nonwords maybe equivalent to very low frequency (VLF) words, words that are probably unknown to the subjects and therefore are functional nonwords. These items are recalled worse than HF or LF words. The situation in recognition is more complex: Performance is worse for VLF words than for LF words and may be as poor or poorer than HF words (e.g., Mandler, Goodman, &Wilkes- Gibbs, 1982; Rao, 1983; Schulman, 1976; Zechmeister, Curt, & Sebastian, 1978). The difficulty of forming interitem associations between VLF words is certainly one reason for their low recallability (e.g., it might not be easy to associate abbatial and Jluviatile in a meaningful sentence or image). The recovery process might also be exceptionally poor when the image sampled does not incorporate a unitized lexical entity.\rHow could our model deal with recognition of VLF words or of nonwords? It is probably premature to speculate considering the paucity of relevant data. On the surface, it seems likely that for items without semantic or lexical rep- resentations, orthographic and phonological codes and similarities would take on primary importance. On the other hand, it might not be difficult to encode many VLF items in terms of a word that is suggested by the VLF item (probably a HF word). This would lend to semantic encoding in terms of the suggested word. Thus it is not difficult to come up with patterns of relationships among the parameters b, c, and d that would give rise to intermediate recognition performance, but it is difficult to\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 45\rselect a hypothesis with any confidence, with- out further data.\rPerhaps the simplest hypothesis holds that the b and d parameters are low for VLF items, but so is the c parameter. This pattern of pa- rameter values would be especially likely if subjects were told to code for meaning (e.g., Mandler et al., 1982), because a self-coding for meaning would not be easy for VLF words. However, if subjects were to attend to ortho- graphic features of the stimuli, then a higher c value might result, and recognition perfor- mance for VLF items would improve (e.g., Rao, 1983).\rForced-Choice Recognition\rSeveral interesting studies concerning word frequency have utilized forced-choice rather than yes-no testing. Before considering these studies, the SAM model must be extended to forced-choice settings. The simplest model for forced-choice performance would have the subject compute separate familiarity values for the two (or more) alternatives and choose the item with the highest value. However, the items being compared might consist of classes that differ widely in familiarity values. One way to handle cases of this kind involves selecting a criterion for each item type, forming an es- timate of the standard deviation of the dis- tractor distribution for each item type, and constructing a standard score for each item of\rthat type. The standard score would be the difference from the criterion divided by the standard deviation estimate; the largest stan- dard score could mark the subject's choice. (Such a scheme could also be used to generate confidence ratings; these could be determined by cutoffs on the standard score dimension. Also, it is possible that some automatic process carries out the standardization by, say, shrink- ing or expanding the different familiarity scores to a common scale before a criterion is chosen.)\rwhich we are in full agreement. He used two groups of subjects. One group replicated the usual word-frequency effect: After a list of mixed HF and LF items, forced-choice tests were given comparing HF targets with HF distractors chosen from the same populations, and LF targets with LF distractors. As usual, recognition was superior for the LF items. A second group received mixed lists of HF and LF items but were given forced-choice tests with distractors chosen for orthographic sim- ilarity. The HF targets were paired with HF distractors that differed in one letter position. The LF targets were paired with nonwords that differed in one letter position (the LF words were low enough in frequency that they may well have been functional nonwords—in any event lexicality could not have been of much help in making a decision in light of the results). In this group, HF tests were su- perior to LF tests; in addition HF performance was higher than in the control group, and LF performance was worse. The results were rep- licated in a subsequent study.\rWe offer the following somewhat speculative hypothesis to explain these data. Suppose that a forced-choice test with an orthographically matched distractor leads the subject to encode the test items in a way that deemphasizes or- thographic features. This might be done be- cause such features are common to both choices and therefore might be felt to be of no help in making a discrimination (in the same way that the common cue in our word- frequency study was no help and may even have harmed recognition performance). The result of deemphasis of orthographic encoding might be improved HF performance, if se- mantic coding is thereby emphasized, and if semantic coding is the best discriminator for HF items (highest c to d ratio). On the other hand, elimination of orthographic encoding might harm LF performance because these items are not well discriminated by semantic features (and would not have been easy to code semantically at study).\rThis hypothesis requires a deeper analysis to justify the stipulated discrimination differ- ences. An item cue can be encoded in terms of a variety of features, containing two im- portant subclasses, orthographic (O) and se- mantic (S). The overall strength relating an item cue to an image can be viewed as an\rInteractions of the Word-Frequency With Distractor Similarity\rEffect\rAn interesting word-frequency study using forced choices has been carried out by Hall (1979). He was interested in demonstrating that recognition performance is crucially de- pendent on choice of distractor, a point about\r\n46 GARY GILLUND AND RICHARD M. SHIFFRIN\rappropriate combination of the individual strengths for O, S, and any other classes of features. At the time of study it seems rea- sonable that LF items are coded less in terms of S and more in terms of O because these items are harder to code semantically and, in addition, tend to be more orthographically distinct. The result would be a higher c to d ratio for the O component than for the S com- ponent. Just the reverse reasoning should apply to HF items, so that a higher c to d ratio would be expected for the S component. For either item type, then, it would be most helpful for recognition performance to encode the item at test only in terms of the component that is the best discriminator. Subjects may not do this most efficiently unless they are led to do so by fairly obvious test manipulations like orthographically matched forced choices. (Also note that this analysis suggests that it is not necessarily optimal to match the study en- coding at test because the d values are inde- pendent of study encoding, and it is the c (and b) to d ratio that determines recognition per- formance).\rForced Choices Between Two Distractors or Between Two Targets\rAnother very interesting set of findings on word-frequency effects were collected by Glanzer and Bowles (1976). Subjects were given mixed lists of LF and HF items. Six different types of forced-choice tests were given as follows: (a) HF target-HF dislractor, (b) LF target-HF distractor, (c) HF target-LF dis- tractor, (d) LF target-LF distractor, (e) LF tar- get-HF target, and (f) LF dislractor-HF dis- tractor. Subjects were not told that the last two \"trick\" choices would be used and, presum- ably, chose the item that they were more sure was on the list. The probabilities of correct choices for the first four conditions were 0.71, 0.82, 0.83, and 0.91, respectively. In addition, a HF distractor was chosen over a LF distractor with probability 0.65, and a LF' target was chosen over a HF target with probability 0.64.\rOur model can be applied to this situation directly, assuming that scaled familiarity dis- tances from a criterion are used to make forced choices. In particular, assume that an evalu-\ration of the familiarity of each of the forced- choice test items is carried out independently. Two criteria are chosen, one for HF items and one for LF items. The subject assesses the fre- quency of a test word, chooses the appropriate criterion, subtracts the criterion from the fa- miliarity value attained, and then scales the resulting difference by dividing by an estimate of the standard deviation of the distractor dis- tribution for that frequency item. After this process is carried out for each test item, the subject compares the scaled values and chooses the item with the highest valueto be the target.\rThe essentials of this situation can be graphed as follows: We take the familiarity distributions for HF targets and distractors (the same distributions that underlie the predictions in Figure 9) and scale them by dividing by the standard deviation of the distractor dis- tribution; an analogous scaling is carried out for the LF items. These two sets of distributions are then aligned so that the respective criteria are in the same location. The result is the four distributions graphed in Figure 31. From left to right these distributions are for LF distrac- tors, HF distractors, HF targets, and LF targets. If we now make the mandated assumption that in a forced choice of items from any two of these distributions the subject will choose the higher (scaled) value, then it is easy to see that the findings of Glanzer and Bowles (1976) will be predicted in qualitative fashion. (For quantitative fits of the data, we would need to estimate parameters).\rWe can hardly claim much insight for the approach shown in Figure 31 because this is exactly the framework proposed by Glanzer and Bowles (1976). Glanzer and Bowles pro- posed a particular model according to which the various distributions were generated—and that underlying model had few similarities to ours—but the general approach is identical. We report these findings because our model does predict the ordering of the four distri- butions necessary to produce the correct pre- dictions.\rWe should note finally that a number of researchers have obtained results that are con- sistent with those obtained by Glanzer and Bowles (e.g., Shepard, 1967). Our model would handle these findings in a manner analogous to that described above.\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 47\r14 21 28 35 42 FAMILIARITY VALUES\rFigure 31. Scaled distributions for high-frequency (HF) and low-frequency (LF) words. (The distributions are the ones that give rise to the predictions in Figures 23 and 24.)\rFinal Remarks\rThe quantitative applications of the model presented in Section 3 amplify considerably the qualitative applications of the Section 2. It seems clear that the model can predict a great deal of data from recognition and recall studies that measure levels of accuracy, in- cluding studies manipulating word frequency.\rAlthough there are a fairly large number of parameters in the model, the essentials of the predictions are due to four working parame- ters, a, b, c, and d, with a, b, and c reflecting coding at study and the match of coding at test to that at study, and d reflecting preex- perimental factors and coding at test. Each of these parameters has a well-defined role, is quite distinct in its effects, and should respond appropriately to experimental manipulations. The context strength is the a parameter, the interitem strength is the b parameter, the self- strength is the c parameter, and the residual strength (based on preexperimental factors) is the d parameter. We have seen in Sections 2 and 3 that the results of experimental manip- ulations can usually be handled in the model by making natural and appropriate adjust- ments in the values of the parameters as re- quired by the changes in the experimental de- signs. Nevertheless, the progress thus far must be considered very preliminary. At the least,\rquantitative applications to many of the par- adigms are required. Beyond this point, it is essential that the model be extended and/or modified to deal with reaction time and con- fidence-rating data. We consider this possibility and other extensions in the Section 4 of this article.\r4. Extensions\rIn Section 4 we briefly discuss possible ex- tensions of the SAM model of recognition to other types of paradigms and response mea- sures.\rEffect\rof Number and Spacing of Target Presentations\rIncreasing the number of target presenta- tions causes both recall and recognition per- formance to rise (e.g., Atkinson & Juola, 1973; Murdock, 1962; Ratcliff and Murdock, 1976). Increasing the spacing of such presentations also causes recall and recognition performance to rise (e.g., Hintzman, 1974; Hintzman, Block, & Summers, 1973; Madigan, 1969).\rThe simplest case occurs when presentations are massed. In this case it seems plausible to treat « successive presentations each of time t as equivalent to one presentation of time «t. When this is done, the predictions for increas-\r\n48 GARY GILLUND AND RICHARD M. SHIFFRIN\ring numbers of presentations are identical to those shown in Figure 13 for increasing pre- sentation time (at least for the case when all of the list items are repeated n times in massed fashion).\rWhen presentations are spaced, the situation is much more complex—so much so that a detailed treatment is not possible in this article. Nevertheless, a tentative conclusion can be reached. It must be realized that the equiva- lence assumption made in the case of massed presentations does not hold. It seems more likely that n spaced presentations would give rise to n separate images, each with a (some- what) independent variability contribution and each with differing interitem strengths to dif- ferent items. For recognition, all of these changes produce a performance increase com- pared to the massed presentation case.\rFrequency Discrimination\rIt ought to be possible to apply our model to the paradigm of frequency discrimination, in which the subject attempts to discriminate between an item presented m times and some other item presented n times. This can be viewed as a generalization of recognition (for whichm=0andn=1).Formandnthat are both greater than 0, both test types are familiar, but more presentations should result in greater familiarity on the average, other fac- tors being equal.\rThe considerations mentioned above during the discussion of multiple presentations also come into play when predicting frequency dis- crimination. Matters are especially difficult because most studies utilize a spaced presen- tation method. For a few reasonable sets of assumptions we have looked at, performance in discriminating n occurrences from (n + 1) occurrences is a decreasing function of n. This prediction matches the findings in the litera- ture (e.g., Hintzman, 1974; Rao, 1983). Other interesting issues in this area, such as (a) the automatism of frequency knowledge (with which the model seems to be consistent: The model predicts that the frequency discrimi- nations should take place under incidental in- structions, as shown by Zacks, Hasher, & Sanft,\r1982), (b) spacing effects (which may be due in part to differential coding at storage; see Hintzman, 1974), and (c) variations in pre-\rsentation time for items of different frequency (the model may predict a choice of the LF item if the times for this item are longer) are beyond the scope of this article.\rMultiple Test Items\rIn the experiment we reported, two items were presented for test in the paired and in the cued conditions. In the paired condition, both items were relevant, but the decision could have been made on the basis of either item alone, because distractor pairs consisted of two new items. In many studies, this pos- sibility is eliminated by using distractor pairs consisting of one new and one old item or two old items from different study pairs. Many experiments of this type involve sentence memory, with distractor sentences consisting of studied words in new arrangements (e.g., J. R. Anderson, 1976). What is relevant for present purposes is the fact that the SAM rec- ognition model produces a familiarity value for an intact group of items that is higher on the average than the familiarity value for a rearranged group of items. This fact provides a basis for discriminating intact pairs from rearranged pairs in recognition tests, a feat that subjects are quite capable of accomplish- ing (e.g., Humphreys, 1976, 1978).\rThe basis for the extra familiarity is the high interitem strength between members of a studied pair; the interitem strength between members of a rearranged test pair is usually lower and often at the residual value. It is eas- iest to follow the argument by way of an ex- ample, as indicated in Table 2.\rAssume for the example that the study list contains just two pairs, A-B and D-E. Rec- ognition tests of pairs could be of four types: A-B (intact), A-D (rearranged), A-F (old- new), and F-G (new-new). Table 2 gives pos- sible retrieval structures for this case under two assumptions: Structure 1assumes that the b parameter for the items studied in a pair is greater than the residual value, whereas Struc- ture 2 assumes that b = d. The familiarityfor the different types of test pairs is indicated at the bottom of the table. Note that for b > d, all types are discriminable (Structure 1), but for b = d, intact and rearranged pairs are not discriminable. Thus the interitem parameter, b, controls this type of discrimination. These\r\nRETRIEV AL MODEL FOR RECOGNITION AND RECALL\r4 9\rTable 2\rRetrieval Structures and Pair Familiarity for a List of Two Word Pairs, A-B and D-E Structure 1: Interitem strength, b. Structure 2: Interitem strength, b,\rwithin pair is higher than residual within pair equals residual value\rImage A B D E Image A B D E\rC 10 10 10 10 C 10 10 10 10 A 10 10 1 1 A 10 1 1 1 B 10 10 1 1 B 1 10 1 1 D 1 1 10 10 D 1 1 10 1 E 1 1 10 10 E 1 1 1 10 F1111F1111 G1111G1111\rFamiliarity\rF(AB) = 2020 (intact) F(AD) = 400 (rearranged) F(AF) = 220\rF(FG) = 40\rqualitative conclusions hold true even if frac- tional weights are assigned to the cues, a point that is important in light of the results of Section 3.\rIn his work on this topic, Humphreys (1976, 1978) contrasted item information, used mainly to discriminate old from new items, and relational information, used mainly to discriminate intact from rearranged pairs. This contrast is captured in our model by the con- trast between the c and b parameters. To this degree our model is consonant with Hum- phreys's, although our detailed assumptions certainly do not match his, and we do not yet know whether our model can fit his data in\rquantitative fashion.15\rWe conclude Section 4 by mentioning one\rother factor in our model that can lead to discrimination of intact from rearranged pairs. This factor is the match of the encoding of a given item at study to the encoding of that item at test. The paired member also presented in these two cases can be viewed as an encoding context (see Section 2). When an intact pair is tested, the encoding of both items might match the study encoding better than would a rearranged pair. This would be reflected in higher values of both the c and b parameters. Note that this encoding factor would be espe- cially important if either the test or the study pairs were chosen to lead to an unusual coding for an item. In cases with random word pair- ings, the dominant encoding might tend to\rFamiliarity F(AB) = 220 F(AD) = 220 F(AF) = 130 F(FG) = 40\roccur at both study and test for both the intact and the rearranged cases, and therefore the match of encodings might not play an im- portant role.\rRecognition of Items From Categorized Lists\rConsider a paradigm in which items from « categories are presented on a list, followed by a yes-no recognition test for both list items and abstractors from each of those categories (e.g., Rabinowitz, 1978). This situation is not difficult to model: Assume that an item has within-category residual strengths that are higher than residual strengths to items in other categories (Le., within-category strengths = rfw; between-category residual strengths = (/b; dw > db). Assume also that when category size varies, different criteria are chosen for the dif- ferent categories. Ignoring for the moment the\r15 Mandler (1980) has suggested that an analogy can be drawn between Humphreys's item information and the familiarity phase of recognition, and Humphreys's rela- tional information and the search phase of recognition. Obviously, our familiarity model can predict the basic phenomena in question with a familiarity approach alone. Note also that whereas we share Humphreys's emphasis on the distinction between relational and item information, and his emphasis on cuing effects, the present model does not incorporate a search phase in cued-item recognition, a key component in Humphreys's model (see Humphreys & Bain, in press).\r\n50 GARY GILLUND AND RICHARD M. SHIFFRIN\rpossibility of using a category cue in addition to item and context cues, predictions are rel- atively straightforward.\rCategory size effects are seen for reasons similar to those that explain list-length effects, although the relative size of the decrement as category size increases is reduced in a multiple- category list compared to a one-category list. For example, the decrease from a 5-flower list to a 10-flower list is lessened if 40 words from other categories are added to each of these lists. The reason is simple: 4> is not zero, so added items in other categories make each category appear to have extra items, the num- ber extra depending on the ratio of afw to <4; the added number does not depend on initial category size. If the total list length is held constant, and the categories making up the list are varied in size, as in Rabinowitz (1978), then performance is still predicted to drop as category size increases (because Jw is larger than rfh), though at a somewhat different rate. These predictions are confirmed by reanalysis of Rabinowitz's data.\rAn interesting question arises if one com- pares performance for a list of n items all from one category with a list of n unrelated items (each tested with distractors similar to the list items). Because residuals between the category items are larger, performance should be lower in this case. Although we do not know of a direct test of this point, Kintsch (1968) com- pared recognition of a random 30-word list with a list composed of six 5-word categories. These gave roughly equal performance, pos- sibly because the improvement due to using a category size of 5 (rather than 30) offset the harm caused by extra-high within-category residual strengths. (By way of comparison, note that our recall model predicts recall of n words from one category to be superior to recall of n random words, for several different reasons— see Raaijmakers & Shiffrin, 1980).\rWhat would be the effect of adding a cat- egory cue to the probe set in addition to the item and context cues? Because the category cue would have higher strengths to images of items in that category than to images of other items, and because this effect would multiply the same effect caused by item cue strengths, the noncategory images would contribute pro- portionally less familiarity. Although a related effect occurrs for distractors, it is much smaller,\rso that performance would increase for the category in question. In addition, category performance would depend more on the size of the tested category and less on the size of the whole list. (Note that fractional cue weights might reduce or eliminate the effect of adding a category cue to the probe set).\rWhen all list items and distractors are from one category, a good case can be made for eliminating a category cue, because it increases familiarity for both targets and distractors, perhaps in a fashion that would reduce per- formance. It could even be argued that it is wise to encode an item cue in such a case so that category information would be de-em- phasized as much as possible. The residuals and the match parameters, m(b) and m(c), could both be reduced by such a de-emphasis, perhaps leading to an overall improvement. Such a possibility depends on a subject's ability to exercise control over encoding of\"items and cues. In any event, performance on a list and a test composed all of items from one category might well be improved by de-emphasis of category information, so that comparisons to performance levels for unrelated-item lists might be closer than otherwise expected.\rContinuous Recognition Paradigms\rVirtually all of the discussion in this article is directed toward study-test paradigms. We have done this because the previous recall work (Raaijmakers & Shiffrin, 1980) suggested that context changes during the course of a given list either do not occur or may be assumed to have negligible impact. A very different situ- ation obtains when a continuous recognition paradigm is used: Words are presented in a continuous sequence and tests are alternated with presentations throughout. In this para- digm we must assume that context is changing throughout the experimental session, so that as study-test lag increases, study context differs more from test context and performance de- creases (e.g., see Hockley, 1982, for a contin- uous recognition paradigm and Shiffrin, 1970, for a continuous recall paradigm).\rWe do not analyze continuous recognition paradigms in this article, but one important point can be made. Most of the effects that are seen as a function of study-test lag can be explained by a change in context. This would\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 5 1\rbe handled in the model by changes in the parameter m(a), which can be instantiated in the simulation by decreases in the value of the context strength in the retrieval structure, as a function of increasing lags. The familiarity would be based on a sum over all of the images of items up to that point in the session. As a target is tested at increasing delays, it con- tributes less and less of the total familiarity despite large self-strength, because the self- strength is multiplied by decreasing context strength.\rShort-Term Recognition\rIn the applications of the model to our data and to the results from the literature, care has been taken to exclude possible contributions to performance that might be due to tests of items in STS. It seems likely that an item still in STS (say, undergoing rehearsal) at the time of a recognition test is given a very rapid and accurate response, regardless of long-term fac- tors that would produce other results. For ex- ample, if the last item presented in a long list had been the first tested, it is very likely that both the accuracy and latency would be well off the distribution expected for other items in such a long list.\rHow might short-term recognition be in- corporated in the model? For recall, we have made the simple assumption that the short- term rehearsal buffer is dumped. For recog- nition accuracy, a related assumption could be made that a correct response would be given if a tested item is currently in STS.\rIt is interesting that a number of theorists have put forth models to handle recognition in both the long-term and short-term spheres (e.g., Murdock, 1974; Ratcliff, 1978). The de- bate between those who prefer two (or more) systems and those who prefer one has been highly active for 20 years without any definitive resolution. It is even conceivable that our fa- miliarity model could be extended to short- term data with good success. Nevertheless, we prefer to retain a two-store system so as to maintain the correspondence with the recall part of the model.\rResponse Times\rA great deal of the modern research into recognition memory involves reaction time\rmeasures (e.g., Atkinson & Juola, 1973; Hockley, 1982; McNicol & Stewart, 1980; Murdock & Dufty, 1972; RatclifT& Murdock, 1976). The present model remains incomplete until a mechanism for the production of re- sponse times is included.\rA Descriptive Approach to Response Times\rWithin the context of the present model, the simplest way to generate predictions for response time involves assignment of times in accord with the distance from the criterion. It seems reasonable that familiarity values far from the criterion should be associated with fast reactions and low variability; values close to the criterion should be associated with slow and variable reaction times.\rFor the reasons discussed in Section 3, it is not sufficient to use only the distance from the criterion; because the variances can differ greatly with item type, the distance must be scaled. Perhaps it is simplest to divide the dis- tance from the criterion by the standard de- viation of the distractor distribution. The re- sulting scaled distance may be called D. Then a model for reaction times might propose that a reaction time distribution be associated with each value of D, with mean ii(D) and variance ff\\D). Most likely, we should set n(D) = /,(£>) and ff\\D) - J2(D), where both j\\ and f2 are monotonically decreasing functions of the ab- solute value of D.\rA model of just this sort was fit quantita- tively to the reaction time results of our ex- periment reported in Section 3. The means and variances of the correct response times were fit quite accurately, as were the patterns of the error-response times (though the ab- solute values of the means and the variances of the error-response times were overpre- dicted). We do not present these results and predictions because the model is purely de- scriptive. Although the approach fit the data reasonably well, all that this demonstrates is a regular and lawful relationship between scaled criterion distance and response time, and hence a regular relationship between ac- curacy and latency measures.\rToward a Process Model of Response Times\rA descriptive model of the type proposed in Section 3 has many drawbacks and limi-\r\n52 GARY GILLUND AND RICHARD M. SHIFFRIN\rtations. To give just one example, there is no obvious way to treat data concerning trade- offs between speed and accuracy (e.g., Corbett, 1977; Dosher, 1981; Pachella, 1974; Reed,\r1973, 1976; Wickelgren, 1977; Wickelgren & Corbett, 1977).\rWe have not yet developed a process model for response time generation, but a random walk model of the type proposed by Link (1975) seems a reasonable candidate. Oneway to implement such a model follows: A scaled familiarity value is obtained for any test item, as in the present model. This scaled value is used to generate a drift rate (and distribution) for a random walk that takes place until either a positive response barrier or a negative re- sponse barrier is reached. The barrier reached determines the response accuracy, and the re- sponse time is determined by the number of steps needed to reach the barrier.\rNote that this model is not equivalent to the model put forth in earlier sections of this article. It is possible to have a positive scaled value of familiarity (which in the model of the earlier sections always produced a positive re- sponse) and yet have the random walk proceed to the negative response barrier (and vice versa). The closer the barriers, the more often such errors occur, and the faster are the re- sponse times. In fact, it is in just this fashion that speed-accuracy trade-off results are dealt with in a random walk approach—barriers are moved in to speed response time and lower accuracy or moved out to slow response time and improve accuracy. We suspect that an ap- propriate choice of barriers and drift rates would allow the random walk model to mimic closely the accuracy predictions of the present model. Whether the accuracy and latency data could be fit simultaneously by a random walk model is a question that must be left for future work.\rCriticisms of Criterion Models for Response Times\rAlthough we have not extended the SAM model to predict response times, we feel con- fident that such an extension is possible, along the lines suggested above. Even if the random walk version of the model is excluded from consideration, the descriptive version can\rhandle most of the results. Our suggested de- scriptive version is similar to a number of strength models that have postulated that re- action time is determined by distance from a criterion (e.g., Norman & Wickelgren, 1969). This approach has usually been used to predict short-term recognition data. It has been dis- missed as a viable candidate to explicate long- term recognition because it (a) predicts error variances to be smaller than correct variances, when the reverse is actually found (Murdock & Dufty, 1972); (b) is not a dynamic theory; that is, there is nothing than can change in the model as variables change (such as test position—Ratcliff& Murdock, 1976); and (c) makes no provision for response bias (Pike, 1973). None of these criticisms hold for an extension of the SAM model to the reaction time domain.\rConfidence Ratings\rMany of the same concerns that were dis- cussed with respect to response times apply to confidence ratings as well. Descriptively, one could suppose that subjects utilize a num- ber of criteria along the familiarity dimension, assigning confidence ratings in accord with the criteria between which a given observation of familiarity falls. Alternatively, in a random walk model, the confidence ratings could be made a function either of the drift rate, the number of steps to reach a barrier, the place- ment of the barrier, or some combination of all three (see Ratcliff, 1978).\r5. Assessment and Generalization\rComparisons of the SAM Model to Other Theories\rOur model postulates a deep-rooted rela- tionship between recall and recognition. Both processes make use of the same types of in- formation during retrieval and both are cue- dependent retrieval processes. In fact, the same quantitative expression serves as the basis for the recall and recognition models. The differ- ence between the two paradigms lies in how the information is used. In recognition, the strength of the total amount of information activated is used to make a familiarity decision. In recall that same information is used to de-\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 53\rtermine sampling and recovery during an ex- tended search.\rThe relationship we have specified is quite different from the relationship specified by embedded two-process theories (Anderson & Bower, 1972; Kintsch, 1970), and two-phase theories (Atkinson & Juola, 1973, 1974;Man- dler, 1980), as discussed in the introduction.\rMoreover, even the familiarity component of such two-phase models (Model 3 of Table 1) differs significantly from our model of fa- miliarity. Atkinson and Juola (see also Atkin- son & Wescourt, 1975) postulate that the fa- miliarity process is based on the level of ac- tivation of the word (that was directly accessed) in the lexicon (i.e., semantic memory)—the decision is thus context independent. Mandler (1980) also argues that the familiarity process is context independent, being based on in- traitem aspects of the stored word rather than intetitem connections (which are used pri- marily in search). Our conceptualization of a familiarity process is diametrically opposed, because the decision is based on a sum of fa- miliarity values across all the episodic images in the list and is not based on the familiarity value of a particular item. Whereas it is true that intraitem factors have an important role in our model, because the self-coding of targets is one of the two crucial factors in raising the familiarity value for targets above that of dis- tractors, such intraitem effects are embedded in a general interitem episodic sum (see Equa- tions 7 and 8). In fact the activation by a dis- tractor of its own semantic image does not\reven enter into the familiarity value on which a decision is based.\rOur conceptualization of the familiarity process is closer to that used in models that propose simultaneous access to multiple memory images or to a single, holistic memory store. In such models the memory decision is based on an activation of many items rather than on direct access to a specific memory image. Such models have been proposed for perceptual tasks (Pike, 1973), short-term memory tasks (Anderson, 1973; Cavanagh,\r1976; Pike, Dalgleish, & Wright, 1977), free recall (Metcalfe & Murdock, 1981; Murdock, 1982), paired-associate learning and forgetting\r(Eich, 1982), and recognition memory (Rat-\r16\rcliff, 1978). Ratcliff's model, although it em-\rphasizes latencies and is restricted to recog- nition, comes closest to predicting the types of data in which we are interested, and is worth a closer examination.\rRatcliff assumes that multiple searches oc- cur in parallel. A test probe activates each image in memory and causes it to begin a random walk between a yes barrier and a no barrier. If any of the walks reaches the yes boundary before all reach the no boundary then a positive response is given; otherwise a negative response is given. Negative responses can be rapid because the negative boundary is much closer than the positive one, although the positive drift rate (if a target is tested) may be faster than all of the negative drift rates. Drift rates are determined by similarity of the test item to each image in memory.\rThe model is similar to ours in that rec- ognition does not involve direct access to one image but rather involves the activation of a large portion of memory. The details of the models differ quite a bit because Ratcliff's model is a process model of activation in real time, whereas we have been more interested in asymptotic familiarity. (Furthermore, Rat- cliff makes no distinction between short- and long-term memory and applies his model to results in both domains.) We do not necessarily regard Ratcliff's approach as inconsistent with ours in a global sense. In fact, as we discussed in Section 4, a real time-process version of our model may well be conceptualized as a random walk. (We prefer a random walk model with one rather than multiple walks because we are not enamored of a model in which negative responses must wait until all of some very large but unspecified number of random walks reach a negative barrier.)\rNext consider the work of Tulving (1974, 1976; Tulving & Watkins, 1973), who sug- gested that recall and recognition involve the same cue-dependent processes and informa- tion. All that distinguishes them is the amount and the type of information available at test\r16\rThe Glanzer and Bowles (1976) model is, in effect, a model of this type. Although only the test item's features are contacted, all the other list items have an effect because they caused tagging of some of the features of the test item when they were first presented.\r\n54 GARY GILLUND AND RICHARD M. SHIFFRIN\rthat serve as cues. Our model is like Tulving's in that we also posit cue-dependent retrieval systems and place a great deal of importance on the types of cues available at test. We agree with Tulving that recall and recognition make use of basically the same types of information. However, we differ in that we postulate quite a different process for recall and recognition, whereas Tulving does not.17\rStrengths and Weaknesses of the SAM Model for Recognition\rWe are reasonably satisfied with the pre- dictions for accuracy measures in recall and recognition. The model has been able to fit a wide variety of data in the domain of episodic memory, in the paradigms of recall and rec- ognition, with a common set of assumptions and parameters and with sensible values of the parameters. That is, when experimental ma-\rnipulations are carried out that ought to change the values of certain parameters, the changes in those parameter values actually do predict the new data.\rAnother strength is the relative simplicity of the model. We have tried not to encumber the model with all sorts of special processes designed to handle particular, troublesome re- sults. It is also gratifying that the model has produced a number of accurate predictions that we did not anticipate prior to carrying out the simulations. Perhaps the greatest strength of the present model is the tight link between recall and recognition processes that is produced by the use of the denominator of the sampling equation as the decision variable for recognitionjudgments. Even though the familiarity mechanism for recognition isvery different from the search process for recall, there is a common basis that underlies both processes. It is this fact that allows us to pro- duce joint predictions for recall and recog- nition with the same model using the same parameters.\rThe weakest part of the present model is its preliminary nature: Predictions for response times and confidence ratings have not yet been produced. Of the applications in the accuracy domain, the main weakness is the lack of a model for choosing criteria. Another potential problem is the failure to include a search com-\rponent during recognition. We consider these problems in the following sections.\rCriterion Placement\rIn the simulations of Section 2 an arbitrary rule was used to choose criteria; in each case a fixed point was chosen between the means of the target and distractor distributions. This rule was used only as a demonstration device; in fact, the subject is assumed to choose a criterion appropriate for each test setting, as was assumed in the quantitative simulations of Section 3. The question then arises: How is an appropriate criterion chosen? This issue is an important one that cannot be addressed in detail in the present work, but we have some suggestions.\rFirst, it is clear that a subject with some knowledge of the expected proportions of tar- gets and distractors can adjust the criterion during the course of testing until a reasonable setting is reached. This solution has several problems. First, the setting of the criterion on the initial test trials is likely to be very im- precise. Second, subjects can produce sensible recognition results when tested with targets only, and no distractors whatever. In this case, it is possible that a criterion could be chosen (over test trials) to produce some arbitrary level of misses, but it is difficult to see why any particular level should be chosen.\rThese considerations suggest a second so- lution, which may appear more plausible. During the list presentation, the subject is constantly being presented with new items (in effect, distractors) for each of which a famil- iarity value can be calculated. Although these familiarity values change during the course of the presentations, the subject can nevertheless use the last several presentations of the list to form an idea of the mean and variance of the distractor distribution. (It may also be possible to get an idea of the characteristics of the target distribution through recall of earlier items in Lhe list). Given this knowledge of the famil-\r17\rNote that Tulving has recently modified his view of the relation between recall and recognition to allow different processes to occur in recall and recognition (Tulving, 1982a,\r1982b).\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 55\riarity distribution for distractors, it should be fairly easy to choose a sensible guess for a criterion setting, a setting that could be fine tuned as needed during the test sequence.\rA somewhat different problem concerns the placement of criteria for different item types as was needed for HF and LF items in Section 3. One problem involves memory load—if many item types are used in a list, how can all the criteria be kept in memory and used appropriately? A second problem concerns speed of response—when a test item is pre- sented how can the item type be ascertained, and the appropriate criteria retrieved, rapidly enough to allow fast recognition responses to be given? An answer to both of these questions might involve an automatic adjustment mechanism that utilizes nonepisodic charac- teristics of the test item to scale the values of familiarity accordingly. Such a solution is at-\rtractive (although not yet implemented) but would not work if the need for different criteria is episodically induced. For example, a list made up of differing numbers of members of different categories requires different criteria for each category. The basis for the choice, however, would not be inherent in the category but instead would be the experimenter's as- signment of presentation numbers to cate- gories. In this case, an automatic mechanism seems unlikely, and an appropriate adjustment in criterion would have to be made by the subject as the category membership of each test item is ascertained.\rIs There a Search Component to Recognition?\rSeveral types of data tend to suggest the presence of search processes and hence the insufficiency of the familiarity-only model (Model 4 of Table 1). One source of evidence concerns false alarms to distractors that are similar to list items. A similar distractor would have a higher than normal familiarity and hence should give rise to increased false alarm rates. However, this is not always the case. If enhanced false alarms are not seen, then it may well be that information recovery from the image of the associated list item is occur- ring. The recovered information might then be used to inhibit a false alarm. Perhaps such\ra process explains the failure to find excess false alarms to synonyms in the study with results shown in Figure 4: Locating the list image paired with the synonym could al- low the subject to discount the familiarity of the cue.\rA different example of this kind is found in a study by Tulving (1982a). After presen- tation of a list of items, a test list was presented consisting of 25% list items as targets, 25% control distractors (random), 25% rhyme dis- tractors, and 25%associated distractors. Group\r1 subjects performed a yes-no recognition judgment. Group 2 used the test items as cues and wrote down next to each a list word that the cue suggested (including the cue itself, if it was a target). An analysis showed that dis- tractors that were better cues for Group 2 (they led to more recalls of their matched-list mem- bers) were given fewer false alarms by subjects in Group 1. Such a result is hard to explain by a familiarity model alone. It seems likely that good cues used as distractors for Group\r1 led on occasion to recovery of the list item matched to that cue; having found a similar list item, the subject might then be able to decide that the cue was a distractor, despite its level of familiarity.\rThese results suggest that sampling of an image may sometimes occur and, thus, inhibit false alarms, but other results suggest that such sampling of the relevant image occurs only occasionally. For example, in a continuous- recognition experiment, false alarms to similar distractors are a nonmonotonic function of lag, with the peak somewhere between one and five intervening items. (MacLeod & Nel- son, 1976, tested lags of zero, five, and higher numbers and found a peak at five. In a similar study, we found a maximum false alarm rate at a lag of one). Presumably at zero lag, the item is not given many false alarms because the relevant list item is still in STS at test. At larger lags, though, the increased familiarity caused by a test at a short lag apparently over- comes whatever tendency there is for the matched-list item to be sampled by the similar distractor and apparently overcomes residual contamination from STS retrieval.\rResults like those obtained by Tulving sug- gest that a search component is at least some- times used during recognition. Whether the\r\n56 GARY GILLUND AND RICHARD M. SHIFFRIN\rsearch component occurs only when famil- iarity does not give a clear answer, as in Figure 1, or whether the image-recovery process oc- curs quickly (on some trials) and precludes use of the familiarity system altogether, is an open\rquestion.\rEvidence of a somewhat different kind ha's\rbeen reported by Mandler et al., (1969). After sorting a list of words into n categories, a sub- ject attempts either recall or recognition, either immediately or after a delay. Larger values of n (up to about 7) lead to better recall at all test delays, but recognition does not depend on n until long delays occur. Mandler argues that search is utilized relatively more and fa- miliarity relatively less as recognition-test delay increases, thereby explaining the result if two additional assumptions are true: (a) Search is better after sorting into more categories, and (b) familiarity judgment is unaffected by sort- ing into more categories. This sort of argument is a bit indirect but may provide evidence for search processes in recognition. Whereas data concerning a lessening of false alarm rates for similar distractors or for good recall cues seem to suggest automatic sampling and recovery occuring in parallel with the familiarity pro- cess, Mandler et al.'s (1969) data seem more compatible with a search carried out after the familiarity process fails.\rThere may be other data that indicate the presence of search during recognition, but the appropriate analyses have not yet been carried out. One paradigm of this kind is that of mul- tiple-item recognition. Both Humphreys (1978; Humphreys & Bain, in press) and Mandler (1980; Mandler, Rabinowitz, & Si- mon, 1981) have analyzed multiple-item rec- ognition by collecting data on the recognition, the cuing ability, and the recall recoverability of each individual item being tested. Through appropriate analysis, it may be possible to ar- gue that joint recognizability is a function of both familiarity and search in combination. We must reserve judgment on this issue until quantitative fits of our model to such data are undertaken, because it is possible that our model could fit the same data, although for different reasons.\rIt is important to emphasize that we are not opposed to the concept of a search com- ponent in recognition. To the contrary, the fundamental nature of the SAM approach re-\rquires that subjects may carry out an extended search during recognition paradigms, if they so desire. Furthermore, it is entirely consistent with our approach that a sample of an image from memory may occur automatically, in parallel with the evaluation of familiarity. Thus, the question is really not whether search processes occur, but the degree to which such processes occur. Perhaps it is surprising that the results reported in this article have shown that most of the experimental findings are ex- plicable with a global familiarity model with no search component at all. This finding should not be used to come to strong conclu- sions concerning the use of search. Because our global familiarity model is designed to predict many findings that are normally thought to indicate the presence of search, it is no easy matter to discriminate our pure familiarity model from one that relies to some degree on search.\rToward a General Model for Recall and Recognition\rRegardless of the status of the data con- cerning evidence in favor of search during rec- ognition, the underlying logic of the SAM model requires that the subject be able to uti- lize search if he or she so chooses. We begin therefore by describing a generalization of the SAM model that incorporates a search com- ponent.\rThe generalized model is depicted in Figure 32. As shown, there are two points at which search processes can contribute: an initial, perhaps automatic, single sample of an image that occurs in parallel with familiarity gen- eration, and an extended search that may fol- low familiarity generation.\rNext we wish to place the model in the larger context of a general information-pro- cessing framework that covers the memory system from initial feature encoding to final retrieval decisions.\rOn initial presentation of an item, a series of encoding and retrieval processes takes place which is largely automatic in the early stages. The general framework is depicted in Figure 33. The set of automatically encoded infor- mation is termed the autofield. We use the terms encoded and retrieved interchangeably here, because initial encoding is a form of re-\r\n5 »«' ll\ra£. X O>\rII.\r!§\rUSE TEST ITEM AND CONTEXT AS CUES\r3-3 I!\rDECISION: HOW SHOULD INFORMATION BE UTILIZED?\r! COMBINE SOURCES O F ' ! INFORMATION '\r• I\rUSE FAMILIARITY\rNO\r1\rEXTENDED MEMORY SEARCH\rUSE IMAGE INFORMATION\rYES\rLET\r- FAMILIARITY VALUE\rRECOVER INFORMATION FROM SAMPLED IMAGE\r21\ro\r70\rI I\rO\rto\r5\rNO\r1I Jr1\rRESPOND\" N O \" RESPOND I1\rRESPOND\r(STARTJ\r\n58\rGARY GO,LUND AND RICHARD M. SHIFFRIN\rtrieval from LTS. When we say, for example, that a straight-line feature is encoded, we mean that the system has processed the physical in- put to the point where a particular straight- line feature in LTS has been activated and made available to the system so that further processing can take place.\rThe information that drives the encoding process includes not only the sensory input from the external environment, including the contextual surround in addition to the nom- inal stimulus defined by the experimenter, but\ralso internal context deriving from the sub- ject's physical state and mental activities at the time of input. The system is assumed to act continuously to encode the changing en- vironment, but we consider only the stages following the occurrence of some event of par- ticular interest, such as the presentation of a word in a recognition experiment.\rIt is our view that the automatic encoding process produces (over a short period of time, perhaps several hundred milliseconds) a large number of different types of informational\rI EPISODIC IMAGE | I JT\rFigure 33. A framework for initial encoding according to the SAM model. (The autoficld represents the result of automatic encoding processes, based on the test cues in the probe set. The autofield contains a region of temporal-contextual information activated from recent episodic images. This information is used both for recognition decisions and as a basis for sampling images during an extended memory search.)\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 59\rfeatures, organized by content, and not by ep- isodic images. Occasionally an episodic image may be activated as part of the automatic en- coding process (see Figure 32), but with this exception, the images of list items and recent events are not in the autofield. However, con- text from these images is in the autofield, in a localized region called episodic context. It is this episodic context, integrated together in appropriate fashion, that is used to make fa- miliarity decisions in recognition. The same episodic context governs search, in the follow- ing manner: An element of context is sampled at random from the contextual region of the autofield and is traced back to the image from whence it arose. This is the sampling process in the memory search.\rConsider next the way in which the various codes are extracted and placed in the autofield. The typical assumption is made that process- ing takes place in an ordered sequence, though strict seriality is not implied. Some of the codes that might be retrieved are indicated in Figure\r33, ranging from low-level sensory features to high-level concepts. Of course the processing depends on the context as well as on previous features in the chain, so as upper stages are reached the features retrieved may become biased by the context as much as they are determined by the item itself. Thus one mean- ing of a word might be retrieved ahead of a more usual meaning, given the presence of a biasing word in the context. Presumably, the later in the stage of processing, the larger is the potential influence of context. The arrows leading from context to the episodic images indicate that the similarity of the test context\rto the context in the stored images causes that context to be activated (in conjunction with the extracted content features that are similar to features in the images; this other path of activation is indicated by the arrows leading from the content features to the episodic im- ages). The arrows leading from the episodic images to the autofield (actually to the episodic context area) show the genesis of the context activation from recent images, that serves as the basis for both familiarity judgments and sampling. The short arrow leading from one of the images to the autofield allows for the possibility that one of the images may be sam- pled as part of the automatic encoding process\rin parallel with the other types of encoding. When this occurs, it may be thought of as the first step in the search process, assuming that\r18\ran extended search is carried out.\rWhy should the context from all episodic\rimages appear in the autofield, but not the images themselves? The answer is based on the associative basis by which the automatic encoding system operates. The autofield is generated through associations to the probe cues (item and context). Because the probe cues contain context, the similar context from a recent image tends to be activated and placed in the autofield. On the other hand, if a word is presented as a distractor, so that none of its images is recent, then the context from its im- ages will not match that used at test, but the content features of the word will match many stored content features in LTS, and a variety of physical and semantic features are placed in the autofield. When both the context and the item in a sorted image match the test cues (i.e., a target or a similar distractor is pre- sented), then both types of information enter the autofield, each in its appropriate region. (Occasionally, in addition, the image itself is activated and is placed in the autofield.)\rIt is assumed that the subject can utilize the information in the autofield once it has been retrieved and can do so according to type of information, depending on the requirements of the task. Thus a lexical decision task would be accomplished through use of the lexical information in the autofield, a physical match task through use of the low-level sensory codes, and so forth.\rThe possibility that our model can be used for tasks other than episodic recognition might eventually prove quite important. As one ex- ample, consider the learning of categories. Suppose Nj items are input and assigned to the rth category. At test either old or new items might be given to the subject to assign to cat- egories. One approach within the SAM model would utilize search. The test item and the\r18\rWhere do the episodic images in LTS come from? We propose that each image represents a sample of the information in the autofield that arises due to some event occurrence. (We make no proposal at this time concerning how many of, and how often, these are stored.)\r\n60 GARY GILLUND AND RICHARD M. SHIFFRIN\rcontext would be used as cues, and some item would be sampled and recovered (eventually). The category of the first item recovered could be given as the response. This model is con- sistent with versions of the exemplar model of Medin and Schaffer (1978) and is essentially isomorphic to the complete-set exemplar model discussed by Homa, Sterling, and Trepel (1981) and Medin, Busemeyer, and Dewey (in press).\rOf course this simple version of a SAM search model is not the only approach within the SAM framework, even if attention is re- stricted to search models only. A simple gen- eralization would let the subject attempt longer searches. For example, N samples could be made, with a decision in favor of the category that is most often recovered. A further gen- eralization would have the subject use a more intelligent decision rule, assigning a category to the test item on the basis of a similarity decision that results from examining the re- covered images. (Note also that the SAM model is perfectly consistent with a mixed- exemplar-prototype model. If the subject should store a prototype separate from the individual images, this could be found during a memory search and could be used to generate a decision).\rAll of these search models for categorization are subject to the same criticisms that were applied to search models for recognition. That is, fast, accurate, and high-confidence category judgments might be difficult to explain on the basis of search alone. One solution is available by utilizing a variant of the familiarity model for recognition. For example, memory could be probed with three cues: context, test item, and category name. A decision in favor of a match would be made when the total system response (i.e., the denominator of the sampling fraction) exceeds some criterion.\rThis brief discussion of category learning is meant to provide an example of the type of generalization that may be possible for the present model. Such generalizations are left for future research.\rReferences\rAllen, L. R., & Carton, R. F. (1968). The influence of word-knowledge on the word-frequency effect in rec- ognition memory. Psychonomic Science, 10, 401-402.\rAnderson, J. A. (1973). A theory for the recognition of\ritems from short memorized lists. Psychological Review,\r80, 417-438.\rAnderson, J. R. (1972). FRAN: A simulation model of\rfree recall. In G. H. Bower (Ed.), The Psychology of Learning and Motivation (Vol.5). New York: Academic Press.\rAnderson, J. R. (1976). Language, memory, & thought. Hillsdale, NJ: Erlbaum.\rAnderson, J. R., & Bower, G. H. (1972). Recognition and retrieval processes in free recall. Psychological Review, 79, 97-123.\rAnderson, J. R., & Bower, G. H. (1974). A prepositional theory of recognition memory. Memory and Cognition, 2, 406-412.\rAnisfeld, M., & Knapp, M. (1968). Association, syn- onymity and directionality in false recognition. Journal of Experimental Psychology, 77, 171-179.\rAntognini, J. (1975). The role of extralist associate cues in retrieval from memory. Unpublished doctoral dis- sertation, YaleUniversity.\rAtkinson, R. C, Herrmann, D. J., & Wescourt, K. T. (1974). Search processes in recognition memory. In R. L. Solso (Ed.), Theories in cognitivepsychology: The Loyola symposium. Hillsdale, NJ: Erlbaum.\rAtkinson, R. C., & Juola, J. F. (1973). Factors influencing speed and accuracy of word recognition. In S. Kornblum (Ed.), Attention and performance (Vol. 4). New York: Academic Press.\rAtkinson, R. C., & Juola, J. F. (1974). Search and decision processes in recognition memory. In D. H. Krantz, R. C. Atkinson, R. D. Luce, & P. Suppes (Eds.), Con- temporary developments in mathematical psychology (Vol. 1): Learning, memory, & thinking. San Francisco: Freeman.\rAtkinson, R. C., & Shiffrin, R. M., (1968). Human mem- ory: A proposed system and its control processes. In K. W. Spence & J. T. Spence (Eds.), The psychology of learning and motivation: Advances in research and theory (Vol. 2). New York: Academic Press.\rAtkinson, R. C., & Wescourt, K. T. (1975). Some remarks on a theoryof memory. In P. M. A. Rabbitt & S. Domic (Eds.), Attention and performance (V ol. 5). London: Ac- ademic Press.\rBalota, D. A., & Neely, J. H. (1980). Test-expectancy and word-frequency effects in recall and recognition.Journal of Experimental Psychology: Human Learning and Memory, 6, 576-587.\rBirnbaum, I. M., & Parker, E. S. (Eds.) (1977). Alcohol and human memory. Hillsdale, NJ: Erlbaum. (1977).\rBower, G. H. (1972). Stimulus-sampling theory of encoding variability. In A. W. Melton & E. Martin (Eds.), Coding processes in human memory. Washington,DC: Winston.\rBuschke, H., & Lenon, R. (1969). Encoding homophones and synonyms for verbal discrimination and recognition. Psychonomic Science, 14, 269-270.\rCavanagh, P. (1976). Holographic and trace strength models of rehearsal effects in the item recognition task. Memory and Cognition, 4, 186-199.\rCermak, G., Schnorr, J., Buschke, H., & Atkinson, R. C. (1970). Recognition memory as influenced by differ- ential attention to semantic and acoustic properties of words. Psychonomic Science, 19, 79-81.\rCorbett, A. T. Retrieval dynamics for rote and visual image mnemonics. (1977). Journal of Verbal Learning and Verbal Behavior, 16, 233-246.\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 6 1\rCramer, P., & Eagle, M. (1972). Relationship between con- ditions of CrS presentation and the category of false recognition errors. Journal of Experimental Psychology, 94, 1-5.\rDavies, G., & Cubbage, A. (1976). Attribute coding at different levels of processing. Quarterly Journal of Ex- perimental Psychology, 28, 653-660.\rDeese, J. (1960). Frequency of usage and number of words in free recall: The role of association. Psychological Re- ports, 7, 337-344.\rDosher, B. A. (1981). The effects of delay and interference: A speed-accuracy study. Cognitive Psychology, 13,551- 582.\rDuncan, C. P. (1974). Retrieval of low-frequency words from mixed lists. Bulletin of the Psychonomic Society, 4, 137-138.\rEagle, M., & Leiter, E. (1964). Recall and recognition in intentional and incidental learning. Journal of Exper- imental Psychology, 68, 58-63.\rEagle, M., & Ortof, E. (1967). The effect of level of attention upon \"phonetic\" recognition errors. Journal of Verbal Learning and Verbal Behavior, 6, 226-231.\rEarhard, B. (1982). Determinants of the word-frequency effect in recognition memory. Memory and Cognition, 10, 115-124.\rEich, J. E. (1980). The cue-dependent nature of state- dependent retrieval. Memory and Cognition, 8, 157-\r173.\rEich, J. M. (1982). A composite holographic associative\rrecall model. Psychological Review, 89, 627-661.' Elias, C. S., & Perfetti, C. A. (1973). Encoding task and recognition memory: The importance of semantic en- coding. Journal of Experimental Psychology, 99, 151-\r156.\rEstes, W. K., & DaPolito, F. (1967). Independent variation\rof information storage and retrieval processes in paired- associatelearning.JournalofExperimental Psychology, 75, 18-26.\rFischler, I., & Juola, J. F. (1971). Effects of repeated tests on recognition time for information in long-term mem- ory. Journal of Experimental Psychology, 91, 54-58.\rCarton, R. F., & Allen,L. R. (1968). Familiarity and word recognition. Quarterly Journal of Experimental Psy- chology, 20, 385-389.\rGillund, G., & Shiffrin, R. M. (1981). Free recall of com- plex pictures and abstract words. Journal of Verbal Learning and Verbal Behavior, 20, 575-592.\rGlanzer, M., & Bowles, N. (1976). Analysis of the word frequency effect in recognition memory. Journal of Ex- perimental Psychology: Human Learning and Memory, 2, 21-31.\rGlenberg, A., & Adams, F. (1978). Type I rehearsal and recognition. Journal of Verbal Learning and Verbal Be- havior, 17, 455-463.\rGlenberg, A., Smith, S. M., & Green, C. (1977). Type 1 rehearsal: Maintenance and more. Journal of Verbal Learning and Verbal Behavior, 16, 339-352.\rGlucksberg, S., & McCloskey, M. (1981). Decisions about ignorance: Knowing that you don't know. Journal of Experimental Psychology: Human Learning and Mem- ory, 7, 311-325.\rGodden, D. R., & Baddeley, A. D. (1975). Context-de- pendent memory in two natural environments: On land and under water. British Journal of Psychology, 66, 325- 331.\rGodden, D. R., & Baddeley, A. D. (1980). When does context influence recognition memory? British Journal of Psychology, 71, 99-104.\rGorman, A. M. (1961). Recognition memory for nouns as a function of abstractness and frequency. Journal of Experimental Psychology, 61, 23-29.\rGregg, V. H. (1976). Word frequency, recognition and recall. In J. Brown (Ed.), Recall and recognition. London: Wiley.\rGregg, V. H., Montgomery, D. C., & Castano, D. (1980). Recall of common and uncommon words from pure and mixed lists. Journal of Verbal Learning and Verbal Behavior, 19, 240-245.\rGrossman, L., & Eagle, M. (1970). Synonymity, anto- nymity, and association in false recognition responses. Journal of Experimental Psychology, 83, 244-248.\rHall, J. F. (1954). Learning as a function of word-frequency. American Journal of Psychology, 67, 138-140.\rHall, J, F. (1979). Recognition as a function of word fre- quency. American Journal of Psychology, 92, 497-505. Hall, J. W, Grossman, L. R., & Elwood, K. D. (1976). Differences in encoding for free recall vs. recognition.\rMemory and Cognition, 4, 507-513.\rHerrmann, D., Frisina, R. D., & Conti, G. (1978). Cat-\regorization and familiarity in recognition involving a well-memorized list. Journal of Experimental Psychol- ogy: Human Learning and Memory, 4, 428-440.\rHerrmann, D. G., McLaughlin, J. P., & Nelson. B. C. (1975). Visual and semantic factors in recognition from long-term memory. Memory and Cognition, 3, 381- 384.\rHintzman, D. L. (1974). Theoretical implications of the spacing effect. In R. L. Solso (Ed.), Theories in cognitive psychology: The Loyola symposium. Hillsdale, N.J.: Erlbaum.\rHintzman, D. L., Block, R. A., & Summers, J. J. (1973). Modality tags and memory for repetitions: Locus of the spacing effect. Journal of Verbal Learning and Verbal Behavior. 12, 229-238.\rHockley, W. E. (1982). Retrieval processes in continuous recognition. Journal of Experimental Psychology: Learning, Memory, and Cognition, 8, 497-512.\rHoma, D. (1973). Organization and long-term memory search. Memory and Cognition, 3, 369-379.\rHoma, D., Sterling, S., & Trepel, L. (1981). Limitations of exemplar-based generalization and the abstraction of categorized information. Journal of Experimental Psy- chology: Human Learning and Memory, 7, 418-439.\rHumphreys, M. S. (1976). Relational information and the context effect in recognition memory. Memory and Cognition, 4, 221-232.\rHumphreys, M. S. (1978). Item and relational information: A case for context independent retrieval. Journal of Verbal Learning and Verbal Behavior, 17, 175-187.\rHumphreys, M. S., & Bain, J. D. (in press). Recognition memory: A cue and information analysis. Memory and Cognition.\rJacoby, L. L., & Dallas, M. (1981). On the relationship between autobiographical memory and perceptual learning. Journal of Experimental Psychology: General, 3, 306-340.\rJuola, J. F., Fischler, I., Wood, C. T., & Atkinson, R. C. (1971). Recognition time for information stored in long- term memory. Perception & Psychophysics, 10, 8-14.\rKintsch, W. (1968). Recognition and free recall of organized lists. Journal of Experimental Psychology, 78, 481-487.\r\n62 GARY GILLUND AND RICHARD M. SHIFFRIN\rKintsch, W. (1970). Models for free recall and recognition. In D. A. Norman (Eds.), Models of human memory. New York: Academic Press.\rKintsch, W. (1974). The representation of meaning in memory. Hillsdale, NJ: Erlbaum.\rLight, L. L., & Carter-Sobell, L. (1970). Effects of changed semantic context on recognition memory. Journal of Verbal Learning and Verbal Behavior, 9, 1-11.\rLink, S. W. (1975). The relative judgment theory of two choice response time. Journal of Mathematical Psy- chology, 12, 114-135.\rLuce, R. D. (1959). Individual choice behavior: A theo- retical analysis. New York: Wiley.\rMaclcod, C. M., & Nelson, T. O. (1976). A nonmonotonic lag function for false alarms to associates. American Journal of Psychology, 89, 127-135.\rMadigan, S. A. (1969). Intrascrial repetition and coding processes in free recall. Journal of Verbal Learning and Verbal Behavior, 8, 828-835.\rMandler, G. (1972). Organization and recognition. In E. Tulving & W. Donaldson (Eds.), Organization of mem- ory. New York: Academic Press.\rMandler, G. (1980). Recognizing: The judgment of previous occurrence. Psychological Review, 87, 252-271.\rMandler, G., & Bocck, W. (1974). Retrieval processes in recognition. Memory and Cognition, 2, 613-615.\rMandler, G., Goodman, G. O., & Wilkes-Gibbs, D. L. (1982). The word-frequency paradox in recognition. Memory and Cognition, 10, 33-42.\rMandler, G., Pearlstone, Z., & Koopmans, H. J. (1969). Effects of organization and semantic similarity on recall and recognition. Journal of Verbal Learning and Verbal Behavior, 8, 410-423.\rMandler, G., Rabinowitz, J. C., & Simon, R. A. (1981). Coordinate organization: The holistic representation of word pairs. American Journal of Psychology, 94, 209- 222.\rMay, R. B., & Tryk, H. E. (1970). Word sequence, word frequency, and free recall. Canadian Journal of Psy- chology, 24, 299-304.\rMcCormack, P. D., & Swenson, A. L. (1972). Recognition memory for common and rare words. Journal of Ex- perimental Psychology, 95, 72-77.\rMcNicol, D., & Stewart, G. W. (1980). Reaction time and the study of memory. In A. T. Welford (Ed.), Reaetion times. London: Academic Press.\rMedin, D. L., Busemeyer, J. R., & Dewey, G. I. (in press). Evaluation of exemplar-based generalization and the abstraction of categorical information. Journal of Ex- perimental Psychology: Learning, Memory, and Cog- nition.\rMedin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. Psychological Review, 85, 207- 238.\rMclcalfe, J., & Murdock, B. B., Jr. (1981). An encoding and retrieval model of single-trial free recall. Journal of Verbal Learning and Verbal Behavior, 20, 161-189.\rMurdock, B. B., Jr. (1962). The serial position effect in free recall. Journal oj Experimental Psychology, 64, 482- 488.\rMurdock, B. B., Jr. (1974). Human memory: Theory and data. Potomac, MD: Erlbaum.\rMurdock, B. B., Jr. (1982). A theory for the storage and retrieval of item and associative information. Psycho- logical Review, 89, 609-626.\rMurdock, B. B., Jr., & Anderson, R. E. (1975). Encoding, storage and retrieval of item information. In R. L. Solso (Ed.), Information processing and cognition: The Loyola symposium. Hillsdale, NJ: Erlbaum.\rMurdock, B. B., Jr., & Dufty, P. O. (1972). Strength theory and recognition memory. Journal of Experimental Psy- chology, 94, 284-290.\rMuter, P. (in press). Recognition and recall of words with a single meaning. Journal of Experimental Psychology: Learning, Memory, and Cognition.\rNairnc, J. S. (1983). Associative processing during rote rehearsal. Journal of Experimental Psychology: Learning, Memory, and Cognition, 9, 3-20.\rNeely, J. H., & Balota, D. A. (1981). Test expectancy and semantic organization effects in recall and recognition. Memory and Cognition, 9, 283-306.\rNeely, J. H., Schmidt, S. R., & Roediger, H. L. III.(1983). Inhibition from related primes in recognition memory. Journal of Experimental Psychology: Learning, Memory, and Cognition, 9, 196-211.\rNelson, D. L., & Davis, M. J. (1972). Transfer and false recognitions based on phonetic identities of words. Journal of Experimental Psychology, 92, 347-353.\rNorman, D. A., & Wickelgren, W. A. (1969). Strength theory of decision rules and latency in short-term mem- ory. Journal of Mathematical Psychology, 6, 192-208.\rOkada, R., & Burrows, D. (1973). Organizational factors in high-speed scanning. Journal of Experimental Psy- chology, 101, 77-81.\rPachella, R. G. (1974). An interpretation of reaction time in information processing research. In B. Kantowitz (Ed.), Human information processing: Tutorials in per-\rformance and cognition. Hillsdale, NJ: Erlbaum.\rPike, R. (1973). Response latency models for signal de-\rtection. Psychological Review, 80, 53-68.\rPike, R., Dalglcish, L., & Wright, J. (1977). A multiple- observations model for response latency and the latencies of correct and incorrect responses in recognition mem-\rory. Memory and Cognition, 5, 580-589.\rRaaijmakers, J. G. W., & Shiflrin, R. M. (1980). SAM: A theory of probabilistic search of associative memory. In G. H. Bower (Ed.), The psychology of learning and\rmotivation (Vol. 14). New York: Academic Press. Raaijmakers, J. G. W., & Shifrrin, R. M. (1981a). Order effects in recall. In A. Long & A. Baddeley (Eds.), At- tentionandperformance(Vol. 9).Hillsdale,NJ:Erlbaum. Raaijmakers, J. G. W., & Shiffrin, R. M. (198Ib). Search of Associative Memory. Psychological Review, 88, 93-\r134.\rRabinowitz, J. C. (1978). Recognition retrieval processes:\rThe function of category size. Unpublished doctoral\rdissertation, University of California, San Diego. Rabinowitz, J. C, Mandler, G., & Barsalou, L. W.(1977). Recognition failure: Another case of retrieval failure. Journal of Verbal Learning and Verbal Behavior, 16,\r639-663.\rRabinowitz, J. C., Mandler, G., & Patterson, K. E. (1977).\rDeterminants of recognition and recall: Accessibility and generation. Journal of Experimental Psychology: General, 106, 302-329.\rRao, K. V.(1983). The word frequency effect in situational frequency estimation. Journal of Experimental Psy- chology: Leaning, Memory, and Cognition, 9, 73-81.\rRatcliff, R. (1978). A theory of memory retrieval. Psy- chological Review, 85, 59-108.\r\nRETRIEVAL MODEL FOR RECOGNITION AND RECALL 63\rRatcliff, R., & Murdock, B. B., Jr. (1976). Retrieval Pro- cesses in recognition memory. PsychologicalReview, 83,\r190-214.\rReder, L. M , Anderson, J. R., & Bjork, R. A. (1974). A\rsemantic interpretation of encoding specificity. Journal\rof Experimental Psychology, 107, 648-656.\rReed, A. V. (1973). Speed-accuracy trade-offin recognition\rmemory. Science, 181, 574-576.\rReed, A. V. (1976). List length and the time course of\rrecognition in immediate memory. Memory and Cog-\rnition, 4, 16-30.\rRoberts, W. A. (1972). Free recall of word lists varying\rin length and rate of presentation: A test of total-time hypotheses. Journal of Experimental Psychology, 92, 365-372.\rRundus, D. (1973). Negative effects of using list items as recall cues. Journal of Verbal Learning and Verbal Be- havior, 12, 43-50.\rSalzburg, P. M. (1976). On the generality of encoding specificity. Journal of Experimental Psychology: Human Learning and Memory, 2, 586-596.\rSchulman, A. I. (1967). Word length and rarity in rec- ognition memory. Psychonomic Science, 9, 211-212. Schulman, A. I. (1976). Memory for rare words previously\rrated for familiarity. Journal of Experimental Psychol-\rogy: Human Learning and Memory, 2, 301-307. Schulman, A. I., & Lovelace, E. A. (1970) Recognition memory for words presented at a slow or rapid rate.\rPsychonomic Science, 21, 99-100.\rShepard, R. N. (1967). Recognition memory for words,\rsentences, and pictures. Journal of verbal Learning and Verbal Behavior, 6, 156-163.\rShim-in, R. M. (1970). Memory search. In D. A. Norman (Ed.), Models of human memory. New York: Academic Press.\rShiffrin, R. M., & Schneider, W. (1977). Controlled and automatic human information processing: 2. Perceptual learning, automatic attending, and a general theory. Psychological Review, 84, 127-190.\rSmith, S. M., Glenberg, A., & Bjork, R. A. (1978). En- vironmental context and human memory. Memory & Cognition, 6, 342-353.\rStrong, E. K., Jr. (1912). The effect of length of series upon recognition memory. Psychological Review, 19, 447-462.\rStrong, E. K., Jr. (1913). The effect of time-interval upon recognition memory. Psychological Review, 20, 339- 372.\rSumby, W. H. (1963). Word frequency and serial position effects. Journal of Verbal Learning and Verbal Behavior, 1, 443-450.\rThorndike, E. L., & Lorge, I. (1944). The teacher's word book of 30,000 words. New York: Columbia University Press.\rTodres, A. K., & Watkins, M. J. (1981). A part-set cuing effect in recognition memory. Journal of Experimental Psychology: Human Learning and Memory, 7, 91-99.\rTownsend, J. T. (1971). A note on the identifiability of parallel and serial processes. Perception & Psychophysics, W, 161-163.\rTulving, E. (1968). When is recall higher than recognition? Psychonomic Science, 10, 53-54.\rTulving, E. (1974). Cue-dependent forgetting. American Scientist, 62, 74-82.\rTulving, E. (1976). Ecphoric processes in recall and rec- ognition. In J. Brown (Ed.), Recall and Recognition. London: Wiley.\rTulving, E. (1982a). Elements of episodic memory. Oxford: Oxford University Press.\rTulving, E. (1982b). Synergistic ecphory in recall and rec- ognition. Canadian Journal of Psychology, 36, 130-147. Tulving, E,, & Thomson, D. M. (1971). Retrieval processes in recognition memory: Effects of associative context.\rJournal of Experimental Psychology, 87, 175-184. Tulving, E., & Thomson, D. M. (1973). Encoding specificity and retrieval processes in episodic memory. Psycholog-\rical Review, 80, 352-373.\rTulving, E., & Watkins, M. J. (1973). Continuity between\rrecall and recognition. American Journal of Psychology,\r86, 739-748.\rTulving, E., & Watkins, M. J. (1975). Structure of memory\rtraces. Psychological Review, 82, 261-275.\rTversky, B. (1973). Encoding processes in recognition and\rrecall. Cognitive Psychology, 5, 275-287.\rUnderwood, B. J. (1957). Interference and forgetting. Psy-\rchological Review, 64, 49-60.\rUnderwood, B. J., & Freund, J. S. (1968). Errors in rec-\rognition learning and retention. Journal of Experimental\rPsychology, 78, 55-63.\rWallace, W. P., Sawyer, T. J., & Robertson, L. C, (1978).\rDistractors in recall, distraetor free recognition, and the word frequency effect. American Journal of Psychology, 91, 295-304.\rWalter, D. A. (1977). Discrete events in word encoding: The locus of elaboration. American Journal of Psy- chology, 90, 655-662.\rWatkins, M. J., & Tulving, E. (1975). Episodic memory: When recognition fails. Journal of Experimental Psy- chology: General, 1, 5-29.\rWickelgren, W. A. (1977). Speed-accuracy tradeoff and information processing dynamics. Acta Psychologica, 41, 67-85.\rWickelgren, W. A., & Corbett, A. T. (1977). Associative interference and retrieval dynamics in yes-no recall and recognition. Journal of Experimental Psychology: Hu- man Learning and Memory, 3, 189-202.\rWinograd, E., & Conn, C. P. (1971). Evidence from rec- ognition memory for specific encoding of unmodified homographs. Journal of Verbal Learning and Verbal Be- havior, 10, 702-706.\rWiseman, S., & Tulving, E. (1975). A test of confusion theory of encoding specificity. Journal of Verbal Learning and Verbal Behavior, 14, 370-381.\rWoodward, A. E., Bjork, R. A., & Jongeward, R. H., Jr. (1973). Recall and recognition as a function of primary rehearsal. Journal of Verbal Learning and Verbal Be- havior, 12, 608-612.\rZacks, R. T., Hasher, L., & Sanft, H. (1982). Automatic encoding of event frequency: Further findings. Journal of Experimental Psychology: Learning, Memory, and Cognition, 8, 106-116.\rZechmeister, E. B., Curt, C., & Sebastian, J. A. (1978). Errors in a recognition memory task are U-shaped function of word-frequency. Bulletin of the Psychonomic Society, 11, 371-373.\r(Appendixes follow on next page)\r\n64 GARY GILLUND AND RICHARD M. SHIFFRIN Appendix A\rTable A-l\rMeans, Criteria, Hit Rates, and False Alarm Rates for the Simulations in Sections 2 and 3\rv=0.25 0.50 0.75\rList length 10\r20 30 40\rTime 1s 2s\r3s\rParameter a = 0.23 0.25 0.27\rParameter b - 0.10 0.15 0.20\rParameter b = 0.20 0.16 0.12\r1.329 1.330 1.331\r0.997 1.330 1.686\r2.067\r0.652 1.686 3.102\r1.168 1.330 1.502\r1.470 1.400 1.330\r1.330 1.175 1.020\rTarget Variable mean\rDistractor Figure 10\r0.647 0.808 0.970\r0.808 0.808 0.808\r0.808 0.808 0.808\r0.539 0.808 1.078\r0.807 0.808 0.810\rFigure 12\r0.435 0.808 1.176 1.560\rmean\rCriterion Hit rate\r0.779 .942 0.973 .942 1.168 .942\r0.838 .861 0.973 .942 1.108 .958\r0.946 .937 0.973 .942 1.001 .945\r0.766 .962 0.973 .942 1.180 .915\r0.972 .997 0.973 .942 0.974 .830\r0.651 .916 0.973 .942 1.302 .954 1.650 .965\r0.564 .864 1.302 .954 2.214 .975\r0.869 .937 0.973 .942 1.080 .943\r1.036 .848 1.004 .913 0.973 .942\r0.973 .942 0.902 .921 0.831 .874\rFalse alarm rate\r.077 .077 .077\r.385 .077 .007\r.114 .077 .050\r.003 .077 .244\r.003 .077 .176\r.007 .077 .171 .264\r.634 ,171 .014\r.122 .077 .051\r.026 .047 .077\r.077 .205 .400\rParameter a=0.20 0.25 0.30\rb=0.10 0.20 0.30\rc=0.10 0.15 0.20\rd = 0.050\r0.075 1.330 0.100 1.515\rc=\rc=\rc=\r0.10 0.15 0.20\r0.51 0.33 0.15\r0.15 0.12 0.09\r0.588 1.176 1.763\rFigure 14\r0.743 0.808 0.872\rFigure 15\r0.808 0.808 0.808\rFigure 16\r0.808 0.808 0.808\r1.064 1.330 1.596\r1.033 1.330 1.627\r1.270 1.330 1.391\r1.145\rFigure 13 (LL = 30)\r\nTable A-l (continued)\rVariable\rParameter a = 0.25 0.20 0.15\rParameter andjunk a=0.25; 0\r0.20; 10 0.15; 20\rParameter and junk a=0.25; 0\r10 20 a=0.20; 0 10 20 a = 0.15; 0 10 20\rTest block 1\r2 3 4\rParameter\rIndividual d = 0.075\rTarget mean\r1.330 1.064 0.798\r1.330 1.488 1.639\r.330\r.755 2.170 .064 .488 .904 0.798 .222 .639\r1.595 1.979 2.348 2.726\r1.330 1.330 1.330 1.330 1.515 1.700\rDistractor Figure 17\r0.808 0.647 0.485\rFigure 18\r0.808 1.069 1.325\rFigure 19\r0.808 1.230 1.647\r0.647 1.069 1.486\r0.485 0.907 1.325\rFigure 20\r1.011 1.385 1.760 2.133\rFigure 22\r0.808 0.822 0.835 0.808 1.078 1.347\rmean\rCriterion\r0.973 0.779 0.584\r0.973 1.163 1.348\r0.973 1.358 1.737\r0.779 1.163 1.543\r0.584 0.969 1.348\r1.186 1.531 1.869 2.211\r0.973 0.979 0.985 0.973 1.180 1.387\r1.302 0.658\r1.302 0.658\rHit rate\r.942 .942 .942\r.942 .951 .948\r.942 .952 .956 .942 .951 .954 .942 .947 .948\r.953 .961 .965 .973\r.942 .937 .934 .942 .915 .878\r.954 .963\r.954 .963\rFalse alarm rate\r.077 .077 .077\r.077 .216 .424\r.077 .179 .290 .077 .216 .350 .077 .278 .424\r.081 .157 .242 .314\r.077 .089 .104 .077 .244 .405\r.171 .043\r.171 .043\rOverall d =\r0.100 0.125 0.075 0.100 0.125\rParameters b and d\r6H = 0.2; rfH = 0.075 AL = 0.1;<4 = 0.035\rParameters b and d\rba = 0.2; rfH = 0-075 bi. = 0.1; 4 = 0.035\rFigure 2 3 J\\niform lists (LL = 30)\r1.686 1.176 0.897 0.549\rFigure 24 Mixed lists (LL = 30)\r1.686 1.176 0.897 0.549\rRETRIEVAL MODEL FOR RECOGNITION AND RECALL\r65\rNote. Except as noted, the predictions shown are based on a list of 20 items presented at 2 s/item with the following parameters: a - 0.25; b = 0.20; c = 0.15; d = 0.075; r = 4; v = 0.5. In each case the criterion equals the sum of the target and distractor means multiplied by 0.455. LL = list length.\r\n66 GARY GILLUND AND RICHARD M. SHIFFRIN Appendix B\rMethod and Procedures for an Experiment on Word Frequency and Test Type\rSubjects\rSubjects were 80 undergraduates from Indiana University.\rMaterials\rWe selected from the Thorndike and Lorge (1944) general count, 520 HF (A or AA) and 520 LF (1- 4 per million) words of between five and eight letters in length. All stimulus generation and presentation and response collection were controlled by a PDF\r11/34 computer.\rDesign\rThe experiment consisted of 10 experimental trials, 2 trials of each type of memory test. Each trial contained a study list, an addition task (to clear the STS), and a memory test.\rEach experimental study list consisted of 16 pairs of words. Four of each of the following combination of words formed the pairs: HF-HF, HF-LF, LF- HF, and LF-LF. The order of presentation of the pairs was randomized for each study list.\rTwo recall tests and three recognition tests were used. During the free-recall tests subjects were given a blank piece of paper and asked to write down all the words they could remember from the previous study list. They were asked to write down words together (on the same line) that had been presented together, if they were able to do so. For the cued- recall test one word from each pair was selected to be a cue. The subjects were asked to provide the paired member as a response. One half of the cues were HF words and the other half were LF words. One half of the HF cues were originally presented in the study list as the first member of a pair, and the other half were originally presented as the second member of a pair. The same selection was used for the LF words. The order of presentation of the cues was randomized, and the entire test list was printed on a sheet of paper for each subject.\rThe single-recognition test was composed of 32 words. One word from each of the study pairs was selected to form the 16 targets. Eight of the targets were HF words, eight were LF words, eight (four HF and four LF) words were originally presented as the first member of a pair, and eight were orig- inally presented as the second member of a pair. In addition, eight HF words and eight LF words that were never presented to that subject as study items were selected from the master lists to be used\ras the 16 distractors. The ordering of the 32 words within the test list was randomized.\rThe paired-recognition test was composed of 32 pairs of words. Sixteen of the pairs were the original 16 target pairs. The remaining pairs were never presented items paired as distractors. The sixteen distractor pairs were of the same frequency com- bination as the targets. The ordering of the target\rand distractor pairs was randomized.\rThe cued-recognition test was composed of 16\rpairs of items. One member of each pair was des- ignated as a cue by placing an asterisk to the left of the first member of a pair or to the right of the second member of a pair. Each cue was selected from the study list in the same fashion that the cues were chosen in the cued-recall task. The 16 lest items were made up of eight targets and eight dis- tractors. Each target was paired with the cue that had been presented with the target on the study list. One half of the targets were HF and one half, LF. For each cue-distractor combination, a HF or a LF word that had never been presented during study was chosen at random to serve as the distractor.\rEvery group of four subjects received a different test order and a different ordering of words within a trial. Thus there were 20 different orders used in the experiment.\rBetween the presentation list and the test list, an arithmetic test was given. Each arithmetic test con- sisted of the mental addition of seven single-digit numbers, presented one at a time for 2 s each.\rProcedure\rSubjects were told that they would be presented with a series of words to study and that they would later be tested. They were told that the words would be presented in pairs and that they should try to associate the pairs together using a mnemonic of some type. Subjects then had the procedure for each type of test explained to them and were given practice trials for each type of test.\rEach trial was initiated with the words GET READY printed on the screen for 2.5 s followed by 16 pairs of words presented for 4 s each. There were 100 ms of blank time between each presentation. Im- mediately after the last pair of words was presented, a series of single digits was presented to the screen, one digit at a time. Seven digits were presented for 2 s each. There were 100 ms of blank time between the end of one digit and the beginning of the next digit. After all digits were presented, WRITETHE\r\nRETRIEV AL MODEL FOR\rRECOGNITION AND RECALL 67\rSUMONTHEANSWERSHEETwas presented on the screen. Subjects were then given 5 s to write their sum on an answer booklet. The type of test the subjects were about to receive was then indicated on the screen and remained there for 5 s. For the free- and cued-recall tests, subjects were given 1.5 min to complete the task.\rFor each of the recognition tasks the single word (for the single-recognition task) or the pair of words (for the paired-recognition and the cued-recognition\rtasks) was presented on the screen for 2.5 s. For each type of test the subjects were to respond yes on the keyboard if the test item had been studied and no if the test item had not been studied. Subjects were told to respond as quickly as possible without sacrificing accuracy, and response times were col- lected. One half of the subjects responded yes with their left hands and no with their right hands, whereas for the remaining subjects the order of re- sponding was reversed.\rAppendix C\rStatistical Analyses of the Recall-Recognition Study\rA series of analyses of variance (ANOVAS) was carried out that confirm the observations given in the text. A separate ANOVA was completed for each type of test. A 2 X 2 (Frequency, HF or LF, X Pair Type, same or different frequency) analysis was car- ried out on the number of items correctly recalled on the cued and free-recall results, separately. Nei- ther the pair type nor the frequency main effects nor their interaction (in all cases p > .1) was sig- nificant. An additional ANOVA was done on the arc- sin transformations of the proportion of words cor- rectly recalled, combining the cued- and free-recall tests. The 2 X 2 X 2 (Frequency X Test X Pair Type) analyses revealed only one significant factor, namely test type. That is, cued-recall performance was better than free-recall performance.\rA 2 X 2 (Frequency X Pair Type) ANOVA was completed for the single-recognition results and only the main effect of frequency was significant, F( 1, 79) = 100.53, p < .001. A similar analysis for the cued-recognition test showed that again only the frequency effect was significant, F(\\, 79) = 15.52, p < .001.\rFor the paired-recognition test a simple one-way ANOVA was completed with three levels of frequency (high, mixed, low). This was done because the H F - LF and LF-HF pairs are indistinguishable with re- gard to pair type. Therefore, they were combined into a single mixed-frequency point. The ANOVA again showed that the frequency effect was signif- icant, F( l , 79) = 15.43, p < .001.\rMatched-group / tests were carried out on the different types of tests to compare their overall level of performance. As expected, the paired-test per- formance was better than the single-test perfor- mance, / (78) = 3.18, p < .005. Because of the variability in the cued tests, the single-recognition performance was not significantly better than the cued-recognition performance (t = .869). We have chosen, however, to fit this difference in means be- cause the results have a logical explanation in our model.\rReceived October 4, 1982 Revision received July 29, 1983 •","eventTime":1450258096000,"mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"distractor","time":1461915876585,"auto":true,"weight":1.0},{"@type":"Tag","text":"item","time":1461915876549,"auto":true,"weight":1.0},{"@type":"Tag","text":"cue","time":1461915876573,"auto":true,"weight":1.0},{"@type":"Tag","text":"strength","time":1461915876644,"auto":true,"weight":1.0},{"@type":"Tag","text":"test","time":1461915876658,"auto":true,"weight":1.0},{"@type":"Tag","text":"recognit","time":1461915876613,"auto":true,"weight":1.0},{"@type":"Tag","text":"paramet","time":1461915876672,"auto":true,"weight":1.0},{"@type":"Tag","text":"lf","time":1461915876561,"auto":true,"weight":1.0},{"@type":"Tag","text":"hf","time":1461915876598,"auto":true,"weight":1.0},{"@type":"Tag","text":"recal","time":1461915876629,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":1,"appId":"bae848ec99dea0d1f415a502a33cbc20f7000b92","timeCreated":1461915874045,"timeModified":1461915874328,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.23799583,"uri":"file:///Users/cheny13/Downloads/infovis_submission251-camera.pdf","plainTextContent":"Beyond Memorability: Visualization Recognition anEnEcodndciondgingRecall Michelle A. Borkin*, Member, IEEE, Zoya Bylinskii*, Nam Wook Kim, Constance May Bainbridge,\rChelsea S. Yeh, Daniel Borkin, Hanspeter Pfister, Senior Member, IEEE, and Aude Oliva\rFig. 1. Illustrative diagram of the experiment design. From left to right: the elements of the visualizations are labeled and categorized, eye-tracking fixations are gathered for 10 seconds of “encoding”, eye-tracking fixations are gathered while visualization recognizability is measured, and finally participants provide text descriptions of the visualizations based on blurred representations to gauge recall.\rAbstract— In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant- generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people’s attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pic- tograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable “at-a-glance” are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.\rIndex Terms—Information visualization, memorability, recognition, recall, eye-tracking study 1 INTRODUCTION\rEXPERIMENT DESIGN\rENCODING RECOGNITION RECALL\r100 “target” visualizations Same 100 targets + 100 “fillers” Correctly recognized blurred targets\rEncodEingcoding ReRcoegcnoigtinointion ReRcaelclall\r10 seconds / image\rOUTPUT: RecallRecall\rEye-tracking fixation locations and durations.\r2 seconds / image OUTPUT:\rEye-tracking fixation locations and durations, and whether visualization is recognized.\r20 min - as many images as participant can complete\rOUTPUT:\rText descriptions of what participant recalls about the visualization.\rLABELED VISUALIZATION DATABASE\r393 visualizations\rVisualizations are taken\rRecogRneitciongnition\rfrom [8], and the label taxonomy described in Table 1 is applied.\rUnderstanding the perceptual and cognitive processing of a visualiza- tion is essential for effective data presentation as well as communi- cation to the viewer. Memorability, a basic cognitive concept, has important implications for both the design of visualizations that will be remembered but also lays the groundwork for understanding higher cognitive functions such as comprehension. In our previous study [8], the memorability scores for hundreds of real-world visualizations were collected on Amazon’s Mechanical Turk (AMT). The results of this research demonstrate that visualizations have inherent memorability, consistent across different groups of observers. We also found that the most memorable visualization types are those that are visually distinct\r• MichelleA.BorkiniswiththeUniversityofBritishColumbia,and Harvard University. E-mail: borkin@cs.ubc.ca.\r• ZoyaBylinskii,ConstanceBainbridge,andAudeOlivaarewiththe Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT), E-mail: {zoya,oliva}@mit.edu\r• Nam Wook Kim, Hanspeter Pfister, and Chelsea S. Yeh are with the School of Engineering & Applied Sciences, Harvard University, E-mail: {namwkim,pfister}@seas.harvard.edu\r• Daniel Borkin is with the University of Michigan. E-mail: drborkin@umich.edu\r• *Equalcontribution.\rManuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication xx xxx 2015; date of current version xx xxx 2015.\rFor information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.\r(e.g., diagrams, tree and network diagrams, etc.), and that elements such as color, visual complexity, and recognizable objects increase a visualization’s memorability. However a few questions remain: What visual elements do people actually pay attention to when examining a visualization? What are the differences in memorability when given more time to view a visualization? What information do people use to recognize a visualization? What exactly do people recall about a visualization?\rTo answer these questions, in this paper we present the results of a three-phase experiment (see experimental design in Fig. 1). In contrast to the previous study, where each visualization was presented for 1 sec- ond (“at-a-glance”), we presented each visualization for 10 seconds (“prolonged exposure”) in the encoding phase of our experiment to allow participants more time to explore visualizations and to facilitate the collection of eye movements. During the recognition phase, par- ticipants viewed the target visualizations, mixed in with previously- unseen (filler) visualizations for 2 seconds each and responded to in- dicate which visualizations they recognized. During both phases, we collected eye fixations to examine which elements a person focuses on when visually encoding and retrieving a visualization from memory. Finally, in the recall phase, participants were presented with the tar- get visualizations correctly identified during the recognition phase in a randomized order and asked to “Describe the visualization in as much detail as possible.” This last experiment phase provides insight into what visualization elements, types, and concepts were easily recalled from memory. Together, the three phases of the experiment, along with a new detailed labeling of the 393 target visualizations from [8], allow us to analyze and quantify what visualization elements people encode, retrieve, and recall from memory.\r\nContributions: This work represents the first study incorporating eye-tracking as well as cognitive experimental techniques to investi- gate which elements of visualizations facilitate subsequent recogni- tion and recall. In addition, we present an analysis of the new labeled visualizations in our database1 in order to characterize visualization design characteristics, including data and message redundancy strate- gies, across different publication venues. Based on the results of our experiment, we are able to offer quantitative evidence in direct support of a number of existing conventional qualitative visualization design guidelines, including that: (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message.\r2 RELATED WORK\rPerception and Memorability of Visualizations: Many important works in the visualization community have studied how different vi- sualization types are perceived, and the effect of different data types and tasks [13, 29, 38]. The effect of “visual embellishments” on the memorability and comprehensibility of visualizations is also an ac- tive area of research [4, 5, 7, 8, 15, 22, 31, 35, 43, 46]. The effect and role of specific visual elements have also been investigated within the context of specific visualization types, e.g., attributes of node link diagrams [1, 16], specific visual elements such as pictographs [18], visual distortions [37], and more broadly [19]. It has been demon- strated that interactions with visualizations affect memory and recall (e.g., [25, 32]). As described in Sec.1, we evaluated the memorabil- ity of visualizations using thousands of un-edited real-world visual- izations from the web [8]. We found that some visualization types are more memorable than others, and particular visual elements (e.g., human recognizable objects, color, etc.) seem to increase a visualiza- tion’s memorability. In this study we build on all of this previous work and move beyond basic memorability by studying what visualization types and features contribute to visualization recognition and recall.\rVisual Cognition and Memory: Work on image memorability has reported high consistency between participants and experimental set- tings with regards to which images are memorable and forgettable. Studies have shown that the memorability rank of images is stable over time [23] and over experimental contexts [12]. Memorability has been demonstrated to be an intrinsic property of scene images [12, 23], faces [3], and graphs [8]. Studies in visual cognition and memory have demonstrated high fidelity of memories over time and massive storage capacity in long-term memory [9, 12, 27, 47]. People remember ob- jects that have defined meanings and use cases [10], people remember a lot of details from visual objects and scenes [27, 47], and people can easily distinguish between different exemplars of the same cate- gory [10, 12, 27]. Also, unique or distinct visual stimuli have been found to be easier to remember in massive memory studies [27, 12, 8]. Brady et al. have shown that when an object is retrieved from memory, many details are retrieved along with it [10]. In this paper we go be- yond the memorability of visualizations [8] and explore what details of visualizations are remembered along with it, and what elements of a visualization lead to better recognition and recall.\rIn the psychology literature, there is a dual process model of mem- ory called process dissociation [24, 34, 45, 30]. The two mem- ory processes include a fast, automatic “familiarity-driven” process, and a slower, conscious “recollection-driven” process. The former is thought to be the result of the perceptual system’s quicker processing of stimuli [24]. Thus, the experiments presented in this paper may be thought of as accessing two memory processes: single-response recog- nition and the longer, detail-retrieving recall. However, even with our 2-second recognition phase, the processes at play are not just percep- tual, as participants have time to go back to some of the textual ele- ments before making their response. We are thus likely capturing both memory processes during both experimental phases, and both play an important role in how and what people remember in visualizations.\r1Massachusetts (Massive) Visualization Dataset (MASSVIS) available at http://massvis.mit.edu.\rFig. 2. Example labeled visualization in the LabelMe system [41]. This pie chart from a government publication has 13 labeled elements includ- ing where the data, annotations, and title are located.\rEye-tracking Evaluations in Visualization: Eye-tracking evalua- tions can be an effective tool for understanding how a person views and visually explores a visualization [6]. They have been used in the vi- sualization community for evaluating specific visualization types such as graphs [20, 21, 28, 39], tree diagrams [11], and parallel coordi- nates [42], for comparing multiple types of visualizations [17], and for evaluating cognitive processes in visualization interactions [25]. There has also been research in the area of understanding different types of tasks and visual search strategies for visualizations through the analy- sis of eye-tracking fixation patterns as well as insights into cognitive processes [39, 40]. The work presented in this paper does not focus on specific tasks, nor on a specific type of visualization, but rather uses eye-tracking for fixation location and duration analysis on hundreds of labeled and categorized real-world visualizations from the web with dozens of study participants. Within the context of our experimental design, we are able to more deeply understand the specific cognitive processes involved in recognition and recall of visualizations.\r3 DATA COLLECTION AND ANNOTATION\rWe used the database of visualizations from our previous memorabil- ity study [8]. The database was generated by scraping multiple sources of real-world visualization publication venues online covering govern- ment reports, infographic blogs, news media, and scientific journals. The diversity and distribution of these visualizations represent a broad set of data visualizations “in the wild”. For our present study, we used the same 393 target visualizations utilized in the previous study [8] along with 393 visualizations selected from the remaining “single” (i.e., stand alone single-panel) visualizations in the collection as filler images (see Sec. 5.3).\rTo gain deeper insight into what elements of a visualization affect its recognition and recall, three experts manually labeled the polygons of each of the visual elements in the 393 target visualizations using the LabelMe system [41]. All labels were reviewed for accuracy and consistency by three visualization experts. Examples of labeled visu- alizations are shown in the leftmost panel of Fig. 1 and in Fig. 2.\rThe labeling taxonomy was based on the visualization taxonomy from [8]. As described in Table 1, the labels classify the visualization elements to be either data encoding, data-related components (e.g., axes, annotations, legends, etc.), textual elements (e.g., title, axis la- bels, paragraphs, etc.), human recognizable objects (HRO), or graphi- cal elements with no data encoding function. Labels could overlap in that a single region could have a number of labels (e.g., an annotation on a graph has an annotation label and a graph label). Additionally, the title of each visualization was manually coded for further analysis.\rWe also documented whether each visualization exhibited one of two types of redundancies: data and message redundancy. A visual- ization exhibits data redundancy if the data being presented is visually encoded in more than one way. This can include the addition of quan- titative values as labels (e.g., numbers on top of bars in a bar chart, as illustrated in Fig. 3, or on sectors in a pie chart), or the use of channels\r\nTable 1. The visualization labeling taxonomy used to annotate our target visualizations. The data subtypes taxonomy is taken from [8].\rLABEL Annotation\rAxis\rData\rData (type)\rGraphical Element Legend\rObject\rText\r[OPTIONAL SUBTYPES] DESCRIPTION\r[Arrow] Outline of any visual elements annotating the data. A specific subtype of “arrow” was included to denote whether the annotation was in the form of an arrow.\r[Time] Outline of where an axis is located including any tick marks and numeric values along the axis. A specific subtype of “time” was included to denote an axis involving time.\rOutline of where the full data plot area is located (e.g., the area between the x-axis and y-axis in a 2D plot).\r[Area, Bar, Circle, Diagram, Distribution, Grid & Matrix, Line, Map, Point, Table, Text, Trees & Networks] Outline of where the actual data values are visually encoded (e.g., bars in a bar graph, points in a scatterplot, etc.).\rOutline of any visual elements that are not related to the visual representation or description of the data.\rOutline of any legends or keys that explain the data’s visual encoding (e.g., color scales, symbol keys, map legends, etc.). [Photograph, Pictogram] Outline of any human recognizable objects (HRO) in the image. Objects are either realistic in representation (photograph) or abstract drawings (pictogram). Descriptions of each object were also recorded.\r[Axis Label, Header Row, Label, Paragraph, Source, Title] Outline of any text in the image. Subtypes cover all the common representations from prose to labels.\rGENDER EQUALITY IN LABOR FORCE PARTICIPATION\rGENDER EQUALITY IN LABOR FORCE PARTICIPATION\rPercent of Visualization covered with Data\r100% 75% 50% 25% 0%\r100% 75% 50% 25% 0%\r82%\rCHINA\r70%\rKOREA\r69%\rJAPAN\r34%\rINDIA\r100%\r75%\r50%\r25%\r0%\r58%\rNews media\r54%\rGovernment\r78%\rScience\r64%\rInfographic\rCHINA\rKOREA\rJAPAN\rINDIA\rRATIO OF FEMALE TO MALE\rRATIO OF FEMALE TO MALE\rRATIO OF FEMALE TO MALE\rRATIO OF FEMALE TO MALE\rPercent of Visualization\rCHINA\rCHINA\rKOREA\rKOREA\rJAPAN\rJAPAN\rINDIA\rINDIA\r70%\rKOREA\rKOREA\r69%\rJAPAN\rJAPAN\r34%\rINDIA\rINDIA\rFig. 4. Percentage of visualization average pixel area covered by the data label. Scientific visualizations on average had the highest percent- age of image area devoted to data presentation.\rsualization type, we see that diagrams, maps, and tables cover a larger percentage of the image area than other visualization types. These types of visualizations tend to have annotations and text labels incor- porated into the data representation itself, thus requiring less of the image area around the data plot for additional explanations.\rAnother observation is the difference in the average total number of elements in a visualization across sources. Visualizations from government sources have on average 11.9 labeled elements per visu- alization, significantly fewer compared to other visualization sources (p < 0.001,t(177) = 4.79 when compared2 to the numerically closest visualization source, science). In contrast, visualizations from info- graphic sources have nearly twice as many elements (M = 38.7) as compared news media (M = 19.7, p < 0.001, t(212) = 5.73) and sci- entific visualizations (M = 18.4, p < 0.001, t(169) = 5.23). The addi- tional elements in the infographic visualizations are mostly in the form of more text, objects, and graphical elements around the data.\rFinally, there is a difference between publication venues in the per- centage of the visualization’s area covered by human recognizable ob- jects (HRO). There are no such objects in the government visualiza- tions, and the percentages are generally less for scientific journal vi- sualizations (M = 14%) as compared to news media (M = 25%) and infographic (M = 29%) visualizations. The human recognizable ob- jects are primarily in the form of company logos (McDonalds, Twitter, Apple, etc.), international flags commonly used in the news media vi- sualizations, and pictograms or photographs of human representations and computer/technology depictions (see also the word cloud visual- ization in the Supplemental Material).\rIn addition to the labels, each visualization was examined to de- termine if it exhibited message redundancy, data redundancy, both, or neither. As shown in Fig. 5, all publication venues included visual-\r2For all the p-values reported in this paper, t-tests were corrected for multi- ple comparisons.\rSource: Gender Statistics 2013, World Bank\rORIGINAL\rSource: Gender Statistics 2013, World Bank\rGENDER EQUALITY IN LABOR FORCE PARTICIPATION\rCHINA LEADS IN FEMALE LABOR FORCE PARTICIPATION WHEREAS INDIA LAGS SIGNIFICANTLY BEHIND IN 2013\rGENDER EQUALITY IN LABOR FORCE PARTICIPATION\rCHINA LEADS IN FEMALE LABOR FORCE PARTICIPATION WHEREAS INDIA LAGS SIGNIFICANTLY BEHIND IN 2013.\r100%\r75%\r50%\r25%\r0%\r100% 75% 50% 25% 0%\r82%\rCHINA\rCHINA\rPublication Venue\rSource: Gender Statistics 2013, World Bank\rSource: Gender Statistics 2013, World Bank\rMESSAGE REDUNDANCY\rDATA & MESSAGE REDUNDANCY\rFig. 3. Illustrative examples of data redundancy (i.e., additional quanti- tative encodings of the data) and message redundancy (i.e., additional qualitative encodings of the main trend or message of the data). More examples are provided in the Supplemental Material.\rsuch as color, size, or opacity to represent a value already exhibited in a visualization such as the x- or y-axis values. In contrast, a visualiza- tion exhibits message redundancy if the main conclusion or message of the visualization is explicitly presented to the viewer in multiple ways: the addition of explanatory annotations, labels, text, and pic- tures. A visualization can exhibit both data and message redundancy (e.g., lower right panel of Fig. 3).\rAnnotating redundancy for each target visualization allows us to evaluate whether redundancy enables more effective recognition and recall. Each of the two types of redundancies were recorded as present or absent, by three visualization experts. The visualizations were re- viewed and discussed until unanimous consensus was found.\r4 ANALYSIS OF LABELED VISUALIZATIONS\rThe labeled visualizations enable us to gain insight into and study the distribution and type of visual elements employed across publication venues and visualization types. These insights also help us under- stand and put into context the observed trends and results of our study (Sec. 6). Examining the proportion of image area covered by the data label, as shown in Fig. 4, we see that it is highest for the scientific journal visualizations. This is probably due to the publishing context of scientific journal figures in which the visualization occurs as part of a paper narrative. Additionally, authors commonly have enforced page limits and figure limit constraints, so maximizing information per unit area is a constraint.\rBreaking down this measure of image area for data display by vi-\rDATA REDUNDANCY\r\n50%\r40% 30% 20% 10%\r0%\rVisualizations with Data and/or Message Redundancy\rScientific\r% Data Redundancy\r% Message Redundancy % Both redundancy\rALL Government Infographic News\rVisualization Source\r0%\r35%\r62%\r48%\r38%\r30%\r25%\r21%\r21%\r21%\r16%\r13%\r12%\r0%\r8%\rFig. 5. The percent of visualizations that exhibit data and message re- dundancies by publication source. The largest percentage of visualiza- tions with message redundancy are from the Infographic and News me- dia venues. Overall and across all visualization sources, there is more frequent use of message redundancy as compared to data redundancy.\rizations with message redundancy, with the highest rates in the info- graphic and news media visualization. This is probably due to both venues prioritizing clear communication of the visualization message. In contrast, the scientific visualizations in our sample had the least message redundancy and no data redundancy. When examining re- dundancy across visualization types, the message redundancy rates are comparable across all visualization types but highest for circle (53% of circle visualizations contain message redundancy), table (51%), line (44%), and bar chart (39%) visualizations.\rsures collected during this phase were the (x,y) fixation locations3 (in pixel coordinates on each visualization) and durations of each fixation (measured in ms).\r5.3 Recognition Phase\rThe recognition phase of the experiment, directly following the encod- ing phase, lasted 10 minutes. Participants were shown the same 100 visualizations they saw in the encoding phase as well as 100 filler visu- alizations. These 200 visualizations were presented in a random per- mutation for 2 seconds each with a 0.5 second fixation cross between visualizations. Participants pressed the spacebar to indicate recogni- tion of a visualization from the previous experimental phase. A feed- back message was presented (i.e., correct, incorrect) after each visu- alization. The 2 second duration time was chosen to be brief enough to quickly collect responses, but long enough to capture meaningful eye-tracking data. The measures collected during this phase were the (x,y) fixation locations and durations for each fixation (in ms), and the number of correctly-recognized visualizations (HITs).\r5.4 Recall Phase\rThe final portion of the experiment, the recall phase, lasted 20 minutes. Participants’ gazes were not recorded. In this phase, all the visualiza- tions the participant correctly recognized in the previous phase were presented in a randomized sequence. Each visualization was presented at 60% of its original size and blurred by a 40-pixel wide Gaussian filter to make the text unreadable. The purpose of blurring visualiza- tions was to allow the visualization to be recognizable, but not contain enough visual detail to enable the extraction of any new information.\rNext to each blurred visualization was an empty text box with the instruction: “Describe the visualization in as much detail as possi- ble.” The goal was to elicit from the participant as much information about the visualization as they could recall from memory. Participants were given 20 minutes in total to write as many descriptions as possi- ble. There was no limit to how much time or text length was spent on each visualization, nor any requirement to complete a certain number of visualizations. The measures collected during this phase were the participant-generated text descriptions of what they could recall of a given visualization.\rDisplaying blurred visualizations at this phase of the experiment has its limitations, including the potential to bias participants to more easily recall visual elements. However, we chose to use this modal- ity due to the following advantages: (1) the study can be performed at-scale, showing participants many visualizations and collecting text responses in a later phase, (2) no assumptions are made about what participants can remember and at what level they extract the content, and thus free-form responses can be collected, and (3) participants can be queried on specific visualizations they have stored in long-term memory via visual hints (i.e., the blurred visualizations).\r5.5 Performance Metrics\rWe computed fixation measures by intersecting fixation locations with labeled visualization elements (Table 1) to determine when fixations landed within each element. We analyze the total duration of a viewer’s fixations landing on a given visual element throughout the en- tire viewing period. We also measure refixations - the total number of times a viewer returns to an element during the entire viewing period (including the first time the element is fixated). Consecutive fixations on the same element are not counted. Note that a single fixation can land on several elements at once (e.g., an annotation on a graph). In this case, we count the fixation as belonging to all of those elements. We collect all of a viewer’s fixations during a particular viewing period (i.e., 10 seconds for encoding, 2 seconds for recognition), and we dis- card as noise fixations lasting less than 150 milliseconds. All fixation measures are averaged across viewers and different sets of visualiza- tions, and compared using Bonferroni-corrected t-tests.\r3The ordering of fixations is also available in the data, but was not used for the present study.\r5\r5.1\rEXPERIMENT OVERVIEW\rExperiment Set-up & Participants\rAs discussed in Sec. 3, we labeled all the visual elements in the 393\rtarget visualizations for which we have memorability scores [8]. We also carefully selected 393 visualizations from the database of single visualizations [8] to use as fillers during the recognition portion of the experiment. These visualizations match the exact distribution of vi- sualization types and original sources as the target visualizations. All target and filler visualizations were resized, while preserving aspect ratios, so that their maximum dimension was 1000 pixels.\rFor the eye-tracking portions of our experiment, we used an SR Research EyeLink1000 desktop eye-tracking system with a chin-rest mount 22 inches from a 19 inch CRT monitor (resolution: 1280 x 1024 pixels). At the beginning of an experimental session, participants performed a randomized 9-point calibration and validation procedure. At regular intervals, a drift check was performed and, if necessary, recalibration took place. Optional breaks were offered to participants.\rA total of 33 participants (17 females, 16 males) participated in the experiment. All of the participants were recruited from the local com- munities of Cambridge and Boston with an average age of 22.9 years (SD = 4.2). All participants had normal color vision. In a single ex- periment lasting 1 hour, a participant would see about 100 randomly- selected (out of a total of 393) target visualizations. A single session of the experiment lasted about 1 hour. Each participant was monetarily compensated $25 for their time.\r5.2 Encoding Phase\rThe first portion of the experiment, the encoding phase, lasted 20 min- utes. As illustrated in Fig. 1, participants were each shown about 100 visualizations, randomly selected from the 393 labeled target visual- izations. For this phase of the experiment, participants examined each visualization for 10 seconds while being eye tracked. Visualizations were separated by a 0.5 second fixation cross to clear their field of view. A 10 second duration proved to be of sufficient length for a par- ticipant to read the visualization’s title, axes, annotations, etc. as well as explore the data encoding, and was short enough to avoid too much redundancy in refixations as well as explorative strategies. The mea-\rPercent of Visualizations\r\nHR\rHR\rLOW QUALITY DESCRIPTION\rHIGH QUALITY DESCRIPTION\r0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1\r0\r0.9\r0.8\r0.7\r0.6\r0.5\r0.4\r0.3\r0.2\rvisual associations\rprolonged exposure\rsemantic associations\rMEMORABILITY ACROSS DIFFERENT ENCODING DURATIONS\rnew HR\rAMT HR\rMEMORABILITY VERSUS DESCRIPTION QUALITY\rMEMORABLE\rFORGETTABLE\ra) AMT HR b) 11\rc)\r50 100 150 200 250 300 350 400\rVisualizIamtiaognessssoorrttedbbyy“AaMt-aT-gHlaRnce” HR\rRaw HR for prolonged exposure study Standard deviation for raw HR points\r1\r0\r50 100 150 200 250 300 350 400\rVisualizImataiognes sorted by “AaMt-aT-gHlaRnce” HR nRewaHwR HR for “at-a-glance” study [8]\rAFMTitHteRd HR for prolonged exposure study\rnew HR\r“at-a-glance” exposure\rno distinct visual or semantic associations\rFig. 6. a) Plot comparing the raw HR (i.e., memorability) scores of target visualizations from the recognition phase of this experiment (after 10 0.9\rseconds of encoding) to the raw HR scores of the same visualizations from the previous experiment [8] (with 1 second of encoding). b) Summarizing\rthe same data by box filtering the raw HR scores makes the main trends clearer. The most memorable “at-a-glance” visualizations are still the most\rmemorable after prolonged exposure, likely due to visual associations. The increase in memorability across experiments for some visualizations can be explained by additional semantic associations (e.g., title, text, etc.) being invoked. c) The top and bottom ranked visualizations across\rdescription quality and memorability, for all four publication source categories. The y-axis represents recognition HR, and the x-axis represents average text description quality at recall. Description quality is also correlated with memorability (Sec. 6.1.4).\rHR\rHR\rHR\r6.1\r6.1.1\r“At-a-glance” vs. “prolonged exposure” memorability\rDoes “at-a-glance” memorability generalize?\r0.8\r0.7\r0.6\rWe also computed the recognition hit rate (HR) for each visual- ization.Thisisthefra0.c5tionofparticipantswhocorrectlyrecognized a visualization when they saw it during the recognition phase of the experiment. This valu0.4e ranges from 0 to 1. See the Supplemental Material for a discussion of the measurement of memorability in this experiment compared t0.3o [8].\rTo quantify the qualitative aspects of the text descriptions collected during the recall phase0.2 of the experiment, three visualization experts went through all 2,449 descriptions. The description quality is a mea-\r(SD = 15.30%) from the previous study [8] (with 1 second of encod- ing).Wecomparethememorabilityscores(HR)ofbothstudiesfor all the target visualizations in Fig. 6a-b. When time to encode a vi- sualization is increased from 1 to 10 seconds, it is natural for the ab- solute memorability scores to increase. However, what we are more interested in is the stability of the relative scores (i.e., ranks) of the visualizations across studies. We find a Spearman rank correlation of 0.62 (p < 0.001) between the memorability scores of the two studies. Note that the inter-participant consistency in the initial study was not perfectly correlated either, with a rank correlation of 0.83 [8]. Thus, the relatively high correlation between the two studies (despite the dif- ference in experimental set-up, participant population, and encoding time) points to the stability of memorability ranks of the visualiza- tions. We also see that this trend holds if we look separately within each source category. The Spearman rank correlations between the HR scores of the two studies are: 0.44 for infographics, 0.38 for news, 0.56 for government, and 0.59 for science ( p < 0.001 for all). Thus, visual- izations that were memorable “at-a-glance” (after only 1 second of encoding) are often the same ones that are memorable after “pro- longed exposure” (10 seconds of encoding). The same holds for the forgettable visualizations. Thus our initial study’s findings generalize to a longer encoding time when people can process more of the visual and textual input. However, now that encoding time has increased, we also notice that 8% of the visualizations that were not previously in the top third most memorable visualizations move up in rank as compared to the previous study. In the next section we discuss why some visu- alizations become more memorable after 10 seconds of encoding than after 1 second, and why others remain forgettable in both cases.\r6.1.2 What are the differences between the most and least recognizable visualizations?\rWe use the eye movements of a participant at recognition time, just before a response is made, as indicators of which parts of the visu- alization trigger the response (i.e., help retrieve the memory of the visualization). We can compare the differences in eye movements for the visualizations that are “at-a-glance” the most and least recogniz- able [8]. By considering the eye movements made on these visual- izations during 10 seconds of encoding and at recognition time, we can see what parts of a visualization are encoded and what parts are required for recognition.\rAs shown in Fig. 7, the heat maps overlaid on the visualizations represent the average of all of the participants’ encoding fixations on the visualization. The fixation patterns in the encoding phase demon-\rsure of how well a part\rvisualization. Quality was rated from 0 to 3 where 0 was an incorrect\ror incoherent description, and 3 was a text description that touched upon the visualization topic, what data or information is presented in the visualization, the main message of the visualization, and one addi- tional specific detail about the visualization. The quality ratings were assigned based on unanimous consensus. Each description was also reviewed for the visual components explicitly discussed or referenced from the visualization including the title and other textual elements. Finally, descriptions were flagged if they were not perfectly accurate (i.e., contained a factual error).\r6 EXPERIMENTAL RESULTS AND DISCUSSION\rIn our previous study [8], we showed that people are highly consistent in which visualizations they recognize and which they forget. Visu- alizations were only shown for 1 second at a time, so we were able to measure how memorable visualizations are “at-a-glance”. In this study we want to test not just recognizability, but we also want to discover which visualization elements people encode and are consec- utively able to recall (top right quadrant of Fig. 6c). We also want to test what aspects of a visualization impact how well the main mes- sage of the visualization is understood. For this purpose we extended encoding time from 1 to 10 seconds and analyzed eye movements at encoding, responses at recognition, and textual descriptions at recall.\r0.\ri\r1\rcipant recalled the content and message of the\r0 50 100 150 200 250 300 350 400\rImages sorted by AMT HR\rDuring the recognition phase of the current study, as discussed in Sec. 5, hit rates (HR) were generated for each visualization (i.e., what percentage of participants correctly identified the visualization as oc- curring during the encoding phase). For the current memorability study discussed in this paper (with 10 seconds of encoding), the mean HR is 79.70% (SD = 17.98%) as compared to a mean HR of 55.61%\r\nMOST RECOGNIZABLE LEAST RECOGNIZABLE\rFig. 7. Examples of the most and least recognizable visualizations from [8]. TOP: Eye-tracking fixation heat maps (i.e., average of all participants’ fixation locations) from the encoding phase of the experiment in which each visualization was presented for 10 seconds. The fixation patterns demonstrate visual exploration of the visualization. BOTTOM: Eye-tracking fixation heat maps from the recognition phase of the experiment in which each visualization was presented for 2 seconds or until response. The most recognizable visualizations all have a single focus in the center indicating quick recognition of the visualization, whereas the least recognizable visualizations have fixation patterns similar to the encoding fixations indicative of visual exploration (e.g., title, text, etc.) for recognition.\rstrate patterns of visual exploration (e.g., view graph, read title, look at legend, etc.), corresponding to the trends described in Sec. 6.2. These visual exploration patterns are seen in both the most and least recog- nizable visualizations.\rThe heat maps generated for recognition fixations are obtained by taking only the fixations until a positive response is made, i.e., only the fixations that lead up to a successful recognition, so that we can deter- mine which parts of a visualization participants look at to recognize a visualization. In Fig. 7 we see a distinct difference between the fixa- tion heat maps of the most and least recognizable visualizations in the recognition phase, where the most recognizable visualizations have a fixation bias towards the center of the visualization. This indicates that a fixation near the center, with the accompanying peripheral context, provides sufficient information to recognize the visualization without requiring further eye movements. In contrast, the least recognizable visualizations have fixation heat maps that look more like the fixation patterns of visual exploration in the encoding phase. These visual- izations are not recognizable “at-a-glance” and participants visually search the visualization for an association, i.e., a component in the visualization that will help with recognition. The fixations along the top of the heat map for the least recognizable visualizations generally correspond to the location of the title and paragraph text describing the visualization in further detail. We conducted statistical tests to verify that the fixation patterns are significantly different between the most and least recognizable visualizations, and we describe these analyses in the Supplemental Material.\rWe see that the recognition fixations are significantly different between the most and least recognizable visualizations in that the least recognizable visualizations require more exploration before they can be retrieved from memory. There is more eye movement during recognition for the least recognizable visualizations indicating that people need to look at more parts of a visualization before they can recognize it. The visual appearance is no longer enough and peo- ple start exploring the text. In the following section we will discuss some possible explanations for this difference and what visualization elements people may be using for recognition.\r6.1.3 Which visualization elements help recognition?\rWe have demonstrated that there is a distinct difference between the fixation patterns of the most and least recognizable visualizations. Which visual elements in the visualization contribute to this differ- ence in fixation patterns, and which elements help explain the overall HR increase for recognition? We believe that people are utilizing two\rtypes of associations to help with visualization recognition: visual as- sociations and semantic associations. The recognizable visualizations tend to contain more human recognizable objects (objects are present in 74% of the top third most recognizable visualizations) compared to the least recognizable visualizations (objects are present in 8% of the bottom third least recognizable visualizations). These human recog- nizable objects are examples of visual associations.\rThe visualizations that are not visually distinct must be recogniz- able due to semantic associations, such as the visualization’s title or similar text. In fact, the elements with the highest total fixation time during recognition are the titles (M = 274ms) followed by human rec- ognizable objects (M = 246ms, p < 0.01). The titles serve as a seman- tic association, and the objects as a visual association for recognition. Note that in Fig. 7 the fixations outside of the central focus in the least recognizable visualizations during recognition correspond to the visu- alizations’ title and other textual elements near the top. Thus these types of semantic associations, i.e., textual components, are used for recognition if the visualization is not visually distinct or does not con- tain sufficient visual associations.\rThe least recognizable visualizations likely do not have a strong vi- sual association nor a strong semantic association to assist with recog- nition (Fig. 6b). The least recognizable visualizations after 10 sec- onds have significant overlap with the least memorable visualizations from [8] (after 1 second). Also, of the 1/3 least recognizable visualiza- tions 54% are from government sources (compare that to only 3% of government sources in the top 1/3 most recognizable visualizations), while of the 1/3 most recognizable visualizations, 49% are from in- fovis sources (and only 2% of the bottom 1/3 least recognizable vi- sualizations are from infovis). Government visualizations tend to use the same templates and similar aesthetics, contributing to confusion between visualizations during recognition. The government visualiza- tions are also heavily composed of the least memorable visualization types including bar and line graphs. The least memorable visualiza- tions “at-a-glance” are also the least recognizable even after pro- longed exposure.\r6.1.4 What do people recall after 10 seconds of encoding?\rAcross 374 target visualizations4, the study participants generated 2,733 text descriptions (see Supplemental Material for examples of participant-generated descriptions). The mean length of a descrip-\r4We removed visualizations for which participants complained about the text being too small to read.\rRECOGNITION ENCODING\r\nTEXTUAL ELEMENTS ARE MENTIONED MOST OFTEN IN DESCRIPTIONS 1400\r1,252 1050\r700\r350\r0\r751\r648\r530\r455\r422\r317\r108\r56\r19\rVISUALIZATION ELEMENT\rFig. 8. The total number of mentions of specific visualization elements in the participant-generated recall text descriptions. Textual elements received the most mentions overall, and specially the title received the most mentions across all visualizations.\rTable 3. Most frequently mentioned visualization elements by publica- tion source (percent of time element was mentioned in the text descrip- tion out of total number of times it was present). Titles, labels, and paragraphs are mentioned most often.\rincluded in the Supplemental Material.\rAt this point, although we cannot make any causality assumptions,\rthe findings above demonstrate that visualization recognition is related to description quality. In other words, visualizations that were most recognizable after only 1 second of viewing also tend to be bet- ter described (after 10 second exposure). It might very well be that a third factor (e.g., relevance of topic to viewer) contributes both to recognizability and description quality. However, even in that case, knowing something about recognizability can tell us something about description quality. Thus, visualizations that were memorable for their visual content after 1 second of viewing are memorable after 10 sec- onds of viewing, and more importantly, their semantic content (the message of the visualizations) is correctly recalled by experimental participants (hence the higher description quality).\r6.2 The effects of visualization elements on memorability and comprehension\rIn this section we investigate which visualization elements attract at- tention during encoding through the analysis of eye movements, and which elements are recalled when a visualization is described from memory through the analysis of textual descriptions. We use the eye movements on a visualization as a proxy for what information is being encoded into memory. From the content of the recall text descriptions, we can infer from which elements of a visualization a participant ex- tracted information. As discussed in Sec. 5, the description quality score corresponds to the extent to which a participant recalled details and the main message of the visualization. These two metrics (eye movements and textual descriptions) together are evidence for which elements of a visualization contribute to a participant’s memory and understanding of a visualization.\r6.2.1 Titles\rWhen participants were shown visualizations with titles during encod- ing, the titles were fixated 59% of the time. Correspondingly, during the recall phase of the experiment, titles were the element most likely to be described if present (see Fig. 8). When presented visualizations with titles, 46% of the time the participants described the contents, or rewording, of the title. For the infographics sources, titles were fixated and described most often (80% and 72% of the time, respectively). A full breakdown by source category of elements described most often is presented in Table 3.\rWhat is the contribution of visualization titles to the quality of the textual descriptions? Across the 330 visualizations with at least 3 de- scriptions, the average description quality of visualizations with titles is 1.90 as compared to 1.30 for visualizations without titles (the dif- ference in quality is statistically significant at p < 0.001). This trend is partially driven by the absence of titles for the scientific visualiza- tions. We believe this might be one explanatory factor as to why the scientific visualizations have the lowest quality descriptions.\rTitles were also more likely to be fixated (76% of the time) when present at the top of the visualization than when present at the bot- tom (fixated 63% of the time). This trend holds across the source categories. Note that government visualizations, the least memorable category with poorer descriptions, contained the most occurrences of titles at the bottom of the visualization (73% of all visualizations that had a title at the bottom are from a government source). Interestingly, for government visualizations, table header rows were fixated more frequently and longer than titles (see Supplemental Material). For gov- ernment sources, titles that were found at the top of the visualization were fixated 85% of the time and described 59% of the time, compared to titles that were found at the bottom of the visualization, which were fixated only 62% of the time and described 46% of the time (see Table 2 in the Supplemental Material for the complete breakdown). Thus, titles are more likely to be paid attention to and later recalled when present at the top of a visualization.\rAcross all textual elements, the title is among the most impor- tant. A good or a bad title can sometimes make all of the difference be- tween a visualization that is recalled correctly from one that is not. We\rRank:\rOverall Infographics News Government Science\r1st\rTitle (46%)\rTitle (72%) Paragraph (45%) Title (55%) Label (27%)\r2nd\rLabel (27%) Label (28%) Title (43%) Legend (26%) Axis (14%)\r3rd\rParagraph (24%) Paragraph (24%) Label (33%) Data (21%) Legend (13%)\rtion is 13.1 words (SD = 9.6), with an average of 7.3 descriptions (SD = 4.0) per visualization. Table 2 contains a breakdown of the description statistics per source category. We can see that info- graphics sources generated the most number of descriptions in total (877), with the largest average number of descriptions per visualiza- tion (M = 11.2, SD = 3.0) and the longest descriptions in terms of word count (M = 14.3, SD = 10.0). More importantly, they also cor- respond to the highest-quality descriptions (M = 2.09, SD = 0.79), statistically higher than the descriptions for the news media visualiza- tions (M = 1.99, SD = 0.88,p < 0.001). As discussed in Sec. 5, the description quality is a measure of how well participants recalled the content and message of the visualization. A higher description quality for infographics implies that participants recalled more details about the source that had the most recognizable visualizations. For reporting statistics related to description quality, we only consider visualizations with at least 3 participant-generated descriptions (so that computations of average description quality are more meaningful). The source with least recognizable visualizations, government, also had the fewest total descriptions (465), the fewest descriptions per visualization (M = 4.7, SD = 2.9), and the shortest descriptions (M = 10.8, SD = 8.4). The average description quality was 1.37 (SD = 0.93), similar to science visualizations. Thus participants were able to recall fewer things about the least recognizable visualizations.\rOverall, the Spearman rank correlation between “at-a-glance” HR [8] and description quality (after prolonged exposure) is 0.34 (p < 0.001), and within just infographics, the source with the highest- quality descriptions, the correlation is 0.24 (p < 0.01). The posi- tive correlation is also present (though not significant) for the other 3 sources, possibly due to insufficient data (fewer visualizations and fewer descriptions per visualization than in the infographics category). Fig. 6c contains example visualizations categorized by how memo- rable they were and the quality of descriptions generated for them (i.e., whether what was memorable was the main content and mes- sage). Sample visualizations with descriptions, sorted by source, are\rTOTAL NUMBER OF MENTIONS\rTitle\rLabel\rParagraph Data\rLegend\rAxis\rHRO\rAnnotation Text\rSource\r\nTable 2. Summary of statistics related to visualization descriptions by source (** = p < 0.001). Mean description quality is computed only on visualizations with at least 3 participant-generated descriptions. Infographics had the most, longest, and highest-quality descriptions.\rSource\rOverall\rInfographics News Government Science\rTotal # of vis.\r374\r78 (20.9%) 120 (32.0%) 99 (26.5%) 77 (20.6%)\rTotal # of desc.\r2733\r877 (32.1%)\r848 (31.0%)\r465 (17.0%)\r543 (19.9%)\rMean # of desc. per vis.\r7.3 (SD:4.0) 11.2 (SD:3.0) 7.1 (SD:3.8) 4.7 (SD:2.9) 7.1 (SD:3.4)\rMean desc. length\r(# words)\r13.1\r(SD: 9.6) 14.3\r(SD: 10.0) 14.1\r(SD: 10.0) 10.8\r(SD: 8.4) 11.5\r(SD: 8.7)\rVis. with no desc.\r6%\r15% 3% 3% 3%\rVis. with at least 3 desc.\r84%\r85% 88% 74% 90%\rCorrelation (HR, # of desc.)\r0.57**\r0.51** 0.43** 0.36** 0.49**\rMean desc. quality (0-3 scale)\r1.77\r(SD: 0.94) 2.09\r(SD: 0.79) 1.99 (SD:0.88) 1.37\r(SD: 0.93) 1.24\r(SD: 0.93)\rInaccurate desc.\r13.1%\r14.9% 15.7% 8.6% 10.1%\robserved that when the title included the main message of the visual- ization (e.g., “66% of Americans feel Romney Performed Better Than Obama in Debates”), more participant-generated descriptions recalled the visualization’s message compared to instances when the title was more generic or less clear (e.g., “Cities”). Select participant-generated descriptions for exemplar visualizations with good and bad titles are presented in the Supplemental Material. Note that prior psychology studies have demonstrated that concrete, imageable words (those that can be easily imagined or visualized) are easier to remember than ab- stract ones [26, 36, 48, 33]. Recent work by Mahowald and colleagues [33] has begun investigating the memorability of individual words, opening up future extensions to designing more memorable titles.\r6.2.2 Pictograms\rPictograms, i.e., human recognizable objects (HRO), did not seem to distract participants during encoding. In fact, averaged over all visual- ization elements, the total fixation time spent on pictograms was less than all the other visualization elements (see Supplemental Material). Infographics is the only source where pictograms were refixated the most and fixated for the longest time in total, but these are also the vi- sualizations with the highest recognizability and quality descriptions.\rOverall (as in Table 4), the average description quality for visual- izations with pictograms (2.01) is statistically higher than for visual- izations without pictograms (1.50, p < 0.001). Within each source cat- egory, the mean description quality for visualizations with pictograms is always higher than for visualizations without pictograms. This is not always a significant difference, but what this demonstrates is that even if pictograms do not always help, they do not seem to hinder de- scription quality. In fact, if we consider the top third best described visualizations, 20% of them have pictograms, compared to only 2% among the visualizations in the bottom third (Supplemental Material). Across all source categories, the visualizations with the best descrip- tions are more likely to contain pictograms than the visualizations with the lowest-quality descriptions. Thus, what participants recall about visualizations is not hindered by pictograms. In other words, with pictograms the message of the visualization is correctly recalled (and in fact, often better). Participants may use pictograms in these cases as visual associations (see Sec. 6.1.3). Pictograms might also provide a form of message redundancy (see Sec. 6.3).\rNote, however, that some pictograms can hurt the understanding of a visualization. Any time people spend on a pictogram is time not spent on the data or the message (unless the pictogram contains part of the data or message), and if the link between the data/message and the pictogram is not clear, people may misunderstand and/or misrecall what the visualization was about. In the Supplemental Material, we qualitatively review the lowest quality text descriptions and see that pictograms that do not relate to the visualization’s data or message can produce confusion and misdirect attention.\r6.2.3 Other Elements\rAcross all visualizations, the elements that were refixated the most were the legend, table header row (often acting as a title), and title.\rTable 4. Across all sources, visualizations with pictograms have similar or better quality descriptions than visualizations without pictograms (** = p < 0.001 when comparing visualizations with and without pictograms).\rSource\rOverall Infographics News Government Science\rMean description quality (0-3 scale)\rWith pictogram\r2.01** 2.09 2.10** 1.46 1.52**\rWithout pictogram\r1.50** 2.07 1.84** 1.36 1.12**\rThe elements that were fixated the longest were the paragraph, legend, and header row (a full breakdown by source is provided in the Sup- plemental Material.) We find that the elements that most commonly contribute to the textual descriptions, across all visualizations, include title, label, and paragraph (see Table 3). For news media sources, the paragraph is used more often than the title and label in the recall de- scriptions. This may be due to the commonly short, and not necessar- ily informative, titles provided. For example, a viewer will gain more from the explanatory paragraph “Numbers of companies in the cotton market that ‘defaulted’ by ignoring arbitration decisions” instead of its title “Broken Promises”. Recall that the science visualizations we have do not contain titles, a common convention in scientific journals, and so participants refer instead to the labels, axis, and legend in their descriptions (since those are often the only elements that contain any explanatory content). Thus, when the title is not sufficient, people turn to the other text in a visualization, including the explanatory paragraph and annotations (labels).\r6.3 The Importance of Redundancy\rThe average description quality is higher for visualizations that con- tain message redundancy (M = 1.99) than for visualizations that do not (M = 1.59, p < 0.001). Similarly, visualizations that contain data redundancy also have better quality descriptions (M = 2.01) than those that do not (M = 1.70, p < 0.001). We can also see this trend by ex- amining redundancy in the visualizations of the top third and bottom third of average description quality rankings. Of the visualizations in the top third of description quality ranks, 57% contain message redun- dancy and 34% contain data redundancy. In contrast, of the visual- izations in the bottom third of description quality ranks, only 22% of visualizations contain message redundancy and 12% contain data re- dundancy. These trends hold within each of the source categories (see Table 5) in that the mean description quality tends to be better for visu- alizations with message/data redundancy than without, and similarly, the visualizations with the best descriptions (the top third) are more likely to contain both forms of redundancy.\rOverall, if we compare the source categories to each other (see Table 5), infographic visualizations contain the most message redun- dancy, followed by news media, government, and science. The ex- act same trend holds for data redundancy. Thus, although no causal\r\nTable 5. The effect of redundancy on the description quality of visualizations: across all sources, the presence of message or data redundancy is linked to higher description quality on average (* = p < 0.01 when comparing visualizations with and without message or data redundancy).\rSource\rMean description quality (0-3 scale)\rAll Message redundancy\rData redundancy\r% with msg redundancy\rAll Top Bottom images 1/3 1/3\r(good) (bad)\r39% 57% 22% 63% 69% 58% 50% 51% 43% 28% 25% 21%\r7% 13% 0%\r% with data redundancy\rAll Top Bottom images 1/3 1/3\r(good) (bad)\r22% 34% 12% 37% 38% 27% 27% 26% 26% 20% 21% 8%\r0% 0% 0%\rimages\rWith Without With\rWithout\r1.70* 2.05 1.97 1.37 1.24\rOverall 1.77 1.99* Infographics 2.09 2.11 News 1.99 2.05 Government 1.37 1.41 Science 1.24 1.60*\r1.59* 2.01* 2.06 2.16 1.90 2.02 1.36 1.40 1.19* NA\rconclusions can be made at this point, we can see that redundancy is related to how well a visualization is recognized and recalled. The importance of data and message redundancy has also been observed for animated and video visualizations [2]. Data and message redun- dancy in visualizations help people grasp on to the main trends and messages, and improves visualization recognition and recall.\r7 CONCLUSIONS\rBased on the results presented in the preceding sections, we summa- rize below the key observations from our study. In addition to our experimental results shedding light on the fundamental theory of how visualizations are encoded, recognized, and recalled from memory, our results also provide direct quantitative empirical evidence in support of many conventional established qualitative visualization design and presentation guidelines. The conclusions listed below, with related es- tablished design principles where appropriate, relate to having a good and clear presentation, making effective use of text and annotations, drawing a viewer’s attention to the important details, providing effec- tive visual hooks for recall, and guiding the viewer through a visual- ization using effective composition and visual narrative.\rVisualizations that are memorable “at-a-glance” have memorable content. Visualizations that are most memorable “at-a-glance” are those that can be quickly retrieved from memory (i.e., require less eye movements to recognize the visualization). Importantly, when these visualizations are retrieved from memory, many details of the visual- ization are retrieved as well. Thus, participant-generated descriptions tend to be higher quality for these visualizations (Sec. 6.1).\rTitles and text are key elements in a visualization and help recall the message. Titles and text attract people’s attention, are dwelled upon during encoding, and correspondingly contribute to recognition and recall. People spend the most amount of time looking at the text in a visualization, and more specifically, the title. If a title is not present, or is in an unexpected location (i.e., not at the top of the visualization), other textual elements receive attention. As exhibited by these results, the content of a title has a significant impact on what a person will take away from, and later recall, about a visualization (Sec. 6.2).\r“Words on graphics are data-ink. It is nearly always helpful to write lit- tle messages on the plotting field to explain the data, to label outliers and interesting data points.” (Edward Tufte [44])\rPictograms do not hinder the memory or understanding of a vi- sualization. Visualizations that contain pictograms tend to be better recognized and described. Pictograms can often serve as visual hooks into memory, allowing a visualization to be retrieved from memory more effectively. If designed well, pictograms can help convey the message of the visualization, as an alternative, and addition to text (Sec. 6.2).\r“The same ink should often serve more than one graphical purpose. A graphical element may carry data information and also perform a design function usually left to non-data-ink. Or it might show several different pieces of data. Such multi-functioning graphical elements, if designed with care and subtlety, can effectively display complex, multivariate data.” (Edward Tufte [44])\rRedundancy helps with visualization recall and understanding.\rWhen redundancy is present, to communicate quantitative values (data redundancy) or the main trends or concepts of a visualization (message redundancy), the data is presented more clearly as measured through better-quality descriptions and a better understanding of the message of the visualization at recall (Sec. 6.3).\r“Redundancy, upon occasion, has its uses; giving a context and order to complexity, facilitating comparisons over various parts of the data, perhaps creating an aesthetic balance.” (Edward Tufte [44])\r“Telling things once is often not enough: redundancy helps restore messages damaged by noise.” (Jean-Luc Doumont [14])\rAll of these findings come down to the following well-phrased com- munication advice:\r“Effective communication is getting messages across. Thus it implies some- one else: it is about an audience, and it suggests that we get this audience to understand something. To ensure that they understand it, we must first get them to pay attention. In turn, getting them to understand is usually nothing but a means to an end: we may want them to remember the material com- municated, be convinced of it, or ultimately, act or at least be able to act on the basis of it.” (Jean-Luc Doumont [14])\rThe previous paper on visualization memorability [8] presented find- ings on which visualizations are most memorable and which are most forgettable, when participants are only given 1 second to encode each visualization. We were able to show that this “at-a-glance” memora- bility is very consistent across participants, and thus likely a property of the visualizations themselves. In this paper, we extended encoding to give participants enough time to process the content (textual compo- nents, messages, and trends) of a visualization. We measured memora- bility and analyzed the information that participants were able to recall about the visualizations. Participants remembered and forgot the same visualizations whether given 1 or 10 seconds for encoding, and more importantly, participants better recalled the main message/content of the visualizations that were more memorable “at-a-glance”. Thus, to get people to understand a visualization, “we must first get them to pay attention” - memorability is one way to get there. Then, with care, one can design and introduce as necessary text, pictograms, and redundancy into the visualization so as to get the messages across.\rACKNOWLEDGMENTS\rThe authors thank Phillip Isola, Alexander Lex, Tamara Munzner, Hendrik Strobelt, and Deqing Sun for their helpful comments on this research and paper. This work was partly supported by the National Science Foundation (NSF) under grant 1016862, Google, and Xerox awards to A.O. M.B. was supported by the NSF Graduate Research Fellowship Program and a Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery grant. Z.B. was supported by the NSERC Postgraduate Doctoral Scholarship (NSERC PGS-D). N.K. was supported by the Kwanjeong Educational Foundation.\r\nREFERENCES\r[1] B. E. Alper, N. Henry Riche, and T. Hollerer. Structuring the space: a study on enriching node-link diagrams with visual references. In CHI ’14, pages 1825–1834. ACM, 2014.\r[2] F.Amini,N.H.Riche,B.Lee,C.Hurter,andP.Irani.Understandingdata videos: Looking at narrative visualization through the cinematography lens. In CHI ’15, pages 1459–1468. ACM, 2015.\r[3] W. A. Bainbridge, P. Isola, and A. Oliva. The intrinsic memorability of face photographs. J. of Exp. Psychology: General, 142(4):1323, 2013.\r[4] S. Bateman, R. L. Mandryk, C. Gutwin, A. Genest, D. McDine, and C. Brooks. Useful junk?: the effects of visual embellishment on com- prehension and memorability of charts. In CHI ’10, pages 2573–2582, 2010.\r[5] A. F. Blackwell and T. Green. Does metaphor increase visual language usability? In Visual Languages, pages 246–253. IEEE, 1999.\r[6] T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. State-of-the-art of visualization for eye tracking data. In Proceed- ings of EuroVis, volume 2014, 2014.\r[7] R. Borgo, A. Abdul-Rahman, F. Mohamed, P. W. Grant, I. Reppa, L. Floridi, and M. Chen. An empirical study on using visual embellish- ments in visualization. IEEE TVCG, 18(12):2759–2768, 2012.\r[8] M. A. Borkin, A. A. Vo, Z. Bylinskii, P. Isola, S. Sunkavalli, A. Oliva, and H. Pfister. What makes a visualization memorable? IEEE TVCG, 19(12):2306–2315, 2013.\r[9] T. F. Brady, T. Konkle, and G. A. Alvarez. A review of visual memory capacity: Beyond individual items and toward structured representations. J. of Vision, 11(5):1–34, 2011.\r[10] T. F. Brady, T. Konkle, G. A. Alvarez, and A. Oliva. Visual long- term memory has a massive storage capacity for object details. PNAS, 105(38):14325–14329, 2008.\r[11] M. Burch, N. Konevtsova, J. Heinrich, M. Hoeferlin, and D. Weiskopf. Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study. IEEE TVCG, 17(12):2440–2448, Dec 2011.\r[12] Z. Bylinskii, P. Isola, C. Bainbridge, A. Torralba, and A. Oliva. Intrinsic and extrinsic effects on image memorability. Vision Research, 2015. (In Press).\r[13] W. S. Cleveland and R. McGill. Graphical perception: Theory, experi- mentation, and application to the development of graphical methods. J. of the Am. Stat. Assoc., 79(387):531–554, 1984.\r[14] J. Doumont. Trees, Maps, and Theorems: Effective Communication for Rational Minds. Principiae, 2009.\r[15] S. Few. The chartjunk debate: A close examination of recent findings. Visual Business Intelligence Newsletter, 2011.\r[16] S.GhaniandN.Elmqvist.Improvingrevisitationingraphsthroughstatic spatial features. In Proceedings of Graphics Interface 2011, pages 175– 182. Canadian Human-Computer Communications Society, 2011.\r[17] J. H. Goldberg and J. I. Helfman. Comparing information graphics: A critical look at eye tracking. In BELIV’10, pages 71–78. ACM, 2010.\r[18] S.Haroz,R.Kosara,andS.L.Franconeri.Isotypevisualization–working memory, performance, and engagement with pictographs. In CHI ’15, pages 1191–1200. ACM, 2015.\r[19] S. Haroz and D. Whitney. How capacity limits of attention influence information visualization effectiveness. IEEE TVCG, 18(12):2402–2410, 2012.\r[20] W. Huang. Using eye tracking to investigate graph layout effects. In APVIS ’07, pages 97–100, Feb 2007.\r[21] W. Huang, P. Eades, and S.-H. Hong. A graph reading behavior: Geodesic-path tendency. In PacificVis ’09, pages 137–144, April 2009.\r[22] J.Hullman,E.Adar,andP.Shah.Benefittinginfoviswithvisualdifficul- ties. IEEE TVCG, 17(12):2213–2222, 2011.\r[23] P. Isola, J. Xiao, D. Parikh, A. Torralba, and A. Oliva. What Makes a Photograph Memorable? PAMI, 36(7):1469–1482, 2014.\r[24] L. L. Jacoby. A process dissociation framework: Separating automatic from intentional uses of memory. J. of Memory and Language, 30:513– 541, 1991.\r[25] S.-H. Kim, Z. Dong, H. Xian, B. Upatising, and J. S. Yi. Does an eye tracker tell the truth about visualizations?: Findings while investigating visualizations for decision making. IEEE TVCG, 18(12):2421–2430, Dec 2012.\r[26] P. Klaver, J. Fell, T. Dietl, S. Schu ̈r, C. Schaller, C. Elger, and G. Ferna ́ndez. Word imageability affects the hippocampus in recogni- tion memory. Hippocampus, 15(6):704–712, 2005.\r[27] T. Konkle, T. F. Brady, G. A. Alvarez, and A. Oliva. Conceptual distinc- tiveness supports detailed visual long-term memory for real-world ob- jects. J. of Exp. Psychology: General, 139(3):558–578, 2010.\r[28] C.Ko ̈rner.Eyemovementsrevealdistinctsearchandreasoningprocesses in comprehension of complex graphs. Applied Cognitive Psychology, 25(6):893–905, 2011.\r[29] S. M. Kosslyn. Understanding charts and graphs. Applied Cognitive Psychology, 3(3):185–225, 1989.\r[30] J. P. Leboe and B. W. A. Whittlesea. The inferential basis of familiarity and recall: Evidence for a common underlying process. J. of Memory and Language, 46:804–829, 2002.\r[31] H. Li and N. Moacdieh. Is chart junk useful? an extended examination of visual embellishment. In Proceedings of the Human Factors and Er- gonomics Society Annual Meeting, volume 58, pages 1516–1520. SAGE Publications, 2014.\r[32] H.R.Lipford,F.Stukes,W.Dou,M.E.Hawkins,andR.Chang.Helping users recall their reasoning process. In 2010 IEEE Symposium on Visual Analytics Science and Technology (VAST), pages 187–194. IEEE, 2010.\r[33] K. Mahowald, P. Isola, E. Fedorenko, A. Oliva, and E. Gibson. Memo- rable words are monogamous: An information-theoretic account of word memorability. 2015. (Under Review).\r[34] G. Mandler. Recognizing: The judgment of previous occurrence. Psy- chological Review, 87:252–271, 1980.\r[35] K. Marriott, H. Purchase, M. Wybrow, and C. Goncu. Memorability of visual features in network diagrams. IEEE TVCG, 18(12):2477–2485, 2012.\r[36] A. Paivio. Mental imagery in associative learning and memory. Psycho- logical Review, 76(3), 1969.\r[37] A. V. Pandey, K. Rall, M. L. Satterthwaite, O. Nov, and E. Bertini. How deceptive are deceptive visualizations?: An empirical analysis of com- mon distortion techniques. In CHI ’15, pages 15–03, 2015.\r[38] S. Pinker. A theory of graph comprehension. Artificial intelligence and the future of testing, pages 73–126, 1990.\r[39] M. Pohl, M. Schmitt, and S. Diehl. Comparing the readability of graph layouts using eyetracking and task-oriented analysis. In Computational Aesthetics in Graphics, Visualization and Imaging, pages 49–56, 2009.\r[40] M. Raschke, T. Blascheck, M. Richter, T. Agapkin, and T. Ertl. Visual analysis of perceptual and cognitive processes. In IJCV, 2014.\r[41] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. Labelme: a database and web-based tool for image annotation. International J. of Computer Vision, 77(1-3):157–173, 2008.\r[42] H.Siirtola,T.Laivo,T.Heimonen,andK.-J.Raiha.Visualperceptionof parallel coordinate visualizations. In Intern. Conf. on Information Visu- alisation, pages 3–9, July 2009.\r[43] D. Skau, L. Harrison, and R. Kosara. An evaluation of the impact of visual embellishments in bar charts. In Proceedings of EuroVis, volume 2015, 2015.\r[44] E. Tufte. The Visual Display of Quantitative Information. Cheshire (Conn.), 2001.\r[45] E.Tulving.Memoryandconsciousness.CanadianPsychology,26:1–12, 1985.\r[46] A. Vande Moere, M. Tomitsch, C. Wimmer, B. Christoph, and T. Grechenig. Evaluating the effect of style in information visualization. IEEE TVCG, 18(12):2739–2748, 2012.\r[47] S. Vogt and S. Magnussen. Long-term memory for 400 pictures on a common theme. Experimental Psychology, 54(4):298–303, 2007.\r[48] I.WalkerandC.Hulme.Concretewordsareeasiertorecallthanabstract words: Evidence for a semantic contribution to short-term serial recall. J. of Exp. Psychology: Learning, Memory, and Cognition, 25(5), 1999.","eventTime":1448462539000,"mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"recogniz","time":1461915874220,"auto":true,"weight":1.0},{"@type":"Tag","text":"fixat","time":1461915874182,"auto":true,"weight":1.0},{"@type":"Tag","text":"memor","time":1461915874263,"auto":true,"weight":1.0},{"@type":"Tag","text":"visual","time":1461915874044,"auto":true,"weight":1.0},{"@type":"Tag","text":"infograph","time":1461915874328,"auto":true,"weight":1.0},{"@type":"Tag","text":"hr","time":1461915874316,"auto":true,"weight":1.0},{"@type":"Tag","text":"messag","time":1461915874288,"auto":true,"weight":1.0},{"@type":"Tag","text":"titl","time":1461915874304,"auto":true,"weight":1.0},{"@type":"Tag","text":"redund","time":1461915874276,"auto":true,"weight":1.0},{"@type":"Tag","text":"pictogram","time":1461915874245,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":18,"appId":"93547d48bcc0a23eb3944cbb31b6addeaa0e06e5","timeCreated":1456673463323,"timeModified":1461915876783,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.052590147,"uri":"file:///Users/cheny13/Documents/References/book_information-visualization-perception-for-design_Ware_Chapter1.pdf","plainTextContent":"SECOND EDITION\rINFORMATION\rVISUALIZATION\rPERCEPTION FOR DESIGN\rCOLIN WARE\rEgocentric object and Pattern map\r\nThe Morgan Kaufmann Series in Interactive Technologies\rSeries Editors: Stuart Card, PARC; Jonathan Grudin, Microsoft;Jakob Nielsen, Nielsen Norman Group\rInformation Visualization: Perception for Design, 2\"dEdition Colin Ware\rInteraction Design for Complex Problem Solving: Developing Useful and Usable Software\rBarbara Mire1\rThe Craft of Information Visualization: Readings and Reflections\rWritten and edited by Ben Bederson and Ben Shneiderman\rHCI Models, Theories, and Frameworks: Towards a Multidisciplinary Science Edited by John M. Carroll\rWeb Bloopers: 60 Common Web Design Mistakes, and How to Avoid Them\rJ e f fJohnson\rObserving the User Experience:\rA Practitioner's Guide to User Research Mike Kuniavsky\rPaper Prototyping: The Fast and Easy Way to Design and Refine User Interfaces\rCarolyn Snyder\rPersuastue Technology: Uszng Computers to , _Cbange -t   3Vs'Thtek and Do\rB: j? Fogg\rCoordznatzng User-Interfaces for Conszstency\rEdaed hy Jaltob Nielsen\rUsabzlzty for the'Web:\rDeszgnlng Web Sztes that Work\rTom Brinck, Darren Gergle, and Scott D.Wood\rUsability Engineering: Scenario-Based Development of human-Computer Interaction Mary Beth Rosson and John M. Carroll\rYour Wish is My Command: Programming by Example Edited by Henry Lieberman\rGUI Bloopers: Don'ts and Dos for Softzuare Developers and Web Designers JeffJohnson\rInformation Visualization: Perception for Design Colin Ware\rRobots for Kids: Exploring New Technologies for Learning\rEdited by Allison Druin and James Hendler\rInformation Appliances and Beyond: Interaction Design for Consumer Products Edited by Eric Bergman\rReadings in Information Visualization:\rUsing Vision to Think\rWritten and edited by Stuart K. Card, Jock D. Mackinlay, and Ben Shneiderman\rThe Design of Children's Technology Edited by Allison Druin\rWeb Site Usability: A Designer's Guide\rJared M. Spool, Tara Scanlon, Will Schroeder, Carolyn Snyder, and Terri DeAngelo\rThe Usability Engineering Lifecycle: A Practitioner's Handbook for User Interface Design Deborah J. Mayhew\rContextual Design: Defining Customer-Centered Systems\rHugh Beyer and Karen Holtzblatt\rHuman-Computer Interface Design: Success Stories, Emerging Methods, and Real World Context\rEdited by Marianne Rudisill, Clayton Lewis, Peter P. Polson, and Timothy D. McKay\r\nI NFORMA TI ON\rPerception for Design\rSecond Edition\rColin Ware\rAMSTERDAM NEW YORK\r.\rHEIDELBERG  LONDON PARIS  SAN DIEGO\rBOSTON\rM :(\rMORGAN KAUFMANN PUBLISHERS\rOXFORD\rSAN FRANCISCO SINGAPORE SYDNEY  TOKYO\rMorganKaufmannisanimprintofElsevier\r\nPublishing Director Publishing Services Manager Project Manager\rEditorial Coordinator Marketing Manager Production Coordinator Cover Design\rText Design\rComposition\rTechnical Illustration Copyeditor\rProofreader\rIndexer\rDiane Cerra\rSimon Crump\rJustin Palmeiro\rMona Buehler\rBrian Grimm\rGraphic World Publishing Services Ross Carron Design\rFrances Baca, Frances Baca Design\rSNP Best-set Typesetter, Ltd., Hong ICong SNP Best-set Typesetter, Ltd., Hong Kong Graphic World Publishing Services Graphic World Publishing Services\rTy Koontz\rMorgan Kaufmann Publishers is an imprint of Elsevier. 500 Sansome Street, Suite 400, San Francisco, CA 94111\rThis book is printed on acid-free paper.\rO 2004 by Elsevier Inc. All rights reserved. @\rDesignations used by companies to distinguish their products are often claimed as trademarks or registered trademarks. In all instances in which Morgan ICaufmann Publishers is aware of a claim, the product names appear in initial capital or all capital letters. Readers, however, should contact the appropriate companies for more complete information regarding trademarks and registration.\rNo part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means- electronic, mechanical, photocopying, scanning, or otherwise- without prior written permission of the publisher.\rPermissions may be sought directly from Elsevier's Science & Technology Rights Department in Oxford, UIC: phone: (+44)1865 843830, fax: (+44)1865 853333, e-mail: permissions@e1sevier.c0m.uk. You may also complete your request on-line via the Elsevier homepage (http://elsevier.com)by selecting \"Customer Support\" and then \"Obtaining Permissions.\"\rLibrary of Congress Cataloging-in-Publication Data Application submitted.\rISBN: 1-55860-819-2\rFor information on all Morgan Kaufmann publications, visit our website at www.mkp.com.\rPrinted in China\r09 08 07 06 05 5 4 3 2\r\nFigure Credits xv\rFor ewor d xvii\rP r e f a c e xi x\rPreface to the first Edition xxi\rCHAPTER 1\rFoundation for a Science of Data Visualization 1 Visualization Stages 4\rExperimental Semiotics Based on Perception 5\rSemiotics of Graphics 6\rPictures as Sensory Languages 8\rSensory versus Arbitrary Symbols 10\rProperties of Sensory and Arbitrary Representation 13 Testing Claims about Sensory Representations 15 Arbitrary Conventional Representations 15\rThe Study of Arbitrary Conventional Symbols 17\rCONTENTS\rA Model of Perceptual Processing 20\rStage 1: Parallel Processing to Extract Low-Level Properties of the Visual Scene 20 Stage 2: Pattern Perception 21\rStage 3: Sequential Goal-Directed Processing 22\rTypes of Data 23 Entities 23\rRelationships 23\rAttributes of Entities or Relationships 24 Operations Considered as Data 25\rMetadata 26 Conclusion 27\r\nxxvi INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rate students and research assistants on visualization-related projects: Daniel Jessome, Richard Guitard, Timothy Lethbridge, Siew Hong Yang, Sean Riley, Serge Limoges, David Fowler, Stephen Osborne, I<. Wing Wong, Dale Chapman, Pat Cavanaugh, Raviil Balakrishnan, Mark Paton, Monica Sardesai, Cyril Gobrecht, Suryan Stalin, Justine Hickey, Yanchao Li, Rohan Parkhi, Kathy Lowther, Li Wang, Greg Parker, Daniel Fleet, Jun Yang, Graham Sweet, Roland Arsenault, Natalie Webber, Poorang Irani, Jordan Lutes, Nhu Le, Irina Padioukova, Glenn Franck, Lyn Bartram, and Matthew Plumlee. Many of the ideas presented here have been refined through their efforts.\rPeter Pirolli, Doug Gillan, and Nahum Gershon made numerous suggestions that helped me improve the manuscript. As a result, the last two chapters, especially, underwent radical revision. I also wish to thank the editorial staff at Morgan Kaufmann: Diane Cerra, Belinda Breyer, and Heather Collins. Finally, my wife, Dianne Ramey, read every word, made it readable, and kept me going.\r\nCHAPTER 1 Foundation for a Science\rof Data Visualization\rIn his book The End of Science, science writer John Horgan (1997) argues that science is finished except for the mopping up of details. He makes a good case where physics is con- cerned. In that discipline, the remaining deep problems may involve generating so much energy as to require the harnessing of entire stars. Similarly, biology has its foundations in DNA and genetics and is now faced with the infinite but often tedious complexity of mapping genes into proteins through intricate pathways.\rWhat Horgan fails to recognize is that cognitive science has fundamental problems that are still to be solved. In particular, the mechanisms of the construction and storage of knowledge remain open questions. He implicitly adopts the physics-centric view of science, which holds that physics is the queen of sciences, and in descending order come chemistry, then biology, with psychology barely acknowledged as a science at all. In this pantheon, sociology is regarded as somewhere on a par with astrology. This attitude is short-sighted. Chemistry builds on physics, enabling our understanding of materials; biology builds on chemistry, enabling us to understand the much greater complexity of living organisms; and psychology builds on neurophysiology, enabling us to understand the processes of cognition. At each level is a separate discipline greater in complexity and level of difficulty than those beneath. It is difficult to conceive of a value scale for which the mechanisms of thought are not of fundamentally greater interest and importance than the interaction of subatomic particles.\rThose who dismiss psychology as a pseudo-science have not being paying attention. Over the past few decades, enormous strides have been made in identifying the brain structures and cognitive mechanisms that have enabled humans to create the huge body of knowledge that now exists. But we need to go one step further and recognize that people with machines, and in groups, are much more cognitively powerful than a single person alone with his or her thoughts. This has been true for a long time. Artifacts such as paper, writing, and geometry instruments have been cognitive tools for centuries. It is not necessary to take the cultural relativists' view to see that sciences are built using socially constructed symbol systems. The review process employed\r\n2 INFORMA TION VISUALIZATION: PERCEPTION FOR DESIGN\rby scientific journals is an obvious example of a social process critical to the construction of knowledge.\rAs Hutchins (1995)so effectivelypointed out, thinking is not something that goes on entirely, or even mostly, inside people's heads. Little intellectual work is accomplished with our eyes and ears closed. Most cognition is done as a kind of interaction with cognitive tools, pencils and paper, calculators, and increasingly, computer-based intellectual supports and information systems. Neither is cognition mostly accomplished alone with a computer. It occurs as a process in systems containing many people and many cognitive tools. Since the beginning of science, dia- grams, mathematical notations, and writing have been essential tools of the scientist. Now we have powerful interactive analytic tools, such as MATLAB, Maple, Mathematica, and S-PLUS, together with databases. The entire fields of genomics and proteomics are built on computer storage and analytic tools. The social apparatus of the scl~ooslystem, the university, the acade- mic journal, and the conference are obviously designed to support cognitive activity.\rBut we should not consider classical science only. Cognition in engineering, banking, busi- ness, and the arts is similarly carried out through distributed cognitive systems. In each case, \"thinking\" occurs through interaction between individuals, using cognitive tools, and operating within social networks. Hence, cognitive systems theory is a much broader discipline than psy- chology. This is emerging as the most interesting, difficult, complex, yet fundamentally the most important, of sciences.\rVisualizations have a small but crucial and expanding role in cognitive systems. Visual dis- plays provide the highest bandwidth channel from the computer to the human. We acquire more information through vision than through all of the other senses combined. The 20 billion or so neurons of the brain devoted to analyzing visual information provide a pattern-finding mecha- nism that is a fundamental component in much of our cognitive activity. Improving cognitive systems often means tightening the loop between a person, computer-based tools, and other indi- viduals. On the one hand, we have the human visual system, a flexible pattern finder, coupled with an adaptive decision-making mechanism. On the other hand are the computational power and vast information resources of the computer and the World Wide Web. Interactive visualiza- tions are increasingly the interface between the two. Improving these interfaces can substantially improve the performance of the entire system.\rUntil recently, the term visualization meant constructing a visual image in the mind (Shorter Oxford English Dictionary, 1972) It has now come to mean something more like a graphical representation of data or concepts. Thus, from being an internal construct of the mind, a visu- alization has become an external artifact supporting decision making. The way visualization func- tions as cognitive tools is the subject of this book.\rOne of the greatest benefits of data visualization is the sheer quantity of information that can be rapidly interpreted if it is presented well. Figure 1.1 shows a visualization derived from a multibeam echo sounder scanning part of Passamoquoddy Bay, between Maine, in the United States, and New Brunswick, Canada, where the tides are the highest in the world. Approximately one million measurements were made. Traditionally, this kind of data is presented in the form of a nautical chart with contours and spot soundings. However, when the data is converted to a\r\nFigure 1.1 Passamoquoddy Bay visualization. Data courtesy o f the Canadian Hydrographic Service.\rheight field and displayed using standard computer graphics techniques, many things become visible that were previously invisible on the chart. A pattern of features called pockmarks can immediately be seen, and it is easy to see how they form lines. Also visible are various problems with the data. The linear ripples (not aligned with the pockmarks) are errors in the data because the roll of the ship that took the measurements was not properly taken into account.\rThe Passamoquoddy Bay image highlights a number of the advantages of visualization: Visualization provides an ability to comprehend huge amounts of data. The important\rinformation from more than a million measurements is immediately available.\rVisualization allows the perception of emergent properties that were not anticipated. In this visualization, the fact that the pockmarks appear in lines is immediately evident. The perception of a pattern can often be the basis of a new insight. In this case, the pockmarks align with the direction of geological faults, suggesting a cause. They may be due to the release of gas.\rVisualization often enables problems with the data itself to become immediately apparent. A visualization commonly reveals things not only about the data itself, but about the way it is collected. With an appropriate visualization, errors and artifacts in the data often\rjump out at you. For this reason, visualizations can be invaluable in quality control.\rVisualization facilitates understanding of both large-scale and small-scale features of the data. It can be especially valuable in allowing the perception of patterns linking local features.\rFoundation for a Science of Data Visualization 3\r\n4 INFORMA TION VISUALIZATION: PERCEPTION FOR DESIGN\rVisualization facilitates hypothesis formation. For example, the visualization in Figure 1.1 was directly responsible for a research paper concerning the geological significance of the pockmark features (Gray et al., 1997).\rThis first chapter has the general goal of defining the scope of a science of visualization based on perceptual principles. Much of it is devoted to outlining the intellectual basis of the endeavor and providing an overview of the kinds of experimental techniques appropriate to visualization research. In the latter half of the chapter, a brief overview of human visual processing is intro- duced to provide a kind of road map to the more detailed analysis of later chapters. The chapter concludes with a categorization of data. It is important to have a conception of the kinds of data we may wish to visualize so that we can talk in general terms about the ways in which whole classes of data should be represented.\rVisualization Stages\rThe process of data visualization includes four basic stages, combined in a number of feedback loops. These are illustrated in Figure 1.2.\rData exploration\r/-\rPreprocessing\rand transformation n\r7\rial environr-\r',\rPhysica\ri '.\rFigure 1.2\rA schematic diagram of the visualization process.\rment\r\nThe four stages consist of:\rThe collection and storage of data itself\rThe preprocessing designed to transform the data into something we can understand The display hardware and the graphics algorithms that produce an image on the screen The human perceptual and cognitive system (the perceiver)\rThe longest feedback loop involves gathering data. A data seeker, such as a scientist or a stock-market analyst, may choose to gather more data to follow up on an interesting lead. Another loop controls the computational preprocessing that takes place prior to visualization. The analyst may feel that if the data is subjected to a certain transformation prior to visualiza- tion, it can be persuaded to give up its meaning. Finally, the visualization process itself may be highly interactive. For example, in 3D data visualization, the scientist may fly to a different vantage point t o better understand the emerging structures. Alternatively, a computer mouse may be used interactively, to select the parameter ranges that are most interesting. Both the physical environment and the social environment are involved in the data-gathering loop. The physical environment is a source of data, while the social environment determines in subtle and complex ways what is collected and how it is interpreted.\rIn this book, the emphasis is on data, perception, and the various tasks to which visualiza- tion may be applied. In general, algorithms are discussed only insofar as they are related to perception. The computer is treated, with some reservations, as a universal tool for producing interactive graphics. This means that once we figure out the best way to visualize data for a particular task, we assume that we can construct algorithms to create the appropriate images. The critical question is how best to transform the data into something that people can understand for optimal decision making. Before plunging into a detailed analysis of human per- ception and how it applies in practice, however, we must establish the conceptual basis for the endeavor.\rThe purpose of this discussion is to stake out a theoretical framework wherein claims about visualizations being \"visually efficient\" or \"natural\" can be pinned down in the form of testable predictions.\rExperimental Semiotics Based on Perception\rThis book is about the science of visualization, as opposed to the craft or art of visualization. But the claim that visualization can be treated as a science may be disputed. Let's look at the alternative view. Some scholars argue that visualization is best understood as a kind of learned language and not as a science at all. In essence, their argument is that visualization is about dia- grams and how they can convey meaning. Generally, diagrams are held to be made up of symbols, and symbols are based on social interaction. The meaning of a symbol is normally understood\rFoundation for a Science of Data Visualization 5\r\n6 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rto be created by convention, which is established in the course of person-to-person communica- tion. Diagrams are arbitrary and are effective in much the same way as the written words on this page are effective- we must learn the conventions of the language, and the better we learn them, the clearer that language will be. Thus, one diagram may ultimately be as good as another; it is just a matter of learning the code, and the laws of perception are largely irrelevant. This view has strong pl~ilosophicapl roponents from the field of semiotics. Although it is not the posi- tion adopted here, the debate can help us define where vision research can assist us in designing better visualizations, and where we would be wise to consult a graphic designer trained in an art college.\rSemiotics of Graphics\rThe study of symbols 'and how they convey meaning is called semiotics. This discipline was orig- inated in the United States by C.S. Peirce and later developed in Europe by the French philoso- pher and linguist Ferdinand de Saussure (1959). Semiotics has been dominated mostly by philosophers and by those who construct arguments based on example rather than on formal experiment. In his great masterwork, Semiology of Graphics, Jacques Bertin (1983) attempted to classify all graphic marks in terms of how they could express data. For the most part, this work is based on his own judgment, although it is a highly trained and sensitive judgment. There are few, if any, references to theories of perception or scientific studies.\rIt is often claimed that visual languages are easy to learn and use. But what do we mean by the term visual language- clearly not the writing on this page. Reading and writing take years of education to master, and it can take almost as long to master some diagrams. Figure 1.3 shows three examples of languages that have some claim to being visual. The first example of visual language is based on a cave painting. We can readily interpret human figures and infer that the people are using bows and arrows to hunt deer. The second example is a schematic diagram showing the interaction between a person and a computer in a virtual environment system; the brain in the diagram is a simplified picture, but it is a part of the anatomy that few have directly perceived. The arrows show data flows and are arbitrary conventions, as are the printed words. The third example is the expression of a matllematical equation that is utterly obscure to all but the initiated. These examples clearly show that some visual languages are easier to \"read\" than others. But why? Perhaps it is simply that we have more experience with the kind of pictorial image shown in the cave painting and less with the mathematical notation. Perhaps the concepts expressed in the cave painting are more familiar than those in the equation.\rThe most profound threat to the idea that there can be a science of visualization originates with Saussure. He defined a principle of arbitrariness as applying to the relationship between the symbol and the thing that is signified. Saussure was also a founding member of a group of struc- turalist philosophers and anthropologists who, although they disagreed on many f~~ndamental issues, were unified in their general insistence that truth is relative to its social context. Meaning\rin one culture may be nonsense in another. A trash can as a visual symbol for deletion is mean- ingful only to those who Itnow how trash cans are used. Thinkers such as LCvi-Strauss, Barthes,\r\nFigure 1.3 Three graphics. Each could be said t o be a visualization.\rFoundation for a Science of Data Visualization 7\r\n8 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rand Lacan have condemned the cultural imperialism and intellectual arrogance implicit in apply- ing our intellects to characterizing other cultures as \"primitive.\" As a result, they have developed the theory that all meaning is relative to the culture. Indeed, meaning is created by society. They claim that we can interpret another culture only in the context of our own culture and using the tools of our own language. Languages are conventional means of communication in which the meanings of symbols are established through custom. Their point is that no one representa- tion is \"better\" than another. All representations have value. All are meaningful to those who understand them and agree to their meanings. Because it seems entirely reasonable to consider visualizations as communications, their argument strikes at the root of the idea that there can be a natural science of visualization with the goal of establishing specific guidelines for better representations.\rPictures as Sensory Languages\rThe question of whether pictures and diagrams are purely conventional, or are perceptual symbols with special properties, has been the subject of considerable scientific investigation. A good place to begin reviewing the evidence is the perception of pictures. There has been a debate over the last century between those who claim that pictures are every bit as arbitrary as words and those who believe that there may be a measure of similarity between pictures and the things that they represent. This debate is crucial to the theory presented here; if even \"realistic\" pic- tures do not embody a sensory language, it will be impossible to make claims that certain dia- grams and other visualizations are better designed perceptually.\rThe nominalist philosopher Nelson Goodman has delivered some of the more forceful attacks on the notion of similarity in pictures (1968):\rRealistic representation, in brief, depends not upon imitation or illusion or information but upon inculcation. Almost any picture may represent almost anything; that is, given picture and object there is usually a system o f representation- a plan of correlation- under which the picture represents the object.\rFor Goodman, realistic representation is a matter of convention; it \"depends on how stereotyped the model of representation is, how commonplace the labels and their uses have become.\" Bieusheuvel (1947)expresses the same opinion: \"The picture, particularly one printed on paper, is a highly conventional symbol, which the child reared in Western culture has learned to inter- pret.\" These statements, taken at face value, invalidate any meaningful basis for saying that a certain visualization is fundamentally better or more natural than another. This would mean that all languages are equally valid and that all are learned. If we accept this position, the best approach to designing visual languages would be to establish graphical conventions early and stick to them. It would not matter what the conventions were, only that we adhered to them in order to reduce the labor of learning new conventions.\rIn support of the nominalist argument, a number of anthropologists have reported expres- sions of puzzlement from people who encounter pictures for the first time. \"A Bush Negro woman\r\nFoztndation for a Science of Data Visualization 9\rturned a photograph this way and that, in attempting to make sense out of the shadings of gray on the piece of paper she held\" (Herskovits, 1948). The evidence related to whether or not we must learn to see pictures has been carefully reviewed and analyzed by Kennedy (1974).He rejects the strong position that pictures and other visual representations are entirely arbitrary. In the case of the reported puzzlement of people who are seeing pictures for the first time, Kennedy argues that these people are amazed by the technology rather than unable to interpret the picture. After all, a photograph is a remarkable artifact. What curious person would not turn it over to see if, perhaps, the reverse side contains some additional interesting information?\rHere are two of the many studies that contradict the nominalist position and suggest that people can interpret pictures without training. Deregowski (1968)reported studies of adults and children, in a remote area of Zambia, who had very little graphic art. Despite this, these people could easily match photographs of toy animals with the actual toys. In an extraordinary but very different kind of experiment, Hochberg and Brooks (1962) raised their daughter nearly to the age of two in a house with no pictures. She was never read to from a picture book and there were no pictures on the walls in the house. Although her parents could not completely block the child's exposure to pictures on trips outside the house, they were careful never to indicate a picture and tell the child that it was a representation of something. Thus, she had no social input telling her that pictures had any kind of meaning. When the child was finally tested, she had a reasonably large vocabulary, and she was asked to identify objects in line drawings and in black- and-white photographs. Despite her lack of instruction in the interpretation of pictures, she was almost always correct in her answers.\rHowever, the issue of how pictures, and especially line drawings, are able to unambiguously represent things is still not fully understood. Clearly, a portrait is a pattern of marks on a page; in a physical sense, it is utterly unlike the flesh-and-blood person it depicts. The most probable explanation is that at some stage in visual processing, the pictorial outline of an object and the object itself excite similar neural processes (Pearson et al., 1990). This view is made plausible by the ample evidence that one of the most important products of early visual processing is the extraction of linear features in the visual array. These may be either the visual boundaries of objects or the lines in a line drawing. The nature of these mechanisms is discussed further in Chapter 6.\rAlthough we may be able to understand certain pictures without learning, it would be a mistake to underestimate the role of convention in representation. Even with the most realistic picture or sculpture, it is very rare for the artifact to be mistaken for the thing that is represented. Trompe l'oeil art is designed to Lcfootlhe eye\" into the illusion that a painting is real. Artists are paid to paint pictures of niches containing statues that look real, and sometimes, for an instant, the viewer will be fooled. On a more mundane level, a plastic laminate on furniture may contain a photograph of wood grain that is very difficult to tell from the real thing. But in general, a picture is intended to represent an object or a scene; it is not intended to be mistaken for it. Many pictures are highly stylized- they violate the laws of perspective and develop particular methods of representation that no one would call realistic.\r\n10 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rFigure 1.4 Two different graphical methods for showing relationships between entities.\rWhen we turn to diagrams and nonpictorial visualizations, it is clear that convention must play a greater role. Figure 1.3(b)is not remotely \"like\" any scene in the real world under any system of measurement. Nevertheless, we can argue that many elements in it are constructed in ways that for perceptual reasons make the diagram easy to interpret. The lines that connect the various components, for example, are a notation that is easy to read, because the visual cortex of the brain contains mechanisms specifically designed to seek out continuous contours. Other possible graphical notations for showing connectivity would be far less effective. Figure 1.4 shows two different conventions for demonstrating relationships between entities. The connecting lines on the left are much more effective than the symbols on the right.\rSensory versus Arbitrary Symbols\rIn this book, the word sensory is used to refer to symbols and aspects of visualizations that derive their expressive power from their ability to use the perceptual processing power of the brain without learning. The word arbitrary is used to define aspects of representation that must be learned, because the representations have no perceptual basis. For example, the written word dog bears no perceptual relationship to any actual animal. Probably very few graphical languages consist of entirely arbitrary conventions, and probably none is entirely sensory. However, the sensory-versus-arbitrary distinction is important. Sensory representations are effective (or mis- leading) because they are well matched to the early stages of neural processing. They tend to be stable across individuals, cultures, and time. A cave drawing of a hunt still conveys much of its meaning across several millennia. Conversely, arbitrary conventions derive their power from culture and are therefore dependent on the particular cultural milieu of an individual.\rThe theory of sensory languages is based on the idea that the human visual system has evolved as an instrument to perceive the physical world. It rejects the idea that the visual system is a truly universal machine. It was once widely held that the brain at birth was an undifferen- tiated neural net, capable of configuring itself to perceive in any world, no matter how strange. According to this theory, if a newborn human infant were to be born into a world with entirely different rules for the propagation of light, that infant would nevertheless learn to see. Partly, this view came from the fact that all cortical brain tissue looks more or less the same, a uniform\r\npinkish gray, so it was thought to be functionally undifferentiated. This tabula rasa view has been overthrown as neurologists have come to understand that the brain has a great many spe- cialized regions. Figure 1.5 shows the major neural pathways between different parts of the brain involved in visual processing (Distler et al., 1993). Although much of the functionality remains unclear, this diagram represents an amazing achievement and summarizes the work of dozens of researchers. These structures are present both in higher primates and in humans. The brain is clearly not an undifferentiated mass; it is more like a collection of highly specialized parallel- processing machines with high-bandwidth interconnections. The entire system is designed to\rFou?tdation for a Science o f Data Visualization 11\rDORSAL PATHWAYS VENTRAL PATHWAYS -\r---------- I\rI\r,-----\r----\rFac\res,\r{ attention,\rVisually guided transient system\rObject perception,\rFiltering for orientation,\rFigure 1.5\rThe major visual pathways of the Macaque monkey. This diagram is included to illustrate the structural complexity of the visual system and because a number of these areas are referenced in different sections of this book. Adapted from Distler et a/. (1993); notes added. Vl- V4, visual areas 1-4; PO, parieto-occipital area; M r middle temporal area (also called V5); De dorsal prestiate area; Pe posterior parietal complex; STS, superiotemporal sulcus complex; 17; inferotemporal cortex.\r\n12 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rextract information from the world in which we live, not from some other environment with entirely different physical properties.\rCertain basic elements are necessary for the visual system to develop normally. For example, cats reared in a world consisting only of vertical stripes develop distorted visual cortices, with an unusual preponderance of vertical-edge detectors. Nevertheless, the basic elements for the development of normal vision are present in all but the most abnormal circumstances. The inter- action of the growing nervous system with everyday reality leads to a more or less standard visual system. This should not surprise us; the everyday world has ubiquitous properties that are common to all environments. All earthly environments consist of objects with well-defined sur- faces, surface textures, surface colors, and a variety of shapes. Objects exhibit temporal persis- tence- they do not randomly appear and vanish, except when there are specific causes. At a more fundamental level, light travels in straight lines and reflects off surfaces in certain ways. The law of gravity continues to operate. Given these ubiquitous properties of the everyday world, the evi- dence suggests that we all develop essentially the same visual systems, irrespective of cultural milieu. Monkeys and even cats have visual structures very similar to those of humans.\rFor example, although Figure 1.5 is based on the visual pathways of the Macaque monkey, a number of lines of evidence show that the same structures exist in humans. First, the same areas can be identified anatomically in humans and animals. Second, specific patterns of blind- ness occur that point to the same areas having the same functions in humans and animals. For example, if the brain is injured in area V4, patients suffer from achromatopsia (Zeki, 1992; Milner and Goodale, 1995). These patients perceive only shades of gray. Also, they cannot recall colors from times before the lesion was formed. Color processing occurs in the same region of the monltey cortex. Third, new research imaging technologies, such as positron emission tomog- raphy (PET)and functional magnetic resonance imaging (fMRI),show that in response to colored or moving patterns, the same areas are active in people as in the Macaque monkey (Zeki, 1992; Beardsley, 1997). The key implication of this is that because we all have the same visual system, it is likely that we all see in the same way, at least as a first approximation. Hence, the same visual designs will be effective for all of us.\rSensory aspects of visualizations derive their expressive power from being well designed to stimulate the visual sensory system. In contrast, arbitrary, conventional aspects of visualizations derive their power from how well they are learned. Sensory and arbitrary representations differ radically in the ways they should be studied. In the former case, we can apply the full rigor of the experimental techniques developed by sensory neuroscience, while in the latter case visualizations and visual symbols can best be studied with the very different interpretive method- ology, derived from the structuralist social sciences. With sensory representations, we can also make claims that transcend cultural and racial boundaries. Claims based on a generalized perceptual processing system will apply to all humans, with obvious exceptions such as color blindness.\rThis distinction between the sensory and social aspects of the symbols used in visualization also has practical consequences for research methodology. It is not worth expending a huge effort carrying out intricate and highly focused experiments to study something that is only this year's\r\nfashion. However, if we can develop generalizations that apply to large classes of visual repre- sentations, and for a long time, the effort is worthwhile.\rIf we accept the distinction between sensory and arbitrary codes, we nevertheless must rec- ognize that most visualizations are hybrids. In the obvious case, they may contain both pictures and words. But in many cases, the sensory and arbitrary aspects of a representation are much more difficult to tease apart. There is an intricate interweaving of learned conventions and hard- wired processing. The distinction is not as clean as we would like, but there are ways of distin- guishing the different kinds of codes.\rProperties of Sensory and Arbitrary Representation\rThe following paragraphs summarize some of the important properties of sensory representations.\rUnderstanding without training: A sensory code is one for which the meaning is perceived without additional training. Usually, all that is necessary is for the audience to understand that some communication is intended. For example, it is immediately clear that the image in Figure 1.6 has an unusual spiral structure. Even though this visually represents a physical process that cannot actually be seen, the detailed shape can be understood because it has been expressed using an artificial shading technique to make it look like a 3D solid object. Our visual systems are built to perceive the shapes of 3D surfaces.\rFoundation for a Science of Data Visualization 13\rFigure 1.6 The expanding wavefront of a chemical reaction is visualized (Cross et al., 1997). Even though this process is alien to most of us, the shape of the structure can be readily perceived.\r\n14 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rFigure 1.7 In the Muller-Lyer illusion, on the left, the horizontal line in the upper figure appears longer than the same line in the lower figure. On the right, the rectangle is distorted into a \"pincushion\" shape.\rResistance to instructional bias: Many sensory phenomena, such as the illusions shown in Figure 1.7, persist despite the knowledge that they are illusory. When such illusions occur in diagrams, they are likely to be misleading. But what is important to the present argument is that some aspects of perception can be taken as bottom-line facts that we ignore at our peril. In general, perceptual phenomena that persist and are highly resistant to change are likely to be hard-wired into the brain.\rSensory immediacy: The processing of certain kinds of sensory information is hard-wired and fast. We can represent information in certain ways that are neurally processed in parallel. This point is illustrated in Figure 1.8, which shows five different textured regions. The two regions on the left are almost impossible to separate. The upright Ts and inverted Ts appear to be a single patch. The region of oblique Ts is easy to differentiate from the neighboring region of inverted Ts. The circles are the easiest to distinguish (Beck,1966). The way in which the visual system divides the visual world into regions is called segmentation. The evidence suggests that this is a function of early rapid-processing systems. (Chapter 5 presents a theory of texture discrimination.)\rCross-cultural validity: A sensory code will, in general, be understood across cultural boundaries. These may be national boundaries or the boundaries between different user groups. Instances in which a sensory code is misunderstood occur when some group has dictated that a sensory code be used arbitrarily in contradiction to the natural interpretation. In this case, the natural response to a particular pattern will, in fact, be wrong.\r\nFigure 1.8 Five regions of texture. Some are easier to distinguish visually than others. Adapted from Beck (1966).\rTesting Claims about Sensory Representations\rEntirely different methodologies are appropriate to the study of representations of the sensory and arbitrary types. In general, the study of sensory representations can employ the scientific methods of vision researchers and biologists. The study of arbitrary conventional representations is best done using the techniques of the social sciences, such as sociology and anthropology; philosophers and cultural critics have much to contribute. Appendix C provides a brief summary of the research methodologies that apply to the study of sensory representations. All are based on the concept of the controlled experiment. For more detailed information on techniques used in vision research and human-factors engineering, see Sekuler and Blake (1990)and Wickens (1992).\rArbitrary Conventional Representations\rArbitrary codes are by definition socially constructed. The word dog is meaningful because we all agree on its meaning and we teach our children the meaning. The word carrot would do just as well, except we have already agreed on a different meaning for that word. In this sense, words are arbitrary; they could be swapped and it would make no difference, so long as they are used consistently from the first time we encounter them. Arbitrary visual codes are often adopted when groups of scientists and engineers construct diagramming conventions for new problems that arise. Examples include circuit diagrams used in electronics, diagrams used to represent molecules in chemistry, and the unified modeling language used in software engi- neering. Of course, many designers will intuitively use perceptually valid forms in the codes, but many aspects of these diagrams are entirely conventional. Arbitrary codes have the following characteristics:\rHard to learn: It takes a child hundreds of hours to learn to read and write, even if the child has already acquired spoken language. The graphical codes of the alphabet and their rules\rFoundation for a Science of Data Visualization 15\r\n16 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rof combination must be laboriously learned. The Chinese character set is reputed to be even harder to work with than the Roman.\rEasy to forget: Arbitrary conventional information that is not overlearned can easily be forgotten. It is also the case that arbitrary codes can interfere with each other. In contrast, sensory codes cannot be forgotten. Sensory codes are hard-wired; forgetting them would be like learning not to see. Still, some arbitrary codes, such as written numbers, are overlearned to the extent that they will never be forgotten. We are stuck with them because the social upheaval involved in replacing them is too great.\rEmbedded in culture and applications: An Asian student in my laboratory was working on an application to visualize changes in computer software. She chose to represent deleted entities with the color green and new entities with red. I suggested to her that red is normally used for a warning, while green symbolizes renewal, so perhaps the reverse coding would be more appropriate. She protested, explaining that green symbolizes death in China, while red symbolizes luck and good fortune. The use of color codes to indicate meaning is highly culture-specific.\rMany graphical symbols are transient and tied to a local culture or application. Think of the graffiti of street culture, or the hundreds of new graphical icons that are being created on the Internet. These tend to stand alone, conveying meaning; there is little or no syntax to bind the symbols into a formal structure. On the other hand, in some cases, arbitrary representations can be almost universal. The Arabic numerals shown in Figure 1.9 are used widely throughout the world. Even if a more perceptually valid code could be constructed, the effort would be wasted. The designer of a new symbology for Air Force or Navy charts must live within the con- fines of existing symbols because of the huge amount of effort invested in the standards. We have many standardized visualization techniques that work well and are solidly embedded in work practices, and attempts to change them would be foolish. In many applications, good design is standardized design.\rCulturally embedded aspects of visualizations persist because they have become embedded in ways in which we think about problems. For many geologists, the topographic contour map is the ideal way to understand relevant features of the earth's surface. They often resist shaded computer graphics representations, even though these appear to be much more intuitively under- standable to most people. Contour maps are embedded in cartographic culture and training.\rFigure 1.9 Two methods for representing the first five digits. The code given below is probably easier to learn. However, it is not easily extended.\r\nFormally powerful: Arbitrary graphical notations can be constructed that embody formally defined, powerful languages. Mathematicians have created hundreds of graphical languages to express and communicate their concepts. The expressive power of mathematics to convey abstract concepts in a formal, rigorous way is unparalleled. However, the languages of mathematics are extremely hard to learn (at least for most people). Clearly, the fact that something is expressed in a visual code does not mean that it is easy to understand.\rCapable of rapid change: One way of looking at the sensorylarbitrary distinction is in terms of the time the two modes have taken to develop. Sensory codes are the products of the millions of years it has taken for our visual systems to evolve. Although the time frames for the evolution of arbitrary conventional representations are much shorter, they can still have lasted for thousands of years (e.g., the number system). But many more have had only a few decades of development. High-performance interactive computer graphics have greatly enhanced our capability to create new codes. We can now control motion and color with great flexibility and precision. For this reason, we are currently witnessing an explosive growth in the invention of new graphical codes.\rThe Study of Arbitrary Conventional Symbols\rThe appropriate methodology for studying arbitrary symbols is very different from that used to study sensory symbols. The tightly focused, narrow questions addressed by psychophysics are wholly inappropriate to investigating visualization in a cultural context. A more appropriate methodology for the researcher of arbitrary symbols may derive from the work of anthropolo- gists such as Clifford Geertz (1973),who advocated \"thick description.\" This approach is based on careful observation, immersion in culture, and an effort to keep \"the analysis of social forms closely tied ... to concrete social events and occasions.\" Also borrowing from the social sciences, Carroll and coworkers have developed an approach to understanding complex user interfaces that they call artifact analysis (Carroll, 1989). In this approach, user interfaces (and presumably visualization techniques) are best viewed as artifacts and studied much as an anthropologist studies cultural artifacts of a religious or practical nature. Formal experiments are out of the question in such circumstances, and if they were actually carried out, they would undoubtedly change the very symbols being studied.\rUnfortunately for researchers, sensory and arbitrary aspects of symbols are closely inter- twined in many representations, and although they have been presented here as distinct cate- gories, the boundary between them is very fuzzy. There is no doubt that culture influences cognition; it is also true that the more we know, the more we may perceive. Pure instances of sensory or arbitrary coding may not exist, but this does not mean that the analysis is invalid. It simply means that for any given example we must be careful to determine which aspects of the visual coding belong in each category.\rIn general, the science of visualization is still in its infancy. There is much about visualiza- tion and visual communication that is more craft than science. For the visualization designer,\rFoundation for a Science o f Data Visualization 17\r\n18 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rtraining in art and design is at least as useful as training in perceptual psychology. For those who wish to do good design, the study of design by example is generally most appropriate. But the science of visualization can inform the process by providing a scientific basis for design rules, and it can suggest entirely new design ideas and methods for displaying data that have not been thought of before. Ultimately, our goal should be to create a new set of conventions for infor- mation visualization, based on sound perceptual principles.\rGibson's Affordnnce Theory\rThe great perception theorist J.J. Gibson brought about radical changes in how we think about perception with his theories of ecological optics, affordances, and direct perception. Aspects of each of these theoretical concepts are discussed throughout this book. We begin with affordance rlieory (Gibson, 1979).\rGibson assumed that we perceive in order to operate on the environment. Perception is designed for action. Gibson called the perceivable possibilities for action affordances; he claimed that we perceive these properties of the environment in a direct and immediate way. This theory is clearly attractive from the perspective of visualization, because the goal of most visualization is decision making. Thinking about perception in terms of action is likely to be much more useful than thinking about how two adjacent spots of light influence each other's appearance (which is the typical approach of classical psychophysicists).\rMuch of Gibson's work was in direct opposition to the approach of theorists who reasoned that we must deal with perception from the bottom up, as with geometry. The pre-Gibsonian theorists tended to have an atomistic view of the world. They thought we should first understand how single points of light were perceived, and then we could work on understanding how pairs of lights interacted and gradually build up to understanding the vibrant, dynamic visual world in which we live.\rGibson took a radically different, top-down approach. He claimed that we do not perceive points of light; rather, we perceive possibilities for action. We perceive surfaces for walking, handles for pulling, space for navigating, tools for manipulating, and so on. In general, our whole evolution has been geared toward perceiving useful possibilities for action. In an experiment that supports this view, Warren (1984)showed that subjects were capable of accurate judgments of the \"climbability\" of staircases. These judgments depended on their own leg lengths. Gibson's affordance theory is tied to a theory of direct perception. He claimed that we perceive affor- dances of the environment directly, not indirectly by piecing together evidence from our senses.\rTranslating the affordance concept into the interface domain, we might construct the fol- lowing principle: to create a good interface, we must create it with the appropriate affordances to make the user's task easy. Thus, if we have a task of moving an object in 3D space, it should have clear handles to use in rotating and lifting the object. Figure 1.10 shows a design for a 3D object-manipulation interface from Houde (1992).When an object is selected, \"handles\" appear that allow the object to be lifted or rotated. The function of these handles is made more explicit by illustrations of gripping hands that show the affordances.\r\nFigure 1.10 Small drawings of hands pop up to show the user what interactions are possible in the prototype interface. Reproduced, with permission, from Houde (1992).\rHowever, Gibson's theory presents problems if it is taken literally. According to Gibson, affor- dances are physical properties of the environment that we directly perceive. Many theorists, unlike Gibson, think of perception as a very active process: the brain deduces certain things about the environment based on the available sensory evidence. Gibson rejected this view in favor of the idea that our visual system is tuned to perceiving the visual world and that we perceive it accurately except under extraordinary circumstances. He preferred to concentrate on the visual system as a whole and not to break perceptual processing down into components and operations. He used the term resonating to describe the way the visual system responds to properties of the environment. This view has been remarkably influential and has radically changed the way vision researchers think about perception. Nevertheless, few would accept it today in its pure form.\rThere are three problems with Gibson's direct perception in developing a theory of visual- ization. The first problem is that even if perception of the environment is direct, it is clear that visualization of data through computer graphics is very indirect. Typically, there are many layers of processing between the data and its representation. In some cases, the source of the data may be microscopic or otherwise invisible. The source of the data may be quite abstract, such as company statistics in a stock-market database. Direct perception is not a meaningful concept in these cases.\rSecond, there are no clear physical affordances in any graphical user interface. To say that a screen button \"affords\" pressing in the same way as a flat surface affords walking is to stretch the theory beyond reasonable limits. In the first place, it is not even clear that a real-world button affords pressing. In another culture, these little bumps might be perceived as rather dull archi- tectural decorations. Clearly, the use of buttons is arbitrary; we must learn that buttons, when pressed, do interesting things in the real world. Things are even more indirect in the computer\rFoundation for a Science o f Data Visualization 19\r\n20 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rworld; we must learn that a picture of a button can be \"pressed\" using a mouse, a cursor, or yet another button. This is hardly a direct interaction with the physical world.\rThird, Gibson's rejection of visual mechanisms is a problem. To take but one example, much that we know about color is based on years of experimentation, analysis, and modeling of the perceptual mechanisms. Color television and many other display technologies are based on an understanding of these mechanisms. To reject the importance of understanding visual mecha- nisms would be to reject a tremendous proportion of vision research as irrelevant. This entire book is based on the premise that an understanding of perceptual mechanisms is basic to a science of visualization.\rDespite these reservations, Gibson's theories color much of this book. The concept of affor- dances, loosely construed, can be extremely useful from a design perspective. The idea suggests that we build interfaces that beg to be operated in appropriate and useful ways. We should make virtual handles for turning, virtual buttons for pressing. If components are designed to work together, this should be made perceptually evident, perhaps by creating shaped sockets that afford the attachment of one object to another. This is the kind of design approach advocated by Norman in his famous book, The Psychology of Everyday Things (1988).Nevertheless, on-screen widgets present affordances only in an indirect sense. They borrow their power from our ability to represent pictorially, or otherwise, the affordances of the everyday world. Therefore, we can be inspired by affordance theory to produce good designs, but we cannot expect much help from that theory in building a science of visualization.\rA Model of Perceptual Processing\rIn this section, we introduce a simplified information-processing model of human visual percep- tion. As Figure 1.5 shows, there are many subsystems in vision and we should always be wary of overgeneralization. Still, an overall conceptual framework is often useful in providing a start- ing point for more detailed analysis. Figure 1.11 gives a broad schematic overview of a three- stage model of perception. In Stage 1, information is processed in parallel to extract basic features of the environment. In Stage 2, active processes of pattern perception pull out structures and segment the visual scene into regions of different color, texture, and motion patterns. In Stage 3, the information is reduced to only a few objects held in visual working memory by active mech- anisms of attention to form the basis of visual thinking.\rStage 1: Parallel Processing to Extract Low-Level Properties\rof the Visual Scene\rVisual information is first processed by large arrays of neurons in the eye and in the primary visual cortex at the back of the brain. Individual neurons are selectively tuned to certain kinds of information, such as the orientation of edges or the color of a patch of light. In Stage 1 pro- cessing, billions of neurons work in parallel, extracting features from every part of the visual\r\n._. ,_ ,;.............\r. . . . . . .\r.-,...... ..\rFoundation for a Science of Data Visualization 21\r1\r..... -\r, ,\rI---!\rt.:-\rr-.\rI..\rI ....I :..;...:.I.,. : I\rStage 1\rStage 2\rStage 3\r:...:.,:.,I..'.! , : .\rFigure 1.11 A three-stage model of human visual information processing.\rfield simultaneously. This parallel processing proceeds whether we like it or not, and it is largely independent of what we choose to attend to (although not of where we look). It is also rapid. If we want people to understand information quickly, we should present it in such a way that it could easily be detected by these large, fast computational systems in the brain.\rImportant characteristics of Stage 1 processing include:\rRapid parallel processing\rExtraction of features, orientation, color, texture, and movement patterns Transitory nature of information, which is briefly held in an iconic store Bottom-up, data-driven model of processing\rStage 2: Pattern Perception\rAt the second stage, rapid active processes divide the visual field into regions and simple pat- terns, such as continuous contours, regions of the same color, and regions of the same texture.\r\n22 INFORMA TION VISUALIZA TION: PERCEPTION FOR DESIGN\rPatterns of motion are also extremely important, although the use of motion as an information code is relatively neglected in visualization. The pattern-finding stage of visual processing is extremely flexible, influenced both by the massive amount of information available from Stage 1 parallel processing and by the top-down action of attention driven by visual queries. Marr (1982)called this stage of processing the 2-112D sketch. Triesman (1985)called it a feature map. Rensink (2002)called it a proto-object flux to emphasize its dynamic nature.\rThere is increasing evidence that tasks involving eye- hand coordination and locomotion may be processed in pathways distinct from those involved in object recognition. This is the two- visual system hypothesis: one system for locomotion and action, called the \"action system,\" and another for symbolic object manipulation, called the \"what system.\" A detailed and convincing account of it can be found in Milner and Goodale (1995).\rImportant characteristics of Stage 2 processing include:\rSlow serial processing\rInvolvement of both working memory and long-term memory More emphasis on arbitrary aspects of symbols\rIn a state of flux, a combination of bottom-up feature processing and top-down attentional mechanisms\rDifferent pathways for object recognition and visually guided motion\rStage 3: Sequential Goal-Directed Processing\rAt the highest level of perception are the objects held in visual working memory by the demands of active attention. In order to use an external visualization, we construct a sequence of visual queries that are answered through visual search strategies. At this level, only a few objects can be held at a time; they are constructed from the available patterns providing answers to the visual queries. For example, if we use a road map to look for a route, the visual query will trigger a search for connected red contours (representing major highways) between two visual symbols (representing cities).\rBeyond the visual processing stages shown in Figure 1.11 are interfaces to other subsystems. The visual object identification process interfaces with the verbal linguistic subsystems of the brain so that words can be connected to images. The perception-for-action subsystem interfaces with the motor systems that control muscle movements.\rThe three-stage model of perceptions is the basis for the structure of this book. Chapters 2, 3, 4, and some of 5 deal mainly with Stage 1 issues. Chapters 5 , 6 , 7, and 8 deal mainly with Stage 2 issues. Chapters 9, 10, and 11 deal with Stage 3 issues. The final three chapters also discuss the interfaces between perceptual and other cognitive processes, such as those involved in language and decision making.\r\nTypes of Data\rIf the goal of visualization research is to transform data into a perceptually efficient visual format, and if we are to make statements with some generality, we must be able to say something about the types of data that can exist for us to visualize. It is useful, but less than satisfying, to be able to say that color coding is good for stock-market symbols but texture coding is good for geo- logical maps. It is far more useful to be able to define broader categories of information, such as continuous-height maps (scalar fields), continuous-flow fields (vector maps), and category data, and then to make general statements such as \"Color coding is good for category information\" and \"Motion coding is good for highlighting selected data.\" If we can give perceptual reasons for these generalities, we have a true science of visualization.\rUnfortunately, the classification of data is a big issue. It is closely related to the classification of knowledge, and it is with great trepidation that we approach the subject. What follows is an informal classification of data classes using a number of concepts that we will find helpful in later chapters. We make no claims that this classification is especially profound or all-encompassing.\rBertin (1977)has suggested that there are two fundamental forms of data: data values and data structures. A similar idea is to divide data into entities and relationships (often called rela- tions).Entities are the objects we wish to visualize; relations define the structures and patterns that relate entities to one another. Sometimes the relationships are provided explicitly; sometimes dis- covering relationships is the very purpose of visualization. We also can talk about the attributes of an entity or a relationship. Thus, for example, an apple can have color as one of its attributes. The concepts of entity, relationship, and attribute have a long history in database design and have been adopted more recently in systems modeling. However, we shall extend these concepts beyond the kinds of data that are traditionally stored in a relational database. In visualization, it is neces- sary to deal with entities that are more complex and we are also interested in seeing complex struc- tured relationships- data structures- not captured by the entity relationship model.\rEntities\rEntities are generally the objects of interest. People can be entities; hurricanes can be entities. Both fish and fishponds can be entities. A group of things can be considered a single entity if it is convenient- for example, a school of fish.\rRelationships\rRelationships form the structures that relate entities. There can be many kinds of relationships. A wheel has a \"part-of\" relationship to a car. One employee of a firm may have a supervisory relationship to another. Relationships can be structural and physical, as in defining the way a house is made of its many component parts, or they can be conceptual, as in defining the rela-\rFoundation for a Science of Data Visualization 23\r\n24 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rtionship between a store and its customers. Relationships can be causal, as when one event causes another, and they can be purely temporal, defining an interval between two events.\rAttributes of Entities or Relationships\rBoth entities and relationships can have attributes. In general, something should be called an attribute (as opposed to an entity itself) when it is a property of some entity and cannot be thought of independently. Thus, the color of an apple is an attribute of the apple. The tempera- ture of water is an attribute of the water. Duration is an attribute of a journey. However, defin- ing what should be an entity and what should be an attribute is not always straightforward. For example, the salary of an employee could be thought of as an attribute of the employee, but we can also think of an amount of money as an entity unto itself, in which case we would have to define a relationship between the employee entity and the sum-of-money entity.\rAttribute Quality\rIt is often desirable to describe data visualization methods in light of the quality of attributes they are capable of conveying. A useful way to consider the quality of data is the taxonomy of number scales defined by the statistician S.S. Stevens (1946). According to Stevens, there are four levels of measurement: nominal, ordinal, interval, and ratio scales.\r1. Nominal: This is the labeling function. Fruit can be classified into apples, oranges, bananas, and so on. There is no sense in which the fruit can be placed in an ordered sequence. Sometimes numbers are used in this way. Thus, the number on the front of a bus generally has a purely nominal value. It identifies the route on which the bus travels.\r2. Ordinal: The ordinal category encompasses numbers used for ordering things in a sequence. It is possible to say that a certain item comes before or after another item. The position of an item in a queue or list is an ordinal quality. When we ask people to rank some group of things (films, political candidates, computers) in order of preference, we are requiring them to create an ordinal scale.\r3. 1nteAal: When we have an interval scale of measurement, it becomes possible to derive the gap between data values. The time of departure and the time of arrival of an aircraft are defined on an interval scale.\r4. Ratio: With a ratio scale, we have the full expressive power of a real number. We can make statements such as \"Object A is twice as large as object B.\" The mass of an object is defined on a ratio scale. Money is defined on a ratio scale. The use of a ratio scale implies a zero value used as a reference.\rIn practice, only three of Stevens's levels of measurement are widely used, and these in somewhat different form. The typical basic data classes most often considered in visualization have been greatly influenced by the demands of computer programming. They are the following:\r\nFoundation for a Science of Data Visualization 25\rCategoy data: This is like Stevens's nominal class.\rInteger data: This is like his ordinal class in that it is discrete and ordered. Real-number data: This combines the properties of interval and ratio scales.\rThese classes of data can be very useful in discussing visualization techniques. For example, here are two generalizations: (1)Using graphic size (as in a bar chart) to display category informa- tion is likely to be misleading, because we tend to interpret size as representing quantity. (2)If we map measurements to color, we can perceive nominal or, at best, ordinal values, with a few discrete steps. Perceiving metric intervals using color is not very effective. Many visualization techniques are capable of conveying only nominal or ordinal data qualities.\rAttribute Dimensions: 1D, 2D, 3D, ...\rAn attribute of an entity can have multiple dimensions. We can have a single scala~quantity, such as the weight of a person. We can have a vector quantity, such as the direction in which that person is traveling. Tensors are higher-order quantities that describe both direction and shear forces, such as occur in materials that are being stressed.\rWe can have a field of scalars, vectors, or tensors. The gravitational field of the earth is a three-dimensional attribute of the earth. In fact, it is a three-dimensional vector field attribute. If we are interested only in the strength of gravity at the earth's surface, it is a two-dimensional scalar attribute. Often the term map is used to describe this kind of field. Thus, we talk about a gravity map or a temperature map.\rOperations Considered as Data\rAn entity relationship model can be used to describe most kinds of data. However, it does not capture the operations that may be performed on entities and relationships. We tend to think of operations as somehow different from the data itself, neither entities nor relationships nor attrib- utes. The following are but a few common operations:\rMathematical operations on numbers- multiplication, division, and so on\rMerging two lists to create a longer list\rInverting a value to create its opposite\rBringing an entity or relationship into existence (such as the mean of a set of numbers) Deleting an entity or relationship (a marriage breaks up)\rTransforming an entity in some way (the chrysalis turns into a butterfly)\rForming a new object out of other objects (a pie is baked from apples and pastry) Splitting a single entity into its component parts (a machine is disassembled)\r\n26 INFORMATION VISUALIZATION: PERCEPTION FOR DESIGN\rIn some cases, these operations tan themselves form a kind of data that we may wish to capture. Chemistry contains a huge catalog of the compounds that result when certain operations are applied to combinations of other compounds. These operations may form part of the data that is stored. Certain operations are easy to visualize: For example, the merging of two entities can easily be represented by showing two visual objects that combine (visukllymerge) into a single entity. Other operations are not at all easy to represent in any visualization. For example, the detailed logical structure of a computer program may be better represented using a written code that has its basis in natural language than using any kind of diagram. What should and should not be visualized is a major topic in Chapter 9.\rOperations and procedures often present a particularly difficult challenge for visualization. It is difficult to express operations effectively in a static diagram, and this is especially a problem in the creation of visual languages. On the other hand, the use of animation opens up the pos- sibility of expressing at least certain operations in an immediately accessible visual manner. We shall deal with the issue of animation and visual languages in Chapter 9.\rMetadata\rWhen we are striving to understand data, certain products are sure to emerge as we proceed. We may discover correlations between variables or clusters of data values. We may postulate certain underlying mechanisms that are not immediately visible. The result is that theoretical entities come into being. Atoms, photons, black holes, and all the basic constructs of physics are like this. As more evidence accumulates, the theoretical entities seem more and more real, but they are nonetheless only observable in the most indirect ways. These theoretical constructs that emerge from data analysis have sometimes been called metadata (Tweedie,1997). They are gen- erally called derived data in the database modeling community. Metadata can be of any kind. It can consist of new entities, such as identified classes of objects, or new relationships, such as pos- tulated interactions between different entities, or new rules. We may impose complex structural relationships on the data, such as tree structures or directed acyclic graphs, or we may find that they already exist in the data.\rThe problem with the view that metadata and primary data are somehow essentially differ- ent is that all data is interpreted to some extent- thereis no such thing as raw data. Every data- gathering instrument embodies some particular interpretation in the way it is built. Also, from the practical viewpoint of the visualization designer, the problems of representation are the same for metadata as for primary data. In both cases, there are entities, relationships, and their attrib- utes to be represented, although some are more abstract than others. Thus the metadata concept is not discussed further in this book.\r\nConcl usi on\rVisualization applies vision research to practical problems of data analysis in much the same way as engineering applies physics to practical problems of building manufacturing plants. Just as engineering has influenced physicists to become more concerned with areas such as semicon- ductor technology, so we may hope that the development of an applied science of data visual- ization can encourage vision researchers to intensify their efforts in addressing such problems as 3D space and task-oriented perception. There is considerable practical benefit in understanding these things. As the importance of visualization grows, so do the benefits of a scientific approach. But there is no time left to lose. New symbol systems are being developed constantly to meet the needs of a society increasingly dependent on data. Once developed, they may stay with us for a very long time, so we should try to get them right.\rWe have introduced a key distinction between the ideas of sensory and arbitrary conven- tional symbols. This is a difficult and sometimes artificial distinction. Readers can doubtless come up with c ~ u n t e r e ~ a m ~alnedsreasons why it is impossible to separate the two. Nonetheless, the distinction is essential. With no basic model of visual processing on which we can support the idea of a good data representation, ultimately the problem of visualization comes down to estab- lishing a consistent notation. If the best representation is simply the one we know best because it is embedded in our culture, then standardization is everything- there is no good representa- tion, only widely shared conventions.\rIn opposition to the view that everything is arbitrary, this book takes the view that all humans do have more or less the same visual system. This visual system has evolved over tens of millions of years to enable creatures to perceive and act within the natural environment. Although very flexible, the visual system is tuned to receiving data presented in certain ways, but not in others. If we can understand how the mechanism works, we can produce better displays and better think- ing tools.\rFoundation for a Science of Data Visualization 27\r\nSECOND EDITION\rORMATIwN\rCEPTION FOR DESIGN\rCOLIN WARE\rdesigners know that yellow text presented against a\rbackground reads clearly and easily, but how many\ron of this classic work, with a new chapter on visual how to evaluate visualizations,and a greatly expanded\rolor and color sequences.\redition is the full-color treatment throughout, to better\ring researcher in the field of human perception who has ether, in a single resource, all current scientific insight\rstion of data visualization.\rORGAN KAUFMANN PUBLISHERS N IMPRINT OF ELSEVIER\rThis unique and essential guide to human visual perception and related cognitive principles will enrich courses on information visualizationand\rexamples will do much to accelerate innovation and adoption of information visualization.\r- Ben Shneiderm- University of Maryl\rprominent contributions to the visual interaction with machines and to infor-\rthe practical design of information visualization systems.\rI\r- from the foreword by 1 Stuart Card, PARC\rColin Ware is the director of the Data Visualization Research Laboratory at the University of New Hampshire. He has published over a hundred articles in scientific and technical\rcolor, texture, motion, and 30 in infor- mation visualization. In addition to his research, Professor Ware has been involvedin developing30interactive visualization systems for ocean map- ping for over twelve years, and has directedthe developmentof the NestedVision3Dsystem for visualizing very large networks of information.","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"symbol","time":1461915876751,"auto":true,"weight":1.0},{"@type":"Tag","text":"pictur","time":1461915876775,"auto":true,"weight":1.0},{"@type":"Tag","text":"data","time":1461915876760,"auto":true,"weight":1.0},{"@type":"Tag","text":"cultur","time":1461915876783,"auto":true,"weight":1.0},{"@type":"Tag","text":"sensori","time":1461915876727,"auto":true,"weight":1.0},{"@type":"Tag","text":"visual","time":1461915876713,"auto":true,"weight":1.0},{"@type":"Tag","text":"arbitrari","time":1461915876734,"auto":true,"weight":1.0},{"@type":"Tag","text":"percept","time":1461915876744,"auto":true,"weight":1.0},{"@type":"Tag","text":"entiti","time":1461915876720,"auto":true,"weight":1.0},{"@type":"Tag","text":"convent","time":1461915876769,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":5,"appId":"1192fc6d87299985771289a9811eefc0f55dd8c9","timeCreated":1461915874645,"timeModified":1461915874729,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.0455444,"uri":"file:///Users/cheny13/Documents/2015vis/TVCG/papers/529-ottley.pdf","plainTextContent":"529\rIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rImproving Bayesian Reasoning:\rThe Effects of Phrasing, Visualization, and Spatial Ability\rAlvitta Ottley, Evan M. Peck, Lane T. Harrison,\rDaniel Afergan, Caroline Ziemkiewicz, Holly A. Taylor, Paul K. J. Han and Remco Chang\rAbstract— Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.\rIndex Terms—Bayesian Reasoning, Visualization, Spatial Ability, Individual Differences\r1 INTRODUCTION\rAs the medical field transitions toward evidence-based and shared de- cision making, effectively communicating conditional probabilities to patients has emerged as a common challenge. To make informed health decisions, it is essential that patients understand health risk information involving conditional probabilities and Bayesian reason- ing [15]. However, understanding such conditional probabilities is challenging for patients [11]. Even more alarming, the burden of com- municating complex statistical information to patients is often placed on physicians even though studies have shown that most struggle with accurate estimations themselves [11].\rStill, both physicians and patients make life-critical judgments based on conditional probabilities. Deficits in diagnostic test sensitiv- ity and specificity (intrinsic characteristics of the test itself) can lead to false negative and false positive test results which do not reflect the ac- tual state of an individual. For low-prevalence diseases, even a highly specific test leads to false positive results for a majority of test recip- ients. Unless a patient fully understands the uncertainties of medical tests, news of a negative result can lead to false reassurance that treat- ment is not necessary, and news of a positive result can bring unjust emotional distress.\rConsider the following mammography problem [17]:\r“The probability of breast cancer is 1% for women at age forty who participate in routine screening. If a woman has breast cancer, the probability is 80% that she will get a positive mammography. If a woman does not have breast cancer, the probability is 9.6% that she will also get a positive mammography.\rA woman in this age group had a positive mammography in a routine\r• AlvittaOttleyiswithTuftsUniversity.E-mail:alvittao@cs.tufts.edu.\r• EvanPeckiswithBucknellUniversity.E-mail:evan.peck@bucknell.edu.\r• LaneT.HarrisoniswithTuftsUniversity.E-mail:lane@cs.tufts.edu.\r• DanielAferganiswithTuftsUniversity.E-mail:afergan@cs.tufts.edu.\r• CarolineZiemkiewicziswithTuftsUniversityandAptimaInc.E-mail:\rcziemkiewicz@aptima.com.\r• HollyA.TayloriswithTuftsUniversity.E-mail:holly.taylor@tufts.edu.\r• PaulK.J.HaniswithMaineMedicalCenterandTuftsMedicalSchool.\rE-mail: hanp@mmc.org.\r• RemcoChangiswithTuftsUniversity.E-mail:remco@cs.tufts.edu.\rscreening. What is the probability that she actually has breast can- cer?”\rMisinterpretation of this and other medical test statistics can have serious adverse consequences such as overdiagnosis [30, 42, 43] or even death. However, there are currently no effective tools for miti- gating this problem. Despite decades of research, the optimal methods for improving interpretation of diagnostic test results remain elusive, and the available evidence is sparse and conflicting.\rPrior work indicates that visualizations may be key for improving performance with Bayesian reasoning problems. For example, re- sults from Brase [1] and work by Garcia-Retamero and Hoffrage [16] suggest that visual aids such as Euler diagrams and icon arrays hold promise. Researchers have also explored visualizations such as deci- sion trees [13, 28], contingency tables [7], “beam cut” diagrams [17] and probability curves [7], and have shown improvements over text- only representations. However, when researchers in the visualization community extended this work to a more diverse sampling of the gen- eral population, they found that adding visualizations to existing text representations did not significantly increase accuracy [29, 32].\rGiven the contradictory findings of prior research, we aim to iden- tify factors that influence performance on Bayesian reasoning tasks. We hypothesize that these discrepancies are due to differences in prob- lem representations (textual or visual), as well as the end users’ in- nate ability to reason through these problems when using visualiza- tions. In particular, we propose that the phrasing of text-only rep- resentations can significantly impact comprehension and that this ef- fect is further confounded when text and visualization are incorpo- rated into a single representation. Furthermore, motivated by prior work [5, 24, 40, 41, 45], we also hypothesize that individual differ- ences (i.e., spatial ability) are mediating factors for performance on Bayesian reasoning tasks.\rTo test our hypotheses, we conducted two experiments to investi- gate how problem representation and individual differences influence performance on Bayesian reasoning tasks. The first experiment fo- cused on text-only representations and how phrasing can impact accu- racy, while the second explores how individual differences and repre- sentations that combine text and visualization affect performance.\rWith Experiment 1 we show that wording can significantly affect users’ accuracy and demonstrate how probing1 can help evaluate dif- ferent representations of Bayesian reasoning problems. Combining techniques that have previously been tested independently, our results\r1Instead of asking a single question (typically the true positive rate), you ask a series of questions designed to guide the user through their calculations.\rManuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of\rManuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of\rpublication xx Aug. 2015; date of current version 25 Oct. 2015. For\rpublication 20 Aug. 2015; date of current version 25 Oct. 2015.\rinformation on obtaining reprints of this article, please send\rFor information on obtaining reprints of this article, please send\re-mail to: tvcg@computer.org.\re-mail to: tvcg@computer.org.\rDigital Object Identifier no. 10.1109/TVCG.2015.2467758\r1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\r\n530 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rshow an increase in the accuracy of the mammography problem from the previously reported 6% [29] to 42%. Our findings demonstrate how the phrasing of a Bayesian problem can partially explain the poor or inconsistent results of prior work and provide a baseline text-only representation for future work.\rIn Experiment 2, we tested six different representations including a new text-only representation that uses indentation to visually depict set relations (Structured-Text), a storyboarding visualization that pro- gressively integrates textual information with a frequency grid visual- ization (Storyboarding), and a visualization-only representation (Vis- Only). The results of our second experiment show that altering the amount of information shown in text and visualization designs can yield accuracies as high as 77%. However, we found that adding vi- sualizations to text resulted in no measurable improvement in perfor- mance, which is consistent with prior work in the visualization com- munity by Micallef et al. [29].\rExamining our study population further, we found that spatial abil- ity impacts users’ speed and accuracy on Bayesian reasoning tasks, with high spatial ability users responding significantly faster and more accurately than low spatial ability users. Analyzing accuracy with re- spect to spatial ability, we discovered that users with high spatial abil- ity tend to perform better than users with low spatial ability across all designs, achieving accuracies from 66%-100%. We discuss the im- plications of these findings for text and visualization design, and how these methods may impact the communication of conditional proba- bility in the medical field and beyond.\rWe make the following contributions to the understanding of how phrasing, visualizations and individual differences influence Bayesian reasoning:\rand patients. They conducted an experiment where half of the partic- ipants received natural frequency formats and the other half received percentages. A further division was made within these groups; half of the participants received the information in numbers while the other half were presented with a visualization (a frequency grid). Their results confirmed the prior results of Gigerenzer and Hoffrage [17] showing that users are more accurate when information is presented using natural frequency formats. With their visualization condition, and they were able to achieve overall accuracies of 62%, one of the highest reported accuracies.\rWork by Brase [1] compared various visualizations for communi- cating Bayesian reasoning. In a comparative study, he analyzed par- ticipants’ accuracies when three different visualizations (icon arrays, Euler diagrams and discretized Euler diagrams 2) were added to tex- tual information. Like natural frequency formats, discrete items rep- resented by the icon array were expected to correspond with humans’ perception of natural sampling, thus improving Bayesian reasoning. In contrast, Euler diagrams were expected to enhance the perception of the nested-set relations that are inherent in Bayesian reasoning prob- lems. The discretized Euler diagram was designed as a hybrid of the two.\rBrase found that icon arrays had the best overall accuracy rate (48%), suggesting that they best facilitate Bayesian reasoning. How- ever there were some inconsistencies with the visualization designs used by Brase [29]. For instance, the Euler diagram was not area pro- portional but the hybrid diagram was, and the number of glyphs in the hybrid diagram differed from the number of glyphs in the frequency grid [29]. Noticing this, researchers in the visualization community extended this work by designing a new, consistent set of visualizations and surveying a more diverse study population.\rMicallef et al. [29] used a combination of natural frequency for- mats, icon arrays and Euler diagrams to improve the designs of Brase [1]. Instead of surveying university undergraduates, they re- cruited crowdsourced participants via Amazon’s Mechanical Turk in an effort to simulate a more diverse lay population [29]. Their study compared 6 different visualization designs and found no significant performance differences among them. Reported accuracies for the 6 designs ranged from 7% to 21% and the control condition (a text-only representation with natural frequency formats) yielded an overall ac- curacy of only 6%.\rThey also found no statistically significant performance difference between the control text-only condition and any of the conditions with visualization designs. This finding implies that simply adding visual aids to existing textual representation did not help under the studied conditions. Their follow up work adds yet another dimension. In a second experiment, they reported significant improvements in the ac- curacies for their visualization conditions when numerical values for the text descriptions were removed. This finding suggests that pre- senting both text with numerical values and visualization together may overload the user and result in incorrect inferences.\rThe findings of Micallef et al. [29] suggest a possible interac- tion between textual information and visualization when representing Bayesian reasoning problems. One possible explanation for this inter- action is that both the mental model required to interpret the textual information in a Bayesian reasoning problem and the mental model required to understand a visualization can compete for the same re- sources [24]. As more information is presented, a user’s performance can degrade since more items will be held in the user’s spatial work- ing memory [21]. In addition to explaining the inconsistencies among prior work by exploring different wording and visualization represen- tations, this paper aims to understand how spatial ability mediates per- formance on Bayesian reasoning problems.\r2.1 Spatial Ability\rIn recent years, an overwhelming body of research has demonstrated how individual differences impact people’s ability to use information\r2These are Euler diagrams with discrete items. They were designed as hy- brid diagrams that combine both the natural sampling affordance of icon arrays and the nested-set relations affordance of traditional Euler diagrams.\r2\r• We identify key factors that influence performance on Bayesian reasoning tasks and explain the inconsistent and conflicting find- ings of prior work.\r• We show that the phrasing of textual Bayesian reasoning prob- lems can significantly affect comprehension, and provide a benchmark text-only problem representation that allows future researchers to reliably test different designs of Bayesian reason- ing problems.\r• We demonstrate that a user’s spatial ability impacts their ability to solve Bayesian reasoning problems with different visual and textual representations. Our findings provide guidance on how to design representations for users of varying spatial ability.\rRELATED WORK\rThere is a substantial body of work aimed at developing novel, more effective methods of communicating Bayesian statistics. Still, there is no authoritative method for effectively communicating Bayesian rea- soning, and prior results are inconsistent at best. Below we survey some of these findings.\rGigerenzer and Hoffrage [17] in their seminal work explored how text-only representations can be improved using natural frequency for- mats. They explored the use of phrases such as 96 out of 1000 instead of 9.6%, hypothesizing that natural frequency formats have greater perceptual correspondence to natural sampling strategies [17]. Their findings demonstrate that using natural frequency significantly im- proves users’ understanding of Bayesian reasoning problems.\rA series of studies has also been conducted to investigate the ef- ficacy of using visualizations to aid reasoning. Various types of vi- sualizations have been tested, including Euler diagrams [1, 25, 29], frequency grids or icon arrays [16, 25, 29, 32, 35], decision trees [13, 35, 37], “beam cut” diagrams [17], probability curves [7], contingency tables [7, 8] and interactive designs [38]. While some researchers have compared several visualization designs [1, 29, 32], many of these vi- sualizations were proposed and tested separately. It is still not clear which best facilitates Bayesian reasoning.\rFor instance, recent work by Garcia-Retamero and Hoffrage [16] in- vestigated how different representations (text versus visualization) af- fect the communication of Bayesian reasoning problems to both doctor\r\nOTTLEY ET AL.: IMPROVING BAYESIAN REASONING: THE EFFECTS OF PHRASING, VISUALIZATION, AND SPATIAL ABILITY 531\rvisualization and visualization systems [2, 5, 6, 18, 31, 33, 40, 41, 47], and a growing number of researchers have advocated for better understanding of these effects [44, 46]. One of the main factors that have been shown to influence visualization use is spatial ability.\rSpatial ability in general refers to the ability to mentally repre- sent and manipulate two- or three-dimensional representations of ob- jects. Spatial ability is a cognitive ability with a number of measur- able dimensions, including spatial orientation, spatial visualization, spatial location memory, targeting, disembedding and spatial percep- tion [26, 40]. People with higher spatial ability can produce more accurate representations and maintain a reliable model of objects as they move and rotate in space.\rThere is considerable evidence that these abilities affect how well a person can reason with abstract representations of information, in- cluding visualizations. Vicente et al. [41] found that low spatial ability corresponded with poor performance on information retrieval tasks in hierarchical file structures. They found that in general high spatial ability users were two times faster than low spatial ability users and that low spatial ability users were more likely to get lost in the hierar- chical file structures.\rChen and Czerwinski [5] found that participants with higher spa- tial ability employed more efficient visual search strategies and were better able to remember visual structures in an interactive node-link visualization. Velez et al. [40] tested users of a three-dimensional vi- sualization and discovered that speed and accuracy were dependent on several factors of spatial ability. Similarly, Cohen and Hegarty [6] found that users’ spatial abilities affects the degree to which interact- ing with an animated visualization helps when performing a mental rotation task, and that participants with high spatial ability were bet- ter able to use a visual representation rather than rely on an internal visualization.\rThis body of research shows that users with higher spatial ability are frequently more effective at using a variety of visualizations. Taken to- gether, they suggest that high spatial ability often correlates with better performance on tasks that involve either searching through spatially arranged information or making sense of new visual representations. Additionally, there is evidence that high spatial ability makes it easier to switch between different representations of complex information. Ziemkiewicz and Kosara [45] tested users’ ability to perform search tasks with hierarachy visualizations when the spatial metaphor im- plied in the task questions differed from that used by the visualization. Most participants performed poorly when the metaphors conflicted, but those with high spatial ability did not. This confirms findings that spatial ability plays a role in understanding text descriptions of spatial information [10].\rIn Bayesian reasoning domain, Kellen [24] found that spatial ability was relevant to the understanding of visualizations of Bayesian reason- ing. He used Euler diagrams and investigated how problem complexity (the number of relationships presented in an Euler diagram) impacts users’ performance. His findings suggest that spatial ability may mod- erate the effect of visualizations on understanding. However, his work only investigated spatial ability as it relates to Bayesian reasoning and the number of relationships depicted in an Euler diagram.\rStill, like the prior reported accuracy findings, the reported results on the effects of spatial ability on understanding Bayesian reasoning have been contradictory. Micallef et al. [29] too investigated the ef- fects of spatial ability. They compared six text and visualization con- ditions and one text-only condition but found no significant effect of participants’ spatial abilities.\r3 RESEARCH GOALS\rThe body of existing work presented in this paper paints a complex portrait of visualization and Bayesian reasoning. First, the results of the prior works are inconsistent. The reported accuracies of the baseline text-only conditions differed significantly: Brase [1] reported 35.4%, Garcia-Retamero and Hoffrage [16] reported 26% while Mi- callef et al. [29] reported accuracies of only 6%. Second, prior work suggests an interaction between textual information and visualization when they are combined into a single representation.\rIn order to progress this important area of research, we must first identify factors that affect a user’s ability to extract information from text and visualization representations of Bayesian reasoning problems. Thus, the primary research goal for this work is to disambiguate the discrepancies among prior works’ results. We hypothesize that the observed discrepancies among prior work are largely due to differ- ences in problem representations. In particular, we hypothesize that the phrasing of text-only representations impacts comprehension. Fur- thermore, we posit that while visualizations can be effective tools for communicating Bayesian reasoning, simply appending visualizations to complex textual information will adversely impact comprehension.\rIn the succeeding sections, we present the results of two experi- ments that were designed to investigate the interacting effect of both problem representation and spatial ability on communicating Bayesian reasoning problems. Together, these experiments address the question of how users make sense of Bayesian reasoning problems under dif- ferent, and sometimes competing, representations of complex infor- mation. Our first experiment establishes a baseline, text-only condi- tion and investigates how various forms of problem phrasing impacts accuracies. With our second experiment, we explore the interaction between textual information and visualizations when they are com- bined in a single representation, and the effect of users’ spatial ability on their performance.\r4 EXPERIMENT 1: TEXT-ONLY REPRESENTATIONS\rA survey of the prior work reveals many inconsistencies among Bayesian problems used for assessing Bayesian reasoning. Many past experiments have used their own Bayesian problems, with dif- fering scenarios, wordings and framings. For instance, in their work, Gigerenzer and Hoffrage [17] used 15 different Bayesian problems, each with differing real-world implications and potential cognitive bi- ases associated with them (e.g. being career oriented leads to choos- ing a course in economics, or carrying heavy books daily relates to a child having bad posture). Micallef et al. [29] and Garcia-Retamero and Hoffrage [16] each used three different Bayesian problems (with only one in common). Brase [1] used a single Bayesian problem not previously tested by other researchers.\rOf the existing work, Brase [1] reported the highest accuracies for his text-only condition, with 35.4% of his participants reaching the correct Bayesian response. In addition to using natural frequencies, Brase [1] used probing as a means of evaluating the effectiveness of representations. Probing is a technique by which a series of ques- tions are posed to the user that are designed to guide that user through the calculation process. Cosmides and Tooby [9] proposed that prob- ing can be used to help users uncover information that is necessary for solving Bayesian inference problems and thereby improves perfor- mance. Rather than asking the participant to calculate the true positive rate from the given information directly (the task that is traditionally given), they used probing to guide their participants’ Bayesian calcu- lations. Ultimately, probing was designed to assess whether the user understands the information as it is presented instead of their mathe- matical skills. Following Brase [1], we examined probing as one of our study conditions.\rIn his study, Brase [1] also used a narrative - a generalizable, hy- pothetical scenario. Instead of presenting information about a specific disease such as breast cancer, he presented a fictional narrative, intro- ducing a population in which individuals are exposed to a new disease (“Disease X”). By using a hypothetical population and a generic dis- ease name, we hypothesize that this generalizes the problem and may have mitigated biases related to a certain disease, thus impacting ac- curacies.\rIn addition to these two techniques (probing and narrative), we adapted framing principles for reducing the complexity of text repre- sentations [9, 39]. Prior studies suggest that framing can significantly impact decision making with probability problems [9]. For example, saying 10 out of 100 people will have the disease versus 90 out of 100 people will not have the disease can elicit very different responses [39], and presenting both frames can help mitigate biases known as framing effects [39]. Using both frames also has the advantage that\r\n532\rIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rTextorig\rText probe\rTextdiseaseX\rText The information presented in this condition is exactly the probe\rTable 1. The three questions used in Experiment 1\r10 out of every 1,000 women at age forty who par- ticipate in routine screening have breast cancer. 8 of every 10 women with breast cancer will get a positive mammography. 95 out of every 990 women without breast cancer will also get a positive mammography.\rHere is a new representative sample of women at age forty who got a positive mammography in rou- tine screening.\rHow many of these women do you expect to actually have breast cancer?     out of\r10 out of every 1,000 women at age forty who par- ticipate in routine screening have breast cancer. 8 of every 10 women with breast cancer will get a positive mammography. 95 out of every 990 women without breast cancer will also get a positive mammography.\rImagine 1000 people are tested for the disease.\r(a) How many people will test positive?\r(b) Of those who test positive, how many will actually have the disease?\rThere is a newly discovered disease, Disease X, which is transmitted by a bacterial infection found in the population. There is a test to detect whether or not a person has the disease, but it is not perfect. Here is some information about the current research on Disease X and efforts to test for the infection that causes it.\rThere is a total of 1000 people in the popula- tion. Out of the 1000 people in the population, 10 people actually have the disease. Out of these 10 people, 8 will receive a positive test result and 2 will receive a negative test result. On the other hand, 990 people do not have the disease (that is, they are perfectly healthy). Out of these 990 people, 95 will receive a positive test result and 895 will receive a negative test result.\rImagine 1000 people are tested for the disease.\r(a) How many people will test positive?\r(b) Of those who test positive, how many will actually have the disease?\rused by Gigerenzer and Hoffrage [17] and Micallef et al. [29], and includes the base rate, true positive rate, and false positive rate. The expected answer was 8 out of 103.\rit explicitly states relationships in the problem that are implicit in the original text.\r4.1 Design\rIn line with our research goals of disambiguating contradictory re- sults in previous research, our first experiment examines how these three techniques (framing, adding a narrative and using probing) can be combined to reduce the complexity of Bayesian reasoning prob- lems. In the context of Bayesian problems, the term complexity can have different meanings, for instance: the number of relationships in the problem [24], the number of steps needed to solve the problem, or the amount of information to be integrated or reconstructed [9]. In the current work, we define complexity as the difficulty of extracting in- formation. This hinges on the notion that the simplicity of a task partly depends on the how the information is presented. We believe that is it important first to establish a baseline text representation (i.e. no visualization) before we consider the effect of adding visualizations.\rWe conducted an online study and tested three different text-only representations of Bayesian reasoning problems:\rTextorig For our base condition, we chose the mammography prob- lem (see Table 1 Textorig) since it has been used in many stud- ies [11, 17, 16, 29, 24] and tests a skill of great importance and generalizability [11]. This specific mammography problem was\rPositiveCount Howmanypeoplewilltestpositive?\rTrue Positive Count Of those who test positive, how many will actually have the disease?\rTextdiseaseX For this condition, we adopted a narrative similar to Brase [1] for the mammography problem, and similarly to Textprobe we used probing. In addition to using these two tech- niques, this condition also provides the user with both positive and negative frames of the problem. Instead of only providing the base rate, the true positive rate, and the false positive rate, the text included the true negative and the false negative rates (see Table 1 TextdiseaseX ). It is important to note that no new data was added. The true negative and false negative rates were implicit in the Textorig and Textprobe conditions.\rTable 2. Demographics for Experiment 1\rN 100\rGender Female: 35%, Male: 65%\rEducation High School: 13%, College: 42%, Gradu-\rate School: 25%, Professional School: 17%,\rPh.D.: 1%, Postdoctoral: 2%\rAge μ : 33.63,σ : 11.8, Range: 19 - 65\r4.1.1 Participants\rWe recruited 100 online participants (37 for Textorig, 30 for Textprobe and 33 for TextdiseaseX ) via Amazon’s Mechanical Turk. Participants received a base pay of $.50 with a bonus of $.50 and $.05 for each cor- rect answer on the main task and surveys respectively. The total possi- ble renumeration was $2.80 which is comparable to the U.S. minimum wage. Participants completed the survey via an external link and per- formed tasks using an online experiment manager developed for this study. Using this tool, participants were regulated by their Mechanical Turk worker identification number and were only allowed to complete the experiment once. Table 2 summarizes the demographic informa- tion for Experiment 1.\r4.1.2 Procedure\rAfter selecting the task from the Mechanical Turk website, partici- pants were instructed to navigate to a specified external link. Once there, they entered their Mechanical Turk worker identification num- ber which was used both for controlling access to the experiment man- ager and for remuneration. After giving informed consent, participants were randomly presented with one of the three Bayesian reasoning problems. They were instructed to take as much time as needed to read and understand the information provided as they would have to answer questions based on the information and that bonuses will be paid for each correct answer. To separate the time spent reading the question from the time spent actually solving the problem, the question was not visible until they clicked the appropriately labeled button to indicate that they were ready. The participants were once again instructed to take as much time as needed and enter the answers in the space pro- vided. The timer ended when the participant entered an answer. Any edits to their responses extended the recorded time. Once they submit- ted the main task, they completed a short demographic questionnaire.\rsame as Textorig but uses probing instead of asking for the true positive rate directly (see Table 1 Textprobe). The participant is first probed for the expected number of people who will be tested positive (103) then she is probed for the true positive count (8). The two probed questions used in the current design are:\r\nOTTLEY ET AL.: IMPROVING BAYESIAN REASONING: THE EFFECTS OF PHRASING, VISUALIZATION, AND SPATIAL ABILITY 533\r4.2 Results\rFor our analysis, responses were only deemed correct if participants entered the expected response for both probed questions.\rWith the Textorig condition, we successfully replicated prior results of Micallef et al. [29] with an accuracy rate of 5.4% as compared to their reported 6% for text-only representations. Modifying the original question by using probing (Textprobe), we presented participants with questions that were easier to understand [9]. Consistent with prior work by Cosmides and Tooby [9], we found that this small change yielded a significantly higher accuracy rate of 26.7%.\rFinally, by changing the problem text with our TextdiseaseX condi- tion, we successfully replicate Brase’s [1] results with an accuracy rate of 42.4% as compared to his reported 53.4%. A chi-square test was conducted across all participants and revealed significant differ- ences between the accuracy rates of the three conditions(χ2(2, N = 100)=13.27 p = 0.001). Performing a pairwise chi-square test with a Bonferroni adjusted alpha (α = 0.017), we found significant differ- ences between Textorig and Textprobe (χ2(1, N = 67) = 5.9, p = 0.015),\rand Textorig and TextdiseaseX (χ2(1, N = 70) = 13.56, p < 0.001). 4.3 Discussion\rIn our first experiment, we found that by simply changing how the problem was presented, we observed an improvement in participants’ overall accuracy from 5.4% to 42.4%. We adapted techniques such as probing, which nudges the user to think more thoroughly about the problem, adding a narrative which generalized the problem, and pre- senting both frames for mitigating framing effects.\rTaken together this gives us insight into how lexical choices of text- only representations of Bayesian reasoning problems govern their ef- fectiveness and may at least partially explain the poor or inconsistent accuracies observed in previous work. By using probing alone, our re- sults showed a significant improvement over our base condition which used direct questioning. This suggests that assessment techniques for Bayesian reasoning problems should be thoroughly scrutinized.\rParticipants were even more accurate when the stimulus combined all three techniques (probing, narrative and framing). This finding provides initial evidence that even with text-only representations (i.e. without visualization aids), the phrasing of the problem can impact comprehension. Indeed, there were several factors that potentially contributed to the increase in communicative competence we observed for TextdiseaseX. For example, using the generic term Disease X in- stead of a specific disease may gave mitigated biases introduced by the mammography problem. Alternatively, the observed increase in accuracy could be attributed to the overall readability of the text or the amount of data presented in the conditions (the TextdiseaseX condition presented the user with slightly more explicit data than the Textorig and Textprobe conditions). Deciphering these was beyond the scope of this project, but will be an important direction for future work.\rIn the following study, we further address our research goals by investigating the effect of adding visualizations for representing\rFig. 1. Accuracies across all conditions in Experiment 1. Combining probing and narrative techniques proved to be effective for reducing the overall complexity of the text and increasing accuracy.\rBayesian reasoning tasks. We use our results from this initial experi- ment by adopting TextdiseaseX as a baseline text-only representation for evaluating different text and visualization designs.\r5 EXPERIMENT 2: TEXT AND VISUALIZATION\rAlthough visualization has been suggested as a solution to the Bayesian reasoning problem, recent findings suggest that, across sev- eral designs, simply adding visualizations to textual Bayesian infer- ence problems yields no significant performance benefit [29, 32]. Mi- callef et al. [29] also found that removing numbers from the textual representation can improve performance. The findings of this prior work suggest an interference between text and visualization compo- nents when they are combined into a single representation.\rDiffering from prior work which focused mainly on comparing different visualization designs [29], our second experiment aimed to progress Bayesian reasoning research by further investigating the ef- fect of presenting text and visualization together. We examined the amount of information presented to the user and the degree to which the textual and visual information are integrated. Grounded by the baseline condition established in Experiment 1 (Table 3 Control- Text), we tested representations that gradually integrate affordances of visualizations or the visualization itself.\rOne affordance of visualizations is that relationships that are im- plicitly expressed in text are often explicated in visual form. Visual- izations make it easier to “see” relationships among groups. To bridge this information gap, we gradually expanded the text-only representa- tion to explicate implied information and relationships.\rSecondary to our main research goals and motivated by the prior work demonstrating a connection between spatial ability and visual design [25, 45], our second experiment also aimed to understand how nuances in spatial ability affect users’ capacity to use different repre- sentations of Bayesian reasoning problems. Since prior research sug- gests that low spatial-ability users may experience difficulty when both the text and visual representations are presented [45], we hypothesize that low spatial-ability users would be more adept at using representa- tions which integrated affordances of the visualization but not the vi- sualization itself. On the other hand, we hypothesize that high spatial- ability users will benefit greatly from representations which merge tex- tual and visual forms, as they are more likely to possess the ability to effectively utilize both representations.\r5.1 Design\rTo test our hypotheses, we present the TextdiseaseX condition from Ex- periment 1, using a variety of representations. Our intent was to ma- nipulate the total amount of information presented, as well as the cou- pling between the problem text and visual representation. Consistent with TextdiseaseX , each condition in Experiment 2 began with an intro- ductory narrative:\rThere is a newly discovered disease, Disease X, which is transmitted by a bacterial infection found in the popula- tion. There is a test to detect whether or not a person has the disease, but it is not perfect. Here is some information about the current research on Disease X and efforts to test for the infection that causes it.\rThe format of the questions asked were also consistent with the TextdiseaseX :\r(a) How many people will test positive?\r(b) Of those who test positive, how many will actually have the disease?\r5.1.1 Conditions\rThere were a total of 6 conditions which were randomly assigned to our participants (see Table 3 for the exact stimuli).\rControl-Text As the name suggests, this is our control condition and uses the same text format as was presented in the TextdiseaseX condition of Experiment 1.\r\n534 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rControl-Text\rTable 3. Table showing the 6 conditions used in Experiment 2\rThere is a total of 100 people in the population. Out of the 100 people in the population, 6 people actually have the disease. Out of these 6 people, 4 will receive a positive test result and 2 will receive a negative test result. On the other hand, 94 people do not have the disease (i.e., they are perfectly healthy). Out of these 94 people, 16 will receive a positive test result and 78 will receive a negative test result.\rComplete-Text\rThere is a total of 100 people in the population. Out of the 100 people in the population, 6 people actually have the disease. Out of these 6 people, 4 will receive a positive test result and 2 will receive a negative test result. On the other hand, 94 people do not have the disease (i.e., they are perfectly healthy). Out of these 94 people, 16 will receive a positive test result and 78 will receive a negative test result.\rAnother way to think about this is... Out of the 100 people in the popu- lation, 20 people will test positive. Out of these 20 people, 4 will actually have the disease and 16 will not have the disease (i.e., they are perfectly healthy). On the other hand, 80 people will test negative. Out of these 80 people, 2 will actually have the disease and 78 will not have the disease (i.e., they are perfectly healthy).\rStructured-Text\rThere is a total of 100 people in the population. Out of the 100 people in the population,\r6 people actually have the disease. Out of these 6 people, 4 will receive a positive test result and\r2 will receive a negative test result.\rOn the other hand, 94 people do not have the disease (i.e., they are perfectly healthy). Out of these 94 people,\r16 will receive a positive test result and 78 will receive a negative test result.\rAnother way to think about this is...\rOut of the 100 people in the population,\r20 people will test positive. Out of these 20 people,\r4 will actually have the disease and\r16 will not have the disease (i.e., they are perfectly healthy).\rOn the other hand, 80 people will test negative. Out of these 80\rStoryboarding\rThere is a total of 100 people in the population.\rOut of the 100 people in the population, 6 people actually have the disease.\rOut of these 6 people, 4 will receive a positive test result and 2 will receive a negative test result.\rOn the other hand, 94 people do not have the disease (i.e., they are perfectly healthy).\rOut of these 94 people, 16 will receive a positive test result and 78 will receive a negative test result.\rpeople,\rVis-Only\rControl+Vis\rThere is a total of 100 people in the population. Out of the 100 people in the population, 6 people actually have the disease. Out of these 6 people, 4 will receive a positive test result and 2 will receive a negative test result. On the other hand, 94 people do not have the disease (i.e., they are perfectly healthy). Out of these 94 people, 16 will receive a positive test result and 78 will receive a negative test result.\r2 will actually have the disease and\r78 will not have the disease (i.e., they are perfectly healthy).\r\nOTTLEY ET AL.: IMPROVING BAYESIAN REASONING: THE EFFECTS OF PHRASING, VISUALIZATION, AND SPATIAL ABILITY 535\rComplete-Text In this condition, the text is expanded to present all possible relationships and framings of the problem, which is a common affordance of visualizations. It is important to note that the text still presents the same amount of information as Control- Text (i.e. the base rate, the true positive rate, the false positive rate, the false negative rate and the true negative rate), however, it presents the data both with respect to having the disease and being tested positive (see Table 3 Complete-Text).\rStructured-Text Here, we further improve the text by integrating an- other affordance of visualizations. Like Complete-Text, the text in this condition enumerates all possible relationships and fram- ings of the problem, however, we enhanced the text by adding visual cues to the representation. Instead of using long-form paragraphs, we used indentation to clarify relationships. Similar to spreadsheets of raw data, the spatialization of the information makes the relationships more apparent.\rVis-Only With this condition, we establish a baseline for using visu- alizations. With the exception of the introductory narrative men- tioned above, there is no additional text in this condition.\rWhile researchers have investigated numerous visualization de- signs for representing Bayesian reasoning (see Section 2), there is still no consensus on which is best. In fact, recent research in the visualization community comparing the effectiveness of 6 different visualization designs found no significant difference between them [29].\rThat said, for this visualization-only condition, we chose to rep- resent the information using an icon array visualization (see Ta- ble 3). A number of researchers have explored their utility for risk communication and Bayesian reasoning [1, 14, 15, 16, 19, 20, 25, 29, 32, 35] and icon-arrays are often used in the medical community for representing risk information.\rThe specific icon-array used in this work consists of a 5 by 20 grid of anthropomorphic figures. We adapted a sequential layout for the different sets (as opposed to a random layout which has been previously used for representing uncertainty [19]) and we used in-place labeling for ease of reference. This is similar to the design used by Brase [1].\rControl+Vis Mirroring prior work [1, 29, 32] that investigated the utility of adding visualization designs to Bayesian problems, here we simply added a visualization to our control text-only representation. The information for this condition is represented using both the Control-Text description and the icon array visu- alization from Vis-Only.\rStoryboarding This condition was designed to simplify Bayesian reasoning by gradually integrating the textual and visual compo- nents of Control+Vis. Such storytelling techniques are becoming increasingly popular in recent years [22, 23, 36] and have even been recently referred to as “the next step for visualization” [27].\rFor our Storyboarding design, no information was added, but the information is presented sequentially, allowing for temporal processing. The text shown in this condition is consistent with Control-Text and the final visualization is the same as Vis-Only.\r5.1.2 Cognitive Ability Measures\rWe measured participants’ spatial ability using the paper folding test (VZ-2) from Ekstrom, French, & Hardon [12]. This survey consists of 2 3-minute sessions with 10 questions each. A similar version of the test has been used as a standard technique to compare spatial ability to Bayesian reasoning skills in other studies [24, 29]. Consistent with prior work [24, 29], a participant’s spatial ability score was calculated by summing the number of correct answers minus the total number of incorrect answers divided by four.\rConsistent with prior studies investigating the effectiveness of Bayesian reasoning problem representations [16, 25, 29], we measured\rparticipants’ numerical skills. This was measured using Brown et al.’s 6-question test [3]. Prior research has demonstrated a correlation be- tween numerical skills and understanding natural frequencies [4] and numerical skills has been shown to correlate with one’s ability to un- derstand medical risk information [3, 34].\rN Gender\rEducation\rTrained+\rAge\rSpatial Ability Numeracy\r377\rFemale: 34.2%, Male: 65%, Unspeci- fied: .8%\rHigh School: 22.3%, College: 51.5%, Grad- uate School: 19.6%, Professional School: 4.2%, Ph.D.: 1.6%, Postdoctoral: .5%, Un- specified: .3%\rYes: 12.2%, No: 87.5%, Unspecified: .3% μ:31,σ:9.87,Range:18to65\rμ : 8.60, σ : 5.25, Range: -3.75 to 20 μ:4.23,σ:1.22,Range:0to6\rTable 4. Demographics for Experiment 2\r+Participants received statistics training 5.1.3 Participants\rWe recruited 377 participants (61-65 per condition) via Amazon’s Me- chanical Turk who had not completed Experiment 1. The recruitment and remuneration techniques used for this experiment follows that of Experiment 1 (see Section 4.1.1). Table 4 summarizes our partici- pants’ demographics.\r5.1.4 Procedure\rThe procedure for this experiment also follows that of Experiment 1 (see Section 4.1.2) except for the following changes. After the main tasks, in addition to the demographics survey, participants completed VZ-2 to measure spatial ability and the numerical skill survey.\r5.2 Hypotheses\rFollowing our high-level claims that text complexity, visual represen- tation, and spatial ability affect accuracy on Bayesian reasoning, we form the following hypotheses:\rH1 Prior work shows no performance increase when visualizations are simply appended to existing text representations [29]. There- fore, removing the textual description completely from the visu- alization condition will mitigate this effect and Vis-Only will be more effective than Control-Text and Control+Vis.\rH2 Since participants are given an increased amount of information in the text, users will perform better on Structured-Text and Complete-Text than on Control-Text.\rH3 High spatial-ability users will perform better overall than low spatial-ability users.\rH4 Designs that include both text and vis (Storyboarding, Con- trol+Vis) will require higher spatial ability than text-only designs (Complete-Text, Structured-Text) and Vis-Only.\r5.3 Results\rWhile recent work [29] has advocated for a more continuous or fine- grained approach to assessing users’ accuracy on Bayesian reasoning tasks (for instance, reporting the differences between users’ responses and correct answers in terms of a log ratio), we report our accuracies in terms of the percentages of correct exact answers. Choosing this bi- nary approach of assessing accuracy has two advantages: (1) it allows us to directly compare our results across the prior body of work as they have all (including [29]) reported their accuracies similarly, and (2) this course-grained approach is especially user-friendly for comparing representations with substantial accuracies as seen in the subsequent sections.\rConsistent with Experiment 1, the proceeding analyses focus only on participants who answered both questions correctly (see Section 5.1\r\n536 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rFig. 2. Accuracies across all conditions. We found that participants were most accurate with Structured-Text, Complete-Text and Vis-Only.\rfor the exact questions asked). In an effort to further simulate a lay population, our analysis excluded participants who reported to have had statistical training.\r5.3.1 Accuracy Across Designs\rAcross all conditions, the average accuracy was remarkably high; 63% of the participants correctly answered both questions. Fig- ure 2 summarizes the accuracies across all conditions. Complete-Text, Structured-Text and Vis-Only yielded the highest overall accuracies ranging from 71% to 77%. Along with Control-Text, Storyboarding yielded the lowest overall accuracies with only 51% and 49% respec- tively of the participants responding correctly to the questions.\rWe performed a chi-square analysis to test for differences in accu- racy across the six conditions. The test revealed that the percentage of participants who correctly answered both questions differed by de- sign (χ2(5, N= 330) = 17.2, p = 0.004). We then performed all pair- wise chi-square tests with a Bonferroni adjusted alpha (α = 0.003) to identify the specific designs that deferred. The analysis revealed a sig- nificant differences between only Storyboarding and Structured-Text (χ2(1, 114) = 8.8, p < 0.003).\rWe found no significant difference in accuracy between Control- Text and Vis-Only, and we found no difference in accuracy among Control-Text, Complete-Text and Structured-Text. Consistent with prior findings [29, 32], we also found no significant difference between the Control-Text and Control+Vis conditions. This suggests that under the studied conditions, using visualizations (with or without textual in- formation) and increasing the amount of explicit textual information in text-only designs did not improve performance, thereby rejecting both H1 and H2.\r5.3.2 Spatial Ability and Accuracy\rTo test our hypothesis that spatial ability affects participants’ capac- ity to extract information from the different representations, we per-\rFig. 3. Average accuracy for the low and high spatial ability groups for each design. Overall, we found that high spatial users were much more likely to correctly answer the question prompts.\rFig. 4. Histograms showing the distribution of spatial ability scores for participants who correctly answered both questions across the six con- ditions. The graphs provide preliminary evidence that Complete-Text and Storyboarding may require higher spatial ability to use them.\rformed a binary logistic regression to predict participants who cor- rectly answered both questions using their spatial abilities score as a predictor. A test of the resulting model against the constant model was statistically significant at p < 0.001, indicating that spatial ability highly correlates with participants’ ability to answer the questions ac- curately. Prediction success overall was 71.5% (87.1% for predicting those who responded correctly and 44.6% for predicting those who responded incorrectly).\rFor a more specific analysis, we split users into two groups (spatiallow and spatialhigh) based on a median split of their spatial abil- ities scores (spatiallow < 9, N = 170 and spatialhigh >= 9, N = 160). Confirming H3, the overall accuracy for the spatialhigh group was 78.8% while spatiallow was 46.9%. Figure 3 summarizes the groups’ accuracies for each of the six conditions.\rWe then performed separate chi-square analyses testing for sig- nificant differences between the accuracies of the six conditions for the spatiallow group and the spatialhigh group. The chi-square test\rfor the spatialhigh group was significant (χ2(5, N = 170) = 26.3, p < 0.001) and multiple comparisons with the Bonferroni adjusted alpha (α = 0.003) revealed significant differences between:\r• Control-Text & Complete-Text (χ2(1, N=63) = 8.8, p < 0.003)\r• Control-Text&Structured-Text(χ2(1,N=54)=12.7,p<0.001)\r• Control-Text&Vis-Only(χ2(1,N=57)=11.07,p<0.001) 2\r• Structured-Text & Storyboarding (χ (1, N=65) = 13.5, p < 0.001)\rThese results indicate that for the spatialhigh group, Structured-Text, Complete-Text, and Vis-Only resulted in improved performance over our control condition (Control-Text), confirming H2 and partially sup- porting H1. More generally, the results imply that spatial ability must be considered when evaluating the effectiveness of Bayesian reasoning designs.\rPerforming a similar analysis with the spatiallow group, we found no significant difference between the accuracies for the six conditions (χ2(5, N= 160) = 6.5, p = 0.262). This indicates that the accuracies for the spatiallow group were similar across the all conditions, suggesting that the proposed designs were ineffective for low spatial-ability users.\r5.3.3 Spatial Ability Across Designs\rGiven our findings that participants’ spatial ability affects their likeli- hood of correctly answering the question prompts using a given rep- resentation, we hypothesize that we can now use spatial ability as a tool for ranking representations based on their complexity. In particu- lar, we use spatial ability as a proxy for measuring and comparing the extraneous cognitive load necessary to effectively use each represen- tation. Figure 4 shows the distribution for spatial ability scores for the correct users on the six conditions.\r\nOTTLEY ET AL.: IMPROVING BAYESIAN REASONING: THE EFFECTS OF PHRASING, VISUALIZATION, AND SPATIAL ABILITY 537\rPrior to our analysis, we removed two outliers whose spatial ability score was more that two standard deviations from the mean score for their respective conditions. We then conducted a one-way Analysis of Variance (ANOVA) to test for differences between the spatial ability scores of participants who correctly answered both questions for each of the six conditions. Our model was statistically significant (F(5, 206) = 2.57, p = 0.028) suggesting that the spatial ability scores differed across conditions.\rPost hoc comparisons using Fisher’s least significant difference (LSD) indicated that the mean scores of the following conditions dif- fered significantly:\r• Control-Text & Control+Vis (p = 0.042) • Complete-Text & Control+Vis (p = 0.005) • Storyboarding&Control+Vis(p=0.002)\rThis finding supports our hypothesis that some representations may require higher spatial ability to use them. However, we partially reject H4. Control+Vis had the lowest average indicating that this repre- senting may be most suitable for users with lower spatial ability. Con- versely, we found that the average spatial ability score for correct users on Storyboarding was higher than all other conditions, suggesting that Storyboarding was the most difficult representation to use.\r5.4 Discussion\rThe results of our Experiment 1 demonstrated how phrasing of Bayesian problems can influence performance. In Experiment 2, we examined whether we could improve problem representations by en- hancing text or combining it with visualization. In an effort to bridge the information gap between text and visual representations, we stud- ied text-only representations that clarified information that usually is more easily seen in a visualization. Our Complete-Text design sought to decrease this information gap by enumerating all probability rela- tionships in the text and our Structured-Text design used indentations to visualize these relationships. Still, when spatial ability was not con- sidered, we found that adding more information did not benefit users.\rWe observed similar results with our visualization conditions. Al- though we hypothesized that the Vis-Only design would be more effec- tive than Control-Text and Control+Vis, our results did not support this hypothesis. Again, when spatial ability was not considered, adding vi- sualizations (with or without textual information) did not improve per- formance. However, a closer examination of our results adds nuance to this finding when individual differences are considered.\r5.4.1 Spatial Ability Matters\rWhile the lack of overall difference across conditions was unexpected, factoring in the effect of spatial ability helped shed light on these find- ings. Across all visualizations, spatial ability was a significant indi- cator of accuracy and completion times. We found that users with low spatial ability generally performed poorly; the accuracy of high spatial-ability users was far higher than the accuracy of low spatial- ability users (78.8% v. 46.9%). Relative to the Control-Text condition, for high spatial users, the Structured-Text, Complete-Text and Vis-Only designs were extremely effective, yielding accuracies of 100%, 90% and 96% respectively. These unprecedented accuracies suggest that, for users with high spatial ability, these designs can solve the problem of Bayesian reasoning. However, it is interesting to note that effec- tive designs were “pure” designs (i.e., they did not combine text and visualizations).\r5.4.2 Text+Vis Interference\rFor high spatial-ability users, we found that representations that com- bined text and visualization (Control+Vis, Storyboarding) actually im- peded users’ understanding of conditional probability when compared to text-only (Complete-Text, Structured-Text) or Vis-Only conditions. Despite the fact that high spatial-ability users performed compara- tively poorly with the Control+Vis design (accuracy decreased by nearly 30% when compared to Complete-Text, Structured-Text, and Vis-Only), such disparity in accuracy was not observed with low spa- tial ability users using Control+Vis. One possible explanation relies\ron considering the problem as a mental modeling task. Users with low spatial ability may have simply chosen the representation in Con- trol+Vis (text or visualization) that best fit their understanding of the problem. On the contrary, high spatial-ability users may have at- tempted (and failed) to integrate the text and visualization represen- tations in order to find the correct answer. This hypothesis would be in line with Kellen’s [25] hypothesis that text and visual representations in a complex problem may compete for the same mental resources, increasing the likelihood of errors.\rThe Storyboarding design proved to be an enormous obstacle for the user. Performing analysis to investigate the spatial ability scores required to successfully extract information from the six designs re- vealed that Storyboarding demand higher spatial ability scores than the other designs. While it is intended to gradually guide users through the Bayesian reasoning problem, the different steps may have inadver- tently introduced distractors to the information that the user is truly looking for and/or forced users into a linear style of reasoning that was incongruent with their mental model of the problem. This added complexity increased cognitive load to a point that accuracy for all users suffered.\rStill, such storytelling techniques have been shown to be effective for communicating real world data [22, 23, 27, 36]. The tasks in this study, however, go beyond typical information dissemination, as users had to understand information known to be inherently challenging for most people. Future work could investigate the utility of storytelling techniques for similar reasoning tasks.\r6 CONCLUSION\rEffectively communicating Bayesian reasoning has been an open chal- lenge for many decades, and existing work is sparse and sometimes contradictory. In this paper we presented results from two experiments that help explain the factors affecting how text and visual representa- tions contribute to performance on Bayesian problems. With our first experiment, we showed that the wording of text-only representations can significantly impact users’ accuracies and may partly be responsi- ble for the poor or inconsistent findings observed by prior work.\rOur second experiment examined the effects of spatial ability on Bayesian reasoning tasks and analyzed performance with a variety of text and visualization conditions. We found that spatial ability signifi- cantly affected users ability to use different Bayesian reasoning repre- sentations. Compared to high spatial-ability users, low spatial-ability users tended to struggle with Bayesian reasoning representations. In fact, high spatial-ability users were almost two times more likely to an- swer correctly than low spatial-ability users. Additionally, we found that text-only or visualization-only designs were more effective than those which blend text and visualization.\rUltimately, our results not only shed light on how problem repre- sentation (both in text phrasing and combining text and visualization) can affect Bayesian reasoning, but also question whether one-size-fits- all visualizations are ideal. Further study is needed to clarify how best to either adapt visualizations or provide customization options to serve users with different needs. The results from these studies can be used for real-world information displays targeted to help people bet- ter understand probabilistic information. They also provide a set of benchmark problem framings that can be used for more comparable future evaluations of visualizations for Bayesian reasoning. Further work in this domain can have significant impact on pressing issues in the medical communication field and other domains where probabilis- tic reasoning is critical.\r7 DATASET\rTo facilitate future work, participants’ data are made available at: http://github.com/TuftsVALT/Bayes.\rACKNOWLEDGMENTS\rThis material is based upon work supported by the National Science Foundation under Grant No. IIS-1218170.\r\n538 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rREFERENCES\r[1] G. L. Brase. Pictorial representations in statistical reasoning. Applied Cognitive Psychology, 23(3):369–381, 2009.\r[2] E. T. Brown, A. Ottley, H. Zhao, Q. Lin, R. Souvenir, A. Endert, and R. Chang. Finding waldo: Learning about users from their interac- tions. IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, 20(12), 2014.\r[3] S.M.Brown,J.O.Culver,K.E.Osann,D.J.MacDonald,S.Sand,A.A. Thornton, M. Grant, D. J. Bowen, K. A. Metcalfe, H. B. Burke, et al. Health literacy, numeracy, and interpretation of graphical breast cancer risk estimates. Patient education and counseling, 83(1):92–98, 2011.\r[4] G.B.Chapman,J.Liu,etal.Numeracy,frequency,andbayesianreason- ing. Judgment and Decision Making, 4(1):34–40, 2009.\r[5] C. Chen and M. Czerwinski. Spatial ability and visual navigation: An empirical study. New Review of Hypermedia and Multimedia, 3(1):67– 89, 1997.\r[6] C. A. Cohen and M. Hegarty. Individual differences in use of external visualisations to perform an internal visualisation task. Applied Cognitive Psychology, 21:701–711, 2007.\r[7] W. Cole. Understanding bayesian reasoning via graphical displays. In ACM SIGCHI Bulletin, volume 20, pages 381–386. ACM, 1989.\r[8] W. Cole and J. Davidson. Graphic representation can lead to fast and accurate bayesian reasoning. In Symp Computer Application in Medical Care, pages 227–231, 1989.\r[9] L. Cosmides and J. Tooby. Are humans good intuitive statisticians after all? rethinking some conclusions from the literature on judgment under uncertainty. cognition, 58(1):1–73, 1996.\r[10] R. De Beni, F. Pazzaglia, V. Gyselinck, and C. Meneghetti. Visuospa- tial working memory and mental representation of spatial descriptions. European Journal of Cognitive Psychology, 17(1):77–95, 2005.\r[11] D. Eddy. Probabilistic reasoning in clinical medicine: Problems and op- portunities, 1982, 249-267. pages 249–267, 1982.\r[12] R. B. Ekstrom, J. W. French, H. H. Harman, and D. Dermen. Manual for kit of factor-referenced cognitive tests. Princeton, NJ: Educational Testing Service, 1976.\r[13] H.Friederichs,S.Ligges,andA.Weissenstein.Usingtreediagramswith- out numerical values in addition to relative numbers improves students numeracy skills a randomized study in medical education. Medical Deci- sion Making, 2013.\r[14] M. Galesic, R. Garcia-Retamero, and G. Gigerenzer. Using icon arrays to communicate medical risks: overcoming low numeracy. Health Psy- chology, 28(2):210, 2009.\r[15] R.Garcia-RetameroandM.Galesic.Whoprofitsfromvisualaids:Over- coming challenges in people’s understanding of risks. Social science & medicine, 70(7):1019–1025, 2010.\r[16] R. Garcia-Retamero and U. Hoffrage. Visual representation of statistical information improves diagnostic inferences in doctors and their patients. Social Science & Medicine, 83:27–33, 2013.\r[17] G.GigerenzerandU.Hoffrage.Howtoimprovebayesianreasoningwith- out instruction: Frequency formats. Psychological Review, 102(4):684, 1995.\r[18] T.M.GreenandB.Fisher.Towardsthepersonalequationofinteraction: The impact of personality factors on visual analytics interface interaction. In IEEE Visual Analytics Science and Technology (VAST), 2010.\r[19] P. K. Han, W. M. Klein, B. Killam, T. Lehman, H. Massett, and A. N. Freedman. Representing randomness in the communication of individ- ualized cancer risk estimates: effects on cancer risk perceptions, worry, and subjective uncertainty about risk. Patient education and counseling, 86(1):106–113, 2012.\r[20] S. T. Hawley, B. Zikmund-Fisher, P. Ubel, A. Jancovic, T. Lucas, and A. Fagerlin. The impact of the format of graphical presentation on health- related knowledge and treatment choices. Patient education and counsel- ing, 73(3):448–455, 2008.\r[21] M. Hegarty. Capacity limits in diagrammatic reasoning. In Theory and application of diagrams, pages 194–206. Springer, 2000.\r[22] J. Hullman and N. Diakopoulos. Visualization rhetoric: Framing effects in narrative visualization. Visualization and Computer Graphics, IEEE Transactions on, 17(12):2231–2240, 2011.\r[23] J. Hullman, S. Drucker, N. H. Riche, B. Lee, D. Fisher, and E. Adar. A deeper understanding of sequence in narrative visualization. Visualiza- tion and Computer Graphics, IEEE Transactions on, 19(12):2406–2415, 2013.\r[24] V. J. Kellen. The effects of diagrams and relational complexity on user performance in conditional probability problems in a non-learning con- text. In Doctoral Thesis. DePaul University, 2012.\r[25] V. J. Kellen, S. Chan, and X. Fang. Facilitating conditional probabil- ity problems with visuals. In Human-Computer Interaction. Interaction Platforms and Techniques, pages 63–71. Springer, 2007.\r[26] D.Kimura.Sexandcognition.MITpress,2000.\r[27] R.KosaraandJ.Mackinlay.Storytelling:Thenextstepforvisualization.\rComputer, 46(5):44–50, 2013.\r[28] L. Martignon and C. Wassner. Teaching decision making and statistical\rthinking with natural frequencies. In Proceedings of the Sixth Interna- tional Conference on Teaching of Statistics. Ciudad del Cabo: IASE. CD ROM, 2002.\r[29] L. Micallef, P. Dragicevic, and J. Fekete. Assessing the effect of visual- izations on bayesian reasoning through crowdsourcing. Visualization and Computer Graphics, IEEE Transactions on, 18(12):2536–2545, 2012.\r[30] A. B. Miller, C. J. Baines, P. Sun, T. To, and S. A. Narod. Twenty five year follow-up for breast cancer incidence and mortality of the canadian national breast screening study: randomised screening trial. BMJ: British Medical Journal, 2014.\r[31] A. Ottley, R. J. Crouser, C. Ziemkiewicz, and R. Chang. Manipulating and controlling for personality effects on visualization tasks. Information Visualization, 2013.\r[32] A.Ottley,B.Metevier,P.K.Han,andR.Chang.Visuallycommunicating bayesian statistics to laypersons. In Technical Report. Tufts University, 2012.\r[33] A. Ottley, H. Yang, and R. Chang. Personality as a predictor of user strategy: How locus of control affects search strategies on tree visualiza- tions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2015.\r[34] V.F.Reyna,W.L.Nelson,P.K.Han,andN.F.Dieckmann.Hownumer- acy influences risk comprehension and medical decision making. Psy- chological bulletin, 135(6):943, 2009.\r[35] P.SedlmeierandG.Gigerenzer.Teachingbayesianreasoninginlessthan two hours. Journal of Experimental Psychology: General, 130(3):380, 2001.\r[36] E.SegelandJ.Heer.Narrativevisualization:Tellingstorieswithdata.Vi- sualization and Computer Graphics, IEEE Transactions on, 16(6):1139– 1148, 2010.\r[37] D. Spiegelhalter, M. Pearson, and I. Short. Visualizing uncertainty about the future. Science, 333(6048):1393–1400, 2011.\r[38] J. Tsai, S. Miller, and A. Kirlik. Interactive visualizations to improve bayesian reasoning. In Proceedings of the Human Factors and Er- gonomics Society Annual Meeting, volume 55, pages 385–389. SAGE Publications, 2011.\r[39] A.TverskyandD.Kahneman.Theframingofdecisionsandthepsychol- ogy of choice. Science, 211(4481):453–458, 1981.\r[40] M. C. Velez, D. Silver, and M. Tremaine. Understanding visualization through spatial ability differences. In IEEE Visualization, pages 511– 518. IEEE, 2005.\r[41] K. J. Vicente, B. C. Hayes, and R. C. Williges. Assaying and isolat- ing individual differences in searching a hierarchical file system. Human Factors: The Journal of the Human Factors and Ergonomics Society, 29(3):349–359, 1987.\r[42] H. G. Welch and W. C. Black. Overdiagnosis in cancer. Journal of the National Cancer Institute, 102(9):605–613, 2010.\r[43] H. G. Welch et al. Overdiagnosis and mammography screening. Bmj, 339, 2009.\r[44] J.S.Yi.Implicationsofindividualdifferencesonevaluatinginformation visualization techniques. In Proceedings of the BELIV Workshop, 2010.\r[45] C. Ziemkiewicz and R. Kosara. Preconceptions and individual differ- ences in understanding visual metaphors. Computer Graphics Forum, 28(3):911–918, 2009. Proceedings EuroVis.\r[46] C. Ziemkiewicz, A. Ottley, R. J. Crouser, K. Chauncey, S. L. Su, and R. Chang. Understanding visualization by understanding individual users. Computer Graphics and Applications, IEEE, 32(6):88–94, 2012.\r[47] C.Ziemkiewicz,A.Ottley,R.J.Crouser,A.R.Yauilla,S.L.Su,W.Rib- arsky, and R. Chang. How visualization layout relates to locus of con- trol and other personality factors. Visualization and Computer Graphics, IEEE Transactions on, 19(7):1109–1121, 2013.","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"bayesian","time":1461915874655,"auto":true,"weight":1.0},{"@type":"Tag","text":"reason","time":1461915874721,"auto":true,"weight":1.0},{"@type":"Tag","text":"vi","time":1461915874712,"auto":true,"weight":1.0},{"@type":"Tag","text":"abil","time":1461915874666,"auto":true,"weight":1.0},{"@type":"Tag","text":"accuraci","time":1461915874697,"auto":true,"weight":1.0},{"@type":"Tag","text":"visual","time":1461915874705,"auto":true,"weight":1.0},{"@type":"Tag","text":"brase","time":1461915874729,"auto":true,"weight":1.0},{"@type":"Tag","text":"diseas","time":1461915874677,"auto":true,"weight":1.0},{"@type":"Tag","text":"text","time":1461915874685,"auto":true,"weight":1.0},{"@type":"Tag","text":"spatial","time":1461915874645,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":33,"appId":"PeyeDF_aabde1d6c2f5a1dc36d0b593e342d0c212d86779","timeCreated":1456743863064,"timeModified":1461915877805,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.03718685,"uri":"file:///Users/cheny13/Downloads/ADA505334.pdf","plainTextContent":"DRDC CR2009-042\rA NON-INTRUSIVE ALERT SYSTEM FOR MARITIME ANOMALIES: LITERATURE REVIEW AND THE DEVELOPMENT AND ASSESSMENT OF INTERFACE DESIGN CONCEPTS\rSYSTÈME D’ALERTE NON INTRUSIVE EN CAS D’ANOMALIES MARITIMES : EXAMEN DE LA DOCUMENTATION ET ÉLABORATION/ÉVALUATION DE CONCEPTS D’INTERFACE\rMichael Matthews, Lora Bruyn Martin, Courtney D. Tario and Andrea L. Brown\rHumansystems® Incorporated 111 Farquhar St., 2nd floor Guelph, ON N1H 3N4\rProject Manager: Ron Boothby, PMP (519) 836 5911\rPWGSC Contract No: W7711-088124/001/TOR\rOn behalf of DEPARTMENT OF NATIONAL DEFENCE\rDRDC Scientific Authorities Sharon McFadden 416-635-2000\rLiesa Lapinski 902-426-3100 ext. 180\rMarch 2009\r\nAuthor\rMichael Matthews, PhD Humansystems® Incorporated\rApproved by\rSharon McFadden DRDC Toronto\rApproved for release by\rChair, Document Review and Library Committee\rThe scientific or technical validity of this Contractor Report is entirely the responsibility of the contractor and the contents do not necessarily have the approval or endorsement of Defence R&D Canada.\r©HER MAJESTY THE QUEEN IN RIGHT OF CANADA (2009) as represented by the Minister of National Defence\r©SA MAJESTE LA REINE EN DROIT DU CANADA (2009) Défense Nationale Canada\r\nAbstract\rThis project involves the investigation of best practices for the development of design concepts for a visualization aid, specifically an alerting system, which would increase the RMP operators’ awareness and understanding of maritime anomalies in the RMP (e.g. vessel not heading to port, grab and dash fishing, etc.). Such an alerting system, however, must make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\rThe objectives of this project were (i) to identify and analyse available literature relevant to non- intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, we were able to extract relevant concepts from the literature relating to alert/alarm design in general. These concepts, combined with general human factors principles, provided direction for a number of design concepts which were then reviewed and evaluated by subject matter experts.\rFuture design efforts should work toward developing an alert system interface design in accordance with the design principles listed above, once these design requirements have been validated.\rHumansystems® Non-Intrusive Alert System Page i\r\nRésumé\rLe projet comprend l’étude des meilleures pratiques applicables à la définition de concepts pour un système d’aide à la visualisation, en l’occurrence un système d’alerte, qui aiderait les opérateurs du TSM à mieux connaître et comprendre les anomalies maritimes indiquées dans le TSM (déroutement d’un navire, braconnage maritime, etc.). Un tel système d’alerte doit toutefois permettre aux opérateurs d’être informés des anomalies éventuelles sans pour autant entraver l’exécution de leurs tâches principales.\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, on a pu extraire de la documentation des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts qui ont ensuite été examinés et évalués.\rLes recherches futures devraient viser à définir une conception d’interface de système d’alerte conformément aux principes de conception présentés ci-dessus, une fois que ces exigences de conception auront été validées.\rPage ii Non-Intrusive Alert System Humansystems®\r\nExecutive Summary\rThe RMP, one of the primary outputs of the Regional Joint Operations Centers (RJOCs), is essentially a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Given the extensive maritime traffic and the large area covered, it is difficult to effectively monitor the RMP to maintain a good understanding of the current situation, including anomalies (e.g. sudden increase in speed, not heading to port of call, etc.). Thus, there is a need for a system that could perform routine checks for anomalous data in the background and then make the information available to operators in a format that makes the anomalies readily comprehensible and gives rise to rapid situation awareness. The crux of the problem is how to implement an alerting system that would make operators aware of anomalies that may be present without impacting on the performance of their primary tasks (i.e. non-intrusive alert).\rThe objectives of this project were (i) to identify and analyse available literature relevant to non- intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe literature review attempted to uncover human factors models, design guidelines, design concepts and empirical research that could be used to inform the design of the functional elements of an alerting system, including:\r• Configuring alert parameters;\r• Receiving information on alert states;\r• Maintaining awareness and performance on the primary task;\r• Comprehending the alert condition;\r• Actioning the alert condition; and\r• Managing alerts.\rResults: The results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, relevant concepts from the literature relating to alert/alarm design in general were extracted. These concepts, combined with general human factors principles, provided direction for a number of design concepts for a future, non-intrusive alerting interface for maritime anomalies.\rA total of four interface design concepts were developed and then reviewed and evaluated by seven subject matter experts. The designs, which were all visual, included an alert indicator, information related to an incoming alert and an alert management window. The design evaluation identified a need to scale the intrusiveness of alerts (i.e. high priority alerts require a more intrusive alert than those of low priority).\rSignificance: Feedback from the SMEs, combined with consideration of general human factors principles, resulted in a list of design requirements for the best way to:\r• Alert RMP operator to new incoming alert\r• Provide operator with awareness of the number of active alerts in the system\r• Provide operator with information specific to an incoming alert\r• Provide operator with information on all active alerts in the system\rHumansystems® Non-Intrusive Alert System Page iii\r\n• Enable operator to manage (i.e. action) any active alerts in the system Future plans: In general, future research and development efforts should focus on:\r(i) establishing and validating detailed design requirements for an alerting system,\r(ii) developing an alert system interface design in accordance with the design principles presented in the report,\r(iii) experimentally evaluating alternate design options for an anomaly alert system in the context of the RMP (i.e. representative of user’s work environment including the potential number of alerts),\r.\r(iv)\rbasic research on better understanding what constitutes intrusiveness and how it relates to factors such as attention and annoyance, particularly as a function of alert interruption frequency.\rPage iv\rNon-Intrusive Alert System Humansystems®\r\nSommaire\rLe Tableau de la situation maritime (TSM), l’un des principaux produits des centres régionaux d'opérations interarmées (CROI), est essentiellement une carte des eaux côtières canadiennes indiquant des contacts, en général des navires. Étant donné l’ampleur du trafic maritime et la vaste zone couverte, il est difficile de contrôler efficacement le TSM de manière à bien comprendre la situation courante, y compris les anomalies (augmentation soudaine de vitesse, déroutement des navires, etc.). Il est donc nécessaire de disposer d’un système capable d’exécuter des vérifications de routine pour déceler les données irrégulières sur l’arrière-plan, puis de transmettre ces informations aux opérateurs sous une forme telle que les anomalies soient immédiatement compréhensibles et qu’on puisse prendre rapidement connaissance de la situation. Le noeud du problème consiste à trouver un moyen de mettre en oeuvre un système d’alerte qui renseigne les opérateurs sur les anomalies possibles sans entraver l’exécution de leurs tâches principales (alerte non intrusive).\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation visait à découvrir des modèles de facteurs humains, des orientations, des concepts et des recherches empiriques qui pourraient être utilisés pour guider la conception des éléments fonctionnels d’un système d’alerte, ce qui comprend :\r• la configuration des paramètres d’alerte;\r• la réception d’information sur les états d’alerte;\r• le maintien de la connaissance de la situation et l’exécution des tâches principales;\r• la compréhension des états d’alerte;\r• l’activation de l’état d’alerte;\r• la gestion des alertes.\rRésultats : L’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, la documentation a permis d’identifier des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts pour une future interface d’alerte non intrusive en cas d’anomalies maritimes.\rEn tout, quatre concepts d’interface ont été définis, puis examinés et évalués par sept experts. Ces concepts, tous de type visuel, comprenaient les suivants : indicateur d’alerte, information liée à une nouvelle alerte et fenêtre de gestion des alertes. L’évaluation de la conception a révélé qu’il fallait prioriser le caractère intrusif des alertes (dans les cas de haute priorité, l’alerte doit être plus intrusive).\rPortée : En tenant compte de la rétroaction des experts, et de principes généraux touchant les facteurs humains, on a établi une liste des exigences à respecter pour la conception d’un système qui remplira le mieux possible les fonctions suivantes :\r• transmettre les nouvelles alertes à l’opérateur du TSM;\rHumansystems® Non-Intrusive Alert System Page v\r\n• informer l’opérateur du nombre d’alertes actives dans le système;\r• fournir à l’opérateur l’information particulière à une nouvelle alerte;\r• fournir à l’opérateur l’information relative à toutes les alertes actives dans le système;\r• permettre à l’opérateur de gérer (intervention) toutes les alertes actives dans le système\rRecherches futures : En général, les futurs travaux de recherche et de développement devraient se concentrer sur les tâches suivantes :\r(v) établir et valider des exigences détaillées pour la conception d’un système d’alerte;\r(vi) définirunconceptd’interfacedesystèmed’alerteconformémentauxprincipesde conception présentés dans le rapport;\r(vii) évaluer expérimentalement différentes options pour la conception d’un système d’alerte en cas d’anomalies dans le contexte du TSM (en tenant compte de l’environnement de travail de l’utilisateur, notamment du nombre éventuel d’alertes);\r(viii)mener des recherches de base visant à mieux comprendre la nature de l’intrusivité et comment celle-ci se rattache à des facteurs tels que l’attention et le dérangement, notamment en fonction de la fréquence des interruptions en cas d’alerte.\rPage vi Non-Intrusive Alert System Humansystems®\r\nTable of Contents\rABSTRACT ......................................................................................................................................................I RÉSUMÉ ...........................................................................................................................................................I EXECUTIVE SUMMARY ...........................................................................................................................III SOMMAIRE ................................................................................................................................................... V TABLE OF CONTENTS ............................................................................................................................ VII LIST OF FIGURES.......................................................................................................................................IX LIST OF TABLES.......................................................................................................................................... X ACKNOWLEDGMENT ...............................................................................................................................XI\r1.\r2.\r3.\r4.\rINTRODUCTION .................................................................................................................................. 1\r1.1 BACKGROUND................................................................................................................................... 1\r1.1.1 Recognized Maritime Picture ...................................................................................................... 1\r1.1.2 Maritime anomalies ..................................................................................................................... 2\r1.1.3 Detection of anomalies ................................................................................................................ 2\r1.2 SCOPE AND OBJECTIVES .................................................................................................................... 3\r1.3 NON-INTRUSIVE ALERTING ............................................................................................................... 3\r1.4 OUTLINE OF REPORT ......................................................................................................................... 3\rTHE BASIC ONTOLOGY OF AN ALERT SYSTEM ...................................................................... 5\r2.1 CONFIGURE ALERT PARAMETERS ...................................................................................................... 6\r2.2 RECEIVE INFORMATION ABOUT THE ALERT STATE ............................................................................ 7\r2.3 COMPREHEND THE ALERT CONDITION............................................................................................... 7\r2.4 ACTION THE ALERT CONDITION......................................................................................................... 7\r2.5 MANAGE ALERTS .............................................................................................................................. 7\r2.6 MAINTAIN PRIMARY TASK ................................................................................................................ 7\r2.7 ORGANISATION OF THE LITERATURE AND THE ONTOLOGY................................................................ 8\rTOWARDS A WORKING DEFINITION OF “NON-INTRUSIVENESS”...................................... 9\r3.1 DEFINING NON-INTRUSIVENESS ........................................................................................................ 9\r3.1.1 Predictions of attentional/resource models ................................................................................. 9\r3.1.2 Predictions of annoyance models .............................................................................................. 10\r3.1.3 Summary of principles ............................................................................................................... 10\rLITERATURE REVIEW .................................................................................................................... 13\r4.1 METHOD ......................................................................................................................................... 13\r4.1.1 Keywords ................................................................................................................................... 13\r4.1.2 Databases .................................................................................................................................. 14\r4.1.3 Search strategy .......................................................................................................................... 14\r4.1.4 Selection and review of articles ................................................................................................. 14\r4.1.4.1 Final evaluation criteria ....................................................................................................................15 Humansystems® Non-Intrusive Alert System Page vii\r\n5.\r6.\r4.2 RESULTS OF THE LITERATURE REVIEW ...........................................................................................16\r4.2.1 Configure alert parameters ........................................................................................................16\r4.2.2 Receive information on alert state..............................................................................................18\r4.2.3 Comprehend alert information content ......................................................................................46\r4.2.4 Maintain primary task ................................................................................................................48\r4.2.5 Manage Alerts ............................................................................................................................53\r4.3 CONCLUSIONS .................................................................................................................................57\r4.3.1 State of current knowledge .........................................................................................................57\r4.3.2 Gaps in the literature .................................................................................................................61\rTHE DESIGN DEVELOPMENT PROCESS.....................................................................................65\r5.1 DEVELOPMENT OF DESIGN CONCEPTS..............................................................................................65\r5.1.1 The alert indicator ......................................................................................................................65\r5.1.2 Visual design concepts ...............................................................................................................67\r5.1.3 Tactile design concept ................................................................................................................73\r5.1.4 The alert information window ....................................................................................................73\r5.1.5 The RMP pop up box ..................................................................................................................76\rEVALUATION AND REVIEW OF DESIGN CONCEPTS BY SUBJECT MATTER EXPERTS .................................................................................................................................................................77\r6.1 PREPARATION FOR DESIGN EVALUATION AND REVIEW...................................................................77\r6.2 METHOD ..........................................................................................................................................77\r6.2.1 Date and Location of SME Evaluation.......................................................................................77\r6.2.2 Participants ................................................................................................................................77\r6.2.3 Materials ....................................................................................................................................77\r6.2.4 Procedure ...................................................................................................................................78\r6.3 RESULTS..........................................................................................................................................79\r6.3.1 Questionnaire data .....................................................................................................................79\r6.3.2 Quantitative data........................................................................................................................80\r6.3.3 Qualitative data..........................................................................................................................85\r6.3.4 General discussion with participants .........................................................................................88\r6.4 CONCLUSIONS AND RECOMMENDATIONS BASED ON EVALUATION...................................................91\r6.4.1 Conclusions ................................................................................................................................91\r6.4.2 Limitations..................................................................................................................................92\r6.4.3 Design recommendations for anomaly alert system ...................................................................92\rOVERALL CONCLUSIONS AND RECOMMENDATIONS..........................................................95\r7.1 CONCLUSIONS .................................................................................................................................95\r7.2 FUTURE WORK ................................................................................................................................95\r7.\rREFERENCES ...............................................................................................................................................97 ANNEX A: ETHICS PROTOCOL............................................................................................................ A-1 ANNEX B: QUESTIONNAIRES FOR SME EVALUATION.................................................................B-1 ANNEX C: GENERAL DISCUSSION INTERVIEW QUESTIONS..................................................... C-1 ACRONYMS ............................................................................................................................................... D-1\rPage viii Non-Intrusive Alert System Humansystems®\r\nList of Figures\rFIGURE 1: RELATIONSHIP OF LITERATURE TO THE ALERT ONTOLOGY ................................................................ 6 FIGURE 2: IRC FRAMEWORK (ADAPTED FROM MCCRICKARD ET AL., 2003A, P. 321) ....................................... 19 FIGURE 3: HAZARD NETWORK (HAUTAMAKI ET AL., 2006, P. 7) ...................................................................... 37 FIGURE 4: INTERRUPTION MANAGEMENT STAGE MODEL (ADAPTED FROM MCFARLANE & LATORELLA, 2002,\rP. 16) ....................................................................................................................................................... 39 FIGURE 5: ALERT LIST (RIVEIRO ET AL., 2008, P. 6; © 2008IEEE) ................................................................... 55 FIGURE 6: DESIGN 1: CUMULATIVE TOTAL INDICATOR.................................................................................... 68 FIGURE 7: DESIGN 2: VERTICAL CUMULATIVE INDICATOR .............................................................................. 69 FIGURE 8: DESIGN 3: HORIZONTAL INDICATOR BAR ........................................................................................ 70 FIGURE 9: DESIGN 4: TICKER & FADING BAR................................................................................................... 71 FIGURE 10: DESIGN 5: POLYGON ...................................................................................................................... 72 FIGURE 11: ALERT INFORMATION WINDOW..................................................................................................... 74 FIGURE 12: RMP TRACK HIGHLIGHT ................................................................................................................ 75 FIGURE 13: ALERT INFORMATION WINDOW AFTER A TRACK IS DELETED......................................................... 75 FIGURE 14: RMP POP UP BOX ........................................................................................................................... 76 FIGURE 15: SUBJECT RANKINGS FOR EFFECTIVENESS IN BRINGING ALERTS TO MY ATTENTION ........................ 83 FIGURE 16: SUBJECT RANKINGS FOR METHODS FOR REPRESENTING THE PRIORITIES ........................................ 83 FIGURE 17: SUBJECT RANKINGS FOR PROVIDING REQUIRED INFORMATION WITHOUT DISTRACTION................. 84\rHumansystems® Non-Intrusive Alert System Page ix\r\nList of Tables\rTABLE 1: KEYWORDS........................................................................................................................................13 TABLE 2: DATABASES SEARCHED .....................................................................................................................14 TABLE 3: OPERATOR INFORMATION PREFERENCE PROFILE (HUDLICKA & MCNEESE, 2002) ............................17 TABLE 4: IRC FRAMEWORK ALERTS AND EXAMPLES (MCCRICKARD ET AL., 2003A) .......................................20 TABLE 5: MODEL-BASED PRINCIPLES FOR HUMAN-CENTRED ALARM SYSTEMS (SHORROCK ET AL., 2002) ......23 TABLE 6: SUGGESTIONS FOR AUDITORY ALERTS (EDWORTHY & HELLIER, 2002).............................................25 TABLE 7: IDENTIFIED PROBLEMS AND DESIGN REQUIREMENTS (HAN ET AL., 2007) ..........................................28 TABLE 8: EXAMPLES OF GUIDELINES (HAN ET AL., 2007) .................................................................................29 TABLE 9: RESULTS FOR OPERATOR TASKS: STUDY 1 (MCCRICKARD ET AL., 2003B) ........................................32 TABLE 10: RESULTS FOR OPERATOR TASKS: STUDY 2 (MCCRICKARD ET AL., 2003B) ......................................33 TABLE 11: RECOMMENDATIONS FOR DESIGN FOR A PLATOON LEADER ALERT (KRAUSMAN ET AL., 2005) .......34 TABLE 12: SUMMARY .......................................................................................................................................45 TABLE 13: LITERATURE IN THE REPORT ............................................................................................................57 TABLE 14: ALERT DESIGN PRINCIPLES ..............................................................................................................58 TABLE 15: SUMMARY OF LITERATURE RELEVANCE AND NEED FOR RESEARCH .................................................63 TABLE 16: MEAN RATINGS FOR USABILITY AND USEFULNESS OF NON-INTRUSIVE ALERTING SYSTEM ...........80 TABLE 17: MEAN RATINGS FOR THE ALERT DESIGN QUESTIONS .....................................................................81 TABLE 18: MEAN RATINGS FOR DESIGN IMPLEMENTATION AND INTRUSIVENESS ..............................................85 TABLE 19: LIKES AND DISLIKES OF ALERT DESIGNS ........................................................................................86 TABLE 20: DESIGN RECOMMENDATIONS FOR FUTURE ITERATIONS OF ALERT SYSTEM INTERFACE....................92\rPage x Non-Intrusive Alert System Humansystems®\r\nAcknowledgment\rWe would like to thank Lt(N) (ret) J. R. Rafuse for providing valuable information that contributed to the development of design concepts for this project and for detailed background information on how operators might prioritize alerts. We should also like to thank all of the people that participated in the evaluation of the design concepts for their time and effort.\rHumansystems® Non-Intrusive Alert System Page xi\r\nThis page intentionally left blank.\rPage xii Non-Intrusive Alert System Humansystems®\r\n1. Introduction 1.1 Background\rDefence Research and Development Canada (DRDC) has an ongoing Applied Research Project in the Maritime Intelligence, Surveillance, and Reconnaissance (MISR) Thrust on information visualization and management for enhanced domain awareness in maritime security. This is a 4- year Research and Development (R&D) project with the goal of enhancing the “maritime picture” through improved quality of information and novel, adaptive ways of visualizing that information. The DRDC team wants to (i) investigate best practices for the design of new visualization aids for operators that may improve their understanding of issues such as information uncertainty and data anomalies, and (ii) test to see if visualizing this information can help improve analysis and situation awareness of the Recognized Maritime Picture (RMP), decision making based on the RMP, and the RMP operators’ efficiency in such activities.\r1.1.1 Recognized Maritime Picture\rThe RMP is one of the primary outputs of the Regional Joint Operations Centers (RJOCs). In its common form, the RMP is a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Other important outputs from the RJOCs are specific reports on Vessels of Interest (VOI). Although produced by the RJOCs, the RMP and VOI reports are also used by other government agencies (e.g., Department of Fisheries and Oceans, the Royal Canadian Mounted Police and Canadian Navy ships at sea to further their own specific interests). Therefore, it is particularly important that the information produced in the RMP and VOI reports is accurate and reliable.\rThe term “Recognized Maritime Picture” implies that there is some element of knowledge of the attributes of a contact, such as the vessel’s name, hull number or class (e.g., tanker, fishing boat, warship). A typical RMP plot would show hundreds of contact symbols, which are colour coded using standard North Atlantic Treaty Organization (NATO) conventions for identity (friend, hostile, neutral, unknown) and may also contain a vector arrow to indicate last known direction of movement. Associated with each plot on the map are metadata, which are shown in a tabular data display containing approximately 22 data fields of information concerning the contact. In addition, the tabular report includes detailed information on the reporting source. There are as many as 20-25 reporting sources (Davenport, Widdis and Rafuse, 2005) that could potentially contribute to a contact report. These reporting sources range from highly detailed and accurate reports from Provincial Airlines (PAL) overflights, Canadian Coastguard and Canadian Navy ships at sea as well as detailed information on a ship’s name and position from ships carrying Automatic Identification System (AIS) transponders to single contact reports from radar sources such as High Frequency Surface Wave Radar (HFSWR) and Electronic Intelligence (ELINT).\rThe many sensor and reporting sources that contribute to the RMP, therefore, push large volumes of data into associated databases. The RMP is constantly being updated, both manually by operators and automatically. Given the extensive maritime traffic and the large area covered, it is difficult to effectively monitor the RMP to maintain a good understanding of the current situation.\rHumansystems® Non-Intrusive Alert System Page 1\r\n1.1.2 Maritime anomalies\rAnomalies are defined as deviations from the norm (Riveiro, Falkman, & Ziemke, 2008) or targets that are not easily classified (Roy, 2008). The more common types of RMP Anomalies include attribute, movement and VOI-related anomalies.\rAttribute related anomalies: In the Concept of Operations (CONOPS) for producing the RMP, a track refers to one ore more contact reports linked together to describe any detected discrete airborne, surface or subsurface object. A track has many attributes, any of which can be missing, incorrect (e.g., misspelled) or inconsistent (changed by different operators, or characterized by different standards/nomenclature/schemas as tracks pass through various RMP management sites). Such problems cause ambiguous tracks in some cases or just bad information on a contact in others. At the moment, the most common alerting method for attribute mismatch is the production of an ambiguous track by Global Command and Control System-Maritime (GCCS-M), a de facto form of alerting that there is a problem with a track, which the operator then must resolve.\rMovement related anomalies include:\r• Unexpected or illogical course and speed changes\r• Failure to reach a reported estimated time of arrival (ETA) at a specified point\r• Loss of reporting\r• Movement that suggests activity of concern, such as very close proximity to other vessels\rin open waters\r• Significant variance in voyage/route compared to historical trace of other recent voyages\rVOI related anomalies: The Maritime Forces Atlantic (MARLANT) “Alert Table” is a list that extracts key fields (usually vessel name) from RMP messages if the vessel has been previously entered on the list (mostly VOIs). This “alert” prompts operators to contact whoever is listed as the interested agency for the VOI.1\r1.1.3 Detection of anomalies\rDetection of anomalies requires identification of whether or not specific behaviours are abnormal. A model of this process has been proposed by Riveiro and colleagues (2008). According to the model, anomaly detection is a complex process, requiring information acquisition (search and detection), analysis and the ability to integrate events within the operator’s situation awareness and mental model of the operational environment. In the present case, this would be bounded by the information provided by existing operational systems (e.g., GCCS-M, the Contact History Database (CHDB)).\rOperators who may be required to perform anomaly detection in conjunction with their visualization aids face a number of different challenges. Rhodes (2007) points out two specific problems associated with the detection and prediction of anomalous behaviour. First, normalcy is dependent on the context in question. It is nearly impossible to create a system that recognizes every type of normal and abnormal behaviour. Thus, a system that is always learning and adapting is needed to deal with such complexity. However, such systems need to function reliably without requiring a high level of operator involvement.\r1 For many operators, this would be their only concept of an alerting method together with the method used to represent ambiguous track generation in GCCS-M.)\rPage 2 Non-Intrusive Alert System Humansystems®\r\nIn addition to anomaly identification, RJOC operational staff has extremely demanding work responsibilities (Davenport et al., 2005; Matthews, Bruyn, Keeble & Rafuse, 2004) that effectively preclude them from spending time doing detective work at uncovering data anomalies. None of the current job functions or tasks performed at the RJOCs includes any effort directed specifically at anomaly detection or analysis with the possible exception of VOIs (Matthews et al., 2004). Thus, there is a need for a system that could perform routine checks for anomalous data in the background and then make the information available to operators in a format that makes the anomalies readily comprehensible and gives rise to rapid situation awareness. The crux of the problem is how to implement an alerting system that would make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\r1.2 Scope and objectives\rThe objectives of the project as a whole were (i) to identify and analyse available literature relevant to non-intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe literature review provided direction for a number of design concepts for a future, non-intrusive alerting interface. This alerting system interface must serve the information requirements of operators and improve their situation awareness of anomalies and their meaning for the RMP. Because operators must multi-task and there exists a potential for a large number of data anomalies to occur, the major constraint for the interface design is that it be “non-intrusive”. A total of five design concepts were developed, four of which were demonstrated for naval SMEs in order to gather user feedback.\r1.3 Non-intrusive alerting\rNon-intrusive alerting is not a clearly defined concept that can be expressed in absolute terms, but is highly context dependent. Essentially, the concept of non-intrusiveness conveys the notion of advising an operator (creating awareness) that an alert condition has occurred in a manner that does not disrupt ongoing task performance. It should be noted that a preliminary search indicated that there is limited literature available that deals specifically with the issue of the intrusiveness of an alert, and how to scale this intrusiveness to a particular set of operational circumstances. Therefore, the review and analysis of much of the following literature was to determine what useful information could be extracted from the literature on more generic aspects of alarm and alerting systems that would be applicable or extensible.\r1.4 Outline of report\rSections 2 and 3 describe the basic ontology of an alert system as well as a working definition of non-intrusiveness. These sections are based on the knowledge that was gained by reviewing the literature and provide a framework for discussing the results of the literature review. Section 4 presents the method, results and conclusion of the literature review. In section 5, we provide examples of design concepts for elements of a non-intrusive alerting system for implementation in a GCCS-M environment. This is supplemented by a separate PowerPoint interactive demonstration that allows a cognitive walkthrough of the design alternatives. Section 6 describes the review and assessment of the design concepts by SMEs from RJOC-Atlantic. The final section of the report gives the overall conclusions and recommendations.\rHumansystems® Non-Intrusive Alert System Page 3\r\nThis page intentionally left blank.\rPage 4 Non-Intrusive Alert System Humansystems®\r\n2. The Basic Ontology of an Alert system 2\rPrior to reviewing the results of the literature search, we provide in this section an outline of the functional elements of an alerting system which we believe brings a useful and coherent structure or framework for the diverse papers reviewed. These ideas, in themselves, stem from insights gained in the literature review. Our objective is to define the functional elements an alerting system and how they relate to each other.\rIn addition, certain constraints and assumptions have guided this analysis concerning the role of the operator and the team, as follows:\r• The operator has a primary task or tasks to fulfil which are NOT the monitoring or management of alerts;\r• The operator does not have any other team members who will monitor or manage alerts; and\r• The goal of the operator is to deal with alerts as efficiently as possible and to return promptly to the primary task.\rThe functional elements of an alerting system serve several different tasks that a user may be required to perform in the set-up, operation and maintenance of an alerting system. These have been adapted, and expanded upon, from McCrickard, Chewar, Somervell and Ndiwalana (2003a) and are outlined below:\r• Configuring alert parameters;\r• Receiving information on alert states;\r• Maintaining awareness and performance on the primary task;\r• Comprehending the alert condition;\r• Actioning the alert condition; and\r• Managing the accumulated alert information.\rThe elements of the ontology are shown in Figure 1 and are outlined below. Explanation of the colour and line coding is provided in section 2.7.\r2 The essential meaning of an ontology is that it is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology (Gruber, 1995).\rHumansystems® Non-Intrusive Alert System Page 5\r\nFigure 1: Relationship of literature to the alert ontology 2.1 Configure alert parameters\rIn an operational environment such as the RJOC, the potential exists for many data anomalies to trigger alerts. Some anomalies will be considered to be high priority, others low. In addition, different circumstances may require different rules to be put into place concerning when an alert is brought to the attention of the operator. Therefore, a critical component of the system is a function that allows managers and operators to define the criteria for an alert and to assign different alerting priorities to these events. The interface for this function should allow operators to simply create rules by picking among menu choices, for example:\r“in area (set co-ordinates), if vessels of (type) have speed = (value) then alert with priority (x)”\rFor the purposes of the present project, this aspect of an alerting system was considered out of scope to be examined in detail; therefore no specific literature search was conducted on this topic, although one useful paper that emerged from the more general search was reviewed. However, we believe that the ability to set alert parameters that are suitably configured to an operational context represents an important aspect of reducing nuisance alarms. In addition, allowing operators to set categories of alarm priority and matching these categories to the way the alarm state is conveyed, represents an important aspect that is highly relevant to the intrusiveness of the alarm.\rPage 6 Non-Intrusive Alert System Humansystems®\r\n2.2 Receive information about the alert state\rThis is the key component of the system and incorporates the means by which an alert state will be brought to the attention of the operator. This function is concerned with just the notification and does not provide details concerning the nature of the condition that gave rise to the alert. However, the alert notification should provide information about the alert priority. The design of the alert indicator is the principle focus of this project\r2.3 Comprehend the alert condition\rHaving been notified of an alert, at some point the operator will decide to process the alert and needs further information about the condition(s) that resulted in the alert. This information needs to be provided in a succinct manner that allows the operator to readily comprehend the type of alert. In the case of the RJOC, the interface should allow the operator to rapidly identify the vessel involved and the area of the RMP concerned. This functionality is a secondary point of focus for this project.\r2.4 Action the alert condition\rThis is a function that allows the operator to investigate the alert and take whatever action is required. This functionality may be integrated into the interface for an alerting system or may involve other work functions independent of the alert system. It could be considered that this function is not really part of an alerting system at all; however, at some point the operator will return to the primary task, therefore the interface will need to provide suitable functionality to support this transition. While the actioning of an alert was out of scope for this project, the regaining of situation awareness of the primary task was considered important and is dealt with more extensively under the function of Maintain Primary Task.\r2.5 Manage alerts\rAs alerts accumulate in the operating environment, a new task relating to their management arises for the operator and possibly system manager. The database of these alerts will need to be structured and formatted in such a manner that it permits the operator to quickly determine, for example, how many alerts are in the system, what are there priorities and when they occurred. Simple tools for the sorting and deletion of records will also be required. This aspect of an alerting system was not the primary focus of the literature review, however, some basic functional elements for the interface for such a system have been developed and are shown later in the report in the section on design concepts. This inclusion was thought to be necessary because effective design of the process for alert management may reduce operator worker load for this activity and indirectly mitigate the information processing costs associated with processing the alarm, thereby potentially reducing the overall alarm intrusiveness into the normal workflow.\r2.6 Maintain primary task\rThe operator’s need to maintain focus on his/her primary task responsibilities is not really part of an alerting system; however it does have implication on the relationship between this primary task and various aspects of the tasks performed in responding to and managing alerts. Some considerations are when to present information about an alert state, i.e., when is the best time to\rHumansystems® Non-Intrusive Alert System Page 7\r\ninterrupt the primary task? Should warnings be provided of impending alerts to allow the operator to better prepare and not suddenly lose primary task focus by a surprise alert? A further consideration is how to enable the operator to regain primary task situation awareness after dealing with an alert. The focus of the present project is only on the last of these three questions.\r2.7 Organisation of the literature and the ontology\rIn order to structure the literature review in terms of its relevance to the ontology, a classification system was applied that broke the literature down into four main categories:\r1. Conceptual models\r2. Generic alert system design guidelines\r3. Specific design concepts for alerts\r4. Relevant empirical research on alert systems\rIn Figure 1, we show the specific links between these categories and those elements of an alerting system that were the focus of the literature search. The primary areas of interest are shown in green boxes and are linked to the literature review by solid lines. The secondary areas are shown in light yellow and the links are shown by broken lines. Note that there is no link to the “tools to service alerts” function, since this was completely beyond the scope of this project.\rPage 8 Non-Intrusive Alert System Humansystems®\r\n3. Towards a Working Definition of “Non- intrusiveness”\r3.1 Defining non-intrusiveness\rPrior to considering any design options for a non-intrusive alert system and in order to better understand the literature review, we believe that it is necessary to clarify what is meant by “intrusive” and to develop a working operational definition that will guide the design process, which is described later in the report. As a starting point, we make the assumption that the intrusiveness of an alert or alarm can be defined as an event which, either immediately, or over time, reduces the ability, or potential capacity, to perform a primary task.\rWith this definition in mind, a central question is then what psychological models are relevant to intrusiveness and how do they predict what would constitute an intrusive or non-intrusive alert. There appear to be two classes of models that are relevant to this issue. The first class of models deals with attention and resource sharing and the second class concerns models of annoyance. Attentional or resource sharing models (e.g., Wickens, 1984) predict interference in cognitive processing of a primary task when an alert competes for common resources. On the other hand, annoyance models would suggest that it is some quality of the alarm (e.g., for auditory alarms, intensity, frequency components, duration and repetition cycle) that produces a psychological state of annoyance that becomes so compelling that attention can no longer be devoted to the primary task (e.g., De-Muer, Botteldooren, Coensel, Berglund, Nilsson & Lercher, 2005).\r3.1.1 Predictions of attentional/resource models\rAccording to attention and resource sharing models, an alarm would be considered intrusive if it directly affects the processing of the primary task by sensory interference. For a visual monitoring task this might be a visual alert that occurs at the primary point of visual focus thereby directly interfering with the perception of visual information required to monitor the display. Other examples of this might be the whole display flashing on and off, a visual message that pops up in the middle of the screen or a siren that occurs over an auditory communication network. A second method of interference suggested by the attentional/resource models would be an alarm that competes in a compelling way for attentional resources, but does not in itself directly interfere with the perceptual or cognitive processing of the primary task. That is, the alarm does not directly impede information processing, but rather vies with the primary task for the operator’s attention. An example of this might be a flashing border on all four sides of a visual display. A third method of interference may result from population stereotypes that have developed concerning what alarms\rlook or sound like, for example red flashing lights, sirens or certain iconic symbols (\rcase it is assumed that the semantic content associated with the stimulus immediately overrides ongoing processing and captures attention. In other words, upon perceiving the alarm, the operator immediately knows it is something important and needs attention.\rWhile it is easy to see how such models can suggest the intrusiveness of an alert in such obvious cases, it is less clear how they would predict the intrusiveness of, for example, a ticker style text message that runs along the bottom of the screen.\r). In this\rHumansystems® Non-Intrusive Alert System Page 9\r\nThe general guidance that can be taken from these models is that non-intrusive alarms should not compete for sensory or cognitive processing capacity of the primary task, should not divert attentional resources and should avoid configurations that may trigger over-learned orientation responses because of familiarity and common usage in society.\rIn developing non-intrusive interfaces for an anomaly alerting system, we have used our knowledge of the RJOC operator’s tasks and current system interface to inform the designs such that they do not compete for attentional resources or cognitive processing capacity of the primary task. However, ultimately the degree to which an alert provides the right level of non-intrusiveness should be judged and validated by the end user in the appropriate operational context.\r3.1.2 Predictions of annoyance models\rThe only predictive annoyance models that we have found deal largely with noise annoyance and apply to more general societal issues than to cognitive work environments. Such models may take into account factors of the strength of the noise source, frequency, adaptation and appraisal by the human, emotions aroused and cognitive processing. However, we have not found any extensive or compelling data that would suggest specific design guidance, other than the obvious. For example, repetitive sounds or signals will generate an annoyance factor over time. However, the degree to which such annoyance produces degradation in or distraction from primary task performance cannot be specified, nor can one say for how long a repetitive signal would have to continue before it becomes annoying, what level of intensity causes annoyance and how individual differences and experience may influence annoyance “thresholds”.\rTherefore, we will proceed to use common sense in the interpretation of what could be potentially annoying in an alert and to avoid such design options. However, as noted above, the degree to which an alert provides the right level of non-intrusiveness should ultimately be judged and validated by the end user in the appropriate operational context. A design that we believe to be non-intrusive could ultimately turn out to be the opposite if the underlying anomaly trigger occurs with high frequency in actual operations. This might be the case when there is a data dump from an information source (e.g., PAL flight or AIS Vessel Monitoring System (AVMS) receiver).\r3.1.3 Summary of principles\rThe following are the main principles that make an alert intrusive.\r• Direct perceptual interference with the ongoing primary task;\r• Direct cognitive interference with the primary task;\r• Continued presence of a stimulus that by repetitiveness, intensity or quality results in a psychological or emotional state that causes an individual to lose attentional focus on the primary task; and\r• The greater the perceptual deviation of the alerting stimulus from the ambient perceptual environment, the more intrusive it will generally be (e.g., providing an auditory alert during a primarily visual task).\rIt follows from this that non-intrusive alerts must have properties that correspond to the converse of the above principles. That is, a non-intrusive alert must not directly interfere with perceptual or cognitive processing of the primary task; draw the operator’s attention away from the primary task due to its repetitiveness, intensity or quality; or differ significantly from the background perceptual\rPage 10 Non-Intrusive Alert System Humansystems®\r\nenvironment. However, as outlined in Section 5, our objective was to design non-intrusive alerts for anomalies that vary in priority level; from low (priority 3) to high (priority 1). Consequently, the priority of the anomalous information must be implied by the level of intrusiveness of the alert itself. That is, alerts for high priority anomalies must be more intrusive than those for low priority information.\rIn addition to avoiding the above factors that would make an alert intrusive, a key element in alert system design, to minimize (if not avoiding entirely) interference in the primary task due to alarm intrusiveness, is to provide an ability for operators to match alarm priorities to the appropriate level of cognitive and perceptual intrusiveness.\rHumansystems® Non-Intrusive Alert System Page 11\r\nThis page intentionally left blank.\rPage 12 Non-Intrusive Alert System Humansystems®\r\n4. Literature Review 4.1 Method\rThis section outlines the methodology used to search the literature as well as select research relevant to the design of non-intrusive alert systems.\r4.1.1 Keywords\rThe following keywords for conducting the literature search were agreed upon with the Technical Authority (TA) and are shown in Table 1.\rTable 1: Keywords\rCore Concept\rPriority 1 keywords\rPriority 2 keywords\rAlerting System Technology\rAlarm/warning/alert system Non-intrusive\rAdvisory system\rTote board\rAuditory/visual alarm Tote\rOperator Interface\rDesign guidelines/concepts Human-computer Operator-machine Interface design\rUser interface Intelligen* Adaptive Etiquette\rHuman factors Smart\rHuman Performance\rFalse alarm\rAttenti*\rMonitor*\rWorkload\rModel*\rSituation* awareness Distract*\rDisrupt*\rAnomaly/change detection Anomal*\rKeywords to be searched independently\rAlarm/warning/alert overload Missed alarm/warning/alert\rIn conducting the search, all Alerting System Technology keywords were paired with Operator Interface keywords and with Human Performance keywords using “AND” logic. The keywords to be searched independently (i.e., alarm/warning/alert overload and missed alarm/warning/alert) were not paired with any other keywords. It was initially proposed that if the literature search using the priority 1 keywords did not produce ideal results, priority 2 keywords would be substituted for priority 1 keywords. However, based upon an abundance of articles found with priority 1 keywords there was no reason to pursue this option.\rHumansystems® Non-Intrusive Alert System Page 13\r\n4.1.2 Databases\rTable 2: Databases searched\rDatabase\rDescription\rPsycINFO\rThe PsycINFO database is a collection of electronically stored bibliographic references, often with abstracts or summaries, to psychological literature from the 1800s to the present. The available literature includes material published in 50 countries, but is all presented in English. Books and chapters published worldwide are also covered in the database, as well as technical reports and dissertations from the last several decades.\rNTIS\rNTIS is an agency for the U.S. Department of Commerce’s Technology Administration. It is the official source for government sponsored U.S. and worldwide scientific, technical, engineering and business related information. The 400,000 article database can be searched for free at the www.ntis.gov. Articles can be purchased from NTIS at costs depending on the length of the article.\rCISTI\rCISTI stands for the Canada Institute for Scientific and Technical Information. It is the library for the National Research Council of Canada and a world source for information in science, technology, engineering and medicine. The database is searchable on-line at cat.cisti.nrc.ca. Articles can be ordered from CISTI for a fee of approximately $12.\rSTINET\rSTINET is a publicly-available database (stinet.dtic.mil) which provides access to citations of documents such as: unclassified unlimited documents that have been entered into the Defence Technology Technical Reports Collection (e.g., dissertations from the Naval Postgraduate School), the Air University Library Index to Military Periodicals, Staff College Automated Military Periodical Index, Department of Defense Index to Specifications and Standards, and Research and Development Descriptive Summaries. The full- text electronic versions of many of these articles are also available from this database.\rGoogle Scholar\rThe World Wide Web was searched using the Google Scholar search engines (scholar.google.com).\rDRDC Research Reports\rDRDC Defence Research Reports is a database of scientific and technical research produced over the past 6- years by and for the Defence Research & Development Canada. It is available online at pubs.drdc-rddc.gc.ca/pubdocs/pcow1_e.html.\r4.1.3 Search strategy\rTo maintain a record of the process, the following information was documented in a spreadsheet:\r• Database searched (e.g., Psych Info);\r• Keyword combination (e.g., Non intrusive AND attenti*);\r• Number of hits;\r• Number of applicable hits;\r• Articles downloaded;\r• Articles/books that require purchase; and\r• If applicable, where in the article the keywords were searched (i.e., only in the article\rkeywords or anywhere in the article).\r4.1.4 Selection and review of articles\rTwo articles were purchased and 68 articles were downloaded giving a total of 70 relevant articles. Each article was then briefly reviewed and classified according to article theme (i.e., Human performance or Operator interface), source of the article (i.e., keyword search, article provided by the TA, or from another article), and whether the article included theoretical or empirical research.\rPage 14 Non-Intrusive Alert System Humansystems®\r\nThe research team first used criteria of relevance and quality to evaluate the 70 articles. Relevance was defined as how closely the article relates to the research objectives outlined in the Statement of Work. Specifically, relevance was assigned the following 4 point scale:\r• 1 = non-intrusive alerts to inform operators (design/performance) mentioned in the abstract or paper;\r• 2 = alerts mentioned in abstract or paper;\r• 3 = alerts mentioned in abstract or paper, but not the focus; and\r• 4 = no alerts mentioned in abstract but still somewhat relevant.\rQuality\rwas defined in terms of where the paper was published using a 3 point scale as follows:\r• 1 = journal;\r• 2 = technical report, summary or conference paper; and\r• 3 = magazine article.\rFollowing the initial search, a more refined search was undertaken to make certain all relevant literature was obtained. This included re-examining all the articles that were rated ‘1’ in terms of relevance (21 in total) and identifying any relevant references and additional relevant keywords that were not included in our original keyword list. This resulted in the addition of the following keywords:\r• Notification system;\r• Alarm cleanup;\r• Nuisance alarm; and • Pre-alarm.\rThese keywords were then paired with all Operator Interface and Human Performance keywords and searched in all of the above mentioned databases.\rFinally, the research team conducted a search for any articles that cited the 21 most relevant articles. Based on this refined search, 4 new articles were identified, providing a total of 74 articles. These 4 additional articles were also classified and evaluated according to the method described above.\rAn EndNote database and a spreadsheet with a record of all 74 articles, classification (as described above), relevance and quality rating as well as any relevant notes have been provided to the Scientific Authority. The accompanying endnote database includes the title, year, author, journal and abstract for the 74 articles.\r4.1.4.1 Final evaluation criteria\rThe research team developed the final criteria to evaluate the articles, based on feedback from the TA. The main concern was that the preliminary criteria would not capture innovative research from other areas (other than the core alert/alarm literature) that might be applicable to the design of non-intrusive alerts. Although this innovative research may not specifically deal with alerts, it could be applied to the warning system. The Quality criteria were maintained for the final search but the relevance criteria were refined to the following 3 point scale:\r• 1 = concepts related to alert intrusiveness (including design/performance/models/theory) specifically mentioned in paper;\r• 2 = concepts related to alerts were the focus of paper; and\r• 3 = concepts related to alerts were not focus of paper.\rHumansystems® Non-Intrusive Alert System Page 15\r\nArticles were then re-evaluated based on the final criteria. As a result, four articles that originally had a lower ranking were found to have concepts in the paper related to alert intrusiveness, while 8 articles received lower relevance scores.\rThe research team also performed an additional keyword search to address another potentially relevant area, namely human centred adaptive-automation. Thus, the additional following keywords were searched:\r• Mission;\r• Context;\r• Operator; and\r• Adaptive.\rThey were paired with:\r• Advisory system;\r• False alarm; and\r• Intrusiveness.\rThese keywords were searched in Google Scholar resulting in two new articles related to alert intrusiveness.\rIn total, there were 24 usable articles obtained from our searches that are related to alert intrusiveness. These articles and 7 more from the TA made up the 31 articles used for the literature review.\r4.2 Results of the Literature Review\rIn this section we provide a description and summary evaluation of each of the key papers. The papers are organized first by functional elements of an alerting system (described in the previous section). Within each section, the literature was further broken down into the four main categories described previously (i.e., conceptual models, generic alert system design guidelines, specific design concepts for alerts, relevant empirical research on alert systems).\r4.2.1 Configure alert parameters\rThis function encompasses the creation of rules for determining which anomalous events will give rise to an alert, and assigning an alert priority for each rule created.\rOf the four categories of literature, we were only able to find relevant papers relating to generic design guidelines. Note that some additional information on the setting of trip points and the reduction in nuisance alerts can be found in Brown, O’Hara and Higgins (2000) which is reviewed in detail in section 4.2.2.2.\r4.2.1.1 Design Guidelines\rConfigure for individual differences\rAs computer systems requiring adaptive user interface technologies mature and proliferate into critical applications, Hudlicka and McNeese (2002) argue that it is critical that user interfaces accommodate individual user characteristics. Citing recent research, Hudlicka and McNeese argue that individual differences impact perceptual processes, cognitive processes, motor processes, low-\rPage 16 Non-Intrusive Alert System Humansystems®\r\nlevel processes (e.g., attention and memory) and higher-level processes (e.g., situation assessment, decision making, and judgment). In fact, they state that ignoring individual differences in human- machine systems can lead to non-optimal behaviour at best and critical errors with disastrous consequences at worst.\rWith respect to configuring alert parameters, one way to address the issue of individual differences is to have operators enter their alert preferences directly into an adaptive interface system. Hucklicka and McNeese (2002) developed just such a system called the Affect and Belief Adaptive Interface System (ABAIS). ABAIS allows users to enter display and modality preferences in a number of categories, which can be seen in Table 3.\rTable 3: Operator information preference profile (Hudlicka & McNeese, 2002)\rInformation Category\rPreferred means of enhancing visibility Preferred colour for alerts\rPreferred alert notification modality Preferred attention capture means\rOptions\rColour, Size, Blinking Red solid, Red outline Visual, Auditory\rMovement at visual periphery, Shift display to foveal region, Enhance icon visibility, Display arrow pointing to desired icon, Superimpose blinking icon\rAlthough no empirical studies have been conducted using ABAIS, it does highlight the usefulness of allowing operators to control the way in which alerts are configured. Alert settings could be based on operator abilities. For example, operators who have auditory difficulties could choose to have alerts provided visually, or operators who are red/green colour blind could choose alternate colour schemes to report the importance of an alert. Alert setting could also be based on operator preferences. For example, it could be that one operator finds a blinking cursor annoying and would much rather have an audio alert.\rAssessment\rWhile this paper recognises the need for the operator to configure appropriate alert parameters, it is not clear that allowing individual preferences for the design of an alert would be feasible in team or multi-operator environments. If individual members of the RJOC team were to have their own set of preferences, then team situation awareness for the current alerts status would be compromised. Further, supervisors would have greater difficulty in understanding the current alert picture. There is also a danger that individuals left to choose their own preferences would select options that may be inappropriate from a Human Factors (HF) perspective.\rConfigure for context\rProviding users with the ability to configure alert parameters can also be important in the battlefield. Commanders and staff monitor and analyze a large amount of digital information in the midst of battle planning, preparation, and execution in order to understand what is relevant and critical to their mission. In order to assist the decision-maker to understand and act in battlefield situations, Akin, Green and Arntz (2005) worked to develop the “System to Help Identify and Empower Leader Decisions” (SHIELD). SHIELD monitors digital data streams and alerts a decision-maker to two specific battle situations requiring immediate action. The first situation is Humansystems® Non-Intrusive Alert System Page 17\r\nwhen a friendly unit violates a boundary, thus risking being mistaken for enemy elements by friendly units (“Cross Boundary Violation Alert”). The second situation is when a unit’s plan for using artillery does not match enemy locations determined by an intelligence section (“Fratricide Prevention Alert”). In both situations, SHIELD initiates an alert that displays a text warning and a flashing icon on an “alert map”.\rAlthough helpful, Akin, Green and Arntz (2005) note that alerts can be intrusive. They, therefore, designed SHIELD to allow leaders to be able to control the intrusiveness of alerts. Leaders are able to dismiss an alert from the screen, have an alert be repeated at what may prove to be a more convenient time, or turn off an alert. In addition to these options, they plan on including an intrusiveness filter in future versions of SHIELD. This intrusiveness filter would allow a leader to define when alerts should and should not be displayed (e.g., within a certain limit of known threat locations; if less than 3 alerts are already displayed on the screen). The intrusiveness filter would also allow leaders to turn off audio alerts, which could be especially important if there is a possibility that noise could signal the leader’s presence to the enemy.\rAssessment\rThe contribution of the paper with respect to alert configuration is useful and provides a good example of an interface that allows users to describe contextual conditions for when an alert may be presented. It should be noted, however, that this does not mitigate the actual intrusiveness of that alert when it does occur.\r4.2.2 Receive information on alert state\rAn operator receives information on an alert’s state through an alert warning, which is defined as the actual mechanisms by which an operator’s attention is captured. Not surprisingly, literature relating to alert warnings was by far the most abundant of any of the alert system functions and included research relating to HF conceptual models, design guidelines, experiments, and design concepts.\r4.2.2.1 HF models\rMcCrickard, Chewar, Somervell and Ndiwalana (2003a) suggest that alerting systems should deliver current and critical information to the user in a manner where the user is not interrupted from their primary task. It is assumed that alerting systems are distracting to the user, however, the authors note that it is not known how much they distract the user and if this distraction is negative. McCrickard et al. (2003a) examine the benefits and drawbacks of alert system interface designs. Three critical parameters for modelling alert system user goals and system designs are interruption, reaction and comprehension.\rInterruption is “an event promoting transition and reallocation of attention focus from a task to the notification” (p. 319). Context plays an important part in interruption. For instance, if the user is driving, the alert (i.e., check engine light) should not be intrusive and interruptive to the main task at hand (i.e., driving safely). However, in the context of the RJOC, an alert informing an operator of a vessel behaving anomalously may require immediate attention and interruption of the main task at hand.\rReaction “is the rapid and accurate response to the stimuli provided by notification systems” (p. 319). Reaction can be measured by the time it takes to shift attention, which can be manipulated through designs incorporating colours, shapes and motion.\rPage 18 Non-Intrusive Alert System Humansystems®\r\nComprehension is “remembering and making sense of the information” (p. 319) which can be measured through recall and content related questions.\rUsing this model, combinations of high (1) and low (0) interruption, reaction and comprehension (IRC) were created to categorize the types of alerts, as shown in Figure 2.\rInterruption\rAlarm\rNoise\rIndicator\rDiversion\rInformation Exhibit\rCritical Activity Monitor\rAmbient Media\rSecondary Display\rReaction\rComprehension\rFigure 2: IRC framework (adapted from McCrickard et al., 2003a, p. 321)\rThe following table is a list of potential alerts and examples from alert systems according to the IRC framework.\rHumansystems® Non-Intrusive Alert System Page 19\r\nTable 4: IRC framework alerts and examples (McCrickard et al., 2003a)\rTypes of Alerts\rIRC Codes\rExamples\rCritical activity monitor\r1113\rA system administrator uses a network monitor on the desktop to enable prompt responses and fixes to computer problems\rAlarm\r110\rA businessman relies on a calendar and email alerts\rInformation exhibit\r101\rA factory supervisor requires critical updates, while performing daily tasks\rSecondary display\r011\rAn editor writes part of a document while monitoring a team progress groupware tool\rDiversion\r100\rA teenager on the home computer enjoys amusing pop-ups\rIndicator\r010\rA tourist uses a GPS (Global Positioning System) to navigate around a new city\rAmbient media\r001\rA telemarketer without a window has changing weather desktop wallpaper\rNoise\r000\rA student doing homework is reassured with internet radio\rThis table displays the alerts in terms of most interruptive, comprehensive, and requiring reaction, to the least interruptive, comprehensive, and requiring reaction. Using Wickens and Hollands (2000) human information processing stage model, McCrickard et al. (2003a) mapped each of the IRC categories onto the information processing model. This enabled the authors to illustrate that different alert types have different information processing routes. For instance, when people process noise (low interruption, reaction and comprehension), it does not reach their working memory as compared to a diversion (high interruption, low reaction and comprehension) which does reach working memory. The two most relevant alert types that produce a low interruption, but require some type of comprehension, are ambient media and a secondary display. Ambient media is processed through the senses and then enters long term working memory. However, in the case of a secondary display, a selection response must be executed in order to get feedback. This means that a secondary display requires more effort on the part of the operator, but not as much as a high interruption, reaction and comprehension alert (i.e., critical activity monitor) would require.\rMcCrickard et al. (2003a) performed a case study to test the IRC category framework. The case study involved two separate evaluations, using questionnaire measures, to determine effectiveness. The first study was conducted at Microsoft with the Scope alert system (van Dantzich, Robbins, Horvitz & Czerwinski, 2002; as cited in McCrickard et al., 2003a). The second study was conducted in McCrickard et al.’s lab using the IRC framework. Although the Scope was meant to be used differently than a standard interface, the questionnaire for the Microsoft study had items assessing standard interface issues. Therefore, the results of the Microsoft study were not useful in identifying design strategies. The questionnaire for the lab study, however, was based on the IRC framework. That is, the questions assessed tradeoffs between interruption, reaction and\r3 For example, a critical activity monitor would be high in interruption (I1), high in reaction (R1) and high in comprehension (C1), while an alarm would be high in interruption (I1), high in reaction (R1) and low in comprehension (C0).\rPage 20 Non-Intrusive Alert System Humansystems®\r\ncomprehension. The questionnaire based on the IRC framework produced better redesign strategies.\rThe Scope is an alerting system which is located in the bottom right corner of a computer screen and presents symbols according to urgency (urgent items are located closer to the middle than non- urgent items). For an illustration of the Scope alerting system, see McCrickard et al. (2003a)\rThe Scope is divided into four sections: email inbox, calendar, task list and general alerts. The goals of the Scope system are to direct user attention to urgencies and minimize user attention for incoming non-urgent alerts. According to the IRC framework, the Scope would be similar to an alarm (IRC 110) and ambient display (IRC 001). The results of the studies and ensuing guidelines are summarized in McCrickard et al. (2003a).\rThe full explanation of the Scope can be found in van Dantzich et al. (2002; as cited in McCrickard et al. 2003a). McCrickard et al. (2003a) conclude that the IRC framework provides a method to evaluate alert systems. The Scope is one of these alert systems that, based on the evaluation, yielded potential redesign strategies such as reducing visual clutter and defining alerts based on shape and colour.\rAssessment\rIn summary, the McCrickard et al. (2003a) model is based on interruption of the alert, reaction to the alert and comprehension of the alert. These parameters create 8 different types of alerts which vary on the IRC levels. For an alert to be non-intrusive it would need to be non-interruptive. A secondary display and ambient media are two types of alerts that may offer a less intrusive way to inform the operator of interesting events. Ambient media allows for comprehension of the material presented, without reaction. Ambient media would be useful in the context of non-urgent alerts or alerts that do not require an action. A secondary display for alerts may not be a viable design solution for RJOC as presently configured.\rIn terms of applicability to maritime anomaly alerts, the Scope approach may be too extensive for the existing RJOC design space, may require too much operator cognitive processing and may provide more information than is required.\rOverall, this paper provides a good conceptual analysis of the human information processing issues that need to be considered with respect to the intrusiveness and comprehensibility of warning indicators.\r4.2.2.2 Design guidelines\rNuisance alerts\rIn the 1980s, reports of air and train crashes came to light indicating that system operators turned off critical alert or warning indicators prior to the accidents. In his seminal paper, Sorkin (1988) outlined two reasons why he believes operators would disable warning signals. The first reason is that alert signals can be highly-aversive and can interfere with important operator duties (e.g., high- level shrill sound produced by warning whistles, multiple alerts that make it difficult to identify the underlying condition for the alerts). The second reason is that operators perceive false alarm rates to be excessively high. A common sense analysis indicates that operators with high workloads will adopt strategies to ignore or disarm excessive alerts, especially if they are perceived to have high false alarm rates. To deal with these issues, Sorkin recommended a number of changes to alert systems, which include:\rHumansystems® Non-Intrusive Alert System Page 21\r\n• Design alert signals in such a way that they are not overly aversive or disruptive. Possible alert techniques suggested include speech message alerts and special alert codes.\r• System designer should consider the effect of the alert rate on the performance of the entire alert system, especially when the operator has a heavy workload.\rAssessment\rSorkin’s (1988) reasons for operators turning off the alerts are still applicable twenty years later. Constant false alarms provide a false sense of security to an operator whereby it is assumed the alerts are not important enough; thus, they do not receive the operator’s attention. Also, designing a less intrusive alerting method can prevent distraction from the primary task. There was considerable research in the area of design guidelines relating to alert warnings; it will be discussed in the following sections.\rGeneral alert principles\rShorrock, Scaife and Cousins (2002) developed high-level principles for the design of soft-desk alert systems that could contribute to a philosophy of alert handling. The principles were derived from information gained from two studies regarding the design and evaluation of Air Traffic Management (ATM) commercial-off-the-shelf (COTS) systems software and hardware. These studies resulted in approximately 100 recommendations for alert handling system design. The recommendations were generalized into a smaller set of design principles, which were structured according to Stanton’s (1994; as cited in Shorrock, Scaife & Cousins, 2002) model of alert-initiated activities.\rStanton’s model consists of six activities:\r1. Observation: the detection of an abnormal condition within the system. At this stage, care should be taken to ensure that colour and flash/blink coding support alert monitoring and searching. Excessive use of colour and blinking can de-sensitize an operator and reduce the ability of the alert to gain the operator’s attention.\r2. Acceptance: acknowledging receipt and awareness of an alert. Alert systems should reduce operator workload to a manageable level – excessive demands for acknowledgement increase workload and operator error.\r3. Analysis: prioritization of an alert based on task and system in which it occurs.\r4. Investigation: activities that aim to discover the underlying cause of an alert in order to deal with the fault.\r5. Correction: the application of the results from the previous stages to address the problem identified by an alert.\r6. Monitoring: assessment of the outcome from the Correction stage.\rShorrock et al. (2002) added co-ordination as a seventh activity to the model. Co-ordination is the transfer of information between operators and the application of collaborative efforts to observe, accept, analyze, investigate or correct faults. Their final list of principles for human-centred alert systems classified according to the revised Stanton model can be seen in Table 5.\rPage 22 Non-Intrusive Alert System Humansystems®\r\nTable 5: Model-based principles for human-centred alarm systems (Shorrock et al., 2002)\rObserve\rAlarms should be presented (time stamped) in chronological order, and recorded in a log in the same order.\rAlarms should signal the need for action.\rAlarms should be relevant and worthy of attention in all the plant states and operating conditions.\rAlarms should be detected rapidly in all operating conditions.\rIt should be possible to distinguish alarms immediately, i.e. different alarms, different operators, alarm priority.\rThe rate at which alarm lists are populated must not exceed the users’ information processing capabilities.\rAuditory alarms should contain enough information for observation and initial analysis and no more.\rAlarms should not annoy, startle or distract unnecessarily.\rAn indication of the alarm should remain until the operator is aware of the condition.\rThe user should have control over automatically updated information so that information important to them at any specific time does not disappear from view.\rIt should be possible to switch off an auditory alarm independent of acceptance, but it should repeat after a reasonable period if the fault is not fixed.\rFailure of an element of the alarm system should be made obvious to the operator.\rAccept\rReduce the number of alarms that require acceptance as far as is practicable.\rAllow multiple selection of alarm entries in alarm lists.\rIt should be possible to view the first unaccepted alarm with a minimum of action.\rIn multi-user systems, only one user should be able to accept and/or clear alarms displayed at multiple workstations.\rIt should only be possible to accept the alarm from where the sufficient alarm information is available.\rIt should be possible to accept alarms with a minimum of action (e.g. double click), from the alarm list or mimic.\rAlarm acceptance should be reflected by a change on the visual display, such as a visual marker and the cancellation of attention-getting mechanisms, which prevails until the system state changes.\rAnalyse\rAlarm presentation, including conspicuity, should reflect alarm priority, with respect to the severity of consequences associated with the delay in recognising the deviant condition.\rWhen the number of alarms is large, provide a means to filter the alarm list display by sub-system or by priority.\rOperators should be able to suppress or shelve certain alarms according to system mode and state, and see which alarms have been suppressed or shelves, with facilities to document the reason for suppression or shelving.\rIt should not be possible for operators to change priorities of any alarms.\rAutomatic signal over-riding should always ensure that the highest priority signal over-rides.\rThe coding strategy should be the same or all display elements.\rFacilities should be provided to allow operators to recall the position of a particular alarm (e.g. periodic divider lines).\rAlarm information such as terms, abbreviations and message structure should be familiar to operators and consistent when applied to alarm lists, mimics and message/event logs.\rThe number of coding techniques should be at the required minimum, but dual (redundant) coding may be necessary to indicate alarm status and improve accurate analysis (e.g. symbols and colours).\rAlarm information should be positioned so as to be easily read from the normal operating position.\rInvestigate\rAlarm point information (e.g., settings, equipment reference) should be available with a minimum of action.\rInformation on the likely cause of an alarm should be available.\rA detailed graphical display pertaining to a displayed alarm should be available with a single action.\rWhen multiple display elements are used, no individual element should be completely obscured by another.\rVisual mimics should be spatially and logically arranged to reflect functional or naturally occurring relationships.\rNavigation between screens should be quick and easy, requiring a minimum of user action.\rCorrect\rHumansystems® Non-Intrusive Alert System Page 23\r\nEvery alarm should have a defined response and provide guidance or indication of what response is required.\rIf two alarms for the same system have the same response, then consideration should be given to grouping them.\rIt should be possible to view status information during fault correction.\rUse cautions for operations that might have detrimental effects.\rAlarm clearance should be indicated on the visual display, both for accepted and unaccepted alarms.\rLocal controls should be positioned within reach of the normal operating position.\rMonitor\rNo primary principles. However, a number of principles primarily associated with observation become relevant to monitoring.\rCoordinate\rProvide high-level overview displays to show location of operators in system, areas of responsibility, etc.\r4.2.2.2.1.1 Assessment\rThese studies provide a solid and useful analysis of the major principles that should be considered in the design of alert systems, many of which are applicable to considerations for non-intrusive systems. Although not the focus of these papers, no specific implementation guidelines are provided for how these principles should be addressed in terms of concrete design concepts or considerations.\rAuditory alerts\rAuditory warnings are used as a means of commanding attention without startling or annoying operators. They are used in domains such as aviation, medicine, and control rooms. Such warnings are meant to convey information about a task or situation, and often must compete with other sensory stimuli in the environment. However, there are problems associated with exposure to loud noise over prolonged periods, including hearing loss, cardiovascular problems (Babisch, 1998; as cited in Edworthy & Hellier, 2002), cognitive problems and performance problems (World Health Organisation, 1993; Smith, 1993; Edworthy, 1997; as cited in Edworthy & Hellier, 2002). In their review of research on auditory warnings in noisy environments, Edworthy and Hellier (2002) make the following suggestions regarding how to deal with issues related to auditory alerts (see Table 6).\rAhlstrom (2003) also conducted work to develop guidelines for auditory alerts within the National Airspace System (NAS). New tools for the NAS are accompanied by new equipment using visual and auditory signals to indicate the status of equipment and of incoming air traffic. The auditory signals are often developed with minimal use of standards or guidelines, or with minimal consideration of the auditory signals on existing equipment. This can result in too many warnings, warnings that are too loud or inaudible, warnings that are confusing, and inappropriate mapping between the warning and its meaning (Meredith & Edworthy, 1994; as cited in Ahlstrom, 2003). Such situations can result in operators disarming or inhibiting alerts, as described by Sorkin (1988).\rAhlstrom (2003) conducted an exploratory study to provide increased understanding and insight into audio alert problems within the NAS. After conducting a literature review, Alhstrom identified 15 issues associated with auditory alerts in a variety of fields (e.g., hospital emergency rooms, aircraft cockpits). Twenty current and former terminal Air Traffic Control Specialists (ATCS) were then asked to rate each of the 15 issues on an 11 point scale ranging from 0 (not a problem) to 10 (severe problem) on how problematic the issue was from them in their work environment.\rPage 24 Non-Intrusive Alert System Humansystems®\r\nTable 6: Suggestions for auditory alerts (Edworthy & Hellier, 2002)\rIssues\rSuggestions\rNoisy environments (e.g., factories, cockpits, flight decks)\rAs ambient noise determines the detectability of auditory warnings, a worst-case spectrum should be used to avoid excessively loud warnings\rEnsure warnings are not too loud as they may be switched off by operators\rAvoid having a number of warnings going off at the same time as they will mask each other\rAcoustic\rLocalizability will be improved by having several audible components in the warning sound, preferably with a fairly low fundamental frequency\rContinuous tones should not be used as warning sounds\rSpeech vs. non- speech\rIt is more difficult to produce intelligible speech warnings for a complex noise environment than it is to fit a non-speech warning to the same noise spectrum\rSpeech warnings can create problems in certain environments. For example, a verbal warning in a hospital ward may cause worry or panic by patients and relatives, whereas a nonverbal warning would not\rWarning sounds can be designed to mimic speech in some way. For example, in an emergency room environment the warning sound for “cardiovascular” can contain 6 pulses, the same as the word, with a pitch pattern consisting of 3 pulses at one pitch followed by 3 at a lower level. Such a warning is easily learned and retained\rSpeech warnings may interfere with ongoing information processing which involves language (Wickens & Hollands Information Processing Model, 2000)\rWarning design protocols\rWarnings must be designed so as to attract attention without causing stress and annoyance\rFalse alarms\rDesign alert systems with high accuracy rates. False alarms provide information of no use whatsoever, serve to increase the noise levels within an environment, and over time will lower performance on a task\rNumber of auditory warnings\rIndividually different alerts for top-priority situations only\rUse specific sounds to identify the category of risk\rFocus on functions rather than equipment. For example, assign specific alerts to specific medical functions rather than to pieces of equipment (as equipment changes frequently in health care)\rParticipants were also provided the following comments on specific problems with auditory alerts in their work environment:\r• Alerts can be annoying at times (e.g., alert continues to sound even after it has been located);\r• Alerts for different problems sound similar and are easily confused;\r• Too many false alarms. After a while, alerts start to lose their meaning;\r• Alerts can interfere with voice communications;\r• Some systems should have auditory alerts but do not;\r• Alerts that are too loud cause stress, frustration and hearing loss.\rHumansystems® Non-Intrusive Alert System Page 25\r\nIn response to these findings, Ahlstrom (2003) provided recommendations to be considered for future alerts used by terminal area operators:\r• Alerting and warning systems should be unambiguous, with a clear indication of the cause for the alert. This can be accomplished by using varying frequency and/or modulation, and periodicity should differ (i.e., avoid continuous signals);\r• The criteria for conditions causing frequent false alarms should be evaluated and effort should be made to reduce the number of irrelevant alerts;\r• Systems should have a simple, consistent means for turning off non-critical auditory alerts without erasing any displayed message accompanying the auditory signal. The system could also include a sensing mechanism that automatically shuts off the auditory portion of an alert when it no longer provides useful information;\r• Strategies should be used to maximize the ability to locate auditory alerts. These include avoiding mid frequencies, positioning alerts to the side rather than in the front or in the back of the operator, minimizing hard surfaces in order to decrease echoes, and providing a centralized alert panel or window indicating the alert status for most alerts;\r• When operators are required to identify an alert based on the auditory portion alone, the number of signals to be identified should not exceed four. Up to nine alerts can be used if they are presented regularly, and up to 12 alerts can be used if relative discrimination (rather than absolute identification) is used.\r4.2.2.2.1.2 Assessment\rThese papers outline many of the critical design and implementation issues with respect to auditory alerts. The first three of Ahlstrom’s recommendations for auditory alerts would also be generically applicable to the implementation of other forms of alerting.\rNuclear control room\rWithin the nuclear industry, the goal of computer-based alert systems is to assist operators by processing alert data and improving the presentation of alert information. Brown, O’Hara and Higgins (2000) conducted an investigation to update and revise the Nuclear Regulatory Commission’s (NRC) guidance for reviewing alert system designs. The resulting revisions were based on NRC research on the effects of alert system design characteristics on operator performance and on research examining the introduction of computer-based human-system interface systems into Nuclear Power Plants. Some of the revised guidelines that would be applicable to naval anomaly detection include setpoint determination, assured functionality, alert reduction, alert signal validation, parameter stability, and alert-status separation.\rSetpoint Determination and Nuisance Alert Avoidance\rThe determination of alert setpoints should consider the trade-off between the timely alerting of an operator to off-normal conditions and the creation of nuisance alerts caused by establishing setpoints so close to the “normal” operating values that occasional excursions of no real consequence are to be expected.\rPage 26 Non-Intrusive Alert System Humansystems®\r\nAssured Functionality under High Alert Condition\rThe alert processing system should ensure that alerts which require immediate operator action or indicate a threat to safety functions are presented in a manner that supports rapid detection and understanding by the operator under all alert loading conditions.\rAlert Reduction\rThe number of alert messages presented to the crew during off-normal conditions should be reduced by alert processing techniques (from a no-processing baseline) to support the crew’s ability to detect, understand, and act upon alerts that are important to the plant condition within the necessary time.\rAlert Signal Validation\rSensor and other input signals should be validated to ensure that spurious alerts are not presented to plant personnel, due to sensor or processing system failure.\rParameter Stability Processing\rThe alert system should incorporate the capability to apply time filtering, time delay, or deadbanding4 to the alert inputs to allow filtering of noise signals and to eliminate unneeded momentary alerts.\rAlert-Status Separation\rStatus indications, messages that indicate the status of plant systems but are not intended to alert the operator to the need to take action, generally should not be presented via the alert system display because they increase the demands on the operators for reading and evaluating alert system messages.\r4.2.2.2.1.3 Assessment\rAlthough the nuclear industry is different from the operational environment of the RJOC, many of the same issues may be applicable, for example, Brown et al.’s (2000) recommendations relating to reducing the number of alerts, rapid detection of alerts, and presenting only meaningful alerts. These recommendations would, in theory, reduce the workload of the operator and make the overall alerting system less intrusive.\rGraphic user interface design for alerts\rA man-machine interface (MMI) is the way in which an operator controls a system and traditionally contains manual buttons, controls, switches, and a monitor through a closed-circuit television. When switching to a graphic user interface (GUI) from a MMI, some mistakes can occur that can adversely impact human performance. A common mistake is the MMI display is shrunk onto the GUI. Although it maintains familiarity for the user, it can result in usability issues, which can lead to efficiency and safety problems.\rHan, Yang and Im (2007) created a six-phase approach to develop a method for GUI design. The six phases include:\r1. UI design guidelines collection for process control rooms5\r4 Deadband is a specified area where the alert would not go off.\rHumansystems® Non-Intrusive Alert System Page 27\r\n2. Usability inspection5\r3. Design rules and guidelines development\r4. User interface design and prototyping\r5. Usability testing and evaluation\r6. Final prototypes and design specs.\rFor the first step, Han et al. (2007) conducted an extensive review of design guidelines for any GUIs used in a process control room. Consequently, they organized approximately 1500 guidelines into 14 chapters and 70 sections. These guideline principles covered topics such as aesthetics, attention, cognitive issues, consistency, display issues, feedback, forgiveness, memory issues, metaphors, simplicity, system messages and help, user control, and user differences.\rHan et al. (2007) then analyzed current user interfaces in a process control room and operator tasks to identify usability problems and define design requirements for a new interface. This new process control room interface prototype takes into account all the current problems experienced by operators. Operator manuals were reviewed and operators were interviewed. Twenty-two operators participated in one-on-one interviews answering questions assessing tasks, alerts systems and requirements. Overall, the usability inspection resulted in the identification of 500 usability problems by 4 practitioners. The most frequently found problems related to attention (e.g., warning signals not salient), consistency (e.g., different terminologies), cognitive issues (e.g., pictures on screens different from actual layout of equipment), memory issues (e.g., same colours have different meanings) and simplicity (e.g., too many colours used). Selected problems are shown in Table 7.\rTable 7: Identified problems and design requirements (Han et al., 2007)\rAlert System Problems\rUsability Problems\rOperator’s Requirements\rWarnings are not categorized by urgency\rScreens are complex\rUnnecessary or redundant interface elements should be removed, and the layout should be reorganized\rIrregular situations may not be informed to operators\rEach screen has different layouts and different methods of information presentation or visualization\rConsistent screen layouts and visualization should be designed\rWarning signals are not attracting the operator’s attention\rToo many colors are used on a screen\rThe color-coding scheme should be developed, and the meaning of colour should be easily understandable\rVisual alerts are not presented with auditory alerts\rOnly one window can be activated at a time\rThe multi-window function should be available\rOnly the mouse can be used to move between input fields\rA ‘Tab’ key should also be available as a method of moving between input fields\rSystem status changes cannot be detected before opening\rand looking at related screens directly\rAll the status changes should be automatically informed on the screen that the operators mainly use\r5 This step can be skipped if a requirements analysis already exists\rPage 28 Non-Intrusive Alert System Humansystems®\r\nOnce the usability problems and operator requirements were identified, Han et al. (2007) created design rules and specific design guidelines for the new interface. Design rules are defined as major premises that designers should always keep in mind when designing. Guidelines refer to the specific methods designers should follow when designing individual interface elements. Design rules were categorized into improvement of task efficiency or reduction of task errors. Each rule was accompanied with several specific design guidelines. Design guidelines generated by Han et al. (2007) addressed critical problems of alert design and colour-coding, as shown in Table 8.\rTable 8: Examples of guidelines (Han et al., 2007)\rDesign Categories\rGuidelines\rAlert Design\rAttention\rWarning should be easily distinguished from the background. For example, visual signals should be bright on a dark screen, and vice versa\rHazard information\rA warning signal should contain hazard information. However, not too much hazard information should be included in just one signal\rConsequences information\rConsequences information should follow the hazard information. However, if operators can infer the consequences from the hazard information, they do not need to be included in the warning signal\rInstructions\rInformation on instructions should not include a too difficult or impossible method to perform. That is, instruction methods as easy as possible should be included\rComprehension\rIf operators’ capability, knowledge, and experience levels are various, warning signals should be easy so that operators who have the lowest capability or experience level can understand them\rMotivation\rWarning signals should induce operators to read or listen to them and to react to them\rBrevity\rAlert information should not exceed two phrases or sentences\rDurability\rWarnings that inform operators of the instantaneous change of process or signals that do not need special reactions should not last for a long time\rColour Design\rGeneral considerations\rColours are used for supporting search tasks, highlighting, or indicating status of the system\rForeground colour\rDo not use blue, magenta, or a shade of pink in displaying information that the user must read\rColour contrast\rExaggerate lightness differences between foreground and background colours, and avoid using colours of similar lightness adjacent to one another, even if they differ in saturation or hue\rColours of interface elements\rDo not design a colour icon that is substantially different from the black-and-white icon. When a colour is added to an icon, it is best to leave the one-pixel black outline and other black lines that form the icon, and fill the icon in with colour\rAs can be seen in Table 8, Han et al. (2007) provide a number of recommendations for alert system design. For example, warnings should be contrasted with background information (e.g., bright warning on a dark screen, vice versa), and should be colour coded (red, yellow, green, white), but\rHumansystems® Non-Intrusive Alert System Page 29\r\nnot in shades of blue or pink. Important for reducing the intrusiveness of alerts are the recommendations that information in the warnings should be easily interpretable and should not exceed 2 sentences, and warnings that do not require an operator’s reaction should not last a long time.\r4.2.2.2.1.4 Assessment\rAlthough the recommendations and guidelines outlined by Han et al. (2007) are not specific to creating less intrusive alerts, the following recommendations can be generalized for the design of less intrusive alerting systems:\r• Decrease the disruptiveness and intrusiveness of alerts.\r• Consider the effect of the alert rate on the operator. Alert processing techniques should be used to reduce the number of alert messages as this will improve ability to detect, understand and act upon important alerts.\r• Be cautious of alert set points. They should be set at a level to avoid nuisance and false alarms. False alarms do not provide useful information and over time will lower performance on a task.\r• Do not present status indications through an alert system display. This increases the demands on an operator for reading and evaluating the message.\r• Auditory alerts should use specific sounds to identify the category of risk. This will aid an operator’s ability to identify alerts.\r• Operators should be able to turn off non-critical alerts without erasing information.\r• The auditory portion of an alert should shut-off automatically when it no longer provides\ruseful information.\r• Operators should be able to distinguish alerts immediately (e.g., different alerts, alert priority).\r• The rate at which alert lists are populated must not exceed the users’ information processing capabilities.\r• Alert acceptance should be reflected by a change on the visual display, such as a visual marker and the cancellation of attention-getting mechanisms, which prevails until the system state changes.\r• Alert presentation, including conspicuity, should reflect alert priority, with respect to the severity of consequences associated with delay in recognizing the deviant condition.\r• Operators should be able to suppress or shelve certain alerts according to system mode and state, and see which alerts have been suppressed or shelved, with facilities to document the reason for suppression or shelving.\r• A detailed graphical display pertaining to a displayed alert should be available with a single action.\rIt should be noted that many of the recommendations are in the form of general principles (e.g., matching alert conspicuity with alert priority) and there is a lack of specific recommendations on how the principle would be implemented in an interface.\rPage 30 Non-Intrusive Alert System Humansystems®\r\n4.2.2.3 Empirical research\rThis section, which reviews empirical research related to alert warnings, is divided into visual, visual and auditory, and predictive alerting system, which relates to the actual alerting system itself and how it can predict the operator’s actions.\rVisual Alerts\rThere are many variations of visual alerts such as text, picture, symbols and icons which may vary in size, colour and position. While developing a pre-alert system that would reduce the frequency of alerts, Hwang, Lin, Liang, Yenn and Hsu (2008) examined whether these pre-alerts should be text or graphic. The idea behind testing both formats was to determine if an operator is more inclined to disregard one format compared to another (e.g., too annoying, not comprehensive, not noticeable), which would in turn lead to more alerts going off. The results of this experiment (see Section 4.2.4.2 Warning of impending alert) showed no significant differences in the text and graphic pre-alert types for reducing the number of alerts. However, with the graphic types, the operators had significantly more correct answers when asked questions about the alerted task. Although both forms of pre-alert systems would be a benefit to the operator, the graphic display includes more information, but requires more space and greater changes to be implemented in the control room. Therefore, Hwang et al. (2008) recommended the text type pre-alert to be implemented in control rooms.\rMcCrickard, Catrambone, Chewar and Stasko (2003b) considered variations of animated text for computer alert systems. Specifically, McCrickard et al. (2003b) examined the visual aspect of an alert, and its effect on interruption, reaction, and comprehension6. There were three forms of animated texts: a smooth ticker (information shifted horizontally), a fading display (information fades), and a rapid serial visual presentation (RSVP)-style “blast” (displays information without smooth animation). Information came in the form of changing news, weather, stocks and sports information. The primary task for participants was searching the World Wide Web for information to answer questions that were asked of them. The alerted task was to monitor the news/weather/stocks/sports information (presented by animated text) and answer questions relating to the message displayed. For example, while participants were trying to answer the question, “In what year was Mount Rushmore carved,” they also had to monitor the weather alerts and press a button when the weather temperature dropped below 30 degrees. At the end of the session, participants were asked to complete awareness questions which assessed the amount of information they recalled from the alerted task (i.e., monitoring news/weather/stocks/sports information). Results did not report a specific text type which yielded the fastest response times, rather strengths and weaknesses were found for each method of animation (see Table 9).\r6 Interruption, reaction and comprehension were defined previously in Section 4.2.1.\rHumansystems® Non-Intrusive Alert System Page 31\r\nTable 9: Results for operator tasks: Study 1 (McCrickard et al., 2003b)\rTasks\rMeasurement\rBest\rWorst\rBrowsing speed\rThe time the information appeared on the screen until the participant typed in the correct answer and pressed OK\rControl then ticker\rFade & blast\rBrowsing comprehension\rThe number of incorrect answers\rControl then blast\rTicker & fade\rLink selections\rThe number of times a participant pressed the Back button\rTicker then fade\rControl & blast\rReaction time to alert\rThe time the alert appeared on the screen until the participant acknowledged it by pressing a button\rBlast (34 seconds)\rTicker (54 seconds)\rBasic awareness hit rate\rRecognition of information in the alert\rTicker\rFade\rDetailed awareness rate\rThe recognition of correct and incorrect answers\rTicker\rFade & blast\rBasic awareness false alarm rate\rInformation participants reported seeing that was not actually presented\rFade\rTicker\rDetailed awareness false alarm rates\rConfidence in understanding information that was not actually understood\rTicker\rFade & blast\rParticipant’s preference\rMost user friendly and least intrusive\rTicker\rBlast\rMcCrickard et al. (2003b) recommended the ticker as the best choice for maintaining awareness, minimizing interruption, facilitating reaction and facilitating comprehension.\rA second experiment by McCrickard et al. (2003b) examined the impact that alert display size and animation speed would have on performance. This experiment only used the ticker text and fade text, as the blast type was rated as the least favourite by participants in the first experiment. Results are shown in Table 10.\rPage 32 Non-Intrusive Alert System Humansystems®\r\nTable 10: Results for operator tasks: Study 2 (McCrickard et al., 2003b)\rTasks\rMeasurement\rBest\rWorst\rBrowsing speed\rThe time the information appeared on the screen until the participant typed in the correct answer and pressed OK\rSlow ticker or any slow\rSmall ticker or any small\rBrowsing comprehension\rThe number of incorrect answers\rSmall fade\rSmall ticker\rLink selections\rThe number of times a participant pressed the Back button\r-\rSmall ticker\rReaction time to alert\rThe time the alert appeared on the screen until the participant acknowledged it by pressing a button\rFade\rNormal or slow ticker\rBasic awareness hit rate\rRecognition of information categories in the alert\rSmall ticker\rSlow ticker\rDetailed awareness rate\rThe recognition of correct and incorrect answers\rSmall fade\rSmall ticker & fade\rBasic awareness false alarm rate\rInformation participant’s reported seeing that was not actually presented\rSlow ticker\rSmall ticker & fade\rDetailed awareness false alarm rates\rConfidence in understanding information that was not actually understood\rSlow fade\rSmall fade\rParticipant’s preference\rMost user friendly and least intrusive\r-\r-\rBased on these results, McCrickard (2003b) recommended that the slow fade may be the best overall alert type.\rIn summary, all three text types did not significantly interrupt the user from the primary task, but still alerted them to important information. Fade, blasts, and small displays were better for rapid identification of the information compared to tickers or other size displays, but worse for comprehension and recall. Overall, alerts that travel across the screen horizontally were found to be the least intrusive and yield the best performance results. However, when the alert text was varied in size and speed, the text that appeared slowly and faded slowly was the best type to use. The finding with respect to ticker style alerts provides guidance to the development of a similar design concept for the present project.\rThe operational definition of alert intrusiveness in terms of the relationship between a primary task and the alerting stimulus is useful and will assist in the development of non-intrusive alerts concepts.\rVisual and Auditory\rThe literature is mixed regarding performance for salient (e.g., auditory) versus less salient (e.g., visual) cues. Banbury, Macken, Tremblay and Jones (2001; as cited in Colcombe & Wickens, 2006) found that discrete auditory stimuli, such as those used in alerts, tend to corrupt memory processes more than visual cues. On the other hand, Helmick-Rich, Burke, Gilad and Hancock (2004; as cited in Colcombe & Wickens, 2006) found that people were more likely to comply with an auditory cue compared to a visual cue. It therefore appears that the degree of disruption due to alerted tasks is a complicated relationship between the characteristics of the ongoing task, the alerted task, and the operator’s strategies and skills. Auditory alerts appear to be more interrupting\rHumansystems® Non-Intrusive Alert System Page 33\r\nthan visual alerts, but do not always impose a cost to ongoing tasks. Colcombe and Wickens (2006) were interested in the way in which parameters of a primary task made them more or less interruptible from an alert, and in turn, how the characteristics of the alert mediated the costs of interruptions to primary tasks.\rFor study 1, participants were warned, visually or auditorily, for potential collision threats from a Cockpit Displays of Traffic Information (CDTI) display. Participants were in one of the following conditions: stable tracking, unstable tracking condition, binary alert (normal state or high-level alert), likelihood alert (normal state, mid-level or high-level alert). Results showed participants were faster to detect auditory alerts than visual alerts. Tracking performance was better in the binary alerting condition than the likelihood alerting condition. Subsequent studies replicated the binary and likelihood finding, but found response times faster for visual than auditory alerts.\rColcombe and Wickens (2006) stated that the binary alert was generally more effective than the likelihood alert. However, this was based on participants responding to the binary alert faster than the mid-level likelihood alert, without taking into consideration comprehension or interruption. No urgency distinction was made with binary alerts and, thus, operators must treat each alert as if it is high-level. Auditory alerts generally supported better conflict detection response than did a visual alert. However, the impact of modality on the concurrent task was modulated by the nature of the task (visual tracking was more disrupted by the auditory alert than by a visual alert).\rSimilarly, Krausman, Elliot and Pettitt (2005) examined visual, auditory and tactile alerts on platoon leader decision making. To a limited extent, the military has implemented a multi-sensory information presentation approach in that system designers are using auditory and visual displays. However, there are situations in which a soldier’s visual and auditory channels are heavily loaded. Therefore, tactile presentations may also be beneficial. Krausman et al. (2005) had twelve infantry officers participate in 3 different scenarios. In each scenario participants played the role of platoon leaders (PL) mounted inside a vehicle. During the scenario, participants sat in front of a primary display, map display, and Unmanned Aerial Vehicle (UAV) display to perform tactical communications and monitor activity on the display. Approximately 9 of the communications sent to the participant in each scenario were preceded by a visual, audio, or tactile alert. Table 11 describes the presentation of each alert.\rTable 11: Recommendations for design for a platoon leader alert (Krausman et al., 2005)\rParticipants received only one type of alert in each scenario (e.g., visual alerts in scenario 1, audio alerts in scenario 2, and tactile alerts in scenario 3). Alerts were continuous and stopped when the participant clicked the “show message” button to receive the information. After each scenario, participants rated and ranked the effectiveness, helpfulness, and necessity of the alerts.\rOverall, visual alerts were the least effective method of alerting participants. Response times were significantly longer for visual alerts than for auditory or tactile alerts; no significant differences\rPage 34 Non-Intrusive Alert System Humansystems®\rAlert purpose\rAlert presentation\rTo alert platoon leader to an incoming message\rVisual alert – solid red box appears on bottom portion of communications console of primary display\rAuditory alert – “beep” from headset Tactile alert – “buzz” from tactical armband\r\nwere found between auditory and tactile alert response times. Participants also rated visual alerts as the less effective method of getting their attention. When ranking alerts, participants ranked auditory alerts to be the most effective method of getting their attention followed by tactile alerts and visual alerts, respectively. Tactile alerts were ranked as the most helpful type of alert, followed by auditory alerts and visual alerts, respectively. However, participants noted that caution should be exercised when implementing auditory and tactile alerts in combat vehicles because alerts might be hard to detect in combat environments (e.g., multiple radio nets might mask the sound of an auditory alert, tactile alerts might be missed in a moving vehicle due to vehicle vibrations). Participants suggested that a combination of alerts might be the most effective option.\rSteefkerk, Esch-Bussemakers and Neerincx (2007) designed a context-aware alert system. This system varied visual and auditory methods, and reported that participants preferred an auditory signal for low urgency messages. More details about this study can be found in Section 4.2.3.2.\rIn summary, the experiments investigating visual alerts recommend alerts that have a slow fade text. Visual alerts that are not recommended include the blast alert and graphical pre-alerts. In general, it was found that auditory alerts support better detection than visual alerts. Experiments investigating auditory alerts recommend an auditory alert for all types of urgencies, and that these alerts vary the presentation based on urgency.\rPredictive Alerting System\rWhile the previous sections addressed visual and auditory alerts, this section presents empirical research related to a predictive alerting system, which can aid the operator in certain tasks. Mitchell (1998) considered the issues surrounding intelligent aids and associates in an operational setting. Intelligent aids and associates, such as displays of up-to-date information about operations, are a type of computer technology that is designed to help operators. Mitchell states that an operator’s immediate response after hearing an alert may be to shut it off, followed by identifying what triggered the alert. Often times software updates follow well behind changes in operational requirements such that by the time software updates are finally introduced, certain alerts may have become extinct or irrelevant. A common issue is that alerts are assumed to be extinct or irrelevant, thus are ignored by operators. To address these issues, Mitchell (1998) designed a system to prevent alert overload, in turn, increasing alert usefulness.\rThe Operator Function Model Expert System (OFMspert) performs many functions, but the most relevant to the present project is the activity tracking device, whereby the system tracks the operator’s activities. The computer would track the activity by asking “What is the operator doing? Why is the operator doing that? What will the operator do next?” (p. 31). The latter question was posed as a way for the system to predict or infer the intent of the operator. Actions that can be interpreted include cognitive actions (i.e., situation assessment) and perceptual actions (i.e., scanning for alerts). Although not discussed in the paper, the system could presumably predict the operator’s future actions, thus presenting an alert at an appropriate time. An alert that is presented to the operator at an appropriate time, would not only be less intrusive, but also more useful. Another useful capability of the system is that it can explain recommendations to an operator, which could reduce the operator’s cognitive workload.\rThe National Aeronautics and Space Administration (NASA) Goddard Space Flight Center (GSFC) was chosen to illustrate the application of the OFMspert. The Multisatellite Operations Control Center (MSOCC) is a system in GSFC that monitors the use and effectiveness of computer systems shared by satellites. Several steps were taken for the design methodology, but the implementation of intelligent aids and associates was of interest for this review. Specifically,\rHumansystems® Non-Intrusive Alert System Page 35\r\ninferring the intent of the operator to reasonably predict their activities and interpret their actions was examined. The OFMspert implementation in MSOCC was studied by Jones, Mitchell and Rubin (1990; as cited in Mitchell, 1998). The experiment evaluated the previous Saisi and Mitchell MSOCC data and verbal protocols from two control participants. Every action had an interpretation from the OFMspert, the participants or a domain expert (from the Saisi and Mitchell MSOCC data). Results showed significantly good matches for system commands and display requests. However, there were poor matches for planning and browsing. Another implementation of the OFMspert was performed by Callantine, Mitchell and Palmer (1997; 1998) and looked at the Boeing B-757 flight deck (as cited in Mitchell, 1998). Results showed the OFMspert correctly interpreted 92% of the actions, when compared with 10 certified pilots. Those that were not correct related to browsing actions.\rAssessment\rThis section, which describes empirical research relating to alert warnings, considers both the visual and auditory components of an alert, as well as how to reduce the operator’s workload. For the visual component of the alert, a slow fade text is recommended. For the auditory component of the alert, a sound for low, medium and high urgencies should be presented with the alert and the sound should vary according to urgency. Lastly, there is a recommendation of having a predictive adaptive system which can infer the actions of the operator. This prediction can allow the operators to be interrupted with a medium or low urgency alert during periods of low workload.\rThe OFMspert technology is a decade old and perhaps could be modified to fit the needs of the maritime domain. Presumably, if this system can track the activities of an operator and predict what actions they will perform, specifically alert scanning; it can have some merit as part of an alerting system. Predicting operator’s actions in tandem with an alerting system could predict when the operator’s workload could be interrupted with an alert. This is especially the case when the operator is already scanning for alerts. Combining the OFMspert with a warning system could be beneficial in that operators are being alerted when their workload allows for it, and the rest of their time is spent on the primary task, except during cases of emergency. However, the major limitation of this approach concerns the accuracy with which the system is able to determine the suitable time for presenting alert information. Even small errors would likely result in operators becoming frustrated with low level alerts that arrive when they are busy and important alerts arriving too late.\r4.2.2.4 Design concepts\rThe literature provided a number of specific design concepts for reducing the intrusiveness of alert warnings. These concepts are related to information presentation, limiting operator information overload, maintaining situation awareness, and reducing the number of alerts.\rInformation presentation\rA key design for reducing the intrusiveness of alerts can be found in Hautamaki, Bagnall and Small’s (2006) research on the Hazard Monitor and Intelligent Alerting System (HMIAS). This system was designed to improve information presentation to combat system (CS) operators using the Combat Control System (CCS). The CCS was designed to help CS operators form a tactical picture of the maritime environment, especially surface and subsurface vessel locations. Included in the CCS is an alert system to notify operators of conditions that violate expected operating ranges. Originally the alert system indicated isolated incidents but did not convey the severity of the situation as a whole to the operator. Over the years, improvements to the CCS have incorporated a great deal of disjointed but related information. However, Hautamaki et al. (2006)\rPage 36 Non-Intrusive Alert System Humansystems®\r\npoint out that there is still room for improvement. In particular, the researchers argue that the Alert Manager window on the Tactical Control and Weapons Control interface has a method for presenting alerts that is too subtle. This subtle alerting mechanism has led to situations in which operators were so focused on an ongoing task that they failed to notice other safety alerts. In addition, Hautamaki et al. (2006) argue that the CCS method of organizing safety alerts by occurrence or contact number makes it difficult for operators to identify the most severe alerts.\rTo address these issues, Hautamaki et al. (2006) developed the Hazard Monitor and Intelligent Alerting System (HMIAS) to improve the overall CCS. HMIAS monitors for, prevents, traps, and captures operator errors in order to prevent the negative consequences associated with errors (see Figure 3 for a generic Hazard Network).\rFigure 3: Hazard Network (Hautamaki et al., 2006, p. 7)\rHMIAS monitors system states for hazards, and alerts operators to the hazards in a timely, context sensitive and multi-modal manner. For example, an initial alert is presented in the form of text on the operator’s screen. If the alert is not acknowledged in a sufficient time period, and the condition persists or worsens, the alert is presented as flashing text, which then proceeds to an audible alert, followed by the addition of verbal instructions.\rTo study the effectiveness of HMIAS, Hautamaki et al. (2006) simulated a mission in which sonar personnel, fire control personnel (including CS operators and CS supervisor), and the Officer of the Deck in a Virginia Class submarine control room track an unfriendly quiet diesel submarine through a strait while remaining undetected. During the mission, operators encounter commercial vessels, deep-draft tankers, and fishing trawlers. After 20 minutes, the scenario concludes when a controlled close aboard encounter with a deep-draft tanker requires an evasive manoeuvre. Participants were tasked with continuously hunting for the best system solution for contacts using target motion analysis. Those assigned to the baseline condition used only current CCS alerts, whereas those assigned to the experimental condition used CCS alerts with HMIAS technology.\rResults indicate that HMIAS enhances operator performance. What is of particular importance to less intrusive alert technology is the HMIAS hazard network. As events monitored by the system increase in severity of consequences, the intrusiveness of the alerts also increase. This allows the operator to easily assess the urgency of an alert and be able to quickly and easily identify the alerts that require immediate attention.\rHumansystems® Non-Intrusive Alert System Page 37\r\nMcFarlane and Latorella (2002) identified some less intrusive methods for presenting alert warnings in their review of interruption management literature. Interruptions are prevalent in many working environments in which humans and computers interact with reactions being both positive and negative. For example, interruptions can provide important information, but they can also cause stress and hinder performance. McFarlane and Latorella (2002) discuss the Aegis weapon system used by the navy as an example. This system interrupts users through an alert tool that presents messages and task assignments on an ongoing basis. Although operators must be informed of the alerts, the alerts are in fact interruptions, occurring several per minute during high-stress operations. McFarlane and Latorella (2002) provide guidelines based upon Latorella’s Interruption Management Stage Model (1996, 1998; as cited in McFarlane & Latorella, 2002). This model explores the process of human interruption in a work environment, as shown in Figure 4.\rPage 38 Non-Intrusive Alert System Humansystems®\r\nAnnunciation Stimulus\rDetected Threshold Exceeded\rAnnunciation Stimulus Processed\rDetection Diversion\rInterpretation Distraction\rIntegration\rImmediate Interrupting Task Performance\rSchedule Interrupting Task Performance\rPreempt Ongoing Task\rDisturbance\rPreempt Ongoing Task\rDisturbance\rSchedule Interruption Task\rDisturbance\rPerform Interruption Task\rDisturbance\rPerform Interruption Task\rDisturbance\rImmediate Performance\rResume Ongoing Task\rDisturbance\rContinue Ongoing Task\rDisruption\rFigure 4: Interruption Management Stage Model (adapted from McFarlane & Latorella, 2002, p. 16)\rMcFarlane and Latorella (2002) propose four design solutions to deal with interruption: immediate interruption, negotiated interruption, mediated interruption, and scheduled interruption.\r• Immediate interruption is required by some tasks. When tasks require this type of interruption, some implementations may make it easier for the operator to resume his or her primary task. For instance, Lee (1992; as cited in McFarlane & Latorella, 2002) found that an active window with an animated border produced less confusion upon resuming a task than an active window with a fixed border. Similar to this, Davies, Findlay and Lambert (1989; as cited in McFarlane & Latorella, 2002) reported that reminders are a useful technique in recovering from interruption. Another design technique to enhance performance of responding to the alert is\rHumansystems® Non-Intrusive Alert System Page 39\r\nwhen information (e.g., numbers) is presented in the same location on the screen, rather than in dispersed areas.\r• Negotiated interruption is that which is controlled by the human. Woods (1995, as cited in McFarlane & Latorella, 2002) hypothesized that humans are better than computers when it comes to diverting attention. Thus, Woods proposes that alerts should be subtle enough to let the human decide when to direct their attention to a secondary task. For instance, displaying alerts separately from the primary task, but in a visible way, allows users to attend to the task if they choose, or ignore it (Lieberman, 1997; as cited in McFarlane & Latorella, 2002). Oberg and Notkin (1992; as cited in McFarlane & Latorella, 2002) created a system design where the alert would pop up near the operator’s curser position. Alerts were also colour coded so that the older an alert was, the more saturated in colour it appeared; urgent alerts changed darker faster than non-urgent alerts. Although this system was not compared to other designs, anecdotes attest to the system’s usefulness. Shneiderman (1992; as cited in McFarlane & Latorella, 2002) listed various techniques to obtain user attention, namely intensity, marking, size, choice of fonts, inverse video, blinking, colour, colour blinking, and audio.\r• Mediated interruption gives control over the interruption to a third source, or mediator (e.g., a personal digital assistant, an answering machine, etc). Czerwinski, Cutrell and Horvitz (2000; as cited in McFarlane & Latorella, 2002) suggested that the system should queue alerts until the user has a natural break. Similar to this, a program that can predict what the user would do next, can interrupt with relevant information at the appropriate time (e.g., Hammer & Small, 1995; as cited in McFarlane & Latorella, 2002).\r• Scheduled interruption can be thought of as a predetermined time where an operator allows for distractions. Alerts to anomalous information received from regularly scheduled Maritime Patrol Aircraft (MPA) flights are examples of interruptions that may be most relevant to the RJOC operators.\rToet (2006) also identified some less intrusive methods for presenting alert warnings in his review of the literature on gaze direction tracking. The author suggests that an alert system that is informed of the operator’s gaze direction will be able to present information to that operator in a way that will maximize responsiveness. Specifically, the interface can reduce visual clutter, enhance the operator’s attentive capacity and direct the operator’s attention. For the computer to perform the action of eye tracking, a non-intrusive video-based tracking system must be installed to monitor the operator’s gaze and direct attention.\rOf particular interest for this review are the techniques used to present visual information on the display screen. These techniques include non-distortion, distortion and gaze contingent techniques.\rNon-Distortion Oriented Techniques\rA semi-transparency (multi-layer displays) allows operators to quickly shift their attention. The display shows two different views (layers), one in the foreground (e.g., overview) and one in the background (e.g., detailed map). Operators can shift their attention between the two views best when views were 50-70% transparent.\rDistortion-Oriented Techniques\rThese techniques combine a detailed (full size or enlarged) representation of the regions of interest with a less detailed (compressed) representation of the remaining regions to draw the operator’s attention to the critical information. Studies have shown that participants who use distortion techniques are faster at navigation (Gutwin & Fedak, 2004; as cited in Toet, 2006).\rPage 40 Non-Intrusive Alert System Humansystems®\r\nGaze Contingent Displays\rThese displays include multiresolution displays, stereoscopic displays, and guiding displays. Multiresolution and stereoscopic displays track the user’s gaze and enhance the area being attended. Attention guiding displays, however, use a non-intrusive method to direct the user’s gaze to critical information. For example, overlaying red dots on a video clip will attract the user’s attention. Attentive interfaces, such as perceptual intelligent interfaces, can adapt their behaviour according to the user. The interface tracks the operator’s interactions over time, thus predicting their future actions.\rThe distortion, non-distortion and gaze contingent displays can be effective for non-intrusive alerting systems. The semi-transparency technique allows operators to quickly and easily shift their attention between tasks. Similar to the fade technique described by McCrickard et al. (2003b), semi-transparent alerts that pop up in the foreground of the display draw attention without being intrusive. Attention guiding displays can be less intrusive methods for directing eye gaze to critical information by directing an operator’s attention to the information by overlapping red dots. Lastly, eye contact displays can prevent irritating alerts by silencing once an operator looks at them.\r4.2.2.4.1.1 Assessment\rHautamaki et al. (2006) provide a useful model for presenting alert information to operators in a timely, context sensitive and multi-modal manner. In particular, they propose that alerts become more intrusive as the severity of the alert consequences increase. For example, an initial alert is presented in the form of text on the operator’s screen. If the alert is not acknowledged in a sufficient time period, and the condition persists or worsens, the alert is presented as a flashing text. The alert then proceeds to an audible alert, followed by the addition of verbal instructions. While there may be no comparable operational requirements in the RJOCs that would require temporal changes in the alert to signify increased urgency, the example categories of “intrusiveness” do provide some specific design options that may be applicable.\rSome of the methods described by McFarlane and Latorella (2002) could potentially be used in a less intrusive alerting system for example, making alerts subtle and presenting them in such a way that they are visibly separate from the main task but still visible to the user and providing coding of alert priority. More questionable is the suggestion to present alert information in the same area of the display as the primary task, which could potentially result in attention being immediately drawn away. Also the accuracy and the reliability of the technology to predict breaks in operator tasking or reduced workload remains unproven, thus recommendations for design concepts based upon this would appear premature.\rThe methods proposed in Toet’s (2006) paper would have limited applicability to an operational environment, where technology for detecting gaze direction cannot be realistically implemented. Although the multi layer or fish eye methods are not compatible with existing constraints within GCCS-M, the use of semi-transparent designs for an alerting system could be feasible.\rLimit operator information overload\rLimiting information overload of the operator is an essential feature of a non-intrusive alerting system. In discussion of a soft-desk control room, Dicken (1999) proposed some ideas for limiting operator information overload. His review was based on the trend in process plants to replace hard- desk operator interfaces (i.e., horse shoe control desk which includes dials, meters, chart recorder, knobs, and buttons) with soft-desk operator interfaces (i.e., computer-based Visual Display Units with management software and user displays). In the past, plant operators were typically in charge Humansystems® Non-Intrusive Alert System Page 41\r\nand ‘drove’ the plant; however with increasing automation, the operator has evolved from driving the plant to being driven by alerts. With the increasing dependence on alert systems, unacceptable problems such as generation rates, nuisance alerts, and poor performance during alert floods have arisen. Consequently, Dicken (1999) provided a careful look at important design concepts that should be considered when switching from hard- to soft-desk alert system facilities. First, he noted that alert lists are prime operating tools for soft-desk systems (see Section 4.2.5 Manage Alerts for detailed information). The system indicates an alert by flashing one of three colours, which corresponds to urgency and matches the list of alerts. To prevent visual overload, alerts are grouped and are attached to the same plant item icon. For example, alerts relating to a single mill are grouped together.\rGrootjen, Bierman and Neerincx (2006) also proposed some ideas for limiting operator information overload. Over multiple projects, Grootjen et al. (2006) identified problems in process control such as information type, information volume, task integration, increased autonomy, increased complexity, low personal costs, low training costs and legislative constraints (for more details, see Grootjen et al., 2006). To address these problems, Grootjen et al. (2006) designed an interface to optimize the operator’s cognitive task load (CTL) by transferring a task or part of a task to another person. This adaptive user interface was designed to:\r• Show only the categories with active alerts (empty categories are not shown). The interface contains the operator’s alerts and the alerts of other operators;\r• Show only the buttons that are relevant for the alerts the operator handles;\r• Provide operators with an icon that allows them to redirect alerts to other operators.\rGrootjen et al. (2006) evaluated the effectiveness of the interface. While participants worked in pairs to solve problems, they were presented with a number of different alerts. Participants received no task allocation (TA) support, task allocation advice from the system, or a notice that the system had reallocated the task. Overall, Grootjen et al.’s (2006) adaptive interface was rated positively. Participants reported that the allocation interface was pleasant to use, not difficult, useful, and allowed them to solve problems faster and better. The reallocation of alerts was not found to require a lot of effort or be confusing. For the automatic alert group, automatic TA of alerts was found to moderately disturb their normal way of working and was rated as moderately annoying. For the advice group, the TA advice was not reported to disturb their normal way of working.\r4.2.2.4.1.2 Assessment\rWith respect to alert intrusiveness, Grootjen et al.’s (2006) adaptive interface offers some useful functions. However, the focus of the work is primarily on process control types of tasks and their associated interfaces, which have little direct comparability to RJOC operators working with GCCS-M.\rMaintain situation awareness of primary task\rMaintaining or enhancing situation awareness of the primary task is another important design feature of an alert warning system. McFarlane and Berger (2004) were concerned with an operator’s ability to maintain situation awareness while using notification systems. Automatic notification systems constantly monitor and generate alerts, but these alerts often interrupt other activities. Although people do not generally perform sustained, simultaneous, multi-channel sampling well on their own, they can do so when provided with specific interface support. An alert- based information stream can deliver tasks and information to support the operator’s ability to a)\rPage 42 Non-Intrusive Alert System Humansystems®\r\nconstantly monitor a dynamic environment, b) collaborate and communicate with other people in the system, and c) supervise background autonomous services. The Human Alerting and Interruption Logistics (HAIL) technology was developed to improve an operator’s ability to maintain situation awareness during high rates of alerting. The HAIL user interface is designed to improve an operator’s ability to process alerts and to reduce the number of interruptions during complex, stressful tactical situations. Compared to the Identification Supervisor (IS) operator for the Aegis Weapon System, HAIL is said to reduce the number of operator interruptions, improve operator situation awareness for each alert and status information, improve control of alerts requiring action responses, and assist in returning to the operator’s original task. For an illustration of the current Aegis alert processing system and the HAIL-enhanced Aegis processing system, see McFarlane and Berger (2004).\rRather than displaying alerts in a single window, the HAIL system pre-processes alerts and displays them in the appropriate window. Operators are then able to negotiate their response to the\ralert by\r• • • • •\rchoosing to:\rSurface the alert (i.e., make it visible); Defer alert and surface next alert; Complete alert and surface next alert; Defer alert; or\rComplete alert.\rMcFarlane and Berger (2004) tested the effectiveness of HAIL with experienced naval operators in an operator simulation using an Aegis alerting system and a HAIL-enhanced Aegis alerting system. In general, results of the HAIL technology were positive. McFarlane and Berger (2004) conclude that HAIL increases warfighter performance by providing operators immunity to the effects of trash alerts, fewer interruptions, better alert situation awareness, and easier recovery of non-alert work after handling an alert. Operators reported that HAIL made it easier for them to distinguish between noise alerts and important alerts.\r4.2.2.4.1.3 Assessment\rThis paper provides some relevant concepts for providing operators with separate functionality for alert actions, alert information and alert management. One caution to be remembered is that this system was designed for more dynamic tactical maritime displays than is the case with GCCS-M, where data is updated at a slower rate (i.e., more time-late data) and tactical decisions and responses do not have to be made with battlefield urgency.\rAlert reduction\rAhnlund, Berguist and Spaanenburg (2003) argue that many alerts are distractive and do not alert the operator to important information. One way to potentially reduce the intrusiveness of alerts and their impact on primary task performance is to reduce the number of alerts to which operators must attend. In particular, nuisance alerts are problematic because they unnecessarily overload operators with alerts, which increases operator workload and has been known to cause operators to turn off alert systems altogether (Sorkin, 1988). Ahnlund, Berguist and Spaanenburg (2003) designed an alarm cleanup methodology and computerized tool to remove nuisance alerts from user interfaces that would not interfere with overall operations.\rHumansystems® Non-Intrusive Alert System Page 43\r\nThe alarm cleanup method uses a software program called the Alarm Cleanup Toolbox (ACT). ACT helps tune alert limits and develops algorithms to reduce the number of alerts. To perform an alarm clean up, the following steps are applied:\r• “Extract signal data during normal operation. This is the difficult part since most control systems are installed without a logging device.\r• Extract information about the signals, such as the current alert limits and the applied signal processing methods.\r• Examine the control system’s built-in functions and programmable capabilities.\r• Perform an off-line analysis of the signals using ACT.\r• Discuss and validate the suggested alert reduction methods and implementation decisions with the operators and personnel with process knowledge.\r• Implement the discussed improvements into the control system” (p. 8).\rTo validate the ACT and alarm clean up method, researchers implemented this technology at a bio- fuelled District Heating Plant (FFC). They were able to track every alert to determine if the alert was removed or delayed. Results showed there were 83% fewer alerts while using ACT.\rChyssler, Burschka, Semling, Lingvall and Burbeck (2004) were also interested in alert and false alarm reduction, specifically within the security field. With respect to security, issues such as alert reduction, false alarm reduction, information correlation, and preventable total service collapse need to be considered. Chyssler et al. (2004) examined alert and false alarm reduction through improving the quality of Intrusion Detection Systems (IDS). According to Chyssler et al. (2004), alerts can be interpreted by their severity, number, frequency, variety, uniqueness and payload. The authors present an architecture that incorporates IDS.\rThe IDS performs the following tasks:\r• Static filtering: A system can produce many irrelevant messages, including false alarms. Tuning a Large Complex Critical Infrastructure (LCCI) is dependent on a static network. Known problems in the system are handled by static filters. These filters omit irrelevant data either by ignoring or deleting the alert. Deleted alerts are permanently removed from the system. Ignored alerts are saved, but not forwarded to the next agent.\r• Adaptive filtering: Unknown problems in the system are handled by adaptive filters. These filters categorize messages as interesting or uninteresting.\r• Aggregation: Aggregation is the combining of frequent alerts into one alert. This reduces the operator’s workload and lessens the intrusiveness of the alerts.\r• Correlation: The correlation agent analyzes the data and determines whether the alert is interesting or uninteresting by comparing the sum to a determined threshold.\rChyssler et al. (2004) applied this architecture to a realistic Internet Protocol (IP) environment where the network experienced internet attacks. Results showed that static filtering reduced the frequency of alerts and that messages were combined into one alert when there was more than a 65% similarity.\rOverall, the method described by Chyssler et al. (2004) shows promising results to reduce the number of alerts and false alarms experienced by operators. Although the study is in the context of\rPage 44 Non-Intrusive Alert System Humansystems®\r\ncomputer networks and internet attacks, it can be applied to the maritime domain. Static filtering (i.e., ignoring or deleting alerts), adaptive filtering (i.e., classifying alerts as interesting or uninteresting), aggregation (i.e., combining frequent alerts into one alert) and correlation (i.e., determining if the vector is interesting or uninteresting using algorithms) data show that when implemented, these techniques can greatly reduce the number of alerts and false alarms.\r4.2.2.4.1.4 Assessment\rThe focus of these papers is the reduction in nuisance and other non-critical alerts through smart technology. As such, these papers will be of more interest to those who will be responsible for the development of the technology and algorithms that will determine which data anomaly conditions in the RMP will merit being brought to the attention of operators., They do not apply directly to the current project where the focus is on interface design approaches to minimise alert intrusiveness.\r4.2.2.5 Summary\rThe following table summarizes some of the key recommendations and design principles from the literature on alert warnings or indicators that are potentially relevant to the design and implementation of initial alert warnings for the RJOC operators.\rTable 12: Summary\rForm of literature\rIssue\rRecommendations or design principles\rModels\rAlert display\rDefine alerts by colour and shape\rLow interruption\rAlerts that are low in interruption are the secondary display and ambient media\rDesign Guidelines\rOperator workload\rKeep set points at a level to avoid nuisance and false alarms\rOperators should be able to suppress or shelve certain alerts\rThe rate at which alert lists are populated must not exceed the users’ information processing capabilities\rOperators should be able to turn off non-critical alerts without erasing information\rAlert display\rDo not present status indications through an alert system display\rAlert acceptance should be reflected by a change on the visual display\rAlert presentation should use specific sounds/colours/etc. to identify the category of risk and priority\rThe auditory portion of an alert should shut-off automatically when it no longer provides useful information\rEmpirical Research\rAlert display\rSlow fade text\rVoice warning\rHigh urgency=sharp sound, medium urgency=soft sound\rPredictive system\rA system which predicts the operator’s actions to interrupt when appropriate\rHumansystems® Non-Intrusive Alert System Page 45\r\nForm of literature\rIssue\rRecommendations or design principles\rDesign Concepts\rOperator workload\rAllow operators the ability to control alert responses through negotiated interruption or surfacing the alert\rAlert display\rUse alert pop ups near cursor position\rColour code alerts by length of time on the screen with alerts getting darker in colour the longer they are on the screen (note: this assumes certain display photometric properties that may or may not be applicable in the RJOCs)\rUse visual distortion techniques to draw operator attention to critical information\rShow only active alerts\r4.2.3 Comprehend alert information content\rWhereas the previous section dealt with issues relating to bringing an alert to the attention of an operator, this section examines relevant research on how to represent the semantic information associated with the conditions that gave rise to the alert. That is, how information should be structured to provide immediate situation awareness of the cause and conditions of the alert. Research relating to alert information content was in the form of design guidelines, empirical research and design concepts.\r4.2.3.1 Design guidelines\rMiller (2005) examined the role of trust and etiquette in adaptive automation. “Etiquette” embodies unwritten codes that define the roles and acceptable behaviour of participants in a social setting. Etiquette rules define the expectations and interpretations of other people’s behaviour. Therefore, etiquette can serve a role in human-computer interaction. For instance, a system using familiar domain jargon indicates that it is experienced with the domain and should be accorded the trust reserved for those in that domain. Etiquette can also define polite and rude behaviours. Humans are prone to interpret computer behaviour similarly to human behaviour (Reeves & Nass, 1996; as cited in Miller, 2005), thus, systems are likely to elicit the same negative or positive reactions if they hold to or defy established etiquette. Therefore, the information content of an alert should comply with system characteristics and jargon with which operators are familiar.\rMiller (2005) defined the following guidelines for adaptive automation. The system needs to:\r• Infer tasks;\r• Allow operator’s to over rule the system;\r• Adapt automation behaviours; and\r• Adapt information presentation.\rAdapting information presentation can be done by:\r• • •\rCommunicating about ongoing and future tasks; Using similar domain jargon;\rUsing text format to report context and tasks; and\rPage 46\rNon-Intrusive Alert System\rHumansystems®\r\n• Overall, maintaining similar human-to-human etiquette. Assessment\rMiller’s (2005) guidelines are based on a study which used etiquette and gained a favourable response from operators. The four bullet points relating to adaptive automation support the notion of having a function in an alerting system that allows local user configuration. The second set of bullets deal more with ensuring that the mode of the communication is consistent with existing operational concepts, language and procedures. In this case, we believe these principles should also be extended to cover the maintenance of existing “human to system to human etiquette”.\r4.2.3.2 Empirical research\rThe literature review uncovered one article describing empirical research relating to alert information content. Steefkerk, Esch-Bussemakers and Neerincx (2007) designed a context-aware alert system and described the system’s implementation. Alert systems should direct the user to important and clear information to allow for quick interpretation and reaction. A prototype was designed on a personal digital assistant (PDA) using visual, auditory and information condensing effects. The alert information included in the prototype varied according to the user’s workload. In low workload situations, the full message (i.e., providing all the information) was presented to the user. Conversely when workload was high, only a summary message was presented to the user, with the exception of high urgency messages. Further, in order to prevent interruption of the user while attending to the PDA, an icon appeared when a new message was received, rather than the full alert. This context-aware prototype in which only a summary message is presented in high workload conditions was compared to a non-adaptive prototype which alerted the user with full messages regardless of workload.\rResults showed that more targets were remembered in the high workload context for those using the adaptive prototype than those who used the non-adaptive prototype. No significant differences were found in terms of the number of correctly remembered messages. Participants who used the adaptive system reported the messages to be less intrusive than those who used the non-adaptive system, especially during high workload conditions. Adaptive systems were also rated as less interruptive and irritating than non-adaptive systems and were generally preferred. Overall, results showed that a system which presented all the information related to the alert during periods of low workload and a summary of the information during high workload was preferred.\rAssessment\rThis study has limited applicability to the present project in the sense that it is based on an assumption that there will be appropriate technology available to assess ongoing operator workload. However, the design concept of providing summary, rather than detailed, alert information in certain conditions may be applicable and will be pursued in the design of a maritime anomaly alerting system for RJOC operators.\r4.2.3.3 Design concepts\rAs described previously, Dicken (1999) discussed the problems that could arise when switching from hard to soft-desks. Dicken also reported how the information related to the alert should be displayed. There are features available to the operator to limit the overload of information. The operator has the option of retrieving more information regarding the alert. This comes in the form of a point and click picklist. The picklist has a number of options that give operators the ability to retrieve more information relevant to the alert, at a time that is convenient to them. The difference\rHumansystems® Non-Intrusive Alert System Page 47\r\nbetween a picklist and an alert list is that the former allows the operator to progressively select further layers of information concerning the alerting state as the situation and time available merit.\rAssessment\rThe merit of this approach is that it would minimize workload by allowing operators to have control over what information is needed at certain times. Thus, the general concept that may be applied to an alert information window is to structure information hierarchically, thereby allowing users progressively more detailed information at lower levels, which can be simply accessed by point and click approaches.\r4.2.4 Maintain primary task\rThis section focuses on the operator’s ability to perform his or her primary task while being presented with alerts. Research suggests that there are three features of an alerting system that will support an operator’s ability to main their primary task: assessing the interruptibility of the operator, preparing an operator for an upcoming alert, and facilitating the operator’s return to the primary task. This section is therefore further subdivided according to these three features.\r4.2.4.1 Assessment of interruptibility\rThis section describes literature relating to human factors and the assessment of interruptibility in the design of an alert system. Of the four categories of literature, we were only able to find relevant papers relating to generic design guidelines.\rDesign guidelines\rThe assessment of interruptibility relates to the impact an alert has on the performance of the operator. The goal is to minimize interruption to maintain performance and situation awareness. The literature provides some evidence that emotional states have a major impact on perception, cognition, and motor processes, which in turn, impact performance. Belief states (i.e., assessment of the current situation) have also been found to impact decision making and response selection. Given the impact that affect and beliefs have on performance, the following authors have looked at how to adapt information presentation to the individual belief states of operators.\rThe Affective and Belief Adaptive Interface System (ABAIS) developed by Hudlicka and McNeese (2002), was designed to address individual differences. Specifically, ABAIS uses an adaptive methodology framework capable of adapting the user interface and information to an operator’s current affective state, key personality traits, situation specific beliefs, and preferences. In particular, the ABAIS system architecture uses an adaptive methodology consisting of the following four modules:\r1.\r2.\r3. Page 48\rUser State Assessment: identifies the user’s affective state and task relevant beliefs (e.g., level of anxiety). The User State Assessment module receives information about the operator and task context to identify the operator’s predominant affective state and situation-relevant beliefs.\rImpact Prediction: identifies the effect of operator state on performance (e.g., focus on threatening stimuli). The Impact Prediction module inputs the identified affective state and operator beliefs to determine, using rule-based reasoning, the impact on task performance.\rStrategy Selection: selects a compensatory strategy (e.g., presentation of additional information to reduce ambiguity). The Strategy Selection module receives the predicted\rNon-Intrusive Alert System Humansystems®\r\nimpact as input and selects a compensatory strategy to counteract resulting performance biases.\r4. GUI/Decision Support System (DSS) Adaptation: modifies the user interface content and format to improve detection, recognition, and assimilation of incoming data to enhance situation awareness. The GUI/DSS module implements a selected compensatory strategy in terms of specific GUI modifications. GUI modifications are based on individual user preferences for information presentation (e.g., blinking, colour change, size change of relevant display or icon).\rPrior to using ABAIS, each operator must provide background information (e.g., individual history information, baseline physiological or diagnostic data). Categories of information include personality, skill, individual history and adaptation preferences. The information is then used to perform a Cognitive, Affective, Personality Task Analysis (CAPTA) to produce a comprehensive description of possible behaviours and behaviour states associated with specific user states, traits and beliefs.\rOnce the user affective and belief states are identified and their likely impact is predicated, ABAIS identifies a compensatory strategy and selects a means of implementing this strategy in terms of specific user interface modifications. This strategy is defined based on stable performance biases and the current task context. Once the compensatory strategy has been identified, ABAIS implements this strategy in terms of specific modifications to the user’s interface. Modifications of the interface to present the information can be made by:\r• Modifying the GUI icons in terms of attributes (e.g., changing colour or size) or modify the icon appearance itself.\r• Modifying the display as a whole by changing size, location, appearance, or contents.\r• Implementing changes to the GUI as a whole or insert additional display elements designed to focus attention on particular areas. For example, reconfigure entire set of instruments to reflect a different system model and insert attention-capturing and attention-directing elements designed to direct the user’s attention to a particular icon or display.\r• Inserting new or modifying existing alert and alert notifications, or adding an icon to a display to represent new information. An example of a notification level adaptation includes adding text regarding desired focus of attention, or adding an icon to a display to represent new information.\rThe ABAIS technology could prove very useful in reducing alert intrusiveness. Prior to presenting information to an operator, the system takes into consideration the operator’s stable beliefs, current affective state, and information presentation preferences. Combining this information potentially provides users with information in the most effective and least intrusive manner. We use the term “potentially” because the ABAIS has not yet been empirically tested with operators.\r4.2.4.1.1.1 Assessment\rThe fundamental assumption of this paper is that affective states can be reliably and accurately assessed and modelled to provide clear parameters for selecting appropriate alert configurations and parameters. This approach is therefore highly speculative and subject to potentially serious operational consequences, if not implemented with a very high level of accuracy and reliability.\rHumansystems® Non-Intrusive Alert System Page 49\r\nOther than gross examples, the paper does not provide any insight or concepts on exactly how different emotional or other states would be mapped onto specific alert designs.\r4.2.4.2 Warning of impending alert\rThis section describes the literature related to how to prepare operators for an impending alert by providing an advanced warning. Of the four categories of papers, only empirical research was found.\rEmpirical research\rHwang, Lin, Liang, Yenn and Hsu (2008) developed a pre-alert system that would reduce the frequency of alerts and examined whether these pre-alerts should be a text or a graphic. Reducing the numbers of alerts is done by aiding the operator in identifying faults before the alert is activated. This method has been applied to many industries and has shown positive results in power plant control rooms. Hwang et al. (2008) designed the pre-alert system based on the following 5 rules in deciding whether or not a system is out of control:\r1. “Any one point falls outside the upper control limits (UCL) or lower control limits (LCL);\r2. Seven points in a row are continually increasing (or decreasing);\r3. Cyclical patterns of points occur;\r4. Two out of three consecutive points fall beyond the two sigma limit;\r5. A run of five points falls beyond the one-sigma limit (Aft, 1998)” (as cited in Hwang et al., 2008, p. 2).\rThe pre-alerts were in the form of a text or graphic. The text alert turned black to yellow and was accompanied by a “ding” sound when the value changed 7 times (dropped or rose consecutively; Rule 2). This text changed from yellow to red and was accompanied with an alert when the value was outside the parameters (Rule 1). The other type of pre-alert was graphical which was designed to provide trend information of the situation. The original alert did not include the pre-alert function.\rTwenty-six graduate students and staff of National Tsing Hua University were randomly assigned to 13 groups which included a reactor operator (RO), assistant reactor operator (ARO) and a supervisor. The primary task of the RO and ARO was to monitor 6 critical parameters (vessel pressure, level, reactor feedwater pump turbine vibration, discharge pressure, turbine vibration or generator vibration). The RO monitored the parameters of the core flow and power during the 12 minute shutdown (normal state). During shutdown, the ARO closed and opened valves and pumps. During the load rejection (abnormal state), 5 alerts went off. The RO had to search for 10 items and find solutions in 10 minutes. The alerted task was deciding if the presented values were greater than or less than each other. Participants then filled out a questionnaire assessing their perceived mental workload.\rOverall results showed that the pre-alert type significantly reduced the number of alerts and the mental workload of the operator and maintained situation awareness. There were no significant differences found between the text and graphic pre-alert types for reducing the number of alerts. However, for the alerted task, operators had significantly more correct answers when deciding if the presented numbers were greater or less than each other when the alert was graphic compared to textual. The operator had significantly more correct answers during shutdown (normal state) than\rPage 50 Non-Intrusive Alert System Humansystems®\r\nin load reduction (abnormal state). Also, the ARO had significantly more correct answers than the RO during load reduction (abnormal state).\rIn summary, Hwang et al. (2008) discovered that a pre-alert system would reduce the number of alerts, thus reducing the intrusiveness experienced by the operators. Although both types of pre- alert systems would be a benefit to the operator, Hwang et al. (2008) recommend the text type pre- alert to be implemented in control rooms.\r4.2.4.2.1.1 Assessment\rThe generalisability of this paper to the operations room remains unknown, since the major thrust is to reduce the number of alerts (presumably in a context where this is a serious problem for operators). If the alert frequency is high, the use of an auditory alert may still result in nuisance alerts for the operator. In addition, the use of trend data to cue the alert may not be applicable to the maritime anomaly context.\rNote that the paper by Mitchell (1998) reviewed in section 4.2.2.3 also has some relevance to the issue of interruptibility.\r4.2.4.3 Facilitate return to primary task\rThis section examines relevant research on how to best support an operator in returning to his/her primary task after attending to an alert. That is, what should be considered in the design of an alert system (which may draw the operator’s attention away from the primary task) so that the operator can easily and quickly return to his/her primary task? We were only able to find research in the form of design concepts rather than models, design guidelines or empirical research.\rDesign concepts\rAfter attending to an alert, operators must typically return to their primary task. This can be problematic for operators as interruptions to deal with alerts are associated with increased errors, reduced efficiency, and increased stress (McFarlane & Latorella, 2002). In addition, interruptions such as attending to an alert can reduce an operator’s situation awareness of the primary task domain. Operators may experience a “resumption lag” once they return to a primary task because they then have to work to re-acquire situation awareness, retrieve suspended task goals, and perform required actions (Monsel, 2003; as cited in St. John, Smallman & Manes, 2005). Returning to a primary task after an interruption can be particularly difficult if the primary task requires the operator to monitor a screen and detect changes. As noted by Smallman and St. John (2005), humans have remarkable difficulty identifying changes, which is particularly true when operators are distracted or interrupted. The longer the disruption, the more problematic it will be for operators to detect changes because their memory of the state prior to the alert will decay. Smallman and St. John (2003) argue that current methods of displaying information only add to an operator’s difficulties when returning to primary tasks because current displays show information in real time, which require operators to remember and mentally integrate previous information with current information. This forces operators to determine for themselves whether or not changes have occurred. In order to address the effects associated with reduced situation awareness (e.g., tunnel vision, resumption lag) and to improve change detection ability, Smallman developed the Change History Explicit (CHEX) human computer interface tool.\rThe CHEX tool augments human attention by detecting significant changes to a situation and logging these changes into a table, which can be sorted and filtered by the operator according to specific variables (e.g., significance, change type, age). Table entries are linked back to objects in\rHumansystems® Non-Intrusive Alert System Page 51\r\nthe geographical display. In order to evaluate the usefulness of the CHEX tool, Smallman and St. John (2003; St. John, Smallman & Manes, 2005) conducted a series of studies comparing the CHEX tool to conventional displays.\rIn the first study, Smallman and St. John (2003) had 80 university students monitor a Geoplot containing a high density of aircrafts (40) or a low density (13). The aircraft slowly moved about an Own-ship signal, changing direction, speed, and turning on and off their fire-control radar (FCR). The participant’s task was to identify aircrafts that turned critically threatening, as quickly as possible. Aircrafts on the Geoplot were assigned an “interest score” to reflect their potential significance to the operator; ten interest points were assigned to aircraft that were 1) flying fast, 2) flying towards own-ship, and 3) had FCR turned on. Once an aircraft had a score of 30, it was defined as a “critical aircraft”. Aircrafts with interest scores of 10 or greater were shown in yellow, whereas those with interest scores of less than 10 were faded. Within each density condition, participants used one of four different change awareness human-computer interaction (HCI) schemes, 1) a baseline of the Geoplot and CRO, 2) baseline plus a static, chronologically sorted Change History Table, 3) baseline plus Change History Table and red circle alerts around all aircraft with changes (circles could be removed by selecting the aircraft), or 4) baseline plus CHEX (a sortable Change History Table linked to the Geoplot). Participants conducted both monitoring and reconstruction tasks. For the monitoring task, participants had to indicate when an aircraft became critical and how many changes the aircraft had made. For the reconstruction task, participants performed mental arithmetic for a minute while the scenario continued to play out of view then returned to the display to indicate when an aircraft became critical and the number of changes. The authors found that adding a Change History Table and change alert circles improved performance in terms of the percent of correctly identified critical aircrafts, but the greatest improvement in performance was seen when the participants were using CHEX in the high density condition. Participants in the CHEX condition had an 80% improvement in change identification speed compared to the baseline condition.\rIn the second study, St. John, Smallman, and Manes (2005) were interested in evaluating the design space of situation awareness recovery tools by comparing CHEX against an alternative tool, Instant Replay. Instant Replay allows operators to return to a monitoring task after an interruption and replay the missed period at high speed to quickly search for any changes to the situation. Participants were allocated into one of five conditions: Baseline (map and the aircraft data display in the lower right corner of the screen), Basic Replay (allowed participants to restart the scenario from the beginning of the last interruption), Explicit Replay (automatically detected and marked significant changes by adding small red triangles to the aircraft symbols and a “pop” sound), Explicit Markers (removed the replay function but kept the red triangles and pop sounds), and CHEX (included a table that logged the time, aircraft identification number, and a short description of the change). For a screenshot of the CHEX display, refer to St. John et al. (2005).\rEach scenario, contained aircrafts moving slowly in the display, interruptions, and changes. Participants response times using the CHEX tool were significantly faster than the other tools, and 57% faster than the Baseline condition. Participants in the CHEX condition also produced fewer misses and fewer errors than participants in any of the other conditions. Participants in the Explicit Markers condition produced few misses, but a high number of errors and moderate response times. Participants in the Baseline condition produced high miss rates, high error rates, and slow response times. Participants in the Basic Replay condition had the slowest response times.\rMcFarlane and Latorella (2002) reviewed the tools that could improve an operators’ ability to return to primary tasks by designing user interfaces to present reminders about the existence and\rPage 52 Non-Intrusive Alert System Humansystems®\r\nstate of interrupted activities. Marlin et al. (1991; as cited in McFarlane & Latorella, 2002) designed a user interface that specifically allowed users to suspend and resume activities. Specifically, this design allows users to explicitly mark when an interruption occurred, which allows the computer to generate appropriate recovery support. Rouncefield (1994; as cited in McFarlane & Latorella, 2002) used a similar method in paper-based offices by having workers mark their work context before leaving to handle an interruption. These markers were found to facilitate recovery of prior work contexts when people returned to their prior tasks. This could be implemented in a computer environment by noting interruptions on an electronic notepad that constantly displays a list of interrupted activities (Cypher, 1986; as cited in McFarlane & Latorella, 2002). Finally, Lee (1992; as cited in McFarlane & Latorella, 2002) found that marking the primary task window with an animated border, instead of a static border, reduced the confusion about which window was active when operators resumed a task after an interruption.\r4.2.4.3.1.1 Assessment\rWhile the problems associated with recovering situation awareness and quickly resuming a primary task that was interrupted by an alert are not the primary focus of the present project, these papers provide some concrete suggestions that could be implemented into a future operator interface.\r• Include a table of significant recent changes.\r• Automatically highlight all changes to a tracked vessel when a change is selected. When a change occurs, a pop sounds and a new row is added to the top of the table. Selecting a row highlights the row in yellow, highlights the aircraft on the map with a yellow circle, and presents the aircraft’s data in the data display.\r• Automatically link and highlight between the “Change Table” and the Geoplot when a vessel in either display is selected. This allows for faster critical vessel identification because operators do not have to search the Geoplot to find the location of the relevant vessel.\r• Including the ability to sort the “change table” as needed by the operator.\rThe following aids, while not suited to the current implementation of GCCS or Concept of Operations in the RJOCs, may also have merit in allowing operators to quickly regain situation awareness of the primary task:\r• Electronically marking primary tasks before attending to an alert;\r• Using an electronic notepad to display interrupted activities; and\r• Mark interrupted task windows with an animated border to allow for quick identification of interrupted tasks.\rFinally, it should be noted that the operational environments that the authors of the above studies had in mind are characterised by multi-tasking on a single or multiple displays, highly dynamic data inputs and the often need for a rapid operator response. For the most part, these are not characteristics that would apply to the RJOCs, except under occasional and special circumstances.\r4.2.5 Manage Alerts\rAlthough not the primary focus of the literature review, several papers were found relating to human factors and the management of alerts in the form of design concepts. Since these could have\rHumansystems® Non-Intrusive Alert System Page 53\r\npotential relevance for the prototype design phase of the project, they have been included in the review.\r4.2.5.1 Design concepts\rIt is becoming increasingly important to introduce intelligent alert handling capability in order to manage alert systems. Liu et al. (2003) developed an Intelligent Alarm Management System (IAMS) for suppressing nuisance alerts and for providing advisory information to help panel operators focus and respond quickly to important alert information. IAMS was developed to incorporate special-purpose algorithms, process knowledge, and control system expertise. It consists of a graphical user interface (GUI), a Data I/O function, an Alarm/Trend/Knowledge Database and six sub-blocks:\r1. Statistical analysis: counts alert numbers in real time for different time periods, tags, message types, and alert statuses.\r2. Nuisance HI/LO analysis: analyzes high or low alerts and suppresses those that are repeating.\r3. IOP (Input Open) analysis: identifies the cause of input open alerts and suppresses those that are nuisance alerts.\r4. Criticality analysis: gives a criticality tag of very important, important, less important, or calculation-related to each alert message.\r5. Standing alert analysis: shows standing alerts, warns of ramping alerts, and resets standing alerts.\r6. Monitor & recover: shows changes to distributed control system (DCS) alert settings and restores alert setting when the nuisance status is cleared.\rIn order to aid operators in making appropriate responses to information, IAMS provides operators with advisory information that:\r• Informs operators which alerts are emergent or critical;\r• Provides operators with early warning for alerts that will lead to violations of high-high or\rlow-low limits;\r• Provides online maintenance reports; and\r• Provides alert statistics.\rAll information is provided to the operator through a GUI. The GUI displays guidance information, criticality, statistics, and an alert management overview, and allows operators to suppress nuisance IOP alerts. The operator controls the alert suppression by clicking the SPR button to enable and the RST button to disable the alert suppression function. The IAMS system also allows operators to obtain a control loop status report over the last work shift (SFT), obtain maintenance information (MTN), monitor alert information such as setting changes (MON), suppress nuisance IOP alerts (IOP), as well as guidance information and alerts that have occurred (GID). Operators also have access to all, calculation-related, ordinary, important, or critical alert messages (CRIT, CAL, ORD, IMP, EMG, respectively). Alert statistics reports can be easily generated for the last 10 seconds, 5 minutes, hour, day, or a special time period (SEC, MIN, HR, DAY, SPE, respectively).\rPage 54 Non-Intrusive Alert System Humansystems®\r\nThe IAMS not only allows operators to suppress nuisance alerts, it allows them to manage the non- nuisance alerts. This alert management system maintains operator situation awareness by providing the necessary information to the operators at their convenience.\rAnother non intrusive alert management method mentioned in the literature is an alert list. Riveiro, Falkman and Ziemke (2008) and Dicken (1999) used an alert list in their design of an alert system.\rAn alert list is an alert management component of an alerting system, designed to manage alerts that have been presented to an operator. Riveiro et al. (2008) designed an anomaly detection system which includes an alert list. The interface display of the anomaly detection system includes a geographical map, controls, detailed information, and the alert list (shown in the bottom left hand corner). This list is shown in detail in Figure 5.\rFigure 5: Alert list (Riveiro et al., 2008, p. 6; © 2008IEEE)\rWhen a vessel is considered anomalous, it is given an identification number and a coloured ellipse, reflecting the probability of the anomaly. A card appears on the alert list containing information such as the object identification (ID), coordinates, probability, main reason, age of alert, and delete/report buttons. By pressing any of these buttons, the operator can obtain more information regarding the alert. The object ID is the object identification number, which identifies each vessel accordingly. The object ID is also highlighted with an urgency colour indicating the probability of the anomaly (red, orange, yellow). The probability of the alert identifies the probability that the alert is anomalous. The position of the ship is given by x- and y-coordinates. Every alert is time- stamped which allows the operator to determine how old the alert is compared to the other alerts. Finally, each alert on the list contains a report and delete button the operator can click to perform the intended action.\rDicken (1999) discussed an alert list in the context of a recent operator desk change, from a hard- to soft-desk. A hard-desk can be described as a horse shoe control desk which includes dials, meters, chart recorder, knobs, and buttons. It consists of a back panel where the indicators can be found along with alerts. A soft-desk is a hard-desk incorporated with Visual Display Unit (VDU) screens. Along with this modernization of the standard hard desk, alert systems have also been changed to include alert management software and user displays.\rAs described by Dicken (1999), the alert interface has two options for soft-desk set up, 1) the standard VDU alert interface, or 2) the alert display and acceptance interface. The latter option is performed by pointing and clicking a mouse on a standard screen. This option also has an alert list as a secondary backup which can be displayed on any screen and is permanently on due to necessity. These lists are limited to a single chronological alert list and are unusable during emergencies. Recommended improvements to the alert list are done by: 1) showing alerts on pages rather than scrolling, 2) keeping the alerts in the same order (no shuffling), 3) adding new alerts to the bottom of the list, 4) removing alerts only by operator action, 5) having a flashing marker,\rHumansystems® Non-Intrusive Alert System Page 55\r\nrather than the alert text flashing, and 6) prioritizing alerts by colour. Dicken (1999) notes that these lists should be filtered, especially during an alert flood. Filters such as priority, category (category rectangles are displayed on the bottom of the list), named list (alerts associated with a task are built into a filter), modes (plant modes, e.g., stable operation, start up), and unaccepted (alerts that have not been accepted) can be useful to prevent operator overload.\rThe alert list also contains a “shelve” option for the inevitable nuisance alerts. Until these are serviced and fixed, operators can choose to ‘shelve’ these alerts to limit their intrusiveness. This shelving option allows operators to ignore false or nuisance alerts that should not be deleted and should not take time away from the primary task. However, unlike ignoring alerts, the shelving option would not remind the operators of the alert because operators are still responsible for the shelved alert list.\rAssessment\rThis paper provides a good description of the issues concerning the management of alerts and potential design solutions. It shows that an alert management system could be one method to ease operator workload and maintain situation awareness. An alert management system can organize alerts into a coherent list which the operator can access at any time to obtain more information. The suggested information contained in these lists are vessel ID number, position, urgency of alert, age of alert, and action buttons (ignore, delete, report, shelve). Alerts on the list should be colour coded according to urgency and this colour should match the actual alert (e.g., red for high urgency, orange for medium urgency, yellow for low urgency). The list should also include a detailed history of all alerts, including those shelved, deleted and reported. Lastly, this list should contain all the information found in the actual alert, including links to pertinent information.\rThere was some lack of detail concerning the implementation of the alert list in both Riveiro et al. (2008) and Dicken (1999) although it should be acknowledged that this was not the main focus of either paper. For example the following issues were not addressed: can an alert list contain a shelved/deleted/reported/ignored list or should they be treated separately? Do the alert lists include alerts that are 1 hour old, 1 day old, 1 week old, etc.? Can an alert system display all the alerts in an appropriate manner without looking cluttered, or are there a maximum number of alerts that can be listed? Further, maritime operators may have a large number of alerts present over the course of a watch, which could pose a problem in terms of where and how the alert list should be displayed. In the example provided, the alert list looks like it could fit 7 alerts across the screen, but the paper does not give any indication of what happens to the alert list when it becomes full.\rIn addition, it is probable that operators would find the alert list more manageable and they would have improved situation awareness if the list were able to be sorted by alert severity. These questions and more need to be examined thoroughly before the appropriate functionality and design requirements for an operational context can be determined.\rFinally, the paper assumes that operators can usefully comprehend categories of alert probability, which remains an assumption for which current empirical experimentation provides no clear guidelines.\rPage 56 Non-Intrusive Alert System Humansystems®\r\n4.3 Conclusions\r4.3.1 State of current knowledge\rThe following summary table shows the number of papers found and reviewed for each of the main functional aspects of an alert system categorised according to paper type (see Table 13).\rTable 13: Literature in the report\rConfigure Alert parameters\rReceive Information on Alert State\rAlert Information Content\rMaintain Primary Task\rAlert Management\rTotal\rModels\r0\r1\r0\r0\r0\r1\rGuidelines\r0\r6\r1\r0\r0\r7\rExperiments\r0\r7\r1\r1\r0\r9\rDesign Concepts\r2\r9\r1\r5\r3\r20\rTotal\r2\r23\r3\r6\r3\r37\rOverall, the largest subset of papers was about design concepts, and the majority of these focused on how to indicate to the user that an alert or alarm had occurred. It was somewhat disappointing to find so few papers that provided conceptual guidance for the design of alert systems and how little empirical research had been done to test the validity or generalisability of design options. One central theme was evident in several papers, namely, the importance of designing alert systems to minimise the operator’s mental workload and to reduce the potential for annoyance.\rWhile we found some empirical research on alerts and their potential impact on operator performance, there is a lack of research explicitly related to non-intrusive alerts. This may be because much of the literature has focused on the traditional problem of how to make an alert intrusive or salient enough to catch the attention of the operator.\rRecent attention has turned to the issue of how a high number of false or nuisance alarms can degrade system and/or operator performance. This has resulted in a body of research attempting to decrease nuisance and false alarms. Within this body of research are design guidelines and concepts that may also be useful in reducing the overall intrusiveness of alerts. To that end, a number of researchers (e.g., Edworthy & Hellier, 2002; Brown et al., 2002; Chyssler et al., 2004; Ahnlund et al., 2003) have recommended ways to reduce the number of alerts an operator is exposed to on a daily basis.\rUsing the ontology outlined earlier in the paper as a reference point, we have extracted from the literature a number of design principles for each of the main alerting system components, as shown in Table 14.\rHumansystems® Non-Intrusive Alert System Page 57\r\nTable 14: Alert design principles\rAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rConfigure alert parameters\rOperator preferences (individual differences)\rInterrupt the operator according to their preferences (i.e., colour, size, time, modality, movement)\rThis guideline should be treated with caution to ensure that operators are not provided with the freedom to create HF inappropriate designs.\rReceive Information on Alert State\rVisual\rColour – differ by priority (e.g., red=high urgency, orange=moderate urgency, green=low urgency) and length of time on screen\rNeed specific guidelines for time on screen or duty cycle for flashing alarms.\rColour coding implemented in design prototypes.\rSize of alert icon – larger icons for higher priority alerts\rNeed specific guidelines on size of alert and how this relates to the display size and the size of windows.\rSolid v. blinking icons\rNeed specific guidelines on blink rates\rChange display when alert has been accepted/actioned (e.g., remove alert from screen, visual marker to note it has been accepted)\rPop-up alert near cursor\rThis may be too intrusive for some tasks.\rNot suitable for RJOC context\rSlow fade v. blast or ticker alerts\rNeed specifications on the dynamics of the fade\rMovement at visual periphery\rMay be too general a recommendation without specifying primary tasks for which this would be appropriate.\rNot suitable for RJOC\rPage 58 Non-Intrusive Alert System Humansystems®\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rcontext\rShift display to foveal region\rThis would have to be implemented with caution because of the potential to impact adversely on the situation awareness of the primary task.\rNot suitable for RJOC context.\rDisplay arrows pointing to alert\rNot suitable for RJOC context\rVisual distortion techniques\rThis would have to be implemented with caution because of the potential to impact adversely on the situation awareness of the primary task.\rNot suitable for RJOC context.\rAuditory (NOTE: none deemed suitable for RJOC context)\rVoice saying “Conflict Conflict”\rLimited application\rHigh urgency=sharp sound, medium urgency=soft sounds\rNeed to define frequency and amplitude characteristics more specifically.\rUse specific sounds to identify category of risk\rPotential impact on increased need for training.\rAutomatically shut-off auditory portion of alert when it no longer provides useful information\rOther modalities\rTactile and olfactory\rTactile may be suitable for warning individual operators who are away from their workstation. Need guidelines and research on the vibrotactile profile and its perceived urgency.\rOlfactory unsuitable\rHumansystems® Non-Intrusive Alert System Page 59\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rfor most environments.\rAlert Information Content\rAdaptable to workload\rHigh workload=summarized information, low workload=full information, high urgency=full information\rAssumes that system is able to assess workload.\rNot suitable for RJOC context.\rDirect operator /constrain information\rUse picklists.\rButtons relating to the schematics, control panel, trends, point information, actions, procedures, and history of the alert\rNot suitable for RJOC context.\rMaintain situation awareness\rAbility to gain information of a vessel by clicking on the vessel on the RMP\rHighly relevant to RJOC. Implemented in design prototypes.\rMaintain Primary Task\rAssess Interruptibility\rThe system accounts for user’s beliefs, affect, preferences, ongoing task priorities\rTechnology not yet available to ensure the level of accuracy required in estimating interruptibility.\rPre-alert\rUse text rather than graph\rNot applicable to RJOC\rReturn to primary task\rInclude a sortable table of recent changes in the situation while operator was away.\rMore suitable for more highly dynamic information environments than the RMP .\rRestore task window to former look\rWhile generally advocating this approach, we caution that depending on the context there is potential for operator disorientation if the picture/RMP suddenly changes focus/range etc without operator input.\rMarking primary task\rElectronically mark task before attending to an alert\rNot applicable to RJOC\rElectronic notepad to record interrupted activities\rNot applicable to RJOC\rMark interrupted task window\rNot applicable to\rPage 60 Non-Intrusive Alert System Humansystems®\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rwith an animated border\rRJOC\rAlert Management\rAlert list\rProvide sortable lists of alerts that show: alert priority, alert context, time of alert and relevant information concerning contact details. Functionality to re-order and delete.\rImplemented in design prototypes\rProvide detailed history of all alerts (shelved, deleted and time actioned)\rRelevant to RJOC but not in scope of present work.\rNot to exceed user’s information processing capabilities\rToo general to be useful!\rEtiquette\rCommunicating about ongoing and future tasks\rThis is more applicable to highly dynamic task contexts.\rUsing similar domain jargon\rEnsure that design approaches are consistent with RJOC CONOPS and GCCS style.\rOur general assessment is that the above represents a somewhat piecemeal and haphazard collection of principles and guidelines, as might be expected since they are an agglomeration from many different papers with quite different application environments and goals. Clearly, there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. However, the detailed guidelines found in Shorrock et al (2002) and Han et al (2007) provide a good starting point for an integrated guidance document, which would then need to be extended and made more context relevant to the RJOCs. In addition, there would be a need to make this guidance more specific than statements such as “alarms should signal the need for action”, “alarms should be detected rapidly...”, “alarms should not annoy, startle or distract unnecessarily” etc. Thus, while these recommendations are sound, there is a lack of information on how they are to be implemented as specific design guidelines.\r4.3.2 Gaps in the literature\rIn general, there was a common theme in the literature relating to alert system design, namely appropriate ways to alert an operator of the situation at hand. Capturing an operator’s attention requires shifting attention to the alert and often times away from the primary task, although not necessarily for a significant period of time. One could argue that the very nature of an alert is to be intrusive. Although the literature presented various methods to lessen the intrusiveness of the alert system, the notion of an explicit ‘non-intrusive’ alert was not mentioned. Further, the impetus for designing less intrusive alerts appears to be minimizing operator annoyance rather than cognitive demands.\rHumansystems® Non-Intrusive Alert System Page 61\r\nAs shown in Table 14 in the previous section, literature in the form of models that could be used to inform the design of a maritime anomaly alert system was minimal. It is clear that the potential impact of poorly designed alerts or alarms on operator performance and behaviour (e.g., turning off nuisance alerts) has been recognized and a number of design concepts have resulted. However, normative models that describe the relationship between alert system parameters and human cognition do not appear to exist. Such models would be extremely valuable in providing a theoretical foundation for the design of an alert system. At present, the most suitable models are generic cognitive information processing approaches those that focus on attention sharing and resource competition. While not the focus of the present work, models of alert annoyance have some relevance to the design of auditory alerts, although they also may lack the specificity to inform design approaches.\rThe majority of the literature that was found related to the alerting cue itself with little focus on the other functional components of an alerting system. Although there is a relation between the alert presentation and the alert information content, most research focused on how to present an alert to an operator, rather than the information that should be included in the alert, when to present the alert (interruption), how to configure the alerts and how to manage old and new alerts.\rThe literature does provide, however, some reasonable guidance in reducing the frequency of alarms and configuring alarms to an operator’s local priorities.\rThere is a significant gap in the literature when it comes to concepts relevant to non-intrusive alert design guidelines. That is, recognition of the need to capture the operator’s attention while not interfering with cognitive processing of the primary task was not the impetus behind the majority of the guidelines. There were a few applicable design guidelines that were found relevant to the alert state and the alert information content and these guided some of the design concepts that were developed.\rFew articles investigated and compared different modalities of alerts. Given the pervasiveness of visual interfaces, it is not surprising that visual alerts were the focus of the bulk of the literature. However, there is research to suggest that auditory and tactile alerts may be useful and perhaps even more appropriate than visual alerts in certain contexts. Factors relating to the intrusiveness of auditory and tactile alerts, however, were beyond the scope of this review and would need to be further investigated.\rIn conclusion, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. Nor, did we find a comprehensive body of information that could provide specific answers on how to scale the intrusiveness of alerts. Thus, the conclusions we have reached concerning design for non-intrusiveness are based upon relevant concepts extracted from the more general alert/alarm literature. In doing so, it should be pointed out that many of the recommendations lacked the specificity to inform design approaches.\rThe following table summarises our assessment of the state of the art of the knowledge base, and what future studies may need to be done in order to provide a comprehensive set of guidelines for the implementation of non-intrusive alerts to operational contexts where such an approach is merited.\rPage 62 Non-Intrusive Alert System Humansystems®\r\nTable 15: Summary of literature relevance and need for research\rAlert Function\rComment\rConfigure alert parameters\rThere are some useful generic guidelines available to guide the design of interfaces that would allow operators to configure alert parameters and priorities. Examples of interfaces for rapid selection of rules and configuration are available.\rResearch question - what is the appropriate number of alert priorities?\rReceive information on alert state\rWhile there is abundant information on the various ways an alert can be brought to the attention of the operator, there is little research on how this can be done non-intrusively.\rResearch is needed to determine the relative intrusiveness of a range of design parameters pertaining to the alert size, location and dynamics.\rMore refined information processing models need to be developed to allow a better understanding of how attention and intrusiveness are related.\rComprehend alert condition\rThere is a sold base of information available in the general alarm/alert literature. In an RMP context, there is a need to examine trade-offs between implementing the information within the RMP window and/or providing a separate window.\rAction the alert condition\rBeyond the scope of the present project\rManage Alerts\rSome useful general principles are found in the literature.\rIn the context of the RJOC, analysis needs to be performed of the information requirements for an alert management system for data anomalies. May also need to consider how this would integrate within the overall “alert” system within the centre.\rMaintain primary task\rAssessment of interruptibility\rThe technology for this is not proven and has the potential for creating adverse operator reactions. The human factors literature in this area was not a priority for this project. However, computer scientists have been interested in this problem and have developed some interesting models.7\rWarning of impending alerts\rThis literature is probably of more relevance to more highly dynamic information environments than the RJOC.\rMethods for return to primary task\rGeneral principles for rapid regain of situation awareness are applicable.\rResearch questions: should RMP be refocused/ranged as before the alert was serviced? Does changing RMP focus between primary task and actioning the alert cause loss of situation awareness. Is there a need to alert operators to any change in RMP status on return from alert?\r7 See, for example, Gievska, S. and Sibert, J. Using task context variables for selecting the best timing for interrupting users. ACM International Conference Proceedings, vol 121, 2005.\rHumansystems® Non-Intrusive Alert System Page 63\r\nThis page intentionally left blank.\rPage 64 Non-Intrusive Alert System Humansystems®\r\n5. The Design Development Process 5.1 Development of design concepts\rBased on a review of the literature, and bounded by the primary focus of the project, we concentrated on developing design ideas for two primary functional elements of an alert system:\r1. An alert indicator which appears superimposed upon the GCCS window and is designed to advise of new alerts non-intrusively; and\r2. An alert information window (AIW) which allows operators to obtain details on specific alerts and to manage alert lists.\r5.1.1 The alert indicator\rWith factors outlined in the previous section in mind, we developed design concepts that map onto three levels of importance of the information that triggers the alarm. Our discussions with an SME and our own analysis of potential requirements suggest that three priority levels represent an appropriate balance of information categories.\rFor each information category (i.e., attribute, movement and VOI related) we have provided operational examples of RMP anomalies taken from Davenport (2008) which have been sorted into the three priority levels by an SME. There are four basic information requirements for all alert concepts:\r1. To bring to the attention of the operator that an alert has occurred;\r2. To provide an indication of the alert priority;\r3. To provide a cumulative numerical indication of how many outstanding alerts are in the system (i.e., have not been processed or cleared); and\r4. To update the cumulative alert indicator when an alert has been cleared.\rThere are several coding schemes that could be potentially used to visually indicate urgency level, including factors such as:\r• Colour stereotypes\r• Colour/luminance increments from the background\r• Size of the alerting stimulus\r• Rate of flashing\r• Locus on the display\rConsiderations for the design of a visual alert indicator for each of the three priority levels are outlined in the next section.\rIn terms of other modalities to indicate alerts, auditory alarms were not considered as we concluded that the perceptual deviation from the operator’s primary task (i.e., a visual monitoring task) would\rHumansystems® Non-Intrusive Alert System Page 65\r\nbe too great and they would therefore be too intrusive. Further, it is anticipated that auditory alarms would be disruptive to other operators in the RJOC.\rA tactile interface, on the other hand, may be feasible in that it can be isolated to the individual operator and therefore not disruptive to other operators. Furthermore, the intrusiveness of a tactile alert can be altered to imply a specific level of priority of information. Considerations for the design of a tactile alert indicator for each of the three priority levels are outlined in Section 5.1.3.\r5.1.1.1 Priority 1 alerts\rA priority 1 alert is an anomaly that is of critical significance. It will require operator attention at the earliest opportunity and may require temporary suspension of the primary task. The occurrence of the alert should be readily perceivable while an operator performs a primary task. The alert should clearly signal that it is of high priority. Some examples of priority 1 anomalies are:\r• Grab and dash fishing - a foreign fishing boat moves from the international zone to Canadian waters (where it is forbidden from fishing) for a few hours just before leaving for its home port.\r• Not heading for port - a vessel is heading in a direction where there is no harbour, or is not heading toward its declared destination. Cargo and Ferry vessels always go from one port to another port, and generally by the shortest available route.\r• Changes destination - a cargo ship changes course in mid-journey or possibly even reverses it’s heading and returning to port.\r• Heading into danger - a ship is heading toward a natural obstacle such as ice or non- navigable water.\r• Regulatory infraction - a ship enters, without permission, a regulated zone such as the Northwest Passage, where ships must register their plans and receive permission to proceed.\r• Infringing a closed zone - a ship is in a zone of the ocean that is closed to its type of commercial activity, whether for environmental, wildlife protection, or national security reasons.\r5.1.1.2 Priority 2 alerts\rA priority 2 alert is an anomaly for which the related information is important but can be dealt with as soon as primary task activity permits. The occurrence of the alert should be readily perceivable while an operator performs a primary task. The alert should clearly signal that it is of intermediate priority. Some examples of anomalies are:\r•\r• •\rUnexplained high speed - a ship that is claiming (e.g., in call-ins or on AIS) to be a normal merchant ship suddenly starts travelling at a high speed more typical of a passenger ship or warship.\rSpeed too slow - a Cargo, Passenger, or Ferry is observed going slowly. As these vessels generally go as fast as they safely can, it may be an indicator of a problem.\rLoitering - a cargo ship stops outside of or far from a harbour, or steams very slowly, rather than proceeding directly into port.\rPage 66\rNon-Intrusive Alert System Humansystems®\r\n• •\r• •\r5.1.1.3\rOutside historical route - a ship that historically follows a consistent route, is deviating or slowing down for no apparent reason.\rOutside shipping lane - a ship that should be in a shipping lane is instead travelling outside the lane. Ships approaching port enter a “Vessel Traffic Management” zone and are required to stay within designated shipping routes.\rZone mismatch to activity - a ship’s location does not match its claimed activity, where that activity can only be carried out in specific regions of the sea, due either to regulations or to physical requirements of the activity itself.\rLittoral rendezvous - many small crafts converge on a larger ship, and then the small crafts spread out at high speeds to many different ports.\rPriority 3 alerts\rA priority 3 alert is an anomaly for which the related information is less important and will be attended to as time and resources permit. The occurrence of the alert should not be as readily perceivable as priority 1 and 2 alerts and should not draw attention to its occurrence. It should be perceivable only when the operator needs to check for alerts. The perceptual properties of the alert should clearly indicate that it is of lowest priority. Some examples of priority 3 anomalies are:\r• Track ends - a ship track ends in mid-ocean. A ship track will normally not end, except at a harbour or by the ship leaving Canadian waters.\r• Proximity to infrastructure - a ship approaches or loiters around Canadian infrastructure, such as oil production equipment, sub sea pipelines, communication cables, etc.\rAgain, we want to emphasize that the above does not constitute a definitive set of anomalies that may be of interest, nor is the specific priority classification being recommended for adoption. These assumptions were made simply to facilitate the development of design concepts.\r5.1.2 Visual design concepts\rMany of the concepts to be described have used colour and/or luminance coding as one basic approach to differentiating priority. Based on existing population stereotypes, the priority coding is as follows:\r• Priority 1: red sector of the spectrum\r• Priority 2: yellow-orange sector of the spectrum\r• Priority 3: unsaturated, neutral areas of the spectrum (e.g., grey)\rThe use of red is considered acceptable, even though red is used in GCCS to code VOI, hostile and suspect tracks, since there is unlikely to be any possible confusion because of where the red alert is located and the shape and context in which it appears.\rA second general principle is to locate the alerting stimulus in the periphery of the display towards the right. We did consider locating it on the menu bar at the top of the screen, but this area is already cluttered and we believed that the spatial separation from the menu bar would in fact encourage cognitive separation, so that typical menu intensive tasks would not be compromised by the adjacent proximity of alerts. Similarly, the alerts themselves would be more salient (enough to capture the operator’s attention without being intrusive) by being spatially separated.\rHumansystems® Non-Intrusive Alert System Page 67\r\nFive different designs have been created and will be discussed in details next.\r5.1.2.1 Design 1: Cumulative Total Indicator\rFigure 6: Design 1: Cumulative Total Indicator\rThe alert category is indicated by colour. For priority 1 alerts, the number in the red box increments and the box blinks at rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the Alert Information Window (AIW). When the operator has finished processing the alert, either one of two conditions exist. One, the alert has been dealt with and is no longer of interest, in which case the counter resets to n (number of current outstanding alerts) -1. Or two, the alert remains in the system, and the indicator no longer flashes and stays at the current value, in which case the next alert would increment this value and flash until attended to by the operator.\rFor priority 2 alerts, the number in the yellow box increments and the box blinks at an approximate rate of .25-.5 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. The remaining functionality is the same as a Priority 1 alert.\rOn a new priority 3 alert, the number in the grey box simply increments. When the operator has processed the alert, the number decreases to n-1.\r000\rPage 68 Non-Intrusive Alert System Humansystems®\r\n5.1.2.2 Design 2: Vertical Cumulative Indicator\rFigure 7: Design 2: Vertical Cumulative Indicator\rThis design concept is similar to design 1, except the count of outstanding alerts is indicated by a vertical progress bar. The above example show several outstanding alerts in all three priority categories. A vertical scale provides an indication of the number of alerts.\rA priority 1 alert is shown in this design by the number 1 blinking red at a rate of 2 Hz and the associated red vertical bar incrementing in height. The flashing of the number 1 to red continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. When the operator has finished processing the alert, one of two conditions will exist. The alert has been dealt with and is no longer of interest, in which case the vertical bar decrements by one unit, or the alert remains in the system, and the indicator bar stays at the current height; in either case the box background reverts to grey. The next alert would again increment the height of the bar and the box would flash red again until attended to.\rA priority 2 alert is shown in this design by the number 2 in the second box blinking yellow at an approximate rate of .25-.5 Hz. and the vertical bar incrementing in height. The flashing continues until the operator acknowledges the alert by clicking on it. The remaining functionality is the same as a Priority 1 alert.\rOn a new priority 3 alert, the vertical bar simply increments. When the operator has processed the alert, the number decreases to the current outstanding number minus 1, or remains the same if the alert is not deleted.\r10 5 0\rHumansystems® Non-Intrusive Alert System Page 69\r\n5.1.2.3 Design 3: Horizontal Indicator Bar\rFigure 8: Design 3: Horizontal Indicator Bar\rThe three alert categories are indicated along the bottom of the display, segregated by position and colour coding. It was assumed that on the east coast contacts on the left hand side of the screen are generally considered higher priority than those on the right side of the screen simply because they are closer to land. For this reason we suggest that priority 1 alerts are situated on the left hand side of the screen for east coast RJOC operators while, for operators on the west coast, high priority alerts are positioned on the right side. That is, the design will be coast dependent. This assumption should be validated with both east and west coast operators.\rA priority 1 alert is indicated in this design by the first empty rectangle in the left most group turning red and blinking at a rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. Again, when the operator has finished processing the alert, one of two conditions will exist. The alert has been dealt with and is of no longer interest, in which case the box is no longer filled with red, or the alert remains in the system, and the box no longer flashes and stays filled. In which case, a new alert would result in the next horizontal box blinking red until attended to by the operator.\rA priority 2 alert is indicated in this design by the first empty rectangle in the middle group turning light yellow and blinking at a rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. The remaining functionality is the same as a Priority 1 alert. As the number of outstanding alerts in this category increases (seven boxes are filled), the colour changes from light yellow to orange.\rOn a new priority 3 alert, a grey box in the right most group is filled. When the operator has processed the alert, the fill is removed from the box, or remains the same if the alert is not deleted.\rPage 70 Non-Intrusive Alert System Humansystems®\r\nAs the number of outstanding alerts in this category increases (seven boxes are filled), the colour changes from light grey to dark grey.\r5.1.2.4 Design 4: Ticker and Fading Bar\rFigure 9: Design 4: Ticker & Fading Bar\rThis design is similar to Design 2 in that the count of outstanding alerts is indicated by a vertical progress bar with a scale to provide an indication of the number of alerts in the system (i.e., unaddressed). In addition, individual incoming priority 1 and 2 alerts are indicated by a bar appearing at the bottom of the screen.\rOn a new priority 1 alert, a red bar appears across the bottom of the screen with a message scrolling from right to left indicating that there is a new priority 1 alert. A brief description of the type of anomaly is also provided (e.g., contact veering off-course). An unacknowledged priority 1 alert would also increment the height of the red bar in the counter on the bottom right of the screen, showing an increase in the number of active priority 1 alerts in the system. The red bar is present and the scrolling continues until the operator acknowledges the alert by clicking it. The number of active priority 1 alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\rOn a new priority 2 alert, an orange bar with text indicating that there is a priority 2 alert fades in and out of the bottom of the screen at a rate of 5 Hz. The bar and message remain on the screen for approximately 2 seconds before fading away and then returning again until the alert is acknowledged by the operator (by clicking on it). An unacknowledged priority 2 alert would also increment the height of the orange bar in the counter on the bottom right of the screen, showing an\rPriority 2 Alert\r10 5 0\rHumansystems® Non-Intrusive Alert System Page 71\r\nincrease in the number of active priority 1 alerts in the system. The number of active priority 2 alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\rOn a new priority 3 alert, the height of the grey bar in the counter at the bottom right of the screen would increment by one indicating an increase in the number of active priority 3 alerts. The operator would only notice this increment if he/she was looking directly at the counter at the moment it increments. Therefore, the operator would be required to intentionally seek out active priority 3 alerts rather than directly being made aware of new alerts. The number of active low priority alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\r5.1.2.5 Design 5: Polygon\rFigure 10: Design 5: Polygon\rThis design is based on principles of ecological interface design in that it is intended to represent both the desired state of the system (i.e., no active alerts in the system) as well as the current state of the system (i.e., the presence of active alerts) in a way that is easily and quickly perceived by the operator. The solid green triangle signifies the desired state (i.e., no active alerts) while the dotted triangle represents the actual state (i.e., if there are active alerts and if so, what type priority of alert). As alerts accumulate, the dotted triangle moves along the axes for which there are alerts. The X-axis (red in colour and labelled with a “1”) represents priority 1 alerts; the Y-axis (orange in colour and labelled with a “2”) represents priority 2 alerts; and the Z-axis (coloured grey and labelled with a “3”) signifies priority 3 alerts. If there are an equal number of active priority 1, 2 and 3 alerts, the dotted triangle is an isosceles triangle; if there are unequal numbers, the dotted triangle becomes skewed toward the priority level for which there is the most active alerts.\r2\r13\rPage 72 Non-Intrusive Alert System Humansystems®\r\nOn a new priority 1 alert, the border of the dotted triangle skews toward the priority 1 axis showing an increase in the number of high priority alerts. The dotted triangle also turns red and flashes at a rate of approximately 5 Hz. The triangle remains red and flashing until the operator acknowledges the alarm by clicking it. The number of active priority 1 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\rUpon an incoming priority 2 alert, the border of the dotted triangle skews toward the priority 2 axis showing an increase in the number of medium priority alerts. The dotted triangle also turns orange and flashes at a rate of approximately 0.5 Hz. The triangle remains orange and flashing until the operator acknowledges the alert by clicking it. The number of active priority 2 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\rOn a new priority 3 alert, the border of the dotted triangle skews toward the priority 3 axis showing an increase in the number of low priority alerts. The dotted triangle also turns grey but does not flash. The operator would only notice this increment if he/she was looking directly at the display at moment it changes shape and colour. Therefore, the operator would be required to intentionally seek out active priority 3 alerts rather than directly being made aware of new alerts. The number of active priority 3 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\r5.1.3 Tactile design concept\rThis design concept assumes that the RJOC operator could carry a pager-type device that would transmit the alerts. The design is based on a brief review of literature on the use of tactons for mobile phone alerts to imply priority (Brown & Kaaresoja, 2006). Generally, priority can be implied by the number, duration and intensity of pulses. That is, higher priority alerts would be indicated by more, longer and more intense pulses.\rOn a new priority 1 alert, the pager would receive two pulses of approximately 30 milliseconds in duration. The intensity of the pulse would be approximately 1.38 V. The pulses would repeat until the operator acknowledges the alarm by clicking it. Interrogating and processing the alarm would have to be done on the operator’s computer workstation.\rOn a new priority 2 alert the pager would receive one pulse of approximately 10 milliseconds in duration. The intensity of the pulse would be approximately 0.98 V. The pulse would repeat until the operator acknowledges the alarm by clicking it. Interrogating and processing the alarm would have to be done on the operator’s computer workstation. The operator would not receive priority 3 alerts via the pager. He/she would therefore have to be at their workstation looking at the screen and intentionally seeking out priority 3 alerts.\r5.1.4 The alert information window\rThe purpose of the Alert Information Window (AIW) window is to provide the operator with specific information about the alert details and to manage alert lists. It is not a window for problem solving or analysis which we assume will take place using existing functionality in the RJOCs.\rFor the AIW, only visual designs were considered. An initial design for an AIW window is shown in Figure 11.\rHumansystems® Non-Intrusive Alert System Page 73\r\nPriority 1\rRMP RMP RMP\r3\r+ Priority2 8 + Priority3 9\r#Alerts Track#\rName\r4689 CP SPIRIT\r3193 EAGLE BOSTON\r4745 OVERSEAS SILVAMAR\rAlert\rHeading into danger\rChanges destination Grab and dash fishing\rClear\rTime 09:48\r10:17 14:21\rDone\rFigure 11: Alert Information Window\rThe window comprises separate areas for each alert priority with the priority level colour coded. Within each category, there is a field for the number of alerts in the system. The information provided includes track number, name of the contact, the specific nature of the anomaly, and time of the report that gave rise to the anomaly.\rThe above example shows what would happen if the operator had selected a priority 1 alert in the RMP window. The priority 1 section is expanded to show all alerts and the most recent alert, that caused the alert indicator to flash, is highlighted. It is important to note that the operator could select another alert in the priority 1 list or another alert category (in which case that section would be automatically expanded to show all of the tracks within that category). At this point the operator may choose to refer back to the RMP to see the location of the alert and the context by clicking the RMP button on the track line. When this happens, the GCCS RMP window is brought back showing the contact in question highlighted, as shown in Figure 12 (see concentric broken circle around the contact of interest in the upper part of the RMP).8\r8 In the PowerPoint presentation for the SMEs, this concentric circle shrinks and expands dynamically around the track symbol to better enable the operator to locate the track in question.\rPage 74 Non-Intrusive Alert System Humansystems®\r\n10 5 0\rFigure 12: RMP track highlight\rWhen the operator has finished analysis on the track of interest, she/he can return back to the AIW (by clicking on the AI indicator) and decide to either leave the track in the system or to clear the alert by way of the CLEAR button. This would then result in the track being deleted from the alert list, as shown in Figure 13.\rFigure 13: Alert Information Window after a track is deleted\rThe operator may choose to process other alerts in the system, or return to the RMP via the button DONE at the bottom of the window.\rHumansystems® Non-Intrusive Alert System Page 75\r\n5.1.5 The RMP pop up box\rThe purpose of this box is to provide the operator with a shortened version of the information about the alert. The AIW provides full information regarding each alert and allows operators to manage all the alerts in the system. The RMP pop up box on the other hand, allows operators a quick and easy method to clear an alert or leave it in the system. The pop up box appears when the operator acknowledges (i.e. clicks on) the indicator for an incoming alert. This was designed as a second method to present operators with relevant information. The research team did not come across any literature regarding this concept. As shown in Figure 14, a pop up box was designed to included the name of the vessel (e.g., Eagle Boston), reason for the alert (e.g., Not heading to port), and time of alert (e.g., 21:15), as well as action buttons (i.e., Clear or Leave).\rs\rFigure 14: RMP pop up box\rThe box’s border would be outlined in the same colour as the priority of the alert, in this case, red for priority 1. If the operator chose to CLEAR the alert, the system would delete the alert and the counter would return back to the previous number, in this case, back to 1. If the operator chose to leave the alert in the system, the counter would remain the same, in this case, stay at 2.\rEAGLE BOSTON: NOT HDNG TO PORT:21:15\rCLEAR\rLEAVE\r10\r5\r0\rPriority 1 Alert – Ves\rPage 76 Non-Intrusive Alert System Humansystems®\r\n6. Evaluation and Review of Design Concepts by Subject Matter Experts\r6.1 Preparation for Design Evaluation and Review\rFollowing the development of the design concepts, a consultative process was begun with the Scientific Authority to determine which concepts should be taken forward for review by Subject Matter Experts (SMEs). As a result, it was decided that the polygon design (because it was thought to non-intuitive) nor the tactile design (because it was considered impractical) would be explored further.\rIn preparation for the evaluation, the individual design elements (alert indicator, alert information window, alert track highlighter and RMP pop up box) were incorporated into a PowerPoint demonstration concept that would simulate the functionality of an alerting system. Each demonstration comprised a new alert initiation (all three priority levels considered sequentially), acknowledgment of the alert, getting information on the alert and clearing the alert.\rTo evaluate the designs, questionnaires were constructed to address issues such as the usability and utility of the design, ease of comprehension and overall preferences.\rAn ethical protocol was submitted to and approved by the Human Research Ethics Committee at DRDC Toronto. The approved ethics protocol is included in Annex A.\r6.2 Method\rThe following section outlines the methodology used in reviewing and evaluating the anomaly alert system design concepts with SMEs.\r6.2.1 Date and Location of SME Evaluation\rSME evaluations were conducted in the MAPLE lab at DRDC Atlantic from February 23-26, 2009.\r6.2.2 Participants\rSeven Subject Matter Experts (SMEs) were recruited to participate in an assessment of the non- intrusive alerting designs. There were 2 Lt(N), 2 retired CF personnel, 2 operators, and a civilian. Combined related experience included a Common Operational Picture Officer, Watch Officer, Surveillance Officers, Bridge Watch Keeper, and Surveillance Database Operators. There were 6 men and 1 woman.\r6.2.3 Materials\rTrial participants were presented with PowerPoint representations of the various design concepts, which demonstrated with animation how the basic functionality would work. The concepts were presented in the following order:\r1. Cumulative total indicator with pop-up window (Figure 6)\rHumansystems® Non-Intrusive Alert System Page 77\r\n2. Cumulative total indicator with AIW (Figure 6)\r3. Vertical cumulative indicator with pop-up window (Figure 7)\r4. Vertical cumulative indicator with AIW (Figure 7)\r5. Horizontal indicator bar with pop-up window (Figure 8)\r6. Horizontal indicator bar with AIW (Figure 8)\r7. Ticker and fading bar with pop-up window (Figure 14)\r8. Ticker and fading bar with AIW (Figure 9)\rAll participants reviewed the design concepts in the same order which means that a potential order effect could not be determined. However, the presence or absence of an order effect was believed to be inconsequential for this exploratory research.\rPart of the evaluation of the designs was accomplished by a five part questionnaire. The first section included demographic questions such as name, rank, number of years experience, etc. The second part of the questionnaire included 15 questions related to the usability and usefulness of a non-intrusive alerting system in general. SMEs were asked to select the rating (on a 5-point scale) they felt most appropriate. Example questions included “These alerts would enhance our knowledge of anomalies” and “This alerting system would be difficult to use.”\rThe third section of the questionnaire assessed participant’s attitudes towards each non-intrusive alert design. Again, SMEs were asked to select the rating (on a 7-point scale) they felt most appropriate. Example questions included “The number of alerts was easy to comprehend” and “The priorities of the alerts were easy to comprehend.”\rThe fourth part of the questionnaire assessed the level of preference for each alert design across three dimensions; overall effectiveness in bringing alerts to the operator’s attention, the method in which different alert priorities are presented, and the degree to which all of the required information about an alert is presented. For each dimension, participants were asked to rank each of the four designs; with ‘1’ indicating the most preferred and ‘4’ the least preferred.\rThe final section of the questionnaire included open-ended questions related to each alert design. For each alert design, participants were asked if they thought the design should be implemented (Yes=1, No=2). They were then asked to list their likes and dislikes for each design. For the final question in this section, participants were asked to rate the intrusiveness of the design based on a 7- point scale (where 1=Not at all Intrusive, 7=Extremely Intrusive). Details of the questionnaires and interview questions can be found Annexes B and C.\r6.2.4 Procedure\rEach SME participated individually in a walkthrough of each of the designs using the PowerPoint presentation on a computer screen, and then completed the usability questionnaire and answered a number of follow up questions in an interview. Sessions lasted on average one hour.\rTwo members of the research team lead the walkthroughs, one leading the process and the other taking notes and audio recording participant responses. Participants were first asked to read a pre- experiment information sheet and then read and sign a voluntary consent form (see Annex A). A member of the research team then described the goals of the project as well as goals of an alert system in general. She then presented a definition of non-intrusive alerts, an overview of the alert\rPage 78 Non-Intrusive Alert System Humansystems®\r\ndesigns, and a basic description of the method to be used for the walkthrough. After the introduction, participants were asked if they had any questions before they were shown each alert design in detail. A member of the research team then walked participants through the four alert designs, each with the RMP pop-up box and then with the AIW. At different points during the walkthrough, participants were questioned relating to the topics such as priority levels, setting alert trip points and parameters for alerts. Participants were allowed to ask questions and make comments throughout the walkthroughs.\rAfter the walkthroughs, participants were asked to complete the five part questionnaire. Participants were then asked some general follow up questions that were not previously addressed during the walkthroughs. If desired, participants were able to revisit the different design concepts during the questionnaire and general discussion phases.\rThe interviewers used the following points to guide the discussion, asking questions when necessary (depending on what had already been discussed during the presentation of the design concepts):\r• Operator’s attendance at their desk\r• Frequency of alerts (by priority)\r• Priority 1 alerts being intrusive\r• Ignoring alerts\r• Priority indications\r• Alert colour scheme (e.g., red, orange, yellow, grey)\r• Font (e.g., size, colour)\r• Terminology\r• Alert Information Window\r• Intuitive versus not intuitive\r• Intrusiveness of alerts\r• Auditory/tactile alerts\r• Flexibility in the location of ticker and horizontal indicator bar\r• RMP centred on contact\r• Information in the RMP pop up box\r• Ability to return to primary task\r• Shift change over problems\rA summary of individual participant responses are provided in detail in Section 6.2.3.\r6.3 Results\r6.3.1 Questionnaire data\rThe questionnaire data are divided into two sections: quantitative and qualitative. With the exception of the intrusiveness ratings, the majority of questionnaire responses were ratings on a 5- point scale. A 7-point scale was used for intrusiveness ratings as the perception of intrusiveness was of primary concern for this project and so it was thought that detecting finer distinctions between people would be desirable. Results suggest, however, that a 5-point scale would likely have been suitable. In addition to rating scales, participants were also asked to rank the designs, and provide their likes and dislikes of each design in an open ended format.\rHumansystems® Non-Intrusive Alert System Page 79\r\n6.3.2 Quantitative data\r6.3.2.1 General Non-Intrusive Design Questions\rThe Usability and Usefulness Questionnaire assessed participants attitudes on the usefulness of a general non-intrusive alerting system (see Table 16).\rTable 16: Mean ratings for Usability and Usefulness of Non-Intrusive Alerting System\rStatement\rAnswer9\rMean\rSt. Dev.\rRange (scale = 1-5)\rThese alerts would enhance our knowledge of anomalies\rStrongly agree\r4.6\r.53\r4-5\rThis alerting system would be used on a daily basis\rStrongly agree\r4.9\r.38\r4-5\rTasks can be performed in a straightforward manner using this alerting system\rStrongly agree\r4.6\r.79\r3-5\rThe thinking required to use this alerting system requires significant effort\rDisagree\r1.9\r.38\r1-2\rThis alerting system would be difficult to use\rStrongly Disagree/Disagree\r2.0\r1.41\r1-5\rThis alerting system will improve my situation awareness\rAgree\r4.4\r.53\r4-5\rThis alerting system would make it easier to identify anomalies\rStrongly agree\r4.7\r.49\r4-5\rI would find this alert system useful\rDisagree\r1.7\r.49\r1-2\rI would not ignore alerts while using this technology\rAgree\r4.0\r1.00\r2-5\rThe Alert Information Window (AIW) was confusing\rDisagree\r2.1\r.38\r2-3\rIt was easy to learn how the AIW was represented\rStrongly agree\r4.7\r.49\r4-5\rThe AIW had all the necessary information\rDisagree\r2.9\r1.07\r2-4\rIt was easy navigating between the RMP and the AIW\rAgree\r4.1\r.38\r4-5\rI prefer clearing and deferring alerts directly from the RMP\rUndecided\r3.1\r1.07\r2-5\rI prefer using the AIW to clear or defer alerts\rUndecided\r3.1\r1.07\r2-5\rAs shown in Table 16, participants showed generally favourable attitudes towards a non-intrusive alerting system. Specifically, participants strongly agreed that the non-intrusive alerting system would enhance their knowledge of maritime anomalies (Mean = 4.6), be used on a daily basis (Mean = 4.9), perform tasks in a straightforward manner (Mean = 4.6), and make it easier to\r9 Scale descriptor is based on the most frequent rating (mode).\rPage 80 Non-Intrusive Alert System Humansystems®\r\nidentify anomalies (Mean = 4.7). Despite these positive answers, participants reported that they would not find the alert system useful (Mean = 1.7). This rating is somewhat surprising and participant comments shed no light on the reasons that may have led to this rating. Therefore, the way in which operators consider the potential usefulness of an alerting system for maritime anomalies clearly needs to be addressed in future work. Participants were also undecided about whether the Alert Information Window (AIW) had all the necessary information and whether they preferred clearing alerts directly from the RMP or the AIW. Follow up interview questions covered these issues and will be discussed in Section 6.2.4.\r6.3.2.2 Specific Design Questions\rThe rest of the questions assessed the participant’s attitudes regarding each specific alerting design. The following table shows the means and standard deviations for each question.\rTable 17: Mean Ratings for the Alert Design Questions\rStatement & Design\rAnswer10\rMean\rSt. Dev.\rRange (scale = 1-5)\rThe number of alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\rStrongly agree\r4.7\r.49\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.69\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.9\r1.07\r2-5\rDesign 4: Ticker & Fading Bar\rAgree\r4.1\r.06\r3-5\rThe presence of an alert was easy to recognize\rDesign 1: Cumulative Total Indicator\rAgree\r4.3\r.49\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.90\r2-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.7\r1.25\r2-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree\r4.4\r1.13\r2-5\rThe priorities of the alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\rStrongly agree\r4.6\r.53\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.4\r.53\r4-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.9\r1.07\r2-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree\r4.4\r.79\r3-5\rIt was easy to find the relevant anomaly\rDesign 1: Cumulative Total Indicator\rAgree\r4.1\r.69\r3-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.0\r.58\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r4.1\r.69\r3-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree/Agree\r4.3\r.76\r3-5\rIt was easy to find information on anomalies\rDesign 1: Cumulative Total Indicator\rAgree\r4.3\r.49\r4-5\r10 Scale descriptor is based on the most frequent rating (mode).\rHumansystems® Non-Intrusive Alert System Page 81\r\nStatement & Design\rAnswer10\rMean\rSt. Dev.\rRange (scale = 1-5)\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.38\r3-4\rDesign 3: Horizontal Indicator Bar\rAgree\r4.0\r.58\r3-5\rDesign 4: Ticker & Fading Bar\rAgree\r4.1\r.69\r3-5\rThe alerting design enhanced my situation awareness of maritime anomalies\rDesign 1: Cumulative Total Indicator\rAgree\r4.1\r.38\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.0\r.58\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r4.0\r.58\r3-5\rDesign 4: Ticker & Fading Bar\rStrongly agree\r4.6\r.53\r4-5\rThe appearance of the alerts is compatible with my current interface\rDesign 1: Cumulative Total Indicator\rStrongly Agree\r4.0\r1.15\r2-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.6\r.98\r2-5\rDesign 3: Horizontal Indicator Bar\rUndecided/Agree\r3.3\r.76\r2-4\rDesign 4: Ticker & Fading Bar\rAgree\r4.0\r1.00\r2-5\rA parametric one-way Analysis of Variance (ANOVAs) and non-parametric tests were conducted and revealed no significant differences in ratings between the four alert designs for any of the above questions. As a result, this section presents only observations on the trends in the data, which need to be validated through further experimentation.\rParticipants rated the Cumulative Total Indicator as the easiest to comprehend, while the Vertical Cumulative Indicator and the Horizontal Indicator Bar were equally more difficult to comprehend. The presence of new alerts was easiest to recognize in the Ticker and Fading Bar, followed by the Cumulative Total Indicator, the Vertical Cumulative Indicator, and then the Horizontal Indicator Bar. Alert priorities in the Cumulative Total Indicator were easiest to comprehend, and most difficult in the Horizontal Indicator Bar. The results also suggest that participants found that the Ticker and Fading Bar was the easiest design in which to find the anomaly, while the Vertical Cumulative Indicator was the most difficult. The Cumulative Total Indicator was rated the easiest to find the relevant information pertaining to the anomaly, while the Vertical Cumulative Indicator was rated the hardest to find the information. The Ticker and Fading Bar enhanced the operator’s situation awareness of maritime anomalies (i.e., incoming alerts and total number of active alerts in the system), while the Vertical Cumulative Indicator and the Horizontal Indicator Bar were tied for least likely to enhance situation awareness. Lastly, the Cumulative Total Indicator and the Ticker and Fading Bar were rated the most compatible with the current interface, while the Horizontal Indicator Bar was rated the least compatible.\rParticipants were also asked to rank the designs in order of preference (i.e., 1, 2, 3 and 4) from the perspective of alert priorities (Figure 15), the degree to which the design grabbed the attention of the operator (Figure 16), and providing relevant information without distraction (Figure 17). All three figures show a frequency count of the specific rank for each of the four alert design concepts.\rPage 82 Non-Intrusive Alert System Humansystems®\r\n6 5 4 3 2 1 0\r1234\rEffectiveness Ranking\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 15: Subject rankings for effectiveness in bringing alerts to my attention\r6 5 4 3 2 1 0\r1234\rPriority Rankings\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 16: Subject rankings for methods for representing the priorities\rHumansystems® Non-Intrusive Alert System Page 83\rFrequency Frequency\r\n4.5 4 3.5 3 2.5 2 1.5 1 0.5 0\r1234\rInformation Rankings\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 17: Subject rankings for providing required information without distraction\rKruskal-Wallis ANOVA and Chi-Square analyses revealed no significant differences between the four alert designs in terms of effectiveness in bringing alerts to the operator’s attention, methods of representing priorities and providing required information without distraction. Though not statistically significant, a trend analysis suggests that the Ticker and Fading Bar was the preferred design for all of these three features while the Vertical Cumulative Indicator was the least preferred. Further empirical research is needed to verify this trend.\r6.3.2.3 Implementation and intrusiveness\rParticipants were asked if each design should be implemented and they were also asked to rate the intrusiveness, as shown in Table 18.\rPage 84 Non-Intrusive Alert System Humansystems®\rFrequency\r\nTable 18: Mean ratings for design implementation and intrusiveness\rStatement & Design\rAnswer11\rMeans\rSt. Dev.\rRange\rShould the design be implemented\rDesign 1: Cumulative Total Indicator\rYes\r1.3\r.49\r1-2\rDesign 2: Vertical Cumulative Indicator\rNo\r1.9\r.38\r1-2\rDesign 3: Horizontal Indicator Bar\rNo\r1.6\r.53\r1-2\rDesign 4: Ticker & Fading Bar\rYes\r1.0\r.00\r1\rIntrusiveness12 (scale = 1-7)\rDesign 1: Cumulative Total Indicator\rNot at all intrusive\r2.1\r.90\r1-4\rDesign 2: Vertical Cumulative Indicator\rSomewhat intrusive\r2.9\r1.07\r1-4\rDesign 3: Horizontal Indicator Bar\rSomewhat intrusive\r3.7\r1.60\r1-5\rDesign 4: Ticker & Fading Bar\rSomewhat intrusive\r3.1\r1.46\r1-5\rA one-way parametric ANOVA and non-parametric tests revealed no significant differences between the four alert designs in terms of which design should be implemented or the intrusiveness of each design. Though not statistically significant, all participants felt that the Ticker and Fading Bar (Mean = 1.0) should be implemented and all but two participants felt that the Cumulative Total Indicator (Mean = 1.3) should be implemented. On the other hand, only one participant indicated that the Vertical Cumulative Indicator (Mean = 1.9) should be implemented and three felt that the Horizontal Indicator Bar (Mean = 1.6) should be implemented13.\rAlthough not statistically significant, participants rated all four designs as not at all intrusive or somewhat intrusive. The horizontal indicator bar was rated the most intrusive and the cumulative total indicator the least intrusive. Interestingly, as noted above, all participants felt that the Ticker and Fading Bar should be implemented even though it did not receive the least intrusive rating.\rIn summary, the Cumulative Total Indicator and the Ticker and Fading Bar were rated more favourably than the Vertical Cumulative Indicator and the Horizontal Indicator Bar. Specifically, in comparing the rank ordering of the designs, the Ticker and Fading Bar was preferred over all other designs, while the Vertical Cumulative Indicator was the least preferred. What is interesting to note is the Ticker and Fading Bar includes the Vertical Cumulative Indicator scale with the only difference being priority colour in the boxes. Based on discussions, participants preferred the actual ticker/fading bar that appeared at the bottom of the screen, rather than the scale, which will be discussed later.\r6.3.3 Qualitative data\rThe following table shows the participants’ qualitative assessments of each alert design.\r11 Scale descriptor is based on the most frequent rating (mode).\r12 The intrusiveness was on a 7-point scale (1 = Not at all intrusive, 4 = Somewhat intrusive, 7 = Extremely intrusive)\r13 Yes was considered a 1, while No was considered a 2.\rHumansystems® Non-Intrusive Alert System Page 85\r\nTable 19: Likes and Dislikes of Alert Designs\rDesign\rLikes\rDislikes\rComments\r1: Cumulative Total Indicator\rSimple and clear; uses less screen; numeric total subtle but effective\rNo hover feature for listing alerts without leaving RMP\rThis feature did not exist for any of the design concepts, but could be readily implemented.\rUnobtrusive; not overbearing but available as a quick visual reference\rThis is not good for operators that sit for hours in front of this system\rComments made during the discussion suggest that this design may be more easily ignored by operators especially over long periods\rExact priority of each alert; unobtrusive display\rNot as immediately visible as the ticker/bar\rCompact, clear number\rSmall numbers could create a change in priority for an operator, contrary to command's need\rNumber boxes could easily be made larger\rDead simple\rNo immediate visual of number of each type of alert without focusing on small numbers. Also, location should be able to be moved at will\rThe size and location of alert could easily be changed in future versions. Also numbers could easily be made larger.\rDoes not fill the RMP with unnecessary info\r2: Vertical Cumulative Indicator\rVisual and numerical representation of alert number and type; small display\rNumber scale could be confused with alert totals or other data\rUnsure what is meant by this comment, especially “other data”\rNot too intrusive\rLocation should be able to be moved at will\rFairly simple\rDifficult to determine number of alerts in bar\rNot annoying\rScale would have to change as alerts increase\rCan become complacent\rDoes not give accurate info on alerts\rThis is not good for operators that sit for hours in front of this system\rComments made during the discussion suggest that this design may be more easily ignored by operators especially over long periods\r3: Horizontal Indicator Bar\rEasy to interpret; unsure if the colour saturation is prominent enough when alerts increase\rTakes up too much screen; scale changes with number s alerts; don’t like gradual colour change\rBut occupies about the same amount of screen as the ticker design.\rPage 86 Non-Intrusive Alert System Humansystems®\r\nDesign\rLikes\rDislikes\rComments\rCatches your eye allows for quick scan without refocusing attention from priority tasking\rNo number available for alerts; colour saturation can be hard to detect; takes up a lot of bottom screen space\rClear; it would be at the forefront of the operator’s mind\rTakes more space\rToo wide; won't notice overflow of alerts easily\rTakes up too much space on RMP screen\r4: Ticker & Fading Bar\rThis would also keep the operator on the ball\rStill has the vertical scale that could be confusing\rThis could easily be removed\rTicker for high priority alerts and ability for operator to see information on cause\rRecommended fading grey bar in and out (like priority 2 ticker) when priority 3 alert arrives\rBut this would produce some potential confusion between different alert priorities. In addition, priority 3 alerts are not defined as events that should immediately capture attention.\rDecreasing intrusiveness; wide area only used for incoming alerts\rDon't like ticker; fading bar should be option; some operators will like and some won't\rAllows immediate connection with track in RMP; less steps to view\rFade bar quite distracting; no real info contained\rHowever, this is no different from having a flashing indicator, as in the cumulative counter design.\rWith information in the bar, the operator does not have to change what they are doing\rThis is not really the case, as the operator will have to shift attention momentarily to process the information in the bar.\rMost prominent; easy to distinguish levels\rProminence may become a negative factor in a high frequency alert context.\rAs shown in Table 19, some of the dislikes were implementation issues and the designs could be easily changed or modified to accommodate the suggestions. There was a comment for the Vertical Cumulative Indicator that needs further explanation. The participant who said, “This is not good for operators that sit for hours in front of this system” and noted “.... the design is not stimulating enough for the mind”. The participant believed that the operator needs to be “stimulated” and their mind “needs to be active”, and believed that with Design 2, this would not happen.\rOverall, the Cumulative Total Indicator was evaluated as being non-intrusive and very simple to use and interpret. However, some participants felt that it was too small and could therefore be easily ignored by operators. It appears that there needs to be a compromise relating to the size of the alert such that it is large and salient enough to be noticed yet not too salient as to distract the operator from his/her primary task or become an annoyance.\rHumansystems® Non-Intrusive Alert System Page 87\r\nThe Vertical Cumulative Indicator was also rated as non-intrusive yet participants found the scale more confusing to interpret than the digital indicator in the Cumulative Total Indicator.\rOnly three participants liked some aspects of the Horizontal Indicator Bar, while most participants felt the display obscured too much of the screen. Although the Ticker and Fading Bar used up as much, if not more space than the Horizontal Indicator Bar, participants did not mention this issue for the former. It is clear that such inconsistencies may have biased or influenced the results. Hence, further research is required to explain these inconsistencies.\rThere were two components to the Ticker and Fading Bar design that did not exist with the other designs. First, the ticker and fading bar itself alerted operators to individual incoming alerts. Second, a counter similar to that of the Vertical Cumulative Indicator was used to depict the total number of active alerts in the system (i.e., alerts that hadn’t been addressed). It is not surprising then that participants generally did not like the scale (which is consistent with the comments on the Vertical Cumulative Indicator). A number of participants recommended that the ticker or fading bar be used for both priority 1 and 2 alerts (i.e., there was no need to have the ticker for only priority 1 alerts and the fading bar for priority 2 alerts). Most participants preferred the Ticker and Fading Bar because of the intrusiveness (i.e., it was easily noticed) and the information contained in the display bar. This result may be different, of course, if there are frequent alerts. Further research in an environment with a representative number of alerts should be conducted to validate these findings.\rAfter participants completed the questionnaires, they were asked some additional follow-up questions. These questions related generally to alerting parameters, information presentation in the RMP and potential challenges of an alerting system in general and of our visual designs.\r6.3.4 General discussion with participants\rThe following points were discussed with participants following the design walkthrough and administration of the questionnaire.\r6.3.4.1 Alert parameters\rFor the purpose of this study, we assumed that the alert parameters operators would want to set included area (co-ordinates), vessel type (e.g., fishing, warship, merchant, etc), speed, and alert priority (1, 2 or 3). Participants stated that they would also like to set alert parameters according to the activity of the vessel, vessel name, threat level (e.g., friendly, neutral, suspect), age of alert, type of anomaly (e.g., not heading to port), flag (i.e., country), course direction, estimated time of arrival, AIS number14, and next port of call. They emphasized that changing alert parameters should be done quickly and easily, and they should have the ability to change these parameters geographically depending on their area of interest at a given time. The example given was that if you were monitoring Europe’s coast, operators would not want alerts relating to all vessels in that area. Thus, the operators require the ability to quickly and easily set these alerts according to their current area of interest.\r14 AIS number is used to identify ship transponder to radio receiving station while MMSI number does the same for satellite receiving stations\rPage 88 Non-Intrusive Alert System Humansystems®\r\n6.3.4.2 Intrusiveness and priority\rParticipants agreed that the level of intrusiveness of an alert must vary according to the priority of the alert. For the purpose of this study, we assumed three priority levels would be appropriate and for the most part, participants agreed. A few participants suggested having 4 or 2 alert priorities instead of 3. Furthermore, several participants added the caveat that priority 3 alerts will be ignored 90% of the time.\r6.3.4.3 Inability to see visual alerts\rWhen asked how often operators could be away from their desks and the RMP, in turn, unable to be notified of alerts, participants reported that there are manning issues on a regular basis and therefore operators are needed elsewhere. Despite this challenge, participants did not like the idea of an auditory or tactile alert (e.g., vibrating pager) because, although the operator would be notified of the alert, he or she would not be able to get back to their workstation to deal with the alert. Furthermore, most participants felt that operators would elect not to use such a pager-type device. Rather, they felt that the visual alert should continue to be displayed on the RMP until the operator acknowledges it (as is the case with the design concepts presented)\r6.3.4.4 Workload\rIn terms of workload, participants admitted that this type of alert design would increase the operator’s workload; however, all believed the anomaly alerting system would be worthwhile because currently most such anomalies are missed.\r6.3.4.5 Colour coding\rAll the participants liked the colour scheme that was used to depict priority levels in the alert designs (i.e., red for priority 1, gold for priority 2 and grey for priority 3). In the current RMP, red, yellow and green are used, but participants felt that green would be inappropriate for priority 3 alerts. As one participant stated, “Green doesn’t communicate the right message. Green means go.”\r6.3.4.6 Readability\rParticipants felt that the size of the displays and fonts were generally acceptable, with the exception of two participants who felt that the numbers in the Cumulative Indicator were too small. Furthermore, one participant suggested that the font in the pop-up windows should be slightly larger.\r6.3.4.7 Location on the screen\rAll participants also liked the idea of having the ability to click and drag the anomaly alert display anywhere on the screen. The alert designs presented to the participants in the trial had the alert display either in the bottom right hand corner or covering the entire width of the bottom of the screen. Participants thought this may impair their situation awareness if they had to focus on the bottom of the RMP for any length of time. Similarly, the current RMP has the ability to centre around a vessel of interest or area of interest. Participants indicated that they would like this to be an option for the non-intrusive design as well. That is, they would like the ability to centre the picture on a contact for which there is anomalous information. This would allow an operator to understand the immediate area and then the surroundings around the contact of interest. One participant mentioned that the option to centre the RMP on the contact of interest could be\rHumansystems® Non-Intrusive Alert System Page 89\r\nimplemented as a button in the AIW. For example, clicking on the name of the ship could centre the RMP on the vessel of interest.\r6.3.4.8 Retrieving information on alert\rAs previously described, participants were shown two options to retrieve information about an alert. In one case, the RMP had a pop up box coming from the vessel of interest which included the name of the vessel, reason for alert (e.g., grab and dash fishing), the time of the alert, and action buttons (i.e., Clear or Leave). While most of the participants thought the RMP pop up box included enough information, one participant recommended that it include course, speed, latitude, longitude, Maritime Mobile Service Identity (MMSI) number (or the AIS #). The second way to get information about an alert was to go to the AIW which was on a separate screen. As designed, the AIW also functions as a tool to manage all alerts as it shows all the alerts in the system according to priority level, total number of alerts by priority, track number, name of vessel, reason for alert, time of alert, and action buttons (i.e., Clear, Done and RMP, which is a button that takes you back to the RMP). Participants recommended that the AIW also include who reported the alert (i.e., source), vessel type, flag, age of alert (instead of time of alert), time of last report (as well as the ability to go to that report), age of track, course, speed, and the history of the alert (e.g., if the vessel has had other alerts before such as speeding up when it should not). Participants mentioned that track number is not useful and should be exchanged for AIS number. It was also recommended that a smaller version of the AIW pop up in the RMP (similar to the pop-up box) rather than taking the operator to a separate screen as navigating away from the RMP can adversely impact their situation awareness.\rAdditionally, the alert designs included a flashing circle around the contact of interest, but only in the designs using the AIW (the pop-up window points to the contact of interest so there is no need for a circle). The circle was red for priority 1 alerts, gold for priority 2 alerts and grey for priority 3 alerts. Two options for this design were shown to participants. First, the circle appeared around the contact of interest as soon as the operator clicked on an incoming alert (i.e., before going to the AIW). The other option was to navigate to the AIW first then click on the RMP button to come back to the RMP on which the flashing circle would appear around the contact. All participants preferred when this circle appeared as soon as the alert was clicked (i.e., before going to the AIW). However, participants indicated that they generally preferred the pop-up window over the AIW as the source for anomaly information which means that the flashing circle would not be required.\rIn summary, for the seven participants there were eleven overall favourite designs, due to participants preferring two of the designs equally. Five of the seven participants preferred the Ticker and Fading Bar, four participants preferred the Cumulative Total Indicator, and one participant preferred the Horizontal Indicator Bar. There was only one participant who favoured the Vertical Cumulative Indicator; however, with the caveat that it had to be in conjunction with the Ticker and Fading Bar.\rThus, overall the Ticker and Fading Bar was favoured because it was the most prominent and participants liked the information provided in the ticker. The participants who favoured the Cumulative Total Indicator did so because it was the least intrusive and they could see the exact number of alerts in the system. The Vertical Cumulative Indicator was disliked because of the scale and the fact that it was difficult to see the exact number of alerts in the system. Further, they were concerned about the amount of space it would take up on the screen if there were a lot of alerts and the scale needed to be increased (a 0-10 scale was used for the design). The Horizontal Indicator Bar was disliked because it was difficult to determine the actual number of alerts and the colour\rPage 90 Non-Intrusive Alert System Humansystems®\r\nsaturation would be difficult to learn. However, one participant believed that this design requires some cognitive effort to interpret which could be positive in that it would “keep the operator’s mind busy” (i.e., maintaining or improving attention and vigilance). On the other hand, this participant felt that the Cumulative Total Indicator and the Vertical Cumulative Indicator could be easily ignored as they do not require as much mental effort to interpret. These comments suggest that the appropriate level of intrusiveness for a maritime alert system for RJOC operators is not established and must be further researched and determined experimentally, particularly under operational contexts of medium to high alert frequency.\r6.4 Conclusions and recommendations based on evaluation\rThis section presents conclusions and a number of recommendations for future anomaly alert system designs based on the SME review and evaluation.\r6.4.1 Conclusions\rIn terms of presentation of incoming alerts, participants favoured both the Ticker and Fading Bar, which was rated as fairly intrusive and the Cumulative Total Indicator, which was rated as relatively non-intrusive. Further research is therefore required to determine the appropriate level of intrusiveness especially once the potential number alerts that may be present in the system is better understood.\rIn terms of presentation of the number of active alerts in a system, participants favoured a numerical display rather than a scale as it was easier to interpret at a quick glance.\rWith regards to getting information about an incoming alert, participants preferred the pop up window in the RMP compared to the AIW. This was primarily because the way in which the AIW was implemented in the prototype; it required the user to navigate to a separate window. Participants felt that this adversely impacted their situation awareness as a result. This suggests that managing alerts should be implemented in such a way that operators do not have to leave the RMP. For example, a separate window or sidebar could pop up in the RMP thereby allowing the operators to still have the RMP in view while managing alerts.\rFinally, participants felt that the AIW would be appropriate for managing alerts, although they suggested a number of additional pieces of information that it should include.\rAlthough the SME feedback from the design review was valuable, the results must be interpreted with caution given the small sample size and inconsistency in the comments. Further research is required to understand these inconsistencies that may have biased or influenced the results.\rAlthough beyond the scope of the current project, it should be noted that an anomaly alerting system will undoubtedly require revision of the current Standard Operating Procedures (SOPs). For example, information relating to anomalies will have to be passed on to incoming watch keepers during watch handover. That is, operators will at least be required to pass on the current alert parameter preferences (i.e., the alert trip points) as well as a summary of the number and types of alerts that have emerged over the course of the shift. Also, the operator may be required to brief the Watch Officer on any Priority 1 alerts immediately and perhaps keep a log of all priority 2 and 3 alerts. Furthermore, which alerts have been briefed to the Watch Officer and the time of the briefing may be an additional piece of information that should be included in the AIW.\rHumansystems® Non-Intrusive Alert System Page 91\r\n6.4.2 Limitations\rThere are three primary issues concerning the validity of the data obtained from the study.\r(i) Reliability: the small sample size means that the data obtained should be treated with caution and should be used to indicate trends in attitudes towards a non-intrusive alert system. A more extensive evaluation should be conducted in the future to verify these trends (particularly as they pertain to specific design elements). This evaluation should also involve operators from the West Coast.\r(ii) Context validity: the evaluation was limited in scope by showing designs as single event representations. However, in considering a future operational environment, there could be potentially a continuing volume of alerts that occur during a watch. Therefore, the validity of the judgments obtained in the walkthrough concerning the appropriate level of intrusiveness of the design alternatives must be treated with caution. A design option that appears to offer the right level of intrusiveness in alerting the operator, when viewed in isolation, may, over time and with a high frequency of alerts, become too distracting.\r(iii) Halo effects: it is possible that if a trial participant favoured a particular design, then all detailed evaluation questions concerning the design could be tainted by this bias. For example, if the design were seen as being insufficiently intrusive to “keep operators on their toes”, then its usefulness and utility may also have been judged lower.\r6.4.3 Design recommendations for anomaly alert system\rConsidering the participant feedback, the results suggest that an anomaly alert design which combines the counter from the Cumulative Total Indicator, to indicate the number of active alerts in the system, with the Ticker and Fading Bar, to notify the operator of incoming alerts (with an option for either one), would be the next logical iteration of a non-intrusive anomaly alerting system design.\rWhile participant feedback from the design walkthroughs can be used to point the way toward future iterations of an alert system design, caution must be used in interpreting this feedback especially given the small sample size and inconsistency in the comments. Hence, design guidelines must also be based on human factors principles and further experimentation in a realistic context (i.e., in the RJOC using the RMP). Table 20 shows a number of recommendations for future alert system designs based on participant feedback as well as human factors principles.\rTable 20: Design recommendations for future iterations of alert system interface\rDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rAlert\rAlert indicator should be visual\rAlert indicator should be visual\rAlert indicator should only disappear after acknowledgement\rAlert indicator should only disappear after acknowledgement\rAbility to click and drag the alert display to a different location on the RMP\rAbility to click and drag the alert display to a different location on the RMP so as not to obscure the RMP (i.e. primary task)\rFont in the pop-up boxes should be a bit\rFont in the pop-up boxes should be\rPage 92 Non-Intrusive Alert System Humansystems®\r\nDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rbigger\rappropriate for distance that operator is sitting from screen and size of fonts used in primary task and ambient illumination\rTime should be in Zulu time\rTime should be consistent with that used for current tasks\rTicker and fading bar should also include the vessel name\rThe required level of information content to be contained with in alert indicator should be further investigated\rOption to centre the screen around a vessel of interest or area of interest\rOption to centre the screen around a vessel of interest or area of interest to support SA\rPriority\r• Priority 1 alerts will be highlighted in red\r• Priority 2 alerts will be highlighted gold\r• Priority 3 alerts will be highlighted in grey\rPriority\r• Priority 1 alerts will be highlighted in red.\r• Priority 2 alerts will be highlighted gold.\r• Priority 3 alerts will be highlighted in grey.\rIncoming vs. Active Alerts\r• Incoming alerts should be indicated in a ticker or fading bar (priority 1 and 2) and/ or as a cumulative number in the count box (priority 1, 2 and 3)\r• Active alerts left in the system will only be indicated as a cumulative number in the count box (priority 1, 2 and 3)\rIncoming vs. Active Alerts\r• Presentation of incoming alerts should be further investigated in a context that is representative of RJOC operator’s actual work environment.\r• Active alerts left in the system will be indicated as a cumulative number in the count box (priority 1, 2 and 3)\rRetrieving Information\rThe RMP pop up box, rather than the AIW, should be used to retrieve information related to an anomaly\rThis solution represents a trade-off between the amount of space required for the appropriate information content concerning the anomaly (which has a potential for obscuring the RMP) and obtaining the information from a separate window, taking the operator’s attention away from the RMP. The appropriate design option will require further investigation before this recommendation can be stated definitively.\rThe RMP pop up box will show AIS number, name of the vessel, reason for alert, age of alert, and action buttons\rThe anomaly information content of the RMP pop up box should be further investigated\rManaging Alerts\rThe AIW should be used to manage alerts\rThe AIW should be used to manage alerts\rThe AIW will show priority level, a button to return to the RMP, number of alerts, age of alerts, AIS number, name of vessel, reason for alert, action buttons, source of alert,\rThe content and functionality of the AIW should be further investigated by determining the specific operational requirements.\rHumansystems® Non-Intrusive Alert System Page 93\r\nDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rvessel type, flag, age of alert, course, speed, alert history and age of track\rThe operator should not have to navigate to a different screen to see the AIW\rThis cannot be supported without further investigation. For example, by the end of the watch, and during a high frequency alerting context, there may be numerous alerts in the system. These would either have to be represented in a small window on the RMP, though which the operator would have to continually screen (i.e., not functionally efficient, or usable) or a large window, which would then obscure a significant portion of the RMP .\rThe AIW may appear as a pop up window over the RMP that can be increased or decreased in size as desired\rThe AIW may appear as a pop up window over the RMP that can be appropriately sized for the information content up to a certain maximum.\rAcknowledging Alerts\rAcknowledgment of an alert will come in the form of LEAVE or CLEAR\rThe most appropriate terms for acknowledging an alert should be intuitive to all users and be consistent with similar functions in the current system, and should therefore be further investigated\rAbility to action alerts through the RMP pop up box\rAbility to action alerts through the RMP pop up box\rAbility to action alerts through the AIW\rAbility to action alerts through the AIW\rPage 94 Non-Intrusive Alert System Humansystems®\r\n7. Overall conclusions and recommendations\rThis section presents a number of overall conclusions based on the literature review, design development process and SME review and evaluation of the alert system design concepts.\r7.1 Conclusions\rOur general assessment of the literature relating to non-intrusive alert system design is that it is a somewhat piecemeal and haphazard collection of principles and guidelines from various application environments and serving a number of different goals. Clearly, there is a lack of a unified design approach and associated recommendations that would be applicable for non- intrusive alerting contexts. However, the detailed guidelines found in Shorrock et al (2002) and Han et al (2007) provide a good starting point for an integrated guidance document, which would then need to be extended and made more context relevant to the RJOCs. In addition, there would be a need to make this guidance more specific than statements such as “alarms should signal the need for action”, “alarms should be detected rapidly...”, “alarms should not annoy, startle or distract unnecessarily” etc. Thus, while these recommendations are sound, there is a lack of information on how they are to be implemented as specific design guidelines.\rThe review of the literature gave rise to some general principles for four alert system design concepts that were the presented to and evaluated by SMEs. The actual translation of these principles into specific designs was very much based on Humansystems’ prior experience with HF design implementation, rather than specific recommendations from the literature reviewed. The design evaluation, combined with consideration of general human factors principles, resulted in a list of design requirements for the best way to:\r• Alert RMP operator to a new incoming alert\r• Provide operator with awareness of the number of active alerts in the system\r• Provide operator with information specific to an incoming alert\r• Provide operator with information on all active alerts in the system\r• Provide operator with a means to acknowledge the occurrence of an alert\r• Enable operator to manage (i.e., action) any active alerts in the system\rFurther research, however, is needed to better clarify design options that would support these design requirements, particularly under more realistic operational conditions of multiple alerts within a watch.\r7.2 Future Work\rThe literature review and SME feedback from the design review were valuable in providing a direction for both future iterations of an anomaly alert system design as well as future research. Specifically, future design efforts should work toward developing an alert system interface design\rHumansystems® Non-Intrusive Alert System Page 95\r\nin accordance with the design principles listed in section 6.3.3 once these design requirements have been validated through further research.\rFuture research efforts should focus on both experimentally evaluating anomaly alert system designs in the context of the RMP (i.e., representative of user’s work environment including the potential number of alerts) as well as broader research relating to intrusiveness and attention. The following list provides a number of research questions that have yet to be resolved:\r• •\r•\r• •\r• • •\rWhat is the appropriate number of alert priorities?\rWhat is the most appropriate level of intrusiveness for different priorities of alerts and how is this influenced by alert frequency?\rWhat is the relative intrusiveness of a range of design parameters such as alert size, location and dynamics?\rHow are attention and intrusiveness related?\rIn an RMP context, what are the trade-offs between implementing the information within the RMP window and/or providing a separate window?\rIn the context of the RJOC, what are the information requirements for an alert management system for data anomalies?\rHow should the anomaly alert system be integrated within the overall “alert” system currently used in GCCS-M?\rGiven the characteristics of the GCCS-M, what are the most appropriate design characteristics (e.g., colour, font size, etc.) for an anomaly alert system?\rPage 96\rNon-Intrusive Alert System Humansystems®\r\nReferences\rAhlstrom, V. (2003). An Initial Survey of National Airspace System Auditory Alarm Issues in Terminal Air Traffic Control. In USDO. Transportation (Ed.).\rAhnlund, J., Bergquist, T., & Spaanenburg, L. (2003). Rule-Based Reduction of Alarm Signals in Industrial Control. Journal of Intelligent and Fuzzy Systems, 14, 73-84.\rAiken, D.S., Green, G.E., Arntz, S.J. and Meliza, L.L. (2005). Real time decision alert, aid and after action review system for combat and training (ARI Technical Report 1165). Alexandria, VA: U.S. Army Research Institute for the Behavioral and Social Sciences.\rBrown, L.M. & Kaaresoja, T. (2006). Feel who's talking: Using tactons for mobile phone alerts. CHI Extended Abstracts, 604-609.\rBrown, W. S., O'Hara, J. M., & Higgins, J. C. (2000). Advanced Alarm Systems: Revision of Guidance and Its Technical Basis. In U. S. N. R. Commission (Ed.). Upton, NY: Brookhaven National Library.\rChyssler, T., Burschka, S., Semling, M., Lingvall, T., & Burbeck, K. (2004). Alarm Reduction and Correlaion in Intrusion Detection Systems. In U. F. a. M. Meier (Ed.), Proceedings from the International GI Workshop on Detection of Intrusions and Malware & Vulnerability Assessment (pp. 9-24). Dortmund, Germany.\rColcombe, A., & Wickens, C. D. (2006). Cockpit Display of Traffic Information Automated Conflict Alerting: Parameters to Maximize Effectiveness and Minimize Disruption in Multi-Task Environments. In N. A. R. Center (Ed.). Moffett Field, CA.\rDavenport, M. (2008). Kinematic behaviour anomaly detection (KBAD) Final Report. DRDC CORA Reference No. KBAD-RP-52-6615.\rDavenport, M., Rafuse, J. Lt(N) & Widdis, E. (2005). RMP baseline update study. Volume 1: Main Report. DRDC Atlantic No. TR-2004-293.\rDe Muer, T., Botteldooren, D., De Coensel, B., Berglund, B., Nilsson, M. & Lercher, P. (2005). A model for noise annoyance based on notice-events. In Proceedings of the 34th International Congress on Noise Control Engineering (Internoise), Rio de Janeiro, Brazil.\rDicken, C. R. (1999). \"Soft\" Control Desk and Alarm Display. Computing & Control Engineering Journal, February 1999, 11-16.\rEdworthy, J., & Hellier, E. (2008). Auditory Warnings in Noisy Environments. Noise and Health, 6, 27-39.\rGrootjen, M., Bierman, E. P. B., & Neerincx, M. A. (2006). Optimizing Cognitive Task Load in Naval Ship Control Centres: Design of an Adaptive Interface, IEA 2006: 16th World Congress on Ergonomics.\rGruber, T. R. (1995). Toward principles for the design of ontologies used for knowledge sharing. International Journal Human-Computer Studies, 43(5-6), 907-928\rHan, S. H., Yang, H., & Im, D.-G. (2007). Designing a Human-Computer Interface for a Process Control Room: A Case Study of a Steel Manufacturing Company. International Journal of Industrial Ergonomics, 37, 383-393.\rHumansystems® Non-Intrusive Alert System Page 97\r\nHautamaki, B. S., Bagnall, T., & Small, R. L. (2006). Human Interface Evaluation Methods for Submarine Combat Systems, Undersea Human Systems Integration Symposium. Mystic, Conneticut.\rHudlicka, E., & McNeese, M. D. (2002). Assessment of User Affective and Belief States for Interface Adaptation: Application to an Air Force Pilot Task. User Modeling and User- Adapted Interaction, 12, 1-47.\rHwang, S.-L., Lin, J.-T., Liang, G.-F., Yau, Y.-J., Yenn, T.-C., & Hsu, C.-C. (2008). Application Control Chart Concepts of Designing a Pre-Alarm System in the Nuclear Power Plant Control Room. Nuclear Engineering and Design, in press.\rKrausman, A. S., Elliott, L. R., & Pettitt, R. A. (2005). Effects of Visual, Auditory, and Tactile Alerts on Platoon Leader Performance and Decision Making. In A. R. Laboratory (Ed.).\rLiu, J., Lim, K. W., Ho, W. K., Tan, K. C., Srinivasan, B., & Tay, A. (2003). The Intelligent Alarm Management System. IEEE Software, 66-71.\rMatthews, M., Bruyn, L., Keeble, R. & Rafuse, J. (2004). Maritime Operation Centre: Function Analysis and Development of Measures of Performance to Evaluate Future Multi-Sensor Integration within a Common Operating Environment. DRDC Contract Report TR-2004- 296.\rMcCrickard, D. S., Catrambone, R., Chewar, C. M., & Stasko, J. T. (2003b). Establishing Tradeoffs that Leverage Attention for Utility: Empirically Evaluating Information Display in Notification Systems. International Journal of Human-Computer Studies, 58, 547-582.\rMcCrickard, D. S., Chewar, C. M., Somervell, J. P., & Ndiwalana, A. (2003a). A Model for Notification Systems Evaluation - Assessing User Goals for Multitasking Activity. ACM Transactions on Computer-Human Interaction, 10(4), 312-338.\rMcFarlane, D. C., & Berger, P. H. (2005). The HSI Implications of a New Naval Combat System Alerting Technology Based on Negotiation-Based Coordination: The Hail Technology, Human Systems Integration Symposium, 2005, American Society of Naval Engineers. Arlington, VA.\rMcFarlane, D. C., & Latorella, K. A. (2002). The scope and importance of human interruption in human-computer interaction design. Human-Computer Interaction, 17, 1-61.\rMiller, C. A. (2005). Trust in Adaptive Automation: The Role of Etiquette in Tuning Trust via Analogic and Affective Methods. Paper presented at the Proceedings of the 1st International Conference on Augmented Cognition, Las Vegas, NV.\rMitchell, C. M. (1998). Model-Based Design of Human Interaction with Complex Systems. In A. P. S. a. W. B. Rouse (Ed.), Handbook of Systems Engineering and Management. New York, NY: John Wilev & Sons.\rRhodes (2007). Biologically-inspired Approcahes to Higher-Level Information Fusion. 10th Interbational Conference on Information Fusion.\rRiveiro, M., Falkman, G., & Ziemke, T. (2008). Improving Maritime Anomaly Detection and Situation Awareness Through Interactive Visualization. Paper presented at the Proceedings of the 11th International Conference on Information Fusion.\rPage 98 Non-Intrusive Alert System Humansystems®\r\nShorrock, S. T., Scaife, R., & Cousins, A. (2002). Model-Based Principles for Human-Centred Alarm Systems from Theory and Practice. Paper presented at the 21st European Conference on Human Decision Making and Control.\rSmallman, H. S., & John, M. S. (2003). CHEX (Change History EXplicit): New HCI Concepts for Change Awareness. Paper presented at the Proceedings of the 46th Annual Meeting of the Human Factors and Ergonomics Society, Santa Monica, CA.\rSmallman, H. S., & John, M. S. (2005). Improving Recovery from Multi-Task Interruptions Using an Intelligent Change Awareness Tool, Proceedings from the 1st International Conference on Augmented Cognition. Las Vegas, NV.\rSorkin, R. D. (1988). Why are People Turning Off Our Alarms? Journal of the Acoustic Society of America, 84(3), 1107-1108.\rSt.John, M., Smallman, H. S., & Manes, D. I. (2005). Recovery from Interruptions to a Dynamic Monitoring Task: The Beguiling Utility of Instant Replay, Proceedings of HFES 2005: 49th Annual Meeting of the Human Factors and Ergonomics Society (pp. 473-477). Santa Monica, CA: Human Factors and Ergonomics Society.\rStreefkerk, J. W., Esch-Bussemakers, M. v., & Neerincx, M. (2007). Context-Aware Notification for Mobile Police Officers. In D. Harris (Ed.), Engineering Psychology and Cognitive Ergonomics 7th International Conference. Beijing, China.\rToet, A. (2006). Gaze Directed Displays as an Enabling Technology for Attention Aware Systems. Computers in Human Behavior, 22, 615-647.\rWickens, C. (1984). Processing resources and attention. In R. Parasuraman & D.R. Davies (Eds.), Varieties of Attention. New York: Academic Press.\rHumansystems® Non-Intrusive Alert System Page 99\r\nThis page intentionally left blank.\rPage 100 Non-Intrusive Alert System Humansystems®\r\nAnnex A: Ethics Protocol\rEXECUTIVE SUMMARY\rProtocol # L676\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study\rPrincipal Investigators: Dr. Michael Matthews, Lora Bruyn Martin, Humansystems® Incorporated (HSI®), Guelph, Ontario. Tel: 519-836-5911\rDefence Research and Development Canada (DRDC) Co-Investigators:\rMs. Sharon McFadden, DRDC Toronto, Tel (416) 635-2189 Ms. Liesa Lapinski, DRDC Atlantic. Tel: (902) 426-3100 x180 Thrust: 11he Maritime Domain Awareness\rObjectives:\rThe goal of this experiment is to explore, in the context of the Recognized Maritime Picture (RMP), non-intrusive ways of presenting to the operator information concerning anomalous behavior of vessels. Anomalous behavior can take many forms, e.g. a sudden increase in speed, a vessel in transit that suddenly stops, a ship that changes its port of destination or a ship heading into regulated waters without appropriate permissions. Because of the large number of vessels and associated track data, it is not currently possible for operators to readily determine when, or where, such anomalies occur.\rOverview:\rSix volunteer participants (no age or gender restrictions) will be required to review a number of different design options for a non-intrusive, anomaly alerting system. The participants will individually do a walk through of the options which will be presented as a PowerPoint mock up of the screen interface. A questionnaire will be administered at the end of each session, to record the volunteers’ subjective evaluation of the different design options in terms of their utility and usability. The experiment will be undertaken at the MAPLE laboratory DRDC and will require one walkthrough session. Each session will comprise approximately 15 minutes of background briefing and a 60-90 minute walkthrough session, which will involve interface walkthroughs and the completion of a questionnaire. This work will be conducted by Humansystems® Incorporated.\rParticipants:\rMale or female Navy or ex-Navy operators will be recruited and paid for their participation. There is no restriction on age or gender. The two ex-Navy operators are both males.\rRisks:\rThis experiment offers minimal risk to the participant’s health and well-being. There is a low risk of eye fatigue or eyestrain, as is associated with doing any visually intensive task on a computer display.\rProtocol # L676\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study\rHumansystems® Non-Intrusive Alert System Page A-1\r\nPrincipal Investigators:\rDr. Michael Matthews, Lora Bruyn Martin, Humansystems Incorporated® (HSI®), Guelph, Ontario. Tel: 519-836-5911\rDRDC Co-Investigators:\rMs. Sharon McFadden, DRDC Toronto, Tel (416) 635-2189 Ms. Liesa Lapinski, DRDC Atlantic, Tel: (902) 426-3100 Thrust: 11he Maritime Domain Awareness\rList of Acronyms:\rDRDC Defence Research and Development Canada HSI® Humansystems Incorporated\rMDA Maritime Domain Awareness\rRMP Recognized Maritime Picture\rRJOCs Regional Joint Operations Centres\rBackground:\rThis applied research project in the Maritime Domain Awareness (MDA) Thrust is studying information visualization and management for enhanced domain awareness in maritime security. The DRDC/HSI® team wants to investigate the best way to provide operators with non-intrusive alerts when certain forms of anomalous vessel behavior occur to see if the information provided by way of the alert can help improve understanding of the Recognized Maritime Picture (RMP), decision making based on the RMP and the efficiency of the RMP operators’ duties.\rThe RMP is a product produced by the Regional Joint Operation Centres (RJOCs). In its common form, it is a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Each contact has a set of metadata associated with it which can include (but is not exclusive to) position, speed, heading, ship name, hull number, threat, flag, destination, origin, type, cargo and a digital image. At worst, the metadata only consist of a position (i.e., there is something out there). At best, the metadata consist of all of the above. The different degrees of metadata are due to the multiple sources of information that feed the RMP. These sources include everything from radar to surveillance flights to self reporting systems to voluntary reports, each providing its own subset of data.\rAnomalies in the movement and behavior of vessels may be of many types and include, for example:\r• Unexplained high speed: a ship that is claiming to be a normal merchant ship suddenly starts travelling at a high speed more typical of a passenger ship or warship.\r• Speed too slow: a Cargo, Passenger, or Ferry is observed going slowly. As these vessels generally go as fast as they safely can, it may be an indicator of a problem.\r• Loitering: a cargo ship stops outside of or far from a harbour, or steams very slowly, rather than proceeding directly into port.\rPage A-2 Non-Intrusive Alert System Humansystems®\r\n• Grab and dash fishing: a foreign fishing boat moves from the international zone to Canadian waters (where it is forbidden from fishing) for a few hours just before leaving for its home port.\r• Not heading to port: a vessel is heading in a direction where there is no harbour, or is not heading toward its declared destination. Cargo and Ferry vessels always go from one port to another port, and generally by the shortest available route.\rThe purpose of the proposed study is to explore the best ways of alerting operators to such anomalies in a non-intrusive manner. Given the potential for many such anomalies during a normal watch, it is imperative that discipline be used in the design of an alerting system to ensure that operators are not hindered in the performance of their primary task by nuisance alerts. Also, it is important that functionality is provided to allow operators to define their own criteria for different alert priorities in different contexts, and that the interface provides good situation awareness of the different alert types. The outcome of this work will serve both the maritime operations communities, as well the scientific communities in the advancement of new methods for the design of non-intrusive alerting systems. This work will be conducted by Humansystems® Incorporated.\rObjectives:\rThe goal of this experiment is to explore, in the context of the RMP, non-intrusive ways of presenting to the operator information concerning anomalous behavior of vessels. Because of the large number of vessel and associated track data, it is not currently possible for operators to readily determine when or where such anomalies occur.\rOverview:\rSix volunteer participants (no restriction on age or gender) will be recruited to review a number of different design options for a non-intrusive, anomaly alerting system. The participants will individually do a walk through of the options which will be presented as a PowerPoint mock up of the screen interface. A questionnaire will be administered at the end of each session, to record the volunteers’ subjective evaluation of the different design options in terms of their utility and usability. The experiment will be undertaken at the MAPLE laboratory DRDC Atlantic and will require one walkthrough session. Each session will comprise approximately 15 minutes of background briefing and a 60-90 minute walkthrough session, which will involve interface walkthroughs and the completion of a questionnaire. This work will be conducted by Humansystems® Incorporated.\rProcedures:\rBackground Briefing\rImmediately prior to the walkthrough session, participants will be given an orientation briefing on the overall study, its objectives and what they will be asked to do. At this stage they will be asked to complete a consent form and an information sheet about the study.\rWalkthrough session: Review of design options\rThe goals of a non-intrusive alerting system will be described to the participant and the major functional components of the system will be described at a high level. Participants will then be guided through a PowerPoint presentation of how a non-intrusive alerting system interface would look and work. For each functional component of the system, participants will interact with an animated PowerPoint slide to simulate the actions of an interface. As participants proceed through\rHumansystems® Non-Intrusive Alert System Page A-3\r\nthe design they will be engaged in discussion concerning how intuitive and easy the interface is to use, how it serves their information needs, what information requirements are not being met and what additional functions they would like to see. It is anticipated that the participant will be presented with 4-5 design options to review in this manner.\rAt the completion of the walkthrough the participants will complete a subjective questionnaire documenting their evaluation of the different design options.\rParticipants:\rApproximately six Navy, or ex-Navy operators, will participate Navy operators will be recruited through a formal request through the DRDC Atlantic Navy Liaison Officer. There will be no restriction on age or gender. Ex-Navy participants will be recruited from a list maintained by HSI of ex-Navy personnel who have indicated a prior willingness to be contacted as potential study participants. . The ex-navy operators are males, Participants will be required to self identify that they have normal colour vision.\rEquipment and Facilities:\rThe apparatus comprises a standard “Windows” workstation with 19” colour screen, a mouse and keyboard input, a work surface to record notes, and an ergonomically designed operator’s chair.\rData collected:\rThe following information will be collected during each walkthrough session:\r- Responses to the questionnaires concerning the participant’s evaluation of the design alternatives\r- Summary of the participants’ comments during free discussion with the walkthrough facilitator\rExperimental Design/Statistical Analysis:\rThe small sample size (limited by the availability of operational personnel) will likely have insufficient power to warrant the use of analytical statistical procedures for estimation of probabilities. However, it should be noted that the present study is designed to be an exploratory approach to defining a preliminary set of good design alternatives, which, at some future date, could be evaluated more completely in a more rigorous experiment.\rRisks and Safety Recommendations:\rThis experiment offers minimal risk to the participant’s health and well-being. There is a low risk of eye fatigue or eyestrain, as would be associated with doing any visually intensive task (e.g. web searching, word processing) on computer display for the period of time used in the walkthrough sessions. This may manifest itself as eye discomfort, dry or itchy eyes, or mild headache. However, the duration of exposure to the presentation will be short. Participants will be encouraged to inform experimenters if they experience any discomfort or eyestrain, or if they have any problems during the investigation. They may be told to stop their activities until problems or conditions are resolved. The risks from participation in this experiment are generally the same as those associated with the performance of normal monitoring of a visual display that a person might do while word processing, surfing the web or playing video games.\rPage A-4 Non-Intrusive Alert System Humansystems®\r\nBenefits of Study:\rThrough their involvement in the study participants will be able to contribute to the validation, development and design of new methods for providing information on vessel anomalies, which in turn provides important human factors data for the re-design and automation of future systems to represent the maritime picture.\rInformed Consent:\rParticipants will be fully briefed on the relevant aspects of the experimental protocol and will be given a copy of this protocol to review. They will be required to sign a voluntary consent form, indicating their willing informed consent, before being allowed to participate in the experiment. No deception is involved.\rConfidentiality:\rAny personal or performance data collected for each participant will be available only to the experimenters and will be held in the strictest confidence. Participants will not be identified by name in the data records; group statistics will be used in future presentations or publications. Individual participant data will be coded anonymously and maintained in a computer file. The file may be accessed only by the project team.\rParticipant Debriefing:\rParticipants will be permitted to ask any questions they wish about the study after they have completed the experiment.\rParticipant Stress Remuneration:\rParticipants will be paid participant pay in the amount of $26.93. This assumes that the total time required will be no more than 3 hours per participant for participation in this study (in accordance with DRDC guidelines and as authorized by DND policies15). The remuneration is calculated as follows:\r3 hours x 2 (stress level) x $2.50 +$11.93 x 1 day = $26.93\rApproximate Time Involvement:\rAll participants will be required to participate in one session of approximately 3 hours with a 15 minute break in the middle.\rMedical screening:\rNo screening is required for this study\rPhysician supervision:\rNo physician supervision is required for this study.\r15 Guide to Stress Compensation for Human Subjects, R. Pigeau. DRDC Toronto.\rHumansystems® Non-Intrusive Alert System Page A-5\r\nThis page intentionally left blank.\rPage A-6 Non-Intrusive Alert System Humansystems®\r\nVoluntary Consent Form\rProtocol Number: L676\rResearch Project Title: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study Principal Investigators:\rDr. Michael Matthews, Humansystems Incorporated, Guelph, Ontario.\rLora Bruyn Martin, Humansystems Incorporated, Guelph, Ontario\rDRDC Co-Investigators:\rMs. Sharon McFadden, DRDC (Toronto)\rMs. Liesa Lapinski. DRDC (Atlantic)\r1. I,__________________________________________________________\r________________________________________________________________\r(Name, Address, Phone number) hereby volunteer to participate as a participant in the experiment entitled “Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study”, the aim of which is to explore the best ways of providing operators with information on anomalous behavior of vessels in a maritime context. I understand that I am required to read the attached protocol in its entirety. I have had the opportunity to study and discuss the attached protocol with the investigators and I have been informed to my satisfaction about the possible discomforts associated with these tests.\r2. I am aware that before starting I will receive a briefing on the aims and procedures for the experiment. I will have the opportunity to ask and receive answers to any questions I may have. I understand that I am free to refuse to participate and may withdraw my consent without prejudice or hard feelings at any time. Should I withdraw my consent, my participation as a participant will cease immediately. I understand that the entire session will last no more than 3 hours.\r3. I have been told that the principal risks associated with this experiment are the possible development of eyestrain or visual fatigue. This may manifest itself as eye discomfort, dry or itchy eyes, or mild headache. I understand that the limited duration of each experiment and rest periods between experiments will mitigate this risk. I understand and accept this risk. I am aware that there are inherent, unknown and currently unforeseen risks by DRDCs Atlantic and Toronto and the Project Investigators that are associated with any scientific research and that all known risks have been explained to my satisfaction.. I have been given examples of potential minor and remote risks associated with the experiment and consider these risks acceptable as well.\r4. I agree to provide responses to questions that are to the best of my knowledge truthful and complete. I have been advised that the experimental data concerning me will be treated as confidential and not revealed to anyone other than the investigators without my consent except as data unidentified as to source. I understand that my name will not be identified or attached in any manner to any publication arising from this study.\r5. I understand that for my participation in this research project, I am entitled to stress remuneration in the form of participant payment of $ 26.93. Stress remuneration is taxable. However, T4A slips are issued only for amounts in excess of $500.00 remuneration per year.\rHumansystems® Non-Intrusive Alert System Page A-7\r\n6. I acknowledge that I have read this form and I understand that my consent is voluntary and has been given under circumstances in which I can exercise free power of choice. I have been informed that I may, at any time, revoke my consent and withdraw from the experiment, and that the investigators may terminate my involvement in the experiment, regardless of my wishes.\r7. I understand that by signing this consent form I have not waived any legal rights I may have as a result of any harm to me occasioned by my participation in this research project beyond all risks I have assumed.\r8. [For Canadian Forces (CF) members only] – I understand that I am considered to be on duty for disciplinary, administrative and Pension Act purposes during my participation in this study. With that said, this duty status has no effect on my right to withdraw for the experiment at any time I wish and it is clear that no action will be taken against me for exercising this right. As well, in the unlikely event that my participation in this study results in a medical condition rendering me unfit for service, I may be released from the CF and my military benefits apply.\rVolunteer’s Name: _________________________________________________\rSignature: _____________________________ Date: _____________________\rName of Witness to Signature: _______________________________________\rSignature: _____________________________ Date: _____________________\rFamily Member or Contact Person (name, address, daytime phone number & relationship):_____________________________________________________________\rPrincipal Investigator: ______________________________________________\rSignature: _____________________________ Date: _____________________\rFOR PARTICIPANT ENQUIRY IF REQUIRED:\rShould I have any questions or concerns regarding this project before, during, or after participation, I understand that I am encouraged to contact any of the contacts below by phone or e-mail, to\rPrincipal Investigators:\rDr. Michael Matthews, Humansystems® Incorporated (HSI®), Guelph, Ontario Tel: (519) 836- 5911, email: mmatthews@humansys.com\rLora Bruyn Martin, Humansystems® Incorporated (HSI®), Guelph, Ontario Tel: (519) 836-5911 Ex. 303, email: lbruyn@humansys.com\rCo-Investigator:\rPage A-8 Non-Intrusive Alert System Humansystems®\r\nMs. Sharon McFadden, DRDC Toronto, Tel (416-635-2189), email: sharon.mcfadden@drdc- rddc.gc.ca\rChair, DRDC Human Research Ethics Committee (HREC): Dr. Jack P. Landolt, phone: 416- 635-2120, email: jack.landolt@drdc-rddc.gc.ca\rI understand that I will be given a copy of this consent form so that I may contact any of the above-mentioned individuals at some time in the future should that be required.\rSecondary Use of Data: I consent/do not consent (delete as appropriate) to the use of this study’s experimental data involving me in unidentified form in future related studies provided review and approval have been given by DRDC HREC.\rVolunteer’s Signature_____________________ Date ____________________\rHumansystems® Non-Intrusive Alert System Page A-9\r\nINFORMATION FOR PARTICIPANTS\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts\rProtocol #L676\rPrincipal Investigators: Dr. Michael Matthews & Lora Bruyn Martin (HSI®), Guelph, Ontario Defence Research and Development Canada (DRDC) Co-Investigators:\rMs. Sharon McFadden (DRDC Toronto) & Ms. Liesa Lapinski (DRDC Atlantic)\rBackground & Purpose of Study:\rThis applied research project in the Maritime Domain Awareness Thrust is studying information visualization and management for enhanced domain awareness in maritime security. The DRDC/HSI® team wants to investigate the best way to provide operators with non-intrusive alerts when certain forms of anomalous vessel behavior occur to see if the information provided by way of the alert can help improve understanding of the Recognized Maritime Picture (RMP), decision making based on the RMP and the efficiency of the RMP operators’ duties. Given the potential for many such anomalies during a normal watch, it is imperative that the design of an alerting system ensures that operators are not hindered in the performance of their primary task by nuisance alerts. Also, it is important that functionality is provided to allow operators to define their own criteria for different alert priorities in different contexts, and that the interface provides good situation awareness of the different alert types. The outcome of this work will serve in the advancement of new methods for the design of non-intrusive alerting systems.\rProcedure:\rImmediately prior to the walkthrough session, you will be given an orientation briefing on the overall study, its objectives and what you will be asked to do. At this stage you will be asked to complete a consent form.\rThe goals of a non-intrusive alerting system will be described to you and the major functional components of the system will be described at a high level. You will then be guided through a PowerPoint presentation of how a non-intrusive alerting system interface would look and work. For each functional component of the system, you will interact with an animated PowerPoint slide to simulate the actions of an interface. As you proceed through the design you will be engaged in discussion concerning the interface, how it serves your information needs, what information requirements are not being met and what additional functions you would like to see. You will be presented with 4-5 design options to review in this manner. You will then complete a subjective questionnaire documenting your evaluation of the different design options.\rThe session will be approximately 2-3 hours with a 15 minute break in the middle.\rPage A-10 Non-Intrusive Alert System Humansystems®\r\nAnnex B: Questionnaires for SME Evaluation\rDemographic Questionnaire\rInstructions: Please read each question below and write the appropriate response in the answer\rsection.\rQuestion\rAnswer\r1. Name.\r2. What is your current rank?\r3. What is your current position?\r4. How long have you been in that position?\r5. Please list any other related positions you have had.\rHumansystems® Non-Intrusive Alert System Page B-1\r\nUsability and Usefulness Questionnaire\rInstructions: Please read each statement below and circle the response you feel is most\rappropriate concerning the usefulness of a general non-intrusive alerting system.\rStatement\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\r1. These alerts would enhance our knowledge of anomalies\r1\r2\r3\r4\r5\r2. This alerting system would be used on a daily basis\r1\r2\r3\r4\r5\r3. Tasks can be performed in a straightforward manner using this alerting system\r1\r2\r3\r4\r5\r4. The thinking required to use this alerting system requires significant effort\r1\r2\r3\r4\r5\r5. This alerting system would be difficult to use\r1\r2\r3\r4\r5\r6. This alerting system will improve my situation awareness\r1\r2\r3\r4\r5\r7. This alerting system would make it easier to identify anomalies\r1\r2\r3\r4\r5\r8. I would find this alert system useful\r1\r2\r3\r4\r5\r9. I would not ignore alerts while using this technology\r1\r2\r3\r4\r5\r10. The Alert Information Window (AIW) was confusing\r1\r2\r3\r4\r5\r11. It was easy to learn how the AIW was represented\r1\r2\r3\r4\r5\r12. The AIW had all the necessary information\r1\r2\r3\r4\r5\r13. It was easy navigating between the RMP and the AIW\r1\r2\r3\r4\r5\r14. I prefer clearing and deferring alerts directly from the RMP\r1\r2\r3\r4\r5\r15. I prefer using the AIW to clear or defer alerts\r1\r2\r3\r4\r5\rPage B-2 Non-Intrusive Alert System Humansystems®\r\nAlert Design Questionnaire\rInstructions: Please read each statement below and circle the response you feel is most\rappropriate concerning the usability of each non intrusive alert design.\rStatement & Design\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\r16. The number of alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r17. The presence of an alert was easy to recognize\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r18. The priorities of the alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r19. It was easy to find the relevant anomaly\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r20. It was easy to find information on anomalies\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r21. The alerting design enhanced my situation awareness of maritime anomalies\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rHumansystems® Non-Intrusive Alert System Page B-3\r\nStatement & Design\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r22. The appearance of the alerts is compatible with my current interface\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\rPage B-4 Non-Intrusive Alert System Humansystems®\r\nRanking Questionnaire\rInstructions: For the following questions, please rank each of the alert designs in order of\rpreference (where 1 = most preferred, 4 = least preferred).\rCumulative Total Indicator Vertical Cumulative Horizontal Indicator Bar Indicator\r23. In terms of overall effectiveness in bringing alerts to my attention\rDesign 1: Cumulative Total Indicator Design 2: Vertical Cumulative Indicator Design 3: Horizontal Indicator Bar Design 4: Ticker & Fading Bar\r24. In comparing the methods for representing the different alert priorities\rDesign 1: Cumulative Total Indicator ____ Design 2: Vertical Cumulative Indicator ____ Design 3: Horizontal Indicator Bar ____ Design 4: Ticker & Fading Bar ____ 25. In terms of providing all of the required information about alerts, without distracting\rme from my primary task\rDesign 1: Cumulative Total Indicator Design 2: Vertical Cumulative Indicator Design 3: Horizontal Indicator Bar Design 4: Ticker & Fading Bar\r____\r____\r____\r____\rTicker and Fading Bar\r____\r____\r____\r____\rHumansystems®\rNon-Intrusive Alert System\rPage B-5\r\nLikes and Dislikes Questionnaire\rInstructions: Please respond to each alert design below by indicating whether or not the\rdesign should be implemented, your likes and dislikes, and the design’s intrusiveness.\r26. Should Design 1: Cumulative Total Indicator be implemented? Yes No\rLikes:________________________________ ____________________________________ ____________________________________\rDislikes:______________________________ ____________________________________ ___________________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\r27. Should Design 2: Vertical Cumulative Indicator be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ __________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\rPage B-6 Non-Intrusive Alert System Humansystems®\r\n28. Should Design 3: Horizontal Indicator Bar be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ __________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\r29. Should Design 4: Ticker & Fading Bar be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ _________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\rHumansystems® Non-Intrusive Alert System Page B-7\r\nThis page intentionally left blank.\rPage B-8 Non-Intrusive Alert System Humansystems®\r\nAnnex C: General Discussion Interview Questions\rSME Interview Questions: Evaluating Interface Designs for Non-Intrusive Alerts Things to emphasize:\r• Operators will define priorities and select rules\r• Show 3 priority levels Rory created\r• Show rule selection window\rGeneral:\r• At desk vs. away from desk\r• Looking at screen vs. not looking at screen\r• Frequency of alerts (by priority)\r• Should priority 1 alerts be intrusive?\r• Ignoring alerts\r• Did you like the way the priorities were indicated?\rInterface design:\r• Alert colour scheme (e.g., red, orange, yellow, grey)\r• Font (e.g., size, colour)\r• Was the terminology used familiar, clear and understandable?\r• Alert Information Window – useful and comprehensiveness?\r• Navigation – can you find the relevant information?\r• Intuitive versus not intuitive\r• Intrusiveness of alerts\r• Should there be auditory/tactile alerts in tandem or separately?\r• Flexibility in the location of ticker and horizontal indicator bar\r• Would operators want RMP centred on contact?\r• Would you want more information in the pop up box in the RMP?\rContext of use:\r• Ability to return to primary task\r• Potential problems from using the non-intrusive alert designs in practice?\ro During an increased workload?\ro When you are not at your computer screen?\ro At shift change over? Beginning or end of shift\rHumansystems® Non-Intrusive Alert System Page C-1\r\nThis page intentionally left blank.\rPage C-2 Non-Intrusive Alert System Humansystems®\r\nAcronyms\rABAIS\rAffect and Belief Adaptive Interface System\rACT\rAlarm Cleanup Toolbox\rAIS\rAutomatic Identification System\rAIW\rAlert Information Window\rANOVA\rAnalysis of Variance\rARO\rAssistant Reactor Operator\rATCS\rAir Traffic Control Specialists\rATM\rAir Traffic Management\rAVMS\rAIS Vessel Monitoring System\rCAPTA\rCognitive, Affective, Personality Task Analysis\rCCS\rCombat Control System\rCDTI\rCockpit Displays of Traffic Information\rCHDB\rContact History Database\rCHEX\rChange History Explicit\rCISTI\rCanada Institute for Scientific and Technical Information\rCONOPS\rConcept of Operations\rCOTS\rCommercial-Off-The-Shelf\rCS\rCombat System\rDCS\rDistributed Control System\rDRDC\rDefence Research and Development Canada\rDSS\rDecision Support System\rELINT\rElectronic Intelligence\rETA\rEstimated Time of Arrival\rFCR\rFire-Control Radar\rFFC\rBio-Fuelled District Heating Plant\rGCCS-M\rGlobal Command and Control System-Maritime\rGSFC\rGoddard Space Flight Center\rGUI\rGraphic User Interface\rHAIL\rHuman Alerting and Interruption Logistics\rHCI\rHuman Computer Interaction\rHF\rHuman Factors\rHFSWR\rHigh Frequency Surface Wave Radar\rHMIAS\rHazard Monitor and Intelligent Alerting System\rIAMS\rIntelligent Alarm Management System\rID\rIdentification\rHumansystems® Non-Intrusive Alert System Page D-1\r\nIDS\rIntrusion Detection Systems\rIOP\rInput Open\rIP\rInternet Protocol\rIRC\rInterruption, Reaction and Comprehension\rIS\rIdentification Supervisor\rLCCI\rLarge Complex Critical Infrastructure\rLCL\rLower Control Limits\rMARLANT\rMaritime Forces Atlantic\rMISR\rMaritime Intelligence, Surveillance, and Reconnaissance\rMMI\rMan-Machine Interface\rMMSI\rMaritime Mobile Service Identity\rMPA\rMaritime Patrol Aircraft\rMSOCC\rMultisatellite Operations Control Center\rNAS\rNational Airspace System\rNASA\rNational Aeronautics and Space Administration\rNATO\rNorth Atlantic Treaty Organization\rNRC\rNuclear Regulatory Commission\rOFMspert\rOperator Function Model Expert System\rPAL\rProvincial Airlines\rPDA\rPersonal Digital Assistant\rPL\rPlatoon Leaders\rR&D\rResearch and Development\rRJOC\rRegional Joint Operations Center\rRMP\rRecognized Maritime Picture\rRO\rReactor Operator\rRSVP\rRapid Serial Visual Presentation\rSHIELD\rSystem to Help Identify and Empower Leader Decisions\rSME\rSubject Matter Expert\rSOP\rStandard Operating Procedure\rTA\rTechnical Authority\rVDU\rVisual Display Unit\rVOI\rVessel of Interest\rUAV\rUnmanned Aerial Vehicle\rUCL\rUpper Control Limits\rPage D-2 Non-Intrusive Alert System Humansystems®\r\nDistribution list\rDocument No.: DRDC Toronto CR 2009-042\rLIST PART 1: Internal Distribution by Centre:\r2 DRDC Toronto Library file copies 1 Sharon McFadden\r1 Dr. Justin Hollands\r1 Dr. Ming Ho\r5 TOTAL LIST PART 1\rLIST PART 2: External Distribution by DRDKIM\r1 DRDKIM\rDRDC Atlantic\r9 Grove St, Dartmouth NS B2Y3Z7 3 Attn: Dr. L. Lapinski\rDr. J. Crebolder\rDr. Bruce Chalmers\rDRDC Ottawa\r1 3701 Carling Avenue, Ottawa, Ontario, K1A 0Z4\rAttn: Chris Helleur\rDefence R&D Canada - Valcartier\r2459 Pie Xi Blvd North 3 Val-Belair, QC G3J 1X5\rAttn: Dennis Gouin Alain Bouchard\rAndrew Wind, DRDC CORA\rMARLANT HQ\r3rd Floor, Room 311\rPO Box 99000 Stn Forces, Halifax, NS, B3K 5X5\rSteven Horn, DRDC CORA\rJTFP J02 OR-1, PO Box 17000 Station Forces, Victoria, BC, B9A 7N2\rCommanding Officer\rTRINITY JOSIC, PO Box 99000 Stn Forces, Halifax, NS, B3K 5X5 Capt(N) K. Greenwood\rJ3, Canada COM HQ, NDHQ - 101 Col. By Dr., Ottawa, ON, K1A 0K2\r1\r1\r1 1\rDRDC Toronto TR 2009-042 1\r\nLCdr A. Gyorkos\r1 OIC Maritime Operations Centre, JTFP HQ, MARPAC/JTFP, PO Box 17000 Station\rForces, Victoria, BC, B9A 7N2 13 TOTAL LIST PART 2\r18 TOTAL COPIES REQUIRED\rDRDC Toronto TR 2009-042 2\r\nDOCUMENT CONTROL DATA\r(Security classification of title, body of abstract and indexing annotation must be entered when the overall document is classified)\r1. ORIGINATOR (The name and address of the organization preparing the document. Organizations for whom the document was prepared, e.g. Centre sponsoring a contractor's report, or tasking agency, are entered in section 8.)\rHumansystems Inc.\r111 Farquhar St., 2nd floor Guelph, Ontario, N1H 3N4\r2. SECURITY CLASSIFICATION\r(Overall security classification of the document including special warning terms if applicable.)\rUNCLASSIFIED\r3. TITLE (The complete document title as indicated on the title page. Its classification should be indicated by the appropriate abbreviation (S, C or U) in parentheses after the title.)\rA non−intrusive alert system for maritime anomalies: literature review and the development and assessment of interface design concepts (U)\rSystème d’alerte non intrusive en cas d’anomalies maritimes: examen de la documentation et élaboration/évaluation de concepts d’interface (U)\r4. AUTHORS (last name, followed by initials – ranks, titles, etc. not to be used)\rMichael Matthews; Lora Bruyn Martin; Courtney D. Tario; Andrea L. Brown\r5. DATE OF PUBLICATION\r(Month and year of publication of document.)\rMarch 2009\r6a. NO. OF PAGES\r(Total containing information, including Annexes, Appendices, etc.)\r140\r6b. NO. OF REFS\r(Total cited in document.)\r37\r7. DESCRIPTIVE NOTES (The category of the document, e.g. technical report, technical note or memorandum. If appropriate, enter the type of report, e.g. interim, progress, summary, annual or final. Give the inclusive dates when a specific reporting period is covered.)\rTechnical Report\r8. SPONSORING ACTIVITY (The name of the department project office or laboratory sponsoring the research and development – include address.)\rDefence R&D Canada – Toronto 1133 Sheppard Avenue West P.O. Box 2000\rToronto, Ontario M3M 3B9\r9a. PROJECT OR GRANT NO. (If appropriate, the applicable research and development project or grant number under which the document\rwas written. Please specify whether project or grant.)\r11he04\r9b. CONTRACT NO. (If appropriate, the applicable number under which the document was written.)\rW7711-088124/001/TOR\r10a. ORIGINATOR'S DOCUMENT NUMBER (The official document number by which the document is identified by the originating activity. This number must be unique to this document.)\rDRDC Toronto CR 2009-042\r10b. OTHERDOCUMENTNO(s).(Anyothernumberswhichmaybe assigned this document either by the originator or by the sponsor.)\r11. DOCUMENT AVAILABILITY (Any limitations on further dissemination of the document, other than those imposed by security classification.) Unlimited\r12. DOCUMENT ANNOUNCEMENT (Any limitation to the bibliographic announcement of this document. This will normally correspond to the Document Availability (11). However, where further distribution (beyond the audience specified in (11) is possible, a wider announcement audience may be selected.))\rUnlimited\rDRDC Toronto TR 2009-042 3\r\n13. ABSTRACT (A brief and factual summary of the document. It may also appear elsewhere in the body of the document itself. It is highly desirable that the abstract of classified documents be unclassified. Each paragraph of the abstract shall begin with an indication of the security classification of the information in the paragraph (unless the document itself is unclassified) represented as (S), (C), (R), or (U). It is not necessary to include here abstracts in both official languages unless the text is bilingual.)\rThis project involves the investigation of best practices for the development of design concepts for a visualization aid, specifically an alerting system, which would increase the RMP operators’ awareness and understanding of maritime anomalies in the RMP (e.g. vessel not heading to port, grab and dash fishing, etc.). Such an alerting system, however, must make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\rThe objectives of this project were (i) to identify and analyse available literature relevant to non-intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, we were able to extract relevant concepts from the literature relating to alert/alarm design in general. These concepts, combined with general human factors principles, provided direction for a number of design concepts which were then reviewed and evaluated by subject matter experts.\rFuture design efforts should work toward developing an alert system interface design in accordance with the design principles listed above, once these design requirements have been validated.\rLe projet comprend l’étude des meilleures pratiques applicables à la définition de concepts pour un système d’aide à la visualisation, en l’occurrence un système d’alerte, qui aiderait les opérateurs du TSM à mieux connaître et comprendre les anomalies maritimes indiquées dans le TSM (déroutement d’un navire, braconnage maritime, etc.). Un tel système d’alerte doit toutefois permettre aux opérateurs d’être informés des anomalies éventuelles sans pour autant entraver l’exécution de leurs tâches principales.\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, on a pu extraire de la documentation des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts qui ont ensuite été examinés et évalués.\rLes recherches futures devraient viser à définir une conception d’interface de système d’alerte conformément aux principes de conception présentés ci-dessus, une fois que ces exigences de conception auront été validées.\r14. KEYWORDS, DESCRIPTORS or IDENTIFIERS (Technically meaningful terms or short phrases that characterize a document and could be helpful in cataloguing the document. They should be selected so that no security classification is required. Identifiers, such as equipment model designation, trade name, military project code name, geographic location may also be included. If possible keywords should be selected from a published thesaurus, e.g. Thesaurus of Engineering and Scientific Terms (TEST) and that thesaurus identified. If it is not possible to select indexing terms which are Unclassified, the classification of each should be indicated as with the title.)\ranomalies; visualization; alerts; non−intrusive; recognized maritime picture\rDRDC Toronto TR 2009-042 4","title":"Microsoft Word - Design Alert Final Report Apr 2 Final to DRDC.doc","isStoredAs":"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo/#LocalFileDataObject","contentHash":"aabde1d6c2f5a1dc36d0b593e342d0c212d86779","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"intrus","time":1461915877646,"auto":true,"weight":1.0},{"@type":"Tag","text":"ticker","time":1461915877805,"auto":true,"weight":1.0},{"@type":"Tag","text":"anomali","time":1461915877735,"auto":true,"weight":1.0},{"@type":"Tag","text":"humansystem","time":1461915877699,"auto":true,"weight":1.0},{"@type":"Tag","text":"rmp","time":1461915877717,"auto":true,"weight":1.0},{"@type":"Tag","text":"alert","time":1461915877630,"auto":true,"weight":1.0},{"@type":"Tag","text":"interrupt","time":1461915877753,"auto":true,"weight":1.0},{"@type":"Tag","text":"alarm","time":1461915877776,"auto":true,"weight":1.0},{"@type":"Tag","text":"oper","time":1461915877682,"auto":true,"weight":1.0},{"@type":"Tag","text":"prioriti","time":1461915877664,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":17,"appId":"aabde1d6c2f5a1dc36d0b593e342d0c212d86779","timeCreated":1455791426132,"timeModified":1461915875523,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.03718685,"uri":"file:///Users/cheny13/Downloads/ADA505334.pdf","plainTextContent":"DRDC CR2009-042\rA NON-INTRUSIVE ALERT SYSTEM FOR MARITIME ANOMALIES: LITERATURE REVIEW AND THE DEVELOPMENT AND ASSESSMENT OF INTERFACE DESIGN CONCEPTS\rSYSTÈME D’ALERTE NON INTRUSIVE EN CAS D’ANOMALIES MARITIMES : EXAMEN DE LA DOCUMENTATION ET ÉLABORATION/ÉVALUATION DE CONCEPTS D’INTERFACE\rMichael Matthews, Lora Bruyn Martin, Courtney D. Tario and Andrea L. Brown\rHumansystems® Incorporated 111 Farquhar St., 2nd floor Guelph, ON N1H 3N4\rProject Manager: Ron Boothby, PMP (519) 836 5911\rPWGSC Contract No: W7711-088124/001/TOR\rOn behalf of DEPARTMENT OF NATIONAL DEFENCE\rDRDC Scientific Authorities Sharon McFadden 416-635-2000\rLiesa Lapinski 902-426-3100 ext. 180\rMarch 2009\r\nAuthor\rMichael Matthews, PhD Humansystems® Incorporated\rApproved by\rSharon McFadden DRDC Toronto\rApproved for release by\rChair, Document Review and Library Committee\rThe scientific or technical validity of this Contractor Report is entirely the responsibility of the contractor and the contents do not necessarily have the approval or endorsement of Defence R&D Canada.\r©HER MAJESTY THE QUEEN IN RIGHT OF CANADA (2009) as represented by the Minister of National Defence\r©SA MAJESTE LA REINE EN DROIT DU CANADA (2009) Défense Nationale Canada\r\nAbstract\rThis project involves the investigation of best practices for the development of design concepts for a visualization aid, specifically an alerting system, which would increase the RMP operators’ awareness and understanding of maritime anomalies in the RMP (e.g. vessel not heading to port, grab and dash fishing, etc.). Such an alerting system, however, must make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\rThe objectives of this project were (i) to identify and analyse available literature relevant to non- intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, we were able to extract relevant concepts from the literature relating to alert/alarm design in general. These concepts, combined with general human factors principles, provided direction for a number of design concepts which were then reviewed and evaluated by subject matter experts.\rFuture design efforts should work toward developing an alert system interface design in accordance with the design principles listed above, once these design requirements have been validated.\rHumansystems® Non-Intrusive Alert System Page i\r\nRésumé\rLe projet comprend l’étude des meilleures pratiques applicables à la définition de concepts pour un système d’aide à la visualisation, en l’occurrence un système d’alerte, qui aiderait les opérateurs du TSM à mieux connaître et comprendre les anomalies maritimes indiquées dans le TSM (déroutement d’un navire, braconnage maritime, etc.). Un tel système d’alerte doit toutefois permettre aux opérateurs d’être informés des anomalies éventuelles sans pour autant entraver l’exécution de leurs tâches principales.\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, on a pu extraire de la documentation des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts qui ont ensuite été examinés et évalués.\rLes recherches futures devraient viser à définir une conception d’interface de système d’alerte conformément aux principes de conception présentés ci-dessus, une fois que ces exigences de conception auront été validées.\rPage ii Non-Intrusive Alert System Humansystems®\r\nExecutive Summary\rThe RMP, one of the primary outputs of the Regional Joint Operations Centers (RJOCs), is essentially a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Given the extensive maritime traffic and the large area covered, it is difficult to effectively monitor the RMP to maintain a good understanding of the current situation, including anomalies (e.g. sudden increase in speed, not heading to port of call, etc.). Thus, there is a need for a system that could perform routine checks for anomalous data in the background and then make the information available to operators in a format that makes the anomalies readily comprehensible and gives rise to rapid situation awareness. The crux of the problem is how to implement an alerting system that would make operators aware of anomalies that may be present without impacting on the performance of their primary tasks (i.e. non-intrusive alert).\rThe objectives of this project were (i) to identify and analyse available literature relevant to non- intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe literature review attempted to uncover human factors models, design guidelines, design concepts and empirical research that could be used to inform the design of the functional elements of an alerting system, including:\r• Configuring alert parameters;\r• Receiving information on alert states;\r• Maintaining awareness and performance on the primary task;\r• Comprehending the alert condition;\r• Actioning the alert condition; and\r• Managing alerts.\rResults: The results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, relevant concepts from the literature relating to alert/alarm design in general were extracted. These concepts, combined with general human factors principles, provided direction for a number of design concepts for a future, non-intrusive alerting interface for maritime anomalies.\rA total of four interface design concepts were developed and then reviewed and evaluated by seven subject matter experts. The designs, which were all visual, included an alert indicator, information related to an incoming alert and an alert management window. The design evaluation identified a need to scale the intrusiveness of alerts (i.e. high priority alerts require a more intrusive alert than those of low priority).\rSignificance: Feedback from the SMEs, combined with consideration of general human factors principles, resulted in a list of design requirements for the best way to:\r• Alert RMP operator to new incoming alert\r• Provide operator with awareness of the number of active alerts in the system\r• Provide operator with information specific to an incoming alert\r• Provide operator with information on all active alerts in the system\rHumansystems® Non-Intrusive Alert System Page iii\r\n• Enable operator to manage (i.e. action) any active alerts in the system Future plans: In general, future research and development efforts should focus on:\r(i) establishing and validating detailed design requirements for an alerting system,\r(ii) developing an alert system interface design in accordance with the design principles presented in the report,\r(iii) experimentally evaluating alternate design options for an anomaly alert system in the context of the RMP (i.e. representative of user’s work environment including the potential number of alerts),\r.\r(iv)\rbasic research on better understanding what constitutes intrusiveness and how it relates to factors such as attention and annoyance, particularly as a function of alert interruption frequency.\rPage iv\rNon-Intrusive Alert System Humansystems®\r\nSommaire\rLe Tableau de la situation maritime (TSM), l’un des principaux produits des centres régionaux d'opérations interarmées (CROI), est essentiellement une carte des eaux côtières canadiennes indiquant des contacts, en général des navires. Étant donné l’ampleur du trafic maritime et la vaste zone couverte, il est difficile de contrôler efficacement le TSM de manière à bien comprendre la situation courante, y compris les anomalies (augmentation soudaine de vitesse, déroutement des navires, etc.). Il est donc nécessaire de disposer d’un système capable d’exécuter des vérifications de routine pour déceler les données irrégulières sur l’arrière-plan, puis de transmettre ces informations aux opérateurs sous une forme telle que les anomalies soient immédiatement compréhensibles et qu’on puisse prendre rapidement connaissance de la situation. Le noeud du problème consiste à trouver un moyen de mettre en oeuvre un système d’alerte qui renseigne les opérateurs sur les anomalies possibles sans entraver l’exécution de leurs tâches principales (alerte non intrusive).\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation visait à découvrir des modèles de facteurs humains, des orientations, des concepts et des recherches empiriques qui pourraient être utilisés pour guider la conception des éléments fonctionnels d’un système d’alerte, ce qui comprend :\r• la configuration des paramètres d’alerte;\r• la réception d’information sur les états d’alerte;\r• le maintien de la connaissance de la situation et l’exécution des tâches principales;\r• la compréhension des états d’alerte;\r• l’activation de l’état d’alerte;\r• la gestion des alertes.\rRésultats : L’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, la documentation a permis d’identifier des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts pour une future interface d’alerte non intrusive en cas d’anomalies maritimes.\rEn tout, quatre concepts d’interface ont été définis, puis examinés et évalués par sept experts. Ces concepts, tous de type visuel, comprenaient les suivants : indicateur d’alerte, information liée à une nouvelle alerte et fenêtre de gestion des alertes. L’évaluation de la conception a révélé qu’il fallait prioriser le caractère intrusif des alertes (dans les cas de haute priorité, l’alerte doit être plus intrusive).\rPortée : En tenant compte de la rétroaction des experts, et de principes généraux touchant les facteurs humains, on a établi une liste des exigences à respecter pour la conception d’un système qui remplira le mieux possible les fonctions suivantes :\r• transmettre les nouvelles alertes à l’opérateur du TSM;\rHumansystems® Non-Intrusive Alert System Page v\r\n• informer l’opérateur du nombre d’alertes actives dans le système;\r• fournir à l’opérateur l’information particulière à une nouvelle alerte;\r• fournir à l’opérateur l’information relative à toutes les alertes actives dans le système;\r• permettre à l’opérateur de gérer (intervention) toutes les alertes actives dans le système\rRecherches futures : En général, les futurs travaux de recherche et de développement devraient se concentrer sur les tâches suivantes :\r(v) établir et valider des exigences détaillées pour la conception d’un système d’alerte;\r(vi) définirunconceptd’interfacedesystèmed’alerteconformémentauxprincipesde conception présentés dans le rapport;\r(vii) évaluer expérimentalement différentes options pour la conception d’un système d’alerte en cas d’anomalies dans le contexte du TSM (en tenant compte de l’environnement de travail de l’utilisateur, notamment du nombre éventuel d’alertes);\r(viii)mener des recherches de base visant à mieux comprendre la nature de l’intrusivité et comment celle-ci se rattache à des facteurs tels que l’attention et le dérangement, notamment en fonction de la fréquence des interruptions en cas d’alerte.\rPage vi Non-Intrusive Alert System Humansystems®\r\nTable of Contents\rABSTRACT ......................................................................................................................................................I RÉSUMÉ ...........................................................................................................................................................I EXECUTIVE SUMMARY ...........................................................................................................................III SOMMAIRE ................................................................................................................................................... V TABLE OF CONTENTS ............................................................................................................................ VII LIST OF FIGURES.......................................................................................................................................IX LIST OF TABLES.......................................................................................................................................... X ACKNOWLEDGMENT ...............................................................................................................................XI\r1.\r2.\r3.\r4.\rINTRODUCTION .................................................................................................................................. 1\r1.1 BACKGROUND................................................................................................................................... 1\r1.1.1 Recognized Maritime Picture ...................................................................................................... 1\r1.1.2 Maritime anomalies ..................................................................................................................... 2\r1.1.3 Detection of anomalies ................................................................................................................ 2\r1.2 SCOPE AND OBJECTIVES .................................................................................................................... 3\r1.3 NON-INTRUSIVE ALERTING ............................................................................................................... 3\r1.4 OUTLINE OF REPORT ......................................................................................................................... 3\rTHE BASIC ONTOLOGY OF AN ALERT SYSTEM ...................................................................... 5\r2.1 CONFIGURE ALERT PARAMETERS ...................................................................................................... 6\r2.2 RECEIVE INFORMATION ABOUT THE ALERT STATE ............................................................................ 7\r2.3 COMPREHEND THE ALERT CONDITION............................................................................................... 7\r2.4 ACTION THE ALERT CONDITION......................................................................................................... 7\r2.5 MANAGE ALERTS .............................................................................................................................. 7\r2.6 MAINTAIN PRIMARY TASK ................................................................................................................ 7\r2.7 ORGANISATION OF THE LITERATURE AND THE ONTOLOGY................................................................ 8\rTOWARDS A WORKING DEFINITION OF “NON-INTRUSIVENESS”...................................... 9\r3.1 DEFINING NON-INTRUSIVENESS ........................................................................................................ 9\r3.1.1 Predictions of attentional/resource models ................................................................................. 9\r3.1.2 Predictions of annoyance models .............................................................................................. 10\r3.1.3 Summary of principles ............................................................................................................... 10\rLITERATURE REVIEW .................................................................................................................... 13\r4.1 METHOD ......................................................................................................................................... 13\r4.1.1 Keywords ................................................................................................................................... 13\r4.1.2 Databases .................................................................................................................................. 14\r4.1.3 Search strategy .......................................................................................................................... 14\r4.1.4 Selection and review of articles ................................................................................................. 14\r4.1.4.1 Final evaluation criteria ....................................................................................................................15 Humansystems® Non-Intrusive Alert System Page vii\r\n5.\r6.\r4.2 RESULTS OF THE LITERATURE REVIEW ...........................................................................................16\r4.2.1 Configure alert parameters ........................................................................................................16\r4.2.2 Receive information on alert state..............................................................................................18\r4.2.3 Comprehend alert information content ......................................................................................46\r4.2.4 Maintain primary task ................................................................................................................48\r4.2.5 Manage Alerts ............................................................................................................................53\r4.3 CONCLUSIONS .................................................................................................................................57\r4.3.1 State of current knowledge .........................................................................................................57\r4.3.2 Gaps in the literature .................................................................................................................61\rTHE DESIGN DEVELOPMENT PROCESS.....................................................................................65\r5.1 DEVELOPMENT OF DESIGN CONCEPTS..............................................................................................65\r5.1.1 The alert indicator ......................................................................................................................65\r5.1.2 Visual design concepts ...............................................................................................................67\r5.1.3 Tactile design concept ................................................................................................................73\r5.1.4 The alert information window ....................................................................................................73\r5.1.5 The RMP pop up box ..................................................................................................................76\rEVALUATION AND REVIEW OF DESIGN CONCEPTS BY SUBJECT MATTER EXPERTS .................................................................................................................................................................77\r6.1 PREPARATION FOR DESIGN EVALUATION AND REVIEW...................................................................77\r6.2 METHOD ..........................................................................................................................................77\r6.2.1 Date and Location of SME Evaluation.......................................................................................77\r6.2.2 Participants ................................................................................................................................77\r6.2.3 Materials ....................................................................................................................................77\r6.2.4 Procedure ...................................................................................................................................78\r6.3 RESULTS..........................................................................................................................................79\r6.3.1 Questionnaire data .....................................................................................................................79\r6.3.2 Quantitative data........................................................................................................................80\r6.3.3 Qualitative data..........................................................................................................................85\r6.3.4 General discussion with participants .........................................................................................88\r6.4 CONCLUSIONS AND RECOMMENDATIONS BASED ON EVALUATION...................................................91\r6.4.1 Conclusions ................................................................................................................................91\r6.4.2 Limitations..................................................................................................................................92\r6.4.3 Design recommendations for anomaly alert system ...................................................................92\rOVERALL CONCLUSIONS AND RECOMMENDATIONS..........................................................95\r7.1 CONCLUSIONS .................................................................................................................................95\r7.2 FUTURE WORK ................................................................................................................................95\r7.\rREFERENCES ...............................................................................................................................................97 ANNEX A: ETHICS PROTOCOL............................................................................................................ A-1 ANNEX B: QUESTIONNAIRES FOR SME EVALUATION.................................................................B-1 ANNEX C: GENERAL DISCUSSION INTERVIEW QUESTIONS..................................................... C-1 ACRONYMS ............................................................................................................................................... D-1\rPage viii Non-Intrusive Alert System Humansystems®\r\nList of Figures\rFIGURE 1: RELATIONSHIP OF LITERATURE TO THE ALERT ONTOLOGY ................................................................ 6 FIGURE 2: IRC FRAMEWORK (ADAPTED FROM MCCRICKARD ET AL., 2003A, P. 321) ....................................... 19 FIGURE 3: HAZARD NETWORK (HAUTAMAKI ET AL., 2006, P. 7) ...................................................................... 37 FIGURE 4: INTERRUPTION MANAGEMENT STAGE MODEL (ADAPTED FROM MCFARLANE & LATORELLA, 2002,\rP. 16) ....................................................................................................................................................... 39 FIGURE 5: ALERT LIST (RIVEIRO ET AL., 2008, P. 6; © 2008IEEE) ................................................................... 55 FIGURE 6: DESIGN 1: CUMULATIVE TOTAL INDICATOR.................................................................................... 68 FIGURE 7: DESIGN 2: VERTICAL CUMULATIVE INDICATOR .............................................................................. 69 FIGURE 8: DESIGN 3: HORIZONTAL INDICATOR BAR ........................................................................................ 70 FIGURE 9: DESIGN 4: TICKER & FADING BAR................................................................................................... 71 FIGURE 10: DESIGN 5: POLYGON ...................................................................................................................... 72 FIGURE 11: ALERT INFORMATION WINDOW..................................................................................................... 74 FIGURE 12: RMP TRACK HIGHLIGHT ................................................................................................................ 75 FIGURE 13: ALERT INFORMATION WINDOW AFTER A TRACK IS DELETED......................................................... 75 FIGURE 14: RMP POP UP BOX ........................................................................................................................... 76 FIGURE 15: SUBJECT RANKINGS FOR EFFECTIVENESS IN BRINGING ALERTS TO MY ATTENTION ........................ 83 FIGURE 16: SUBJECT RANKINGS FOR METHODS FOR REPRESENTING THE PRIORITIES ........................................ 83 FIGURE 17: SUBJECT RANKINGS FOR PROVIDING REQUIRED INFORMATION WITHOUT DISTRACTION................. 84\rHumansystems® Non-Intrusive Alert System Page ix\r\nList of Tables\rTABLE 1: KEYWORDS........................................................................................................................................13 TABLE 2: DATABASES SEARCHED .....................................................................................................................14 TABLE 3: OPERATOR INFORMATION PREFERENCE PROFILE (HUDLICKA & MCNEESE, 2002) ............................17 TABLE 4: IRC FRAMEWORK ALERTS AND EXAMPLES (MCCRICKARD ET AL., 2003A) .......................................20 TABLE 5: MODEL-BASED PRINCIPLES FOR HUMAN-CENTRED ALARM SYSTEMS (SHORROCK ET AL., 2002) ......23 TABLE 6: SUGGESTIONS FOR AUDITORY ALERTS (EDWORTHY & HELLIER, 2002).............................................25 TABLE 7: IDENTIFIED PROBLEMS AND DESIGN REQUIREMENTS (HAN ET AL., 2007) ..........................................28 TABLE 8: EXAMPLES OF GUIDELINES (HAN ET AL., 2007) .................................................................................29 TABLE 9: RESULTS FOR OPERATOR TASKS: STUDY 1 (MCCRICKARD ET AL., 2003B) ........................................32 TABLE 10: RESULTS FOR OPERATOR TASKS: STUDY 2 (MCCRICKARD ET AL., 2003B) ......................................33 TABLE 11: RECOMMENDATIONS FOR DESIGN FOR A PLATOON LEADER ALERT (KRAUSMAN ET AL., 2005) .......34 TABLE 12: SUMMARY .......................................................................................................................................45 TABLE 13: LITERATURE IN THE REPORT ............................................................................................................57 TABLE 14: ALERT DESIGN PRINCIPLES ..............................................................................................................58 TABLE 15: SUMMARY OF LITERATURE RELEVANCE AND NEED FOR RESEARCH .................................................63 TABLE 16: MEAN RATINGS FOR USABILITY AND USEFULNESS OF NON-INTRUSIVE ALERTING SYSTEM ...........80 TABLE 17: MEAN RATINGS FOR THE ALERT DESIGN QUESTIONS .....................................................................81 TABLE 18: MEAN RATINGS FOR DESIGN IMPLEMENTATION AND INTRUSIVENESS ..............................................85 TABLE 19: LIKES AND DISLIKES OF ALERT DESIGNS ........................................................................................86 TABLE 20: DESIGN RECOMMENDATIONS FOR FUTURE ITERATIONS OF ALERT SYSTEM INTERFACE....................92\rPage x Non-Intrusive Alert System Humansystems®\r\nAcknowledgment\rWe would like to thank Lt(N) (ret) J. R. Rafuse for providing valuable information that contributed to the development of design concepts for this project and for detailed background information on how operators might prioritize alerts. We should also like to thank all of the people that participated in the evaluation of the design concepts for their time and effort.\rHumansystems® Non-Intrusive Alert System Page xi\r\nThis page intentionally left blank.\rPage xii Non-Intrusive Alert System Humansystems®\r\n1. Introduction 1.1 Background\rDefence Research and Development Canada (DRDC) has an ongoing Applied Research Project in the Maritime Intelligence, Surveillance, and Reconnaissance (MISR) Thrust on information visualization and management for enhanced domain awareness in maritime security. This is a 4- year Research and Development (R&D) project with the goal of enhancing the “maritime picture” through improved quality of information and novel, adaptive ways of visualizing that information. The DRDC team wants to (i) investigate best practices for the design of new visualization aids for operators that may improve their understanding of issues such as information uncertainty and data anomalies, and (ii) test to see if visualizing this information can help improve analysis and situation awareness of the Recognized Maritime Picture (RMP), decision making based on the RMP, and the RMP operators’ efficiency in such activities.\r1.1.1 Recognized Maritime Picture\rThe RMP is one of the primary outputs of the Regional Joint Operations Centers (RJOCs). In its common form, the RMP is a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Other important outputs from the RJOCs are specific reports on Vessels of Interest (VOI). Although produced by the RJOCs, the RMP and VOI reports are also used by other government agencies (e.g., Department of Fisheries and Oceans, the Royal Canadian Mounted Police and Canadian Navy ships at sea to further their own specific interests). Therefore, it is particularly important that the information produced in the RMP and VOI reports is accurate and reliable.\rThe term “Recognized Maritime Picture” implies that there is some element of knowledge of the attributes of a contact, such as the vessel’s name, hull number or class (e.g., tanker, fishing boat, warship). A typical RMP plot would show hundreds of contact symbols, which are colour coded using standard North Atlantic Treaty Organization (NATO) conventions for identity (friend, hostile, neutral, unknown) and may also contain a vector arrow to indicate last known direction of movement. Associated with each plot on the map are metadata, which are shown in a tabular data display containing approximately 22 data fields of information concerning the contact. In addition, the tabular report includes detailed information on the reporting source. There are as many as 20-25 reporting sources (Davenport, Widdis and Rafuse, 2005) that could potentially contribute to a contact report. These reporting sources range from highly detailed and accurate reports from Provincial Airlines (PAL) overflights, Canadian Coastguard and Canadian Navy ships at sea as well as detailed information on a ship’s name and position from ships carrying Automatic Identification System (AIS) transponders to single contact reports from radar sources such as High Frequency Surface Wave Radar (HFSWR) and Electronic Intelligence (ELINT).\rThe many sensor and reporting sources that contribute to the RMP, therefore, push large volumes of data into associated databases. The RMP is constantly being updated, both manually by operators and automatically. Given the extensive maritime traffic and the large area covered, it is difficult to effectively monitor the RMP to maintain a good understanding of the current situation.\rHumansystems® Non-Intrusive Alert System Page 1\r\n1.1.2 Maritime anomalies\rAnomalies are defined as deviations from the norm (Riveiro, Falkman, & Ziemke, 2008) or targets that are not easily classified (Roy, 2008). The more common types of RMP Anomalies include attribute, movement and VOI-related anomalies.\rAttribute related anomalies: In the Concept of Operations (CONOPS) for producing the RMP, a track refers to one ore more contact reports linked together to describe any detected discrete airborne, surface or subsurface object. A track has many attributes, any of which can be missing, incorrect (e.g., misspelled) or inconsistent (changed by different operators, or characterized by different standards/nomenclature/schemas as tracks pass through various RMP management sites). Such problems cause ambiguous tracks in some cases or just bad information on a contact in others. At the moment, the most common alerting method for attribute mismatch is the production of an ambiguous track by Global Command and Control System-Maritime (GCCS-M), a de facto form of alerting that there is a problem with a track, which the operator then must resolve.\rMovement related anomalies include:\r• Unexpected or illogical course and speed changes\r• Failure to reach a reported estimated time of arrival (ETA) at a specified point\r• Loss of reporting\r• Movement that suggests activity of concern, such as very close proximity to other vessels\rin open waters\r• Significant variance in voyage/route compared to historical trace of other recent voyages\rVOI related anomalies: The Maritime Forces Atlantic (MARLANT) “Alert Table” is a list that extracts key fields (usually vessel name) from RMP messages if the vessel has been previously entered on the list (mostly VOIs). This “alert” prompts operators to contact whoever is listed as the interested agency for the VOI.1\r1.1.3 Detection of anomalies\rDetection of anomalies requires identification of whether or not specific behaviours are abnormal. A model of this process has been proposed by Riveiro and colleagues (2008). According to the model, anomaly detection is a complex process, requiring information acquisition (search and detection), analysis and the ability to integrate events within the operator’s situation awareness and mental model of the operational environment. In the present case, this would be bounded by the information provided by existing operational systems (e.g., GCCS-M, the Contact History Database (CHDB)).\rOperators who may be required to perform anomaly detection in conjunction with their visualization aids face a number of different challenges. Rhodes (2007) points out two specific problems associated with the detection and prediction of anomalous behaviour. First, normalcy is dependent on the context in question. It is nearly impossible to create a system that recognizes every type of normal and abnormal behaviour. Thus, a system that is always learning and adapting is needed to deal with such complexity. However, such systems need to function reliably without requiring a high level of operator involvement.\r1 For many operators, this would be their only concept of an alerting method together with the method used to represent ambiguous track generation in GCCS-M.)\rPage 2 Non-Intrusive Alert System Humansystems®\r\nIn addition to anomaly identification, RJOC operational staff has extremely demanding work responsibilities (Davenport et al., 2005; Matthews, Bruyn, Keeble & Rafuse, 2004) that effectively preclude them from spending time doing detective work at uncovering data anomalies. None of the current job functions or tasks performed at the RJOCs includes any effort directed specifically at anomaly detection or analysis with the possible exception of VOIs (Matthews et al., 2004). Thus, there is a need for a system that could perform routine checks for anomalous data in the background and then make the information available to operators in a format that makes the anomalies readily comprehensible and gives rise to rapid situation awareness. The crux of the problem is how to implement an alerting system that would make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\r1.2 Scope and objectives\rThe objectives of the project as a whole were (i) to identify and analyse available literature relevant to non-intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe literature review provided direction for a number of design concepts for a future, non-intrusive alerting interface. This alerting system interface must serve the information requirements of operators and improve their situation awareness of anomalies and their meaning for the RMP. Because operators must multi-task and there exists a potential for a large number of data anomalies to occur, the major constraint for the interface design is that it be “non-intrusive”. A total of five design concepts were developed, four of which were demonstrated for naval SMEs in order to gather user feedback.\r1.3 Non-intrusive alerting\rNon-intrusive alerting is not a clearly defined concept that can be expressed in absolute terms, but is highly context dependent. Essentially, the concept of non-intrusiveness conveys the notion of advising an operator (creating awareness) that an alert condition has occurred in a manner that does not disrupt ongoing task performance. It should be noted that a preliminary search indicated that there is limited literature available that deals specifically with the issue of the intrusiveness of an alert, and how to scale this intrusiveness to a particular set of operational circumstances. Therefore, the review and analysis of much of the following literature was to determine what useful information could be extracted from the literature on more generic aspects of alarm and alerting systems that would be applicable or extensible.\r1.4 Outline of report\rSections 2 and 3 describe the basic ontology of an alert system as well as a working definition of non-intrusiveness. These sections are based on the knowledge that was gained by reviewing the literature and provide a framework for discussing the results of the literature review. Section 4 presents the method, results and conclusion of the literature review. In section 5, we provide examples of design concepts for elements of a non-intrusive alerting system for implementation in a GCCS-M environment. This is supplemented by a separate PowerPoint interactive demonstration that allows a cognitive walkthrough of the design alternatives. Section 6 describes the review and assessment of the design concepts by SMEs from RJOC-Atlantic. The final section of the report gives the overall conclusions and recommendations.\rHumansystems® Non-Intrusive Alert System Page 3\r\nThis page intentionally left blank.\rPage 4 Non-Intrusive Alert System Humansystems®\r\n2. The Basic Ontology of an Alert system 2\rPrior to reviewing the results of the literature search, we provide in this section an outline of the functional elements of an alerting system which we believe brings a useful and coherent structure or framework for the diverse papers reviewed. These ideas, in themselves, stem from insights gained in the literature review. Our objective is to define the functional elements an alerting system and how they relate to each other.\rIn addition, certain constraints and assumptions have guided this analysis concerning the role of the operator and the team, as follows:\r• The operator has a primary task or tasks to fulfil which are NOT the monitoring or management of alerts;\r• The operator does not have any other team members who will monitor or manage alerts; and\r• The goal of the operator is to deal with alerts as efficiently as possible and to return promptly to the primary task.\rThe functional elements of an alerting system serve several different tasks that a user may be required to perform in the set-up, operation and maintenance of an alerting system. These have been adapted, and expanded upon, from McCrickard, Chewar, Somervell and Ndiwalana (2003a) and are outlined below:\r• Configuring alert parameters;\r• Receiving information on alert states;\r• Maintaining awareness and performance on the primary task;\r• Comprehending the alert condition;\r• Actioning the alert condition; and\r• Managing the accumulated alert information.\rThe elements of the ontology are shown in Figure 1 and are outlined below. Explanation of the colour and line coding is provided in section 2.7.\r2 The essential meaning of an ontology is that it is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology (Gruber, 1995).\rHumansystems® Non-Intrusive Alert System Page 5\r\nFigure 1: Relationship of literature to the alert ontology 2.1 Configure alert parameters\rIn an operational environment such as the RJOC, the potential exists for many data anomalies to trigger alerts. Some anomalies will be considered to be high priority, others low. In addition, different circumstances may require different rules to be put into place concerning when an alert is brought to the attention of the operator. Therefore, a critical component of the system is a function that allows managers and operators to define the criteria for an alert and to assign different alerting priorities to these events. The interface for this function should allow operators to simply create rules by picking among menu choices, for example:\r“in area (set co-ordinates), if vessels of (type) have speed = (value) then alert with priority (x)”\rFor the purposes of the present project, this aspect of an alerting system was considered out of scope to be examined in detail; therefore no specific literature search was conducted on this topic, although one useful paper that emerged from the more general search was reviewed. However, we believe that the ability to set alert parameters that are suitably configured to an operational context represents an important aspect of reducing nuisance alarms. In addition, allowing operators to set categories of alarm priority and matching these categories to the way the alarm state is conveyed, represents an important aspect that is highly relevant to the intrusiveness of the alarm.\rPage 6 Non-Intrusive Alert System Humansystems®\r\n2.2 Receive information about the alert state\rThis is the key component of the system and incorporates the means by which an alert state will be brought to the attention of the operator. This function is concerned with just the notification and does not provide details concerning the nature of the condition that gave rise to the alert. However, the alert notification should provide information about the alert priority. The design of the alert indicator is the principle focus of this project\r2.3 Comprehend the alert condition\rHaving been notified of an alert, at some point the operator will decide to process the alert and needs further information about the condition(s) that resulted in the alert. This information needs to be provided in a succinct manner that allows the operator to readily comprehend the type of alert. In the case of the RJOC, the interface should allow the operator to rapidly identify the vessel involved and the area of the RMP concerned. This functionality is a secondary point of focus for this project.\r2.4 Action the alert condition\rThis is a function that allows the operator to investigate the alert and take whatever action is required. This functionality may be integrated into the interface for an alerting system or may involve other work functions independent of the alert system. It could be considered that this function is not really part of an alerting system at all; however, at some point the operator will return to the primary task, therefore the interface will need to provide suitable functionality to support this transition. While the actioning of an alert was out of scope for this project, the regaining of situation awareness of the primary task was considered important and is dealt with more extensively under the function of Maintain Primary Task.\r2.5 Manage alerts\rAs alerts accumulate in the operating environment, a new task relating to their management arises for the operator and possibly system manager. The database of these alerts will need to be structured and formatted in such a manner that it permits the operator to quickly determine, for example, how many alerts are in the system, what are there priorities and when they occurred. Simple tools for the sorting and deletion of records will also be required. This aspect of an alerting system was not the primary focus of the literature review, however, some basic functional elements for the interface for such a system have been developed and are shown later in the report in the section on design concepts. This inclusion was thought to be necessary because effective design of the process for alert management may reduce operator worker load for this activity and indirectly mitigate the information processing costs associated with processing the alarm, thereby potentially reducing the overall alarm intrusiveness into the normal workflow.\r2.6 Maintain primary task\rThe operator’s need to maintain focus on his/her primary task responsibilities is not really part of an alerting system; however it does have implication on the relationship between this primary task and various aspects of the tasks performed in responding to and managing alerts. Some considerations are when to present information about an alert state, i.e., when is the best time to\rHumansystems® Non-Intrusive Alert System Page 7\r\ninterrupt the primary task? Should warnings be provided of impending alerts to allow the operator to better prepare and not suddenly lose primary task focus by a surprise alert? A further consideration is how to enable the operator to regain primary task situation awareness after dealing with an alert. The focus of the present project is only on the last of these three questions.\r2.7 Organisation of the literature and the ontology\rIn order to structure the literature review in terms of its relevance to the ontology, a classification system was applied that broke the literature down into four main categories:\r1. Conceptual models\r2. Generic alert system design guidelines\r3. Specific design concepts for alerts\r4. Relevant empirical research on alert systems\rIn Figure 1, we show the specific links between these categories and those elements of an alerting system that were the focus of the literature search. The primary areas of interest are shown in green boxes and are linked to the literature review by solid lines. The secondary areas are shown in light yellow and the links are shown by broken lines. Note that there is no link to the “tools to service alerts” function, since this was completely beyond the scope of this project.\rPage 8 Non-Intrusive Alert System Humansystems®\r\n3. Towards a Working Definition of “Non- intrusiveness”\r3.1 Defining non-intrusiveness\rPrior to considering any design options for a non-intrusive alert system and in order to better understand the literature review, we believe that it is necessary to clarify what is meant by “intrusive” and to develop a working operational definition that will guide the design process, which is described later in the report. As a starting point, we make the assumption that the intrusiveness of an alert or alarm can be defined as an event which, either immediately, or over time, reduces the ability, or potential capacity, to perform a primary task.\rWith this definition in mind, a central question is then what psychological models are relevant to intrusiveness and how do they predict what would constitute an intrusive or non-intrusive alert. There appear to be two classes of models that are relevant to this issue. The first class of models deals with attention and resource sharing and the second class concerns models of annoyance. Attentional or resource sharing models (e.g., Wickens, 1984) predict interference in cognitive processing of a primary task when an alert competes for common resources. On the other hand, annoyance models would suggest that it is some quality of the alarm (e.g., for auditory alarms, intensity, frequency components, duration and repetition cycle) that produces a psychological state of annoyance that becomes so compelling that attention can no longer be devoted to the primary task (e.g., De-Muer, Botteldooren, Coensel, Berglund, Nilsson & Lercher, 2005).\r3.1.1 Predictions of attentional/resource models\rAccording to attention and resource sharing models, an alarm would be considered intrusive if it directly affects the processing of the primary task by sensory interference. For a visual monitoring task this might be a visual alert that occurs at the primary point of visual focus thereby directly interfering with the perception of visual information required to monitor the display. Other examples of this might be the whole display flashing on and off, a visual message that pops up in the middle of the screen or a siren that occurs over an auditory communication network. A second method of interference suggested by the attentional/resource models would be an alarm that competes in a compelling way for attentional resources, but does not in itself directly interfere with the perceptual or cognitive processing of the primary task. That is, the alarm does not directly impede information processing, but rather vies with the primary task for the operator’s attention. An example of this might be a flashing border on all four sides of a visual display. A third method of interference may result from population stereotypes that have developed concerning what alarms\rlook or sound like, for example red flashing lights, sirens or certain iconic symbols (\rcase it is assumed that the semantic content associated with the stimulus immediately overrides ongoing processing and captures attention. In other words, upon perceiving the alarm, the operator immediately knows it is something important and needs attention.\rWhile it is easy to see how such models can suggest the intrusiveness of an alert in such obvious cases, it is less clear how they would predict the intrusiveness of, for example, a ticker style text message that runs along the bottom of the screen.\r). In this\rHumansystems® Non-Intrusive Alert System Page 9\r\nThe general guidance that can be taken from these models is that non-intrusive alarms should not compete for sensory or cognitive processing capacity of the primary task, should not divert attentional resources and should avoid configurations that may trigger over-learned orientation responses because of familiarity and common usage in society.\rIn developing non-intrusive interfaces for an anomaly alerting system, we have used our knowledge of the RJOC operator’s tasks and current system interface to inform the designs such that they do not compete for attentional resources or cognitive processing capacity of the primary task. However, ultimately the degree to which an alert provides the right level of non-intrusiveness should be judged and validated by the end user in the appropriate operational context.\r3.1.2 Predictions of annoyance models\rThe only predictive annoyance models that we have found deal largely with noise annoyance and apply to more general societal issues than to cognitive work environments. Such models may take into account factors of the strength of the noise source, frequency, adaptation and appraisal by the human, emotions aroused and cognitive processing. However, we have not found any extensive or compelling data that would suggest specific design guidance, other than the obvious. For example, repetitive sounds or signals will generate an annoyance factor over time. However, the degree to which such annoyance produces degradation in or distraction from primary task performance cannot be specified, nor can one say for how long a repetitive signal would have to continue before it becomes annoying, what level of intensity causes annoyance and how individual differences and experience may influence annoyance “thresholds”.\rTherefore, we will proceed to use common sense in the interpretation of what could be potentially annoying in an alert and to avoid such design options. However, as noted above, the degree to which an alert provides the right level of non-intrusiveness should ultimately be judged and validated by the end user in the appropriate operational context. A design that we believe to be non-intrusive could ultimately turn out to be the opposite if the underlying anomaly trigger occurs with high frequency in actual operations. This might be the case when there is a data dump from an information source (e.g., PAL flight or AIS Vessel Monitoring System (AVMS) receiver).\r3.1.3 Summary of principles\rThe following are the main principles that make an alert intrusive.\r• Direct perceptual interference with the ongoing primary task;\r• Direct cognitive interference with the primary task;\r• Continued presence of a stimulus that by repetitiveness, intensity or quality results in a psychological or emotional state that causes an individual to lose attentional focus on the primary task; and\r• The greater the perceptual deviation of the alerting stimulus from the ambient perceptual environment, the more intrusive it will generally be (e.g., providing an auditory alert during a primarily visual task).\rIt follows from this that non-intrusive alerts must have properties that correspond to the converse of the above principles. That is, a non-intrusive alert must not directly interfere with perceptual or cognitive processing of the primary task; draw the operator’s attention away from the primary task due to its repetitiveness, intensity or quality; or differ significantly from the background perceptual\rPage 10 Non-Intrusive Alert System Humansystems®\r\nenvironment. However, as outlined in Section 5, our objective was to design non-intrusive alerts for anomalies that vary in priority level; from low (priority 3) to high (priority 1). Consequently, the priority of the anomalous information must be implied by the level of intrusiveness of the alert itself. That is, alerts for high priority anomalies must be more intrusive than those for low priority information.\rIn addition to avoiding the above factors that would make an alert intrusive, a key element in alert system design, to minimize (if not avoiding entirely) interference in the primary task due to alarm intrusiveness, is to provide an ability for operators to match alarm priorities to the appropriate level of cognitive and perceptual intrusiveness.\rHumansystems® Non-Intrusive Alert System Page 11\r\nThis page intentionally left blank.\rPage 12 Non-Intrusive Alert System Humansystems®\r\n4. Literature Review 4.1 Method\rThis section outlines the methodology used to search the literature as well as select research relevant to the design of non-intrusive alert systems.\r4.1.1 Keywords\rThe following keywords for conducting the literature search were agreed upon with the Technical Authority (TA) and are shown in Table 1.\rTable 1: Keywords\rCore Concept\rPriority 1 keywords\rPriority 2 keywords\rAlerting System Technology\rAlarm/warning/alert system Non-intrusive\rAdvisory system\rTote board\rAuditory/visual alarm Tote\rOperator Interface\rDesign guidelines/concepts Human-computer Operator-machine Interface design\rUser interface Intelligen* Adaptive Etiquette\rHuman factors Smart\rHuman Performance\rFalse alarm\rAttenti*\rMonitor*\rWorkload\rModel*\rSituation* awareness Distract*\rDisrupt*\rAnomaly/change detection Anomal*\rKeywords to be searched independently\rAlarm/warning/alert overload Missed alarm/warning/alert\rIn conducting the search, all Alerting System Technology keywords were paired with Operator Interface keywords and with Human Performance keywords using “AND” logic. The keywords to be searched independently (i.e., alarm/warning/alert overload and missed alarm/warning/alert) were not paired with any other keywords. It was initially proposed that if the literature search using the priority 1 keywords did not produce ideal results, priority 2 keywords would be substituted for priority 1 keywords. However, based upon an abundance of articles found with priority 1 keywords there was no reason to pursue this option.\rHumansystems® Non-Intrusive Alert System Page 13\r\n4.1.2 Databases\rTable 2: Databases searched\rDatabase\rDescription\rPsycINFO\rThe PsycINFO database is a collection of electronically stored bibliographic references, often with abstracts or summaries, to psychological literature from the 1800s to the present. The available literature includes material published in 50 countries, but is all presented in English. Books and chapters published worldwide are also covered in the database, as well as technical reports and dissertations from the last several decades.\rNTIS\rNTIS is an agency for the U.S. Department of Commerce’s Technology Administration. It is the official source for government sponsored U.S. and worldwide scientific, technical, engineering and business related information. The 400,000 article database can be searched for free at the www.ntis.gov. Articles can be purchased from NTIS at costs depending on the length of the article.\rCISTI\rCISTI stands for the Canada Institute for Scientific and Technical Information. It is the library for the National Research Council of Canada and a world source for information in science, technology, engineering and medicine. The database is searchable on-line at cat.cisti.nrc.ca. Articles can be ordered from CISTI for a fee of approximately $12.\rSTINET\rSTINET is a publicly-available database (stinet.dtic.mil) which provides access to citations of documents such as: unclassified unlimited documents that have been entered into the Defence Technology Technical Reports Collection (e.g., dissertations from the Naval Postgraduate School), the Air University Library Index to Military Periodicals, Staff College Automated Military Periodical Index, Department of Defense Index to Specifications and Standards, and Research and Development Descriptive Summaries. The full- text electronic versions of many of these articles are also available from this database.\rGoogle Scholar\rThe World Wide Web was searched using the Google Scholar search engines (scholar.google.com).\rDRDC Research Reports\rDRDC Defence Research Reports is a database of scientific and technical research produced over the past 6- years by and for the Defence Research & Development Canada. It is available online at pubs.drdc-rddc.gc.ca/pubdocs/pcow1_e.html.\r4.1.3 Search strategy\rTo maintain a record of the process, the following information was documented in a spreadsheet:\r• Database searched (e.g., Psych Info);\r• Keyword combination (e.g., Non intrusive AND attenti*);\r• Number of hits;\r• Number of applicable hits;\r• Articles downloaded;\r• Articles/books that require purchase; and\r• If applicable, where in the article the keywords were searched (i.e., only in the article\rkeywords or anywhere in the article).\r4.1.4 Selection and review of articles\rTwo articles were purchased and 68 articles were downloaded giving a total of 70 relevant articles. Each article was then briefly reviewed and classified according to article theme (i.e., Human performance or Operator interface), source of the article (i.e., keyword search, article provided by the TA, or from another article), and whether the article included theoretical or empirical research.\rPage 14 Non-Intrusive Alert System Humansystems®\r\nThe research team first used criteria of relevance and quality to evaluate the 70 articles. Relevance was defined as how closely the article relates to the research objectives outlined in the Statement of Work. Specifically, relevance was assigned the following 4 point scale:\r• 1 = non-intrusive alerts to inform operators (design/performance) mentioned in the abstract or paper;\r• 2 = alerts mentioned in abstract or paper;\r• 3 = alerts mentioned in abstract or paper, but not the focus; and\r• 4 = no alerts mentioned in abstract but still somewhat relevant.\rQuality\rwas defined in terms of where the paper was published using a 3 point scale as follows:\r• 1 = journal;\r• 2 = technical report, summary or conference paper; and\r• 3 = magazine article.\rFollowing the initial search, a more refined search was undertaken to make certain all relevant literature was obtained. This included re-examining all the articles that were rated ‘1’ in terms of relevance (21 in total) and identifying any relevant references and additional relevant keywords that were not included in our original keyword list. This resulted in the addition of the following keywords:\r• Notification system;\r• Alarm cleanup;\r• Nuisance alarm; and • Pre-alarm.\rThese keywords were then paired with all Operator Interface and Human Performance keywords and searched in all of the above mentioned databases.\rFinally, the research team conducted a search for any articles that cited the 21 most relevant articles. Based on this refined search, 4 new articles were identified, providing a total of 74 articles. These 4 additional articles were also classified and evaluated according to the method described above.\rAn EndNote database and a spreadsheet with a record of all 74 articles, classification (as described above), relevance and quality rating as well as any relevant notes have been provided to the Scientific Authority. The accompanying endnote database includes the title, year, author, journal and abstract for the 74 articles.\r4.1.4.1 Final evaluation criteria\rThe research team developed the final criteria to evaluate the articles, based on feedback from the TA. The main concern was that the preliminary criteria would not capture innovative research from other areas (other than the core alert/alarm literature) that might be applicable to the design of non-intrusive alerts. Although this innovative research may not specifically deal with alerts, it could be applied to the warning system. The Quality criteria were maintained for the final search but the relevance criteria were refined to the following 3 point scale:\r• 1 = concepts related to alert intrusiveness (including design/performance/models/theory) specifically mentioned in paper;\r• 2 = concepts related to alerts were the focus of paper; and\r• 3 = concepts related to alerts were not focus of paper.\rHumansystems® Non-Intrusive Alert System Page 15\r\nArticles were then re-evaluated based on the final criteria. As a result, four articles that originally had a lower ranking were found to have concepts in the paper related to alert intrusiveness, while 8 articles received lower relevance scores.\rThe research team also performed an additional keyword search to address another potentially relevant area, namely human centred adaptive-automation. Thus, the additional following keywords were searched:\r• Mission;\r• Context;\r• Operator; and\r• Adaptive.\rThey were paired with:\r• Advisory system;\r• False alarm; and\r• Intrusiveness.\rThese keywords were searched in Google Scholar resulting in two new articles related to alert intrusiveness.\rIn total, there were 24 usable articles obtained from our searches that are related to alert intrusiveness. These articles and 7 more from the TA made up the 31 articles used for the literature review.\r4.2 Results of the Literature Review\rIn this section we provide a description and summary evaluation of each of the key papers. The papers are organized first by functional elements of an alerting system (described in the previous section). Within each section, the literature was further broken down into the four main categories described previously (i.e., conceptual models, generic alert system design guidelines, specific design concepts for alerts, relevant empirical research on alert systems).\r4.2.1 Configure alert parameters\rThis function encompasses the creation of rules for determining which anomalous events will give rise to an alert, and assigning an alert priority for each rule created.\rOf the four categories of literature, we were only able to find relevant papers relating to generic design guidelines. Note that some additional information on the setting of trip points and the reduction in nuisance alerts can be found in Brown, O’Hara and Higgins (2000) which is reviewed in detail in section 4.2.2.2.\r4.2.1.1 Design Guidelines\rConfigure for individual differences\rAs computer systems requiring adaptive user interface technologies mature and proliferate into critical applications, Hudlicka and McNeese (2002) argue that it is critical that user interfaces accommodate individual user characteristics. Citing recent research, Hudlicka and McNeese argue that individual differences impact perceptual processes, cognitive processes, motor processes, low-\rPage 16 Non-Intrusive Alert System Humansystems®\r\nlevel processes (e.g., attention and memory) and higher-level processes (e.g., situation assessment, decision making, and judgment). In fact, they state that ignoring individual differences in human- machine systems can lead to non-optimal behaviour at best and critical errors with disastrous consequences at worst.\rWith respect to configuring alert parameters, one way to address the issue of individual differences is to have operators enter their alert preferences directly into an adaptive interface system. Hucklicka and McNeese (2002) developed just such a system called the Affect and Belief Adaptive Interface System (ABAIS). ABAIS allows users to enter display and modality preferences in a number of categories, which can be seen in Table 3.\rTable 3: Operator information preference profile (Hudlicka & McNeese, 2002)\rInformation Category\rPreferred means of enhancing visibility Preferred colour for alerts\rPreferred alert notification modality Preferred attention capture means\rOptions\rColour, Size, Blinking Red solid, Red outline Visual, Auditory\rMovement at visual periphery, Shift display to foveal region, Enhance icon visibility, Display arrow pointing to desired icon, Superimpose blinking icon\rAlthough no empirical studies have been conducted using ABAIS, it does highlight the usefulness of allowing operators to control the way in which alerts are configured. Alert settings could be based on operator abilities. For example, operators who have auditory difficulties could choose to have alerts provided visually, or operators who are red/green colour blind could choose alternate colour schemes to report the importance of an alert. Alert setting could also be based on operator preferences. For example, it could be that one operator finds a blinking cursor annoying and would much rather have an audio alert.\rAssessment\rWhile this paper recognises the need for the operator to configure appropriate alert parameters, it is not clear that allowing individual preferences for the design of an alert would be feasible in team or multi-operator environments. If individual members of the RJOC team were to have their own set of preferences, then team situation awareness for the current alerts status would be compromised. Further, supervisors would have greater difficulty in understanding the current alert picture. There is also a danger that individuals left to choose their own preferences would select options that may be inappropriate from a Human Factors (HF) perspective.\rConfigure for context\rProviding users with the ability to configure alert parameters can also be important in the battlefield. Commanders and staff monitor and analyze a large amount of digital information in the midst of battle planning, preparation, and execution in order to understand what is relevant and critical to their mission. In order to assist the decision-maker to understand and act in battlefield situations, Akin, Green and Arntz (2005) worked to develop the “System to Help Identify and Empower Leader Decisions” (SHIELD). SHIELD monitors digital data streams and alerts a decision-maker to two specific battle situations requiring immediate action. The first situation is Humansystems® Non-Intrusive Alert System Page 17\r\nwhen a friendly unit violates a boundary, thus risking being mistaken for enemy elements by friendly units (“Cross Boundary Violation Alert”). The second situation is when a unit’s plan for using artillery does not match enemy locations determined by an intelligence section (“Fratricide Prevention Alert”). In both situations, SHIELD initiates an alert that displays a text warning and a flashing icon on an “alert map”.\rAlthough helpful, Akin, Green and Arntz (2005) note that alerts can be intrusive. They, therefore, designed SHIELD to allow leaders to be able to control the intrusiveness of alerts. Leaders are able to dismiss an alert from the screen, have an alert be repeated at what may prove to be a more convenient time, or turn off an alert. In addition to these options, they plan on including an intrusiveness filter in future versions of SHIELD. This intrusiveness filter would allow a leader to define when alerts should and should not be displayed (e.g., within a certain limit of known threat locations; if less than 3 alerts are already displayed on the screen). The intrusiveness filter would also allow leaders to turn off audio alerts, which could be especially important if there is a possibility that noise could signal the leader’s presence to the enemy.\rAssessment\rThe contribution of the paper with respect to alert configuration is useful and provides a good example of an interface that allows users to describe contextual conditions for when an alert may be presented. It should be noted, however, that this does not mitigate the actual intrusiveness of that alert when it does occur.\r4.2.2 Receive information on alert state\rAn operator receives information on an alert’s state through an alert warning, which is defined as the actual mechanisms by which an operator’s attention is captured. Not surprisingly, literature relating to alert warnings was by far the most abundant of any of the alert system functions and included research relating to HF conceptual models, design guidelines, experiments, and design concepts.\r4.2.2.1 HF models\rMcCrickard, Chewar, Somervell and Ndiwalana (2003a) suggest that alerting systems should deliver current and critical information to the user in a manner where the user is not interrupted from their primary task. It is assumed that alerting systems are distracting to the user, however, the authors note that it is not known how much they distract the user and if this distraction is negative. McCrickard et al. (2003a) examine the benefits and drawbacks of alert system interface designs. Three critical parameters for modelling alert system user goals and system designs are interruption, reaction and comprehension.\rInterruption is “an event promoting transition and reallocation of attention focus from a task to the notification” (p. 319). Context plays an important part in interruption. For instance, if the user is driving, the alert (i.e., check engine light) should not be intrusive and interruptive to the main task at hand (i.e., driving safely). However, in the context of the RJOC, an alert informing an operator of a vessel behaving anomalously may require immediate attention and interruption of the main task at hand.\rReaction “is the rapid and accurate response to the stimuli provided by notification systems” (p. 319). Reaction can be measured by the time it takes to shift attention, which can be manipulated through designs incorporating colours, shapes and motion.\rPage 18 Non-Intrusive Alert System Humansystems®\r\nComprehension is “remembering and making sense of the information” (p. 319) which can be measured through recall and content related questions.\rUsing this model, combinations of high (1) and low (0) interruption, reaction and comprehension (IRC) were created to categorize the types of alerts, as shown in Figure 2.\rInterruption\rAlarm\rNoise\rIndicator\rDiversion\rInformation Exhibit\rCritical Activity Monitor\rAmbient Media\rSecondary Display\rReaction\rComprehension\rFigure 2: IRC framework (adapted from McCrickard et al., 2003a, p. 321)\rThe following table is a list of potential alerts and examples from alert systems according to the IRC framework.\rHumansystems® Non-Intrusive Alert System Page 19\r\nTable 4: IRC framework alerts and examples (McCrickard et al., 2003a)\rTypes of Alerts\rIRC Codes\rExamples\rCritical activity monitor\r1113\rA system administrator uses a network monitor on the desktop to enable prompt responses and fixes to computer problems\rAlarm\r110\rA businessman relies on a calendar and email alerts\rInformation exhibit\r101\rA factory supervisor requires critical updates, while performing daily tasks\rSecondary display\r011\rAn editor writes part of a document while monitoring a team progress groupware tool\rDiversion\r100\rA teenager on the home computer enjoys amusing pop-ups\rIndicator\r010\rA tourist uses a GPS (Global Positioning System) to navigate around a new city\rAmbient media\r001\rA telemarketer without a window has changing weather desktop wallpaper\rNoise\r000\rA student doing homework is reassured with internet radio\rThis table displays the alerts in terms of most interruptive, comprehensive, and requiring reaction, to the least interruptive, comprehensive, and requiring reaction. Using Wickens and Hollands (2000) human information processing stage model, McCrickard et al. (2003a) mapped each of the IRC categories onto the information processing model. This enabled the authors to illustrate that different alert types have different information processing routes. For instance, when people process noise (low interruption, reaction and comprehension), it does not reach their working memory as compared to a diversion (high interruption, low reaction and comprehension) which does reach working memory. The two most relevant alert types that produce a low interruption, but require some type of comprehension, are ambient media and a secondary display. Ambient media is processed through the senses and then enters long term working memory. However, in the case of a secondary display, a selection response must be executed in order to get feedback. This means that a secondary display requires more effort on the part of the operator, but not as much as a high interruption, reaction and comprehension alert (i.e., critical activity monitor) would require.\rMcCrickard et al. (2003a) performed a case study to test the IRC category framework. The case study involved two separate evaluations, using questionnaire measures, to determine effectiveness. The first study was conducted at Microsoft with the Scope alert system (van Dantzich, Robbins, Horvitz & Czerwinski, 2002; as cited in McCrickard et al., 2003a). The second study was conducted in McCrickard et al.’s lab using the IRC framework. Although the Scope was meant to be used differently than a standard interface, the questionnaire for the Microsoft study had items assessing standard interface issues. Therefore, the results of the Microsoft study were not useful in identifying design strategies. The questionnaire for the lab study, however, was based on the IRC framework. That is, the questions assessed tradeoffs between interruption, reaction and\r3 For example, a critical activity monitor would be high in interruption (I1), high in reaction (R1) and high in comprehension (C1), while an alarm would be high in interruption (I1), high in reaction (R1) and low in comprehension (C0).\rPage 20 Non-Intrusive Alert System Humansystems®\r\ncomprehension. The questionnaire based on the IRC framework produced better redesign strategies.\rThe Scope is an alerting system which is located in the bottom right corner of a computer screen and presents symbols according to urgency (urgent items are located closer to the middle than non- urgent items). For an illustration of the Scope alerting system, see McCrickard et al. (2003a)\rThe Scope is divided into four sections: email inbox, calendar, task list and general alerts. The goals of the Scope system are to direct user attention to urgencies and minimize user attention for incoming non-urgent alerts. According to the IRC framework, the Scope would be similar to an alarm (IRC 110) and ambient display (IRC 001). The results of the studies and ensuing guidelines are summarized in McCrickard et al. (2003a).\rThe full explanation of the Scope can be found in van Dantzich et al. (2002; as cited in McCrickard et al. 2003a). McCrickard et al. (2003a) conclude that the IRC framework provides a method to evaluate alert systems. The Scope is one of these alert systems that, based on the evaluation, yielded potential redesign strategies such as reducing visual clutter and defining alerts based on shape and colour.\rAssessment\rIn summary, the McCrickard et al. (2003a) model is based on interruption of the alert, reaction to the alert and comprehension of the alert. These parameters create 8 different types of alerts which vary on the IRC levels. For an alert to be non-intrusive it would need to be non-interruptive. A secondary display and ambient media are two types of alerts that may offer a less intrusive way to inform the operator of interesting events. Ambient media allows for comprehension of the material presented, without reaction. Ambient media would be useful in the context of non-urgent alerts or alerts that do not require an action. A secondary display for alerts may not be a viable design solution for RJOC as presently configured.\rIn terms of applicability to maritime anomaly alerts, the Scope approach may be too extensive for the existing RJOC design space, may require too much operator cognitive processing and may provide more information than is required.\rOverall, this paper provides a good conceptual analysis of the human information processing issues that need to be considered with respect to the intrusiveness and comprehensibility of warning indicators.\r4.2.2.2 Design guidelines\rNuisance alerts\rIn the 1980s, reports of air and train crashes came to light indicating that system operators turned off critical alert or warning indicators prior to the accidents. In his seminal paper, Sorkin (1988) outlined two reasons why he believes operators would disable warning signals. The first reason is that alert signals can be highly-aversive and can interfere with important operator duties (e.g., high- level shrill sound produced by warning whistles, multiple alerts that make it difficult to identify the underlying condition for the alerts). The second reason is that operators perceive false alarm rates to be excessively high. A common sense analysis indicates that operators with high workloads will adopt strategies to ignore or disarm excessive alerts, especially if they are perceived to have high false alarm rates. To deal with these issues, Sorkin recommended a number of changes to alert systems, which include:\rHumansystems® Non-Intrusive Alert System Page 21\r\n• Design alert signals in such a way that they are not overly aversive or disruptive. Possible alert techniques suggested include speech message alerts and special alert codes.\r• System designer should consider the effect of the alert rate on the performance of the entire alert system, especially when the operator has a heavy workload.\rAssessment\rSorkin’s (1988) reasons for operators turning off the alerts are still applicable twenty years later. Constant false alarms provide a false sense of security to an operator whereby it is assumed the alerts are not important enough; thus, they do not receive the operator’s attention. Also, designing a less intrusive alerting method can prevent distraction from the primary task. There was considerable research in the area of design guidelines relating to alert warnings; it will be discussed in the following sections.\rGeneral alert principles\rShorrock, Scaife and Cousins (2002) developed high-level principles for the design of soft-desk alert systems that could contribute to a philosophy of alert handling. The principles were derived from information gained from two studies regarding the design and evaluation of Air Traffic Management (ATM) commercial-off-the-shelf (COTS) systems software and hardware. These studies resulted in approximately 100 recommendations for alert handling system design. The recommendations were generalized into a smaller set of design principles, which were structured according to Stanton’s (1994; as cited in Shorrock, Scaife & Cousins, 2002) model of alert-initiated activities.\rStanton’s model consists of six activities:\r1. Observation: the detection of an abnormal condition within the system. At this stage, care should be taken to ensure that colour and flash/blink coding support alert monitoring and searching. Excessive use of colour and blinking can de-sensitize an operator and reduce the ability of the alert to gain the operator’s attention.\r2. Acceptance: acknowledging receipt and awareness of an alert. Alert systems should reduce operator workload to a manageable level – excessive demands for acknowledgement increase workload and operator error.\r3. Analysis: prioritization of an alert based on task and system in which it occurs.\r4. Investigation: activities that aim to discover the underlying cause of an alert in order to deal with the fault.\r5. Correction: the application of the results from the previous stages to address the problem identified by an alert.\r6. Monitoring: assessment of the outcome from the Correction stage.\rShorrock et al. (2002) added co-ordination as a seventh activity to the model. Co-ordination is the transfer of information between operators and the application of collaborative efforts to observe, accept, analyze, investigate or correct faults. Their final list of principles for human-centred alert systems classified according to the revised Stanton model can be seen in Table 5.\rPage 22 Non-Intrusive Alert System Humansystems®\r\nTable 5: Model-based principles for human-centred alarm systems (Shorrock et al., 2002)\rObserve\rAlarms should be presented (time stamped) in chronological order, and recorded in a log in the same order.\rAlarms should signal the need for action.\rAlarms should be relevant and worthy of attention in all the plant states and operating conditions.\rAlarms should be detected rapidly in all operating conditions.\rIt should be possible to distinguish alarms immediately, i.e. different alarms, different operators, alarm priority.\rThe rate at which alarm lists are populated must not exceed the users’ information processing capabilities.\rAuditory alarms should contain enough information for observation and initial analysis and no more.\rAlarms should not annoy, startle or distract unnecessarily.\rAn indication of the alarm should remain until the operator is aware of the condition.\rThe user should have control over automatically updated information so that information important to them at any specific time does not disappear from view.\rIt should be possible to switch off an auditory alarm independent of acceptance, but it should repeat after a reasonable period if the fault is not fixed.\rFailure of an element of the alarm system should be made obvious to the operator.\rAccept\rReduce the number of alarms that require acceptance as far as is practicable.\rAllow multiple selection of alarm entries in alarm lists.\rIt should be possible to view the first unaccepted alarm with a minimum of action.\rIn multi-user systems, only one user should be able to accept and/or clear alarms displayed at multiple workstations.\rIt should only be possible to accept the alarm from where the sufficient alarm information is available.\rIt should be possible to accept alarms with a minimum of action (e.g. double click), from the alarm list or mimic.\rAlarm acceptance should be reflected by a change on the visual display, such as a visual marker and the cancellation of attention-getting mechanisms, which prevails until the system state changes.\rAnalyse\rAlarm presentation, including conspicuity, should reflect alarm priority, with respect to the severity of consequences associated with the delay in recognising the deviant condition.\rWhen the number of alarms is large, provide a means to filter the alarm list display by sub-system or by priority.\rOperators should be able to suppress or shelve certain alarms according to system mode and state, and see which alarms have been suppressed or shelves, with facilities to document the reason for suppression or shelving.\rIt should not be possible for operators to change priorities of any alarms.\rAutomatic signal over-riding should always ensure that the highest priority signal over-rides.\rThe coding strategy should be the same or all display elements.\rFacilities should be provided to allow operators to recall the position of a particular alarm (e.g. periodic divider lines).\rAlarm information such as terms, abbreviations and message structure should be familiar to operators and consistent when applied to alarm lists, mimics and message/event logs.\rThe number of coding techniques should be at the required minimum, but dual (redundant) coding may be necessary to indicate alarm status and improve accurate analysis (e.g. symbols and colours).\rAlarm information should be positioned so as to be easily read from the normal operating position.\rInvestigate\rAlarm point information (e.g., settings, equipment reference) should be available with a minimum of action.\rInformation on the likely cause of an alarm should be available.\rA detailed graphical display pertaining to a displayed alarm should be available with a single action.\rWhen multiple display elements are used, no individual element should be completely obscured by another.\rVisual mimics should be spatially and logically arranged to reflect functional or naturally occurring relationships.\rNavigation between screens should be quick and easy, requiring a minimum of user action.\rCorrect\rHumansystems® Non-Intrusive Alert System Page 23\r\nEvery alarm should have a defined response and provide guidance or indication of what response is required.\rIf two alarms for the same system have the same response, then consideration should be given to grouping them.\rIt should be possible to view status information during fault correction.\rUse cautions for operations that might have detrimental effects.\rAlarm clearance should be indicated on the visual display, both for accepted and unaccepted alarms.\rLocal controls should be positioned within reach of the normal operating position.\rMonitor\rNo primary principles. However, a number of principles primarily associated with observation become relevant to monitoring.\rCoordinate\rProvide high-level overview displays to show location of operators in system, areas of responsibility, etc.\r4.2.2.2.1.1 Assessment\rThese studies provide a solid and useful analysis of the major principles that should be considered in the design of alert systems, many of which are applicable to considerations for non-intrusive systems. Although not the focus of these papers, no specific implementation guidelines are provided for how these principles should be addressed in terms of concrete design concepts or considerations.\rAuditory alerts\rAuditory warnings are used as a means of commanding attention without startling or annoying operators. They are used in domains such as aviation, medicine, and control rooms. Such warnings are meant to convey information about a task or situation, and often must compete with other sensory stimuli in the environment. However, there are problems associated with exposure to loud noise over prolonged periods, including hearing loss, cardiovascular problems (Babisch, 1998; as cited in Edworthy & Hellier, 2002), cognitive problems and performance problems (World Health Organisation, 1993; Smith, 1993; Edworthy, 1997; as cited in Edworthy & Hellier, 2002). In their review of research on auditory warnings in noisy environments, Edworthy and Hellier (2002) make the following suggestions regarding how to deal with issues related to auditory alerts (see Table 6).\rAhlstrom (2003) also conducted work to develop guidelines for auditory alerts within the National Airspace System (NAS). New tools for the NAS are accompanied by new equipment using visual and auditory signals to indicate the status of equipment and of incoming air traffic. The auditory signals are often developed with minimal use of standards or guidelines, or with minimal consideration of the auditory signals on existing equipment. This can result in too many warnings, warnings that are too loud or inaudible, warnings that are confusing, and inappropriate mapping between the warning and its meaning (Meredith & Edworthy, 1994; as cited in Ahlstrom, 2003). Such situations can result in operators disarming or inhibiting alerts, as described by Sorkin (1988).\rAhlstrom (2003) conducted an exploratory study to provide increased understanding and insight into audio alert problems within the NAS. After conducting a literature review, Alhstrom identified 15 issues associated with auditory alerts in a variety of fields (e.g., hospital emergency rooms, aircraft cockpits). Twenty current and former terminal Air Traffic Control Specialists (ATCS) were then asked to rate each of the 15 issues on an 11 point scale ranging from 0 (not a problem) to 10 (severe problem) on how problematic the issue was from them in their work environment.\rPage 24 Non-Intrusive Alert System Humansystems®\r\nTable 6: Suggestions for auditory alerts (Edworthy & Hellier, 2002)\rIssues\rSuggestions\rNoisy environments (e.g., factories, cockpits, flight decks)\rAs ambient noise determines the detectability of auditory warnings, a worst-case spectrum should be used to avoid excessively loud warnings\rEnsure warnings are not too loud as they may be switched off by operators\rAvoid having a number of warnings going off at the same time as they will mask each other\rAcoustic\rLocalizability will be improved by having several audible components in the warning sound, preferably with a fairly low fundamental frequency\rContinuous tones should not be used as warning sounds\rSpeech vs. non- speech\rIt is more difficult to produce intelligible speech warnings for a complex noise environment than it is to fit a non-speech warning to the same noise spectrum\rSpeech warnings can create problems in certain environments. For example, a verbal warning in a hospital ward may cause worry or panic by patients and relatives, whereas a nonverbal warning would not\rWarning sounds can be designed to mimic speech in some way. For example, in an emergency room environment the warning sound for “cardiovascular” can contain 6 pulses, the same as the word, with a pitch pattern consisting of 3 pulses at one pitch followed by 3 at a lower level. Such a warning is easily learned and retained\rSpeech warnings may interfere with ongoing information processing which involves language (Wickens & Hollands Information Processing Model, 2000)\rWarning design protocols\rWarnings must be designed so as to attract attention without causing stress and annoyance\rFalse alarms\rDesign alert systems with high accuracy rates. False alarms provide information of no use whatsoever, serve to increase the noise levels within an environment, and over time will lower performance on a task\rNumber of auditory warnings\rIndividually different alerts for top-priority situations only\rUse specific sounds to identify the category of risk\rFocus on functions rather than equipment. For example, assign specific alerts to specific medical functions rather than to pieces of equipment (as equipment changes frequently in health care)\rParticipants were also provided the following comments on specific problems with auditory alerts in their work environment:\r• Alerts can be annoying at times (e.g., alert continues to sound even after it has been located);\r• Alerts for different problems sound similar and are easily confused;\r• Too many false alarms. After a while, alerts start to lose their meaning;\r• Alerts can interfere with voice communications;\r• Some systems should have auditory alerts but do not;\r• Alerts that are too loud cause stress, frustration and hearing loss.\rHumansystems® Non-Intrusive Alert System Page 25\r\nIn response to these findings, Ahlstrom (2003) provided recommendations to be considered for future alerts used by terminal area operators:\r• Alerting and warning systems should be unambiguous, with a clear indication of the cause for the alert. This can be accomplished by using varying frequency and/or modulation, and periodicity should differ (i.e., avoid continuous signals);\r• The criteria for conditions causing frequent false alarms should be evaluated and effort should be made to reduce the number of irrelevant alerts;\r• Systems should have a simple, consistent means for turning off non-critical auditory alerts without erasing any displayed message accompanying the auditory signal. The system could also include a sensing mechanism that automatically shuts off the auditory portion of an alert when it no longer provides useful information;\r• Strategies should be used to maximize the ability to locate auditory alerts. These include avoiding mid frequencies, positioning alerts to the side rather than in the front or in the back of the operator, minimizing hard surfaces in order to decrease echoes, and providing a centralized alert panel or window indicating the alert status for most alerts;\r• When operators are required to identify an alert based on the auditory portion alone, the number of signals to be identified should not exceed four. Up to nine alerts can be used if they are presented regularly, and up to 12 alerts can be used if relative discrimination (rather than absolute identification) is used.\r4.2.2.2.1.2 Assessment\rThese papers outline many of the critical design and implementation issues with respect to auditory alerts. The first three of Ahlstrom’s recommendations for auditory alerts would also be generically applicable to the implementation of other forms of alerting.\rNuclear control room\rWithin the nuclear industry, the goal of computer-based alert systems is to assist operators by processing alert data and improving the presentation of alert information. Brown, O’Hara and Higgins (2000) conducted an investigation to update and revise the Nuclear Regulatory Commission’s (NRC) guidance for reviewing alert system designs. The resulting revisions were based on NRC research on the effects of alert system design characteristics on operator performance and on research examining the introduction of computer-based human-system interface systems into Nuclear Power Plants. Some of the revised guidelines that would be applicable to naval anomaly detection include setpoint determination, assured functionality, alert reduction, alert signal validation, parameter stability, and alert-status separation.\rSetpoint Determination and Nuisance Alert Avoidance\rThe determination of alert setpoints should consider the trade-off between the timely alerting of an operator to off-normal conditions and the creation of nuisance alerts caused by establishing setpoints so close to the “normal” operating values that occasional excursions of no real consequence are to be expected.\rPage 26 Non-Intrusive Alert System Humansystems®\r\nAssured Functionality under High Alert Condition\rThe alert processing system should ensure that alerts which require immediate operator action or indicate a threat to safety functions are presented in a manner that supports rapid detection and understanding by the operator under all alert loading conditions.\rAlert Reduction\rThe number of alert messages presented to the crew during off-normal conditions should be reduced by alert processing techniques (from a no-processing baseline) to support the crew’s ability to detect, understand, and act upon alerts that are important to the plant condition within the necessary time.\rAlert Signal Validation\rSensor and other input signals should be validated to ensure that spurious alerts are not presented to plant personnel, due to sensor or processing system failure.\rParameter Stability Processing\rThe alert system should incorporate the capability to apply time filtering, time delay, or deadbanding4 to the alert inputs to allow filtering of noise signals and to eliminate unneeded momentary alerts.\rAlert-Status Separation\rStatus indications, messages that indicate the status of plant systems but are not intended to alert the operator to the need to take action, generally should not be presented via the alert system display because they increase the demands on the operators for reading and evaluating alert system messages.\r4.2.2.2.1.3 Assessment\rAlthough the nuclear industry is different from the operational environment of the RJOC, many of the same issues may be applicable, for example, Brown et al.’s (2000) recommendations relating to reducing the number of alerts, rapid detection of alerts, and presenting only meaningful alerts. These recommendations would, in theory, reduce the workload of the operator and make the overall alerting system less intrusive.\rGraphic user interface design for alerts\rA man-machine interface (MMI) is the way in which an operator controls a system and traditionally contains manual buttons, controls, switches, and a monitor through a closed-circuit television. When switching to a graphic user interface (GUI) from a MMI, some mistakes can occur that can adversely impact human performance. A common mistake is the MMI display is shrunk onto the GUI. Although it maintains familiarity for the user, it can result in usability issues, which can lead to efficiency and safety problems.\rHan, Yang and Im (2007) created a six-phase approach to develop a method for GUI design. The six phases include:\r1. UI design guidelines collection for process control rooms5\r4 Deadband is a specified area where the alert would not go off.\rHumansystems® Non-Intrusive Alert System Page 27\r\n2. Usability inspection5\r3. Design rules and guidelines development\r4. User interface design and prototyping\r5. Usability testing and evaluation\r6. Final prototypes and design specs.\rFor the first step, Han et al. (2007) conducted an extensive review of design guidelines for any GUIs used in a process control room. Consequently, they organized approximately 1500 guidelines into 14 chapters and 70 sections. These guideline principles covered topics such as aesthetics, attention, cognitive issues, consistency, display issues, feedback, forgiveness, memory issues, metaphors, simplicity, system messages and help, user control, and user differences.\rHan et al. (2007) then analyzed current user interfaces in a process control room and operator tasks to identify usability problems and define design requirements for a new interface. This new process control room interface prototype takes into account all the current problems experienced by operators. Operator manuals were reviewed and operators were interviewed. Twenty-two operators participated in one-on-one interviews answering questions assessing tasks, alerts systems and requirements. Overall, the usability inspection resulted in the identification of 500 usability problems by 4 practitioners. The most frequently found problems related to attention (e.g., warning signals not salient), consistency (e.g., different terminologies), cognitive issues (e.g., pictures on screens different from actual layout of equipment), memory issues (e.g., same colours have different meanings) and simplicity (e.g., too many colours used). Selected problems are shown in Table 7.\rTable 7: Identified problems and design requirements (Han et al., 2007)\rAlert System Problems\rUsability Problems\rOperator’s Requirements\rWarnings are not categorized by urgency\rScreens are complex\rUnnecessary or redundant interface elements should be removed, and the layout should be reorganized\rIrregular situations may not be informed to operators\rEach screen has different layouts and different methods of information presentation or visualization\rConsistent screen layouts and visualization should be designed\rWarning signals are not attracting the operator’s attention\rToo many colors are used on a screen\rThe color-coding scheme should be developed, and the meaning of colour should be easily understandable\rVisual alerts are not presented with auditory alerts\rOnly one window can be activated at a time\rThe multi-window function should be available\rOnly the mouse can be used to move between input fields\rA ‘Tab’ key should also be available as a method of moving between input fields\rSystem status changes cannot be detected before opening\rand looking at related screens directly\rAll the status changes should be automatically informed on the screen that the operators mainly use\r5 This step can be skipped if a requirements analysis already exists\rPage 28 Non-Intrusive Alert System Humansystems®\r\nOnce the usability problems and operator requirements were identified, Han et al. (2007) created design rules and specific design guidelines for the new interface. Design rules are defined as major premises that designers should always keep in mind when designing. Guidelines refer to the specific methods designers should follow when designing individual interface elements. Design rules were categorized into improvement of task efficiency or reduction of task errors. Each rule was accompanied with several specific design guidelines. Design guidelines generated by Han et al. (2007) addressed critical problems of alert design and colour-coding, as shown in Table 8.\rTable 8: Examples of guidelines (Han et al., 2007)\rDesign Categories\rGuidelines\rAlert Design\rAttention\rWarning should be easily distinguished from the background. For example, visual signals should be bright on a dark screen, and vice versa\rHazard information\rA warning signal should contain hazard information. However, not too much hazard information should be included in just one signal\rConsequences information\rConsequences information should follow the hazard information. However, if operators can infer the consequences from the hazard information, they do not need to be included in the warning signal\rInstructions\rInformation on instructions should not include a too difficult or impossible method to perform. That is, instruction methods as easy as possible should be included\rComprehension\rIf operators’ capability, knowledge, and experience levels are various, warning signals should be easy so that operators who have the lowest capability or experience level can understand them\rMotivation\rWarning signals should induce operators to read or listen to them and to react to them\rBrevity\rAlert information should not exceed two phrases or sentences\rDurability\rWarnings that inform operators of the instantaneous change of process or signals that do not need special reactions should not last for a long time\rColour Design\rGeneral considerations\rColours are used for supporting search tasks, highlighting, or indicating status of the system\rForeground colour\rDo not use blue, magenta, or a shade of pink in displaying information that the user must read\rColour contrast\rExaggerate lightness differences between foreground and background colours, and avoid using colours of similar lightness adjacent to one another, even if they differ in saturation or hue\rColours of interface elements\rDo not design a colour icon that is substantially different from the black-and-white icon. When a colour is added to an icon, it is best to leave the one-pixel black outline and other black lines that form the icon, and fill the icon in with colour\rAs can be seen in Table 8, Han et al. (2007) provide a number of recommendations for alert system design. For example, warnings should be contrasted with background information (e.g., bright warning on a dark screen, vice versa), and should be colour coded (red, yellow, green, white), but\rHumansystems® Non-Intrusive Alert System Page 29\r\nnot in shades of blue or pink. Important for reducing the intrusiveness of alerts are the recommendations that information in the warnings should be easily interpretable and should not exceed 2 sentences, and warnings that do not require an operator’s reaction should not last a long time.\r4.2.2.2.1.4 Assessment\rAlthough the recommendations and guidelines outlined by Han et al. (2007) are not specific to creating less intrusive alerts, the following recommendations can be generalized for the design of less intrusive alerting systems:\r• Decrease the disruptiveness and intrusiveness of alerts.\r• Consider the effect of the alert rate on the operator. Alert processing techniques should be used to reduce the number of alert messages as this will improve ability to detect, understand and act upon important alerts.\r• Be cautious of alert set points. They should be set at a level to avoid nuisance and false alarms. False alarms do not provide useful information and over time will lower performance on a task.\r• Do not present status indications through an alert system display. This increases the demands on an operator for reading and evaluating the message.\r• Auditory alerts should use specific sounds to identify the category of risk. This will aid an operator’s ability to identify alerts.\r• Operators should be able to turn off non-critical alerts without erasing information.\r• The auditory portion of an alert should shut-off automatically when it no longer provides\ruseful information.\r• Operators should be able to distinguish alerts immediately (e.g., different alerts, alert priority).\r• The rate at which alert lists are populated must not exceed the users’ information processing capabilities.\r• Alert acceptance should be reflected by a change on the visual display, such as a visual marker and the cancellation of attention-getting mechanisms, which prevails until the system state changes.\r• Alert presentation, including conspicuity, should reflect alert priority, with respect to the severity of consequences associated with delay in recognizing the deviant condition.\r• Operators should be able to suppress or shelve certain alerts according to system mode and state, and see which alerts have been suppressed or shelved, with facilities to document the reason for suppression or shelving.\r• A detailed graphical display pertaining to a displayed alert should be available with a single action.\rIt should be noted that many of the recommendations are in the form of general principles (e.g., matching alert conspicuity with alert priority) and there is a lack of specific recommendations on how the principle would be implemented in an interface.\rPage 30 Non-Intrusive Alert System Humansystems®\r\n4.2.2.3 Empirical research\rThis section, which reviews empirical research related to alert warnings, is divided into visual, visual and auditory, and predictive alerting system, which relates to the actual alerting system itself and how it can predict the operator’s actions.\rVisual Alerts\rThere are many variations of visual alerts such as text, picture, symbols and icons which may vary in size, colour and position. While developing a pre-alert system that would reduce the frequency of alerts, Hwang, Lin, Liang, Yenn and Hsu (2008) examined whether these pre-alerts should be text or graphic. The idea behind testing both formats was to determine if an operator is more inclined to disregard one format compared to another (e.g., too annoying, not comprehensive, not noticeable), which would in turn lead to more alerts going off. The results of this experiment (see Section 4.2.4.2 Warning of impending alert) showed no significant differences in the text and graphic pre-alert types for reducing the number of alerts. However, with the graphic types, the operators had significantly more correct answers when asked questions about the alerted task. Although both forms of pre-alert systems would be a benefit to the operator, the graphic display includes more information, but requires more space and greater changes to be implemented in the control room. Therefore, Hwang et al. (2008) recommended the text type pre-alert to be implemented in control rooms.\rMcCrickard, Catrambone, Chewar and Stasko (2003b) considered variations of animated text for computer alert systems. Specifically, McCrickard et al. (2003b) examined the visual aspect of an alert, and its effect on interruption, reaction, and comprehension6. There were three forms of animated texts: a smooth ticker (information shifted horizontally), a fading display (information fades), and a rapid serial visual presentation (RSVP)-style “blast” (displays information without smooth animation). Information came in the form of changing news, weather, stocks and sports information. The primary task for participants was searching the World Wide Web for information to answer questions that were asked of them. The alerted task was to monitor the news/weather/stocks/sports information (presented by animated text) and answer questions relating to the message displayed. For example, while participants were trying to answer the question, “In what year was Mount Rushmore carved,” they also had to monitor the weather alerts and press a button when the weather temperature dropped below 30 degrees. At the end of the session, participants were asked to complete awareness questions which assessed the amount of information they recalled from the alerted task (i.e., monitoring news/weather/stocks/sports information). Results did not report a specific text type which yielded the fastest response times, rather strengths and weaknesses were found for each method of animation (see Table 9).\r6 Interruption, reaction and comprehension were defined previously in Section 4.2.1.\rHumansystems® Non-Intrusive Alert System Page 31\r\nTable 9: Results for operator tasks: Study 1 (McCrickard et al., 2003b)\rTasks\rMeasurement\rBest\rWorst\rBrowsing speed\rThe time the information appeared on the screen until the participant typed in the correct answer and pressed OK\rControl then ticker\rFade & blast\rBrowsing comprehension\rThe number of incorrect answers\rControl then blast\rTicker & fade\rLink selections\rThe number of times a participant pressed the Back button\rTicker then fade\rControl & blast\rReaction time to alert\rThe time the alert appeared on the screen until the participant acknowledged it by pressing a button\rBlast (34 seconds)\rTicker (54 seconds)\rBasic awareness hit rate\rRecognition of information in the alert\rTicker\rFade\rDetailed awareness rate\rThe recognition of correct and incorrect answers\rTicker\rFade & blast\rBasic awareness false alarm rate\rInformation participants reported seeing that was not actually presented\rFade\rTicker\rDetailed awareness false alarm rates\rConfidence in understanding information that was not actually understood\rTicker\rFade & blast\rParticipant’s preference\rMost user friendly and least intrusive\rTicker\rBlast\rMcCrickard et al. (2003b) recommended the ticker as the best choice for maintaining awareness, minimizing interruption, facilitating reaction and facilitating comprehension.\rA second experiment by McCrickard et al. (2003b) examined the impact that alert display size and animation speed would have on performance. This experiment only used the ticker text and fade text, as the blast type was rated as the least favourite by participants in the first experiment. Results are shown in Table 10.\rPage 32 Non-Intrusive Alert System Humansystems®\r\nTable 10: Results for operator tasks: Study 2 (McCrickard et al., 2003b)\rTasks\rMeasurement\rBest\rWorst\rBrowsing speed\rThe time the information appeared on the screen until the participant typed in the correct answer and pressed OK\rSlow ticker or any slow\rSmall ticker or any small\rBrowsing comprehension\rThe number of incorrect answers\rSmall fade\rSmall ticker\rLink selections\rThe number of times a participant pressed the Back button\r-\rSmall ticker\rReaction time to alert\rThe time the alert appeared on the screen until the participant acknowledged it by pressing a button\rFade\rNormal or slow ticker\rBasic awareness hit rate\rRecognition of information categories in the alert\rSmall ticker\rSlow ticker\rDetailed awareness rate\rThe recognition of correct and incorrect answers\rSmall fade\rSmall ticker & fade\rBasic awareness false alarm rate\rInformation participant’s reported seeing that was not actually presented\rSlow ticker\rSmall ticker & fade\rDetailed awareness false alarm rates\rConfidence in understanding information that was not actually understood\rSlow fade\rSmall fade\rParticipant’s preference\rMost user friendly and least intrusive\r-\r-\rBased on these results, McCrickard (2003b) recommended that the slow fade may be the best overall alert type.\rIn summary, all three text types did not significantly interrupt the user from the primary task, but still alerted them to important information. Fade, blasts, and small displays were better for rapid identification of the information compared to tickers or other size displays, but worse for comprehension and recall. Overall, alerts that travel across the screen horizontally were found to be the least intrusive and yield the best performance results. However, when the alert text was varied in size and speed, the text that appeared slowly and faded slowly was the best type to use. The finding with respect to ticker style alerts provides guidance to the development of a similar design concept for the present project.\rThe operational definition of alert intrusiveness in terms of the relationship between a primary task and the alerting stimulus is useful and will assist in the development of non-intrusive alerts concepts.\rVisual and Auditory\rThe literature is mixed regarding performance for salient (e.g., auditory) versus less salient (e.g., visual) cues. Banbury, Macken, Tremblay and Jones (2001; as cited in Colcombe & Wickens, 2006) found that discrete auditory stimuli, such as those used in alerts, tend to corrupt memory processes more than visual cues. On the other hand, Helmick-Rich, Burke, Gilad and Hancock (2004; as cited in Colcombe & Wickens, 2006) found that people were more likely to comply with an auditory cue compared to a visual cue. It therefore appears that the degree of disruption due to alerted tasks is a complicated relationship between the characteristics of the ongoing task, the alerted task, and the operator’s strategies and skills. Auditory alerts appear to be more interrupting\rHumansystems® Non-Intrusive Alert System Page 33\r\nthan visual alerts, but do not always impose a cost to ongoing tasks. Colcombe and Wickens (2006) were interested in the way in which parameters of a primary task made them more or less interruptible from an alert, and in turn, how the characteristics of the alert mediated the costs of interruptions to primary tasks.\rFor study 1, participants were warned, visually or auditorily, for potential collision threats from a Cockpit Displays of Traffic Information (CDTI) display. Participants were in one of the following conditions: stable tracking, unstable tracking condition, binary alert (normal state or high-level alert), likelihood alert (normal state, mid-level or high-level alert). Results showed participants were faster to detect auditory alerts than visual alerts. Tracking performance was better in the binary alerting condition than the likelihood alerting condition. Subsequent studies replicated the binary and likelihood finding, but found response times faster for visual than auditory alerts.\rColcombe and Wickens (2006) stated that the binary alert was generally more effective than the likelihood alert. However, this was based on participants responding to the binary alert faster than the mid-level likelihood alert, without taking into consideration comprehension or interruption. No urgency distinction was made with binary alerts and, thus, operators must treat each alert as if it is high-level. Auditory alerts generally supported better conflict detection response than did a visual alert. However, the impact of modality on the concurrent task was modulated by the nature of the task (visual tracking was more disrupted by the auditory alert than by a visual alert).\rSimilarly, Krausman, Elliot and Pettitt (2005) examined visual, auditory and tactile alerts on platoon leader decision making. To a limited extent, the military has implemented a multi-sensory information presentation approach in that system designers are using auditory and visual displays. However, there are situations in which a soldier’s visual and auditory channels are heavily loaded. Therefore, tactile presentations may also be beneficial. Krausman et al. (2005) had twelve infantry officers participate in 3 different scenarios. In each scenario participants played the role of platoon leaders (PL) mounted inside a vehicle. During the scenario, participants sat in front of a primary display, map display, and Unmanned Aerial Vehicle (UAV) display to perform tactical communications and monitor activity on the display. Approximately 9 of the communications sent to the participant in each scenario were preceded by a visual, audio, or tactile alert. Table 11 describes the presentation of each alert.\rTable 11: Recommendations for design for a platoon leader alert (Krausman et al., 2005)\rParticipants received only one type of alert in each scenario (e.g., visual alerts in scenario 1, audio alerts in scenario 2, and tactile alerts in scenario 3). Alerts were continuous and stopped when the participant clicked the “show message” button to receive the information. After each scenario, participants rated and ranked the effectiveness, helpfulness, and necessity of the alerts.\rOverall, visual alerts were the least effective method of alerting participants. Response times were significantly longer for visual alerts than for auditory or tactile alerts; no significant differences\rPage 34 Non-Intrusive Alert System Humansystems®\rAlert purpose\rAlert presentation\rTo alert platoon leader to an incoming message\rVisual alert – solid red box appears on bottom portion of communications console of primary display\rAuditory alert – “beep” from headset Tactile alert – “buzz” from tactical armband\r\nwere found between auditory and tactile alert response times. Participants also rated visual alerts as the less effective method of getting their attention. When ranking alerts, participants ranked auditory alerts to be the most effective method of getting their attention followed by tactile alerts and visual alerts, respectively. Tactile alerts were ranked as the most helpful type of alert, followed by auditory alerts and visual alerts, respectively. However, participants noted that caution should be exercised when implementing auditory and tactile alerts in combat vehicles because alerts might be hard to detect in combat environments (e.g., multiple radio nets might mask the sound of an auditory alert, tactile alerts might be missed in a moving vehicle due to vehicle vibrations). Participants suggested that a combination of alerts might be the most effective option.\rSteefkerk, Esch-Bussemakers and Neerincx (2007) designed a context-aware alert system. This system varied visual and auditory methods, and reported that participants preferred an auditory signal for low urgency messages. More details about this study can be found in Section 4.2.3.2.\rIn summary, the experiments investigating visual alerts recommend alerts that have a slow fade text. Visual alerts that are not recommended include the blast alert and graphical pre-alerts. In general, it was found that auditory alerts support better detection than visual alerts. Experiments investigating auditory alerts recommend an auditory alert for all types of urgencies, and that these alerts vary the presentation based on urgency.\rPredictive Alerting System\rWhile the previous sections addressed visual and auditory alerts, this section presents empirical research related to a predictive alerting system, which can aid the operator in certain tasks. Mitchell (1998) considered the issues surrounding intelligent aids and associates in an operational setting. Intelligent aids and associates, such as displays of up-to-date information about operations, are a type of computer technology that is designed to help operators. Mitchell states that an operator’s immediate response after hearing an alert may be to shut it off, followed by identifying what triggered the alert. Often times software updates follow well behind changes in operational requirements such that by the time software updates are finally introduced, certain alerts may have become extinct or irrelevant. A common issue is that alerts are assumed to be extinct or irrelevant, thus are ignored by operators. To address these issues, Mitchell (1998) designed a system to prevent alert overload, in turn, increasing alert usefulness.\rThe Operator Function Model Expert System (OFMspert) performs many functions, but the most relevant to the present project is the activity tracking device, whereby the system tracks the operator’s activities. The computer would track the activity by asking “What is the operator doing? Why is the operator doing that? What will the operator do next?” (p. 31). The latter question was posed as a way for the system to predict or infer the intent of the operator. Actions that can be interpreted include cognitive actions (i.e., situation assessment) and perceptual actions (i.e., scanning for alerts). Although not discussed in the paper, the system could presumably predict the operator’s future actions, thus presenting an alert at an appropriate time. An alert that is presented to the operator at an appropriate time, would not only be less intrusive, but also more useful. Another useful capability of the system is that it can explain recommendations to an operator, which could reduce the operator’s cognitive workload.\rThe National Aeronautics and Space Administration (NASA) Goddard Space Flight Center (GSFC) was chosen to illustrate the application of the OFMspert. The Multisatellite Operations Control Center (MSOCC) is a system in GSFC that monitors the use and effectiveness of computer systems shared by satellites. Several steps were taken for the design methodology, but the implementation of intelligent aids and associates was of interest for this review. Specifically,\rHumansystems® Non-Intrusive Alert System Page 35\r\ninferring the intent of the operator to reasonably predict their activities and interpret their actions was examined. The OFMspert implementation in MSOCC was studied by Jones, Mitchell and Rubin (1990; as cited in Mitchell, 1998). The experiment evaluated the previous Saisi and Mitchell MSOCC data and verbal protocols from two control participants. Every action had an interpretation from the OFMspert, the participants or a domain expert (from the Saisi and Mitchell MSOCC data). Results showed significantly good matches for system commands and display requests. However, there were poor matches for planning and browsing. Another implementation of the OFMspert was performed by Callantine, Mitchell and Palmer (1997; 1998) and looked at the Boeing B-757 flight deck (as cited in Mitchell, 1998). Results showed the OFMspert correctly interpreted 92% of the actions, when compared with 10 certified pilots. Those that were not correct related to browsing actions.\rAssessment\rThis section, which describes empirical research relating to alert warnings, considers both the visual and auditory components of an alert, as well as how to reduce the operator’s workload. For the visual component of the alert, a slow fade text is recommended. For the auditory component of the alert, a sound for low, medium and high urgencies should be presented with the alert and the sound should vary according to urgency. Lastly, there is a recommendation of having a predictive adaptive system which can infer the actions of the operator. This prediction can allow the operators to be interrupted with a medium or low urgency alert during periods of low workload.\rThe OFMspert technology is a decade old and perhaps could be modified to fit the needs of the maritime domain. Presumably, if this system can track the activities of an operator and predict what actions they will perform, specifically alert scanning; it can have some merit as part of an alerting system. Predicting operator’s actions in tandem with an alerting system could predict when the operator’s workload could be interrupted with an alert. This is especially the case when the operator is already scanning for alerts. Combining the OFMspert with a warning system could be beneficial in that operators are being alerted when their workload allows for it, and the rest of their time is spent on the primary task, except during cases of emergency. However, the major limitation of this approach concerns the accuracy with which the system is able to determine the suitable time for presenting alert information. Even small errors would likely result in operators becoming frustrated with low level alerts that arrive when they are busy and important alerts arriving too late.\r4.2.2.4 Design concepts\rThe literature provided a number of specific design concepts for reducing the intrusiveness of alert warnings. These concepts are related to information presentation, limiting operator information overload, maintaining situation awareness, and reducing the number of alerts.\rInformation presentation\rA key design for reducing the intrusiveness of alerts can be found in Hautamaki, Bagnall and Small’s (2006) research on the Hazard Monitor and Intelligent Alerting System (HMIAS). This system was designed to improve information presentation to combat system (CS) operators using the Combat Control System (CCS). The CCS was designed to help CS operators form a tactical picture of the maritime environment, especially surface and subsurface vessel locations. Included in the CCS is an alert system to notify operators of conditions that violate expected operating ranges. Originally the alert system indicated isolated incidents but did not convey the severity of the situation as a whole to the operator. Over the years, improvements to the CCS have incorporated a great deal of disjointed but related information. However, Hautamaki et al. (2006)\rPage 36 Non-Intrusive Alert System Humansystems®\r\npoint out that there is still room for improvement. In particular, the researchers argue that the Alert Manager window on the Tactical Control and Weapons Control interface has a method for presenting alerts that is too subtle. This subtle alerting mechanism has led to situations in which operators were so focused on an ongoing task that they failed to notice other safety alerts. In addition, Hautamaki et al. (2006) argue that the CCS method of organizing safety alerts by occurrence or contact number makes it difficult for operators to identify the most severe alerts.\rTo address these issues, Hautamaki et al. (2006) developed the Hazard Monitor and Intelligent Alerting System (HMIAS) to improve the overall CCS. HMIAS monitors for, prevents, traps, and captures operator errors in order to prevent the negative consequences associated with errors (see Figure 3 for a generic Hazard Network).\rFigure 3: Hazard Network (Hautamaki et al., 2006, p. 7)\rHMIAS monitors system states for hazards, and alerts operators to the hazards in a timely, context sensitive and multi-modal manner. For example, an initial alert is presented in the form of text on the operator’s screen. If the alert is not acknowledged in a sufficient time period, and the condition persists or worsens, the alert is presented as flashing text, which then proceeds to an audible alert, followed by the addition of verbal instructions.\rTo study the effectiveness of HMIAS, Hautamaki et al. (2006) simulated a mission in which sonar personnel, fire control personnel (including CS operators and CS supervisor), and the Officer of the Deck in a Virginia Class submarine control room track an unfriendly quiet diesel submarine through a strait while remaining undetected. During the mission, operators encounter commercial vessels, deep-draft tankers, and fishing trawlers. After 20 minutes, the scenario concludes when a controlled close aboard encounter with a deep-draft tanker requires an evasive manoeuvre. Participants were tasked with continuously hunting for the best system solution for contacts using target motion analysis. Those assigned to the baseline condition used only current CCS alerts, whereas those assigned to the experimental condition used CCS alerts with HMIAS technology.\rResults indicate that HMIAS enhances operator performance. What is of particular importance to less intrusive alert technology is the HMIAS hazard network. As events monitored by the system increase in severity of consequences, the intrusiveness of the alerts also increase. This allows the operator to easily assess the urgency of an alert and be able to quickly and easily identify the alerts that require immediate attention.\rHumansystems® Non-Intrusive Alert System Page 37\r\nMcFarlane and Latorella (2002) identified some less intrusive methods for presenting alert warnings in their review of interruption management literature. Interruptions are prevalent in many working environments in which humans and computers interact with reactions being both positive and negative. For example, interruptions can provide important information, but they can also cause stress and hinder performance. McFarlane and Latorella (2002) discuss the Aegis weapon system used by the navy as an example. This system interrupts users through an alert tool that presents messages and task assignments on an ongoing basis. Although operators must be informed of the alerts, the alerts are in fact interruptions, occurring several per minute during high-stress operations. McFarlane and Latorella (2002) provide guidelines based upon Latorella’s Interruption Management Stage Model (1996, 1998; as cited in McFarlane & Latorella, 2002). This model explores the process of human interruption in a work environment, as shown in Figure 4.\rPage 38 Non-Intrusive Alert System Humansystems®\r\nAnnunciation Stimulus\rDetected Threshold Exceeded\rAnnunciation Stimulus Processed\rDetection Diversion\rInterpretation Distraction\rIntegration\rImmediate Interrupting Task Performance\rSchedule Interrupting Task Performance\rPreempt Ongoing Task\rDisturbance\rPreempt Ongoing Task\rDisturbance\rSchedule Interruption Task\rDisturbance\rPerform Interruption Task\rDisturbance\rPerform Interruption Task\rDisturbance\rImmediate Performance\rResume Ongoing Task\rDisturbance\rContinue Ongoing Task\rDisruption\rFigure 4: Interruption Management Stage Model (adapted from McFarlane & Latorella, 2002, p. 16)\rMcFarlane and Latorella (2002) propose four design solutions to deal with interruption: immediate interruption, negotiated interruption, mediated interruption, and scheduled interruption.\r• Immediate interruption is required by some tasks. When tasks require this type of interruption, some implementations may make it easier for the operator to resume his or her primary task. For instance, Lee (1992; as cited in McFarlane & Latorella, 2002) found that an active window with an animated border produced less confusion upon resuming a task than an active window with a fixed border. Similar to this, Davies, Findlay and Lambert (1989; as cited in McFarlane & Latorella, 2002) reported that reminders are a useful technique in recovering from interruption. Another design technique to enhance performance of responding to the alert is\rHumansystems® Non-Intrusive Alert System Page 39\r\nwhen information (e.g., numbers) is presented in the same location on the screen, rather than in dispersed areas.\r• Negotiated interruption is that which is controlled by the human. Woods (1995, as cited in McFarlane & Latorella, 2002) hypothesized that humans are better than computers when it comes to diverting attention. Thus, Woods proposes that alerts should be subtle enough to let the human decide when to direct their attention to a secondary task. For instance, displaying alerts separately from the primary task, but in a visible way, allows users to attend to the task if they choose, or ignore it (Lieberman, 1997; as cited in McFarlane & Latorella, 2002). Oberg and Notkin (1992; as cited in McFarlane & Latorella, 2002) created a system design where the alert would pop up near the operator’s curser position. Alerts were also colour coded so that the older an alert was, the more saturated in colour it appeared; urgent alerts changed darker faster than non-urgent alerts. Although this system was not compared to other designs, anecdotes attest to the system’s usefulness. Shneiderman (1992; as cited in McFarlane & Latorella, 2002) listed various techniques to obtain user attention, namely intensity, marking, size, choice of fonts, inverse video, blinking, colour, colour blinking, and audio.\r• Mediated interruption gives control over the interruption to a third source, or mediator (e.g., a personal digital assistant, an answering machine, etc). Czerwinski, Cutrell and Horvitz (2000; as cited in McFarlane & Latorella, 2002) suggested that the system should queue alerts until the user has a natural break. Similar to this, a program that can predict what the user would do next, can interrupt with relevant information at the appropriate time (e.g., Hammer & Small, 1995; as cited in McFarlane & Latorella, 2002).\r• Scheduled interruption can be thought of as a predetermined time where an operator allows for distractions. Alerts to anomalous information received from regularly scheduled Maritime Patrol Aircraft (MPA) flights are examples of interruptions that may be most relevant to the RJOC operators.\rToet (2006) also identified some less intrusive methods for presenting alert warnings in his review of the literature on gaze direction tracking. The author suggests that an alert system that is informed of the operator’s gaze direction will be able to present information to that operator in a way that will maximize responsiveness. Specifically, the interface can reduce visual clutter, enhance the operator’s attentive capacity and direct the operator’s attention. For the computer to perform the action of eye tracking, a non-intrusive video-based tracking system must be installed to monitor the operator’s gaze and direct attention.\rOf particular interest for this review are the techniques used to present visual information on the display screen. These techniques include non-distortion, distortion and gaze contingent techniques.\rNon-Distortion Oriented Techniques\rA semi-transparency (multi-layer displays) allows operators to quickly shift their attention. The display shows two different views (layers), one in the foreground (e.g., overview) and one in the background (e.g., detailed map). Operators can shift their attention between the two views best when views were 50-70% transparent.\rDistortion-Oriented Techniques\rThese techniques combine a detailed (full size or enlarged) representation of the regions of interest with a less detailed (compressed) representation of the remaining regions to draw the operator’s attention to the critical information. Studies have shown that participants who use distortion techniques are faster at navigation (Gutwin & Fedak, 2004; as cited in Toet, 2006).\rPage 40 Non-Intrusive Alert System Humansystems®\r\nGaze Contingent Displays\rThese displays include multiresolution displays, stereoscopic displays, and guiding displays. Multiresolution and stereoscopic displays track the user’s gaze and enhance the area being attended. Attention guiding displays, however, use a non-intrusive method to direct the user’s gaze to critical information. For example, overlaying red dots on a video clip will attract the user’s attention. Attentive interfaces, such as perceptual intelligent interfaces, can adapt their behaviour according to the user. The interface tracks the operator’s interactions over time, thus predicting their future actions.\rThe distortion, non-distortion and gaze contingent displays can be effective for non-intrusive alerting systems. The semi-transparency technique allows operators to quickly and easily shift their attention between tasks. Similar to the fade technique described by McCrickard et al. (2003b), semi-transparent alerts that pop up in the foreground of the display draw attention without being intrusive. Attention guiding displays can be less intrusive methods for directing eye gaze to critical information by directing an operator’s attention to the information by overlapping red dots. Lastly, eye contact displays can prevent irritating alerts by silencing once an operator looks at them.\r4.2.2.4.1.1 Assessment\rHautamaki et al. (2006) provide a useful model for presenting alert information to operators in a timely, context sensitive and multi-modal manner. In particular, they propose that alerts become more intrusive as the severity of the alert consequences increase. For example, an initial alert is presented in the form of text on the operator’s screen. If the alert is not acknowledged in a sufficient time period, and the condition persists or worsens, the alert is presented as a flashing text. The alert then proceeds to an audible alert, followed by the addition of verbal instructions. While there may be no comparable operational requirements in the RJOCs that would require temporal changes in the alert to signify increased urgency, the example categories of “intrusiveness” do provide some specific design options that may be applicable.\rSome of the methods described by McFarlane and Latorella (2002) could potentially be used in a less intrusive alerting system for example, making alerts subtle and presenting them in such a way that they are visibly separate from the main task but still visible to the user and providing coding of alert priority. More questionable is the suggestion to present alert information in the same area of the display as the primary task, which could potentially result in attention being immediately drawn away. Also the accuracy and the reliability of the technology to predict breaks in operator tasking or reduced workload remains unproven, thus recommendations for design concepts based upon this would appear premature.\rThe methods proposed in Toet’s (2006) paper would have limited applicability to an operational environment, where technology for detecting gaze direction cannot be realistically implemented. Although the multi layer or fish eye methods are not compatible with existing constraints within GCCS-M, the use of semi-transparent designs for an alerting system could be feasible.\rLimit operator information overload\rLimiting information overload of the operator is an essential feature of a non-intrusive alerting system. In discussion of a soft-desk control room, Dicken (1999) proposed some ideas for limiting operator information overload. His review was based on the trend in process plants to replace hard- desk operator interfaces (i.e., horse shoe control desk which includes dials, meters, chart recorder, knobs, and buttons) with soft-desk operator interfaces (i.e., computer-based Visual Display Units with management software and user displays). In the past, plant operators were typically in charge Humansystems® Non-Intrusive Alert System Page 41\r\nand ‘drove’ the plant; however with increasing automation, the operator has evolved from driving the plant to being driven by alerts. With the increasing dependence on alert systems, unacceptable problems such as generation rates, nuisance alerts, and poor performance during alert floods have arisen. Consequently, Dicken (1999) provided a careful look at important design concepts that should be considered when switching from hard- to soft-desk alert system facilities. First, he noted that alert lists are prime operating tools for soft-desk systems (see Section 4.2.5 Manage Alerts for detailed information). The system indicates an alert by flashing one of three colours, which corresponds to urgency and matches the list of alerts. To prevent visual overload, alerts are grouped and are attached to the same plant item icon. For example, alerts relating to a single mill are grouped together.\rGrootjen, Bierman and Neerincx (2006) also proposed some ideas for limiting operator information overload. Over multiple projects, Grootjen et al. (2006) identified problems in process control such as information type, information volume, task integration, increased autonomy, increased complexity, low personal costs, low training costs and legislative constraints (for more details, see Grootjen et al., 2006). To address these problems, Grootjen et al. (2006) designed an interface to optimize the operator’s cognitive task load (CTL) by transferring a task or part of a task to another person. This adaptive user interface was designed to:\r• Show only the categories with active alerts (empty categories are not shown). The interface contains the operator’s alerts and the alerts of other operators;\r• Show only the buttons that are relevant for the alerts the operator handles;\r• Provide operators with an icon that allows them to redirect alerts to other operators.\rGrootjen et al. (2006) evaluated the effectiveness of the interface. While participants worked in pairs to solve problems, they were presented with a number of different alerts. Participants received no task allocation (TA) support, task allocation advice from the system, or a notice that the system had reallocated the task. Overall, Grootjen et al.’s (2006) adaptive interface was rated positively. Participants reported that the allocation interface was pleasant to use, not difficult, useful, and allowed them to solve problems faster and better. The reallocation of alerts was not found to require a lot of effort or be confusing. For the automatic alert group, automatic TA of alerts was found to moderately disturb their normal way of working and was rated as moderately annoying. For the advice group, the TA advice was not reported to disturb their normal way of working.\r4.2.2.4.1.2 Assessment\rWith respect to alert intrusiveness, Grootjen et al.’s (2006) adaptive interface offers some useful functions. However, the focus of the work is primarily on process control types of tasks and their associated interfaces, which have little direct comparability to RJOC operators working with GCCS-M.\rMaintain situation awareness of primary task\rMaintaining or enhancing situation awareness of the primary task is another important design feature of an alert warning system. McFarlane and Berger (2004) were concerned with an operator’s ability to maintain situation awareness while using notification systems. Automatic notification systems constantly monitor and generate alerts, but these alerts often interrupt other activities. Although people do not generally perform sustained, simultaneous, multi-channel sampling well on their own, they can do so when provided with specific interface support. An alert- based information stream can deliver tasks and information to support the operator’s ability to a)\rPage 42 Non-Intrusive Alert System Humansystems®\r\nconstantly monitor a dynamic environment, b) collaborate and communicate with other people in the system, and c) supervise background autonomous services. The Human Alerting and Interruption Logistics (HAIL) technology was developed to improve an operator’s ability to maintain situation awareness during high rates of alerting. The HAIL user interface is designed to improve an operator’s ability to process alerts and to reduce the number of interruptions during complex, stressful tactical situations. Compared to the Identification Supervisor (IS) operator for the Aegis Weapon System, HAIL is said to reduce the number of operator interruptions, improve operator situation awareness for each alert and status information, improve control of alerts requiring action responses, and assist in returning to the operator’s original task. For an illustration of the current Aegis alert processing system and the HAIL-enhanced Aegis processing system, see McFarlane and Berger (2004).\rRather than displaying alerts in a single window, the HAIL system pre-processes alerts and displays them in the appropriate window. Operators are then able to negotiate their response to the\ralert by\r• • • • •\rchoosing to:\rSurface the alert (i.e., make it visible); Defer alert and surface next alert; Complete alert and surface next alert; Defer alert; or\rComplete alert.\rMcFarlane and Berger (2004) tested the effectiveness of HAIL with experienced naval operators in an operator simulation using an Aegis alerting system and a HAIL-enhanced Aegis alerting system. In general, results of the HAIL technology were positive. McFarlane and Berger (2004) conclude that HAIL increases warfighter performance by providing operators immunity to the effects of trash alerts, fewer interruptions, better alert situation awareness, and easier recovery of non-alert work after handling an alert. Operators reported that HAIL made it easier for them to distinguish between noise alerts and important alerts.\r4.2.2.4.1.3 Assessment\rThis paper provides some relevant concepts for providing operators with separate functionality for alert actions, alert information and alert management. One caution to be remembered is that this system was designed for more dynamic tactical maritime displays than is the case with GCCS-M, where data is updated at a slower rate (i.e., more time-late data) and tactical decisions and responses do not have to be made with battlefield urgency.\rAlert reduction\rAhnlund, Berguist and Spaanenburg (2003) argue that many alerts are distractive and do not alert the operator to important information. One way to potentially reduce the intrusiveness of alerts and their impact on primary task performance is to reduce the number of alerts to which operators must attend. In particular, nuisance alerts are problematic because they unnecessarily overload operators with alerts, which increases operator workload and has been known to cause operators to turn off alert systems altogether (Sorkin, 1988). Ahnlund, Berguist and Spaanenburg (2003) designed an alarm cleanup methodology and computerized tool to remove nuisance alerts from user interfaces that would not interfere with overall operations.\rHumansystems® Non-Intrusive Alert System Page 43\r\nThe alarm cleanup method uses a software program called the Alarm Cleanup Toolbox (ACT). ACT helps tune alert limits and develops algorithms to reduce the number of alerts. To perform an alarm clean up, the following steps are applied:\r• “Extract signal data during normal operation. This is the difficult part since most control systems are installed without a logging device.\r• Extract information about the signals, such as the current alert limits and the applied signal processing methods.\r• Examine the control system’s built-in functions and programmable capabilities.\r• Perform an off-line analysis of the signals using ACT.\r• Discuss and validate the suggested alert reduction methods and implementation decisions with the operators and personnel with process knowledge.\r• Implement the discussed improvements into the control system” (p. 8).\rTo validate the ACT and alarm clean up method, researchers implemented this technology at a bio- fuelled District Heating Plant (FFC). They were able to track every alert to determine if the alert was removed or delayed. Results showed there were 83% fewer alerts while using ACT.\rChyssler, Burschka, Semling, Lingvall and Burbeck (2004) were also interested in alert and false alarm reduction, specifically within the security field. With respect to security, issues such as alert reduction, false alarm reduction, information correlation, and preventable total service collapse need to be considered. Chyssler et al. (2004) examined alert and false alarm reduction through improving the quality of Intrusion Detection Systems (IDS). According to Chyssler et al. (2004), alerts can be interpreted by their severity, number, frequency, variety, uniqueness and payload. The authors present an architecture that incorporates IDS.\rThe IDS performs the following tasks:\r• Static filtering: A system can produce many irrelevant messages, including false alarms. Tuning a Large Complex Critical Infrastructure (LCCI) is dependent on a static network. Known problems in the system are handled by static filters. These filters omit irrelevant data either by ignoring or deleting the alert. Deleted alerts are permanently removed from the system. Ignored alerts are saved, but not forwarded to the next agent.\r• Adaptive filtering: Unknown problems in the system are handled by adaptive filters. These filters categorize messages as interesting or uninteresting.\r• Aggregation: Aggregation is the combining of frequent alerts into one alert. This reduces the operator’s workload and lessens the intrusiveness of the alerts.\r• Correlation: The correlation agent analyzes the data and determines whether the alert is interesting or uninteresting by comparing the sum to a determined threshold.\rChyssler et al. (2004) applied this architecture to a realistic Internet Protocol (IP) environment where the network experienced internet attacks. Results showed that static filtering reduced the frequency of alerts and that messages were combined into one alert when there was more than a 65% similarity.\rOverall, the method described by Chyssler et al. (2004) shows promising results to reduce the number of alerts and false alarms experienced by operators. Although the study is in the context of\rPage 44 Non-Intrusive Alert System Humansystems®\r\ncomputer networks and internet attacks, it can be applied to the maritime domain. Static filtering (i.e., ignoring or deleting alerts), adaptive filtering (i.e., classifying alerts as interesting or uninteresting), aggregation (i.e., combining frequent alerts into one alert) and correlation (i.e., determining if the vector is interesting or uninteresting using algorithms) data show that when implemented, these techniques can greatly reduce the number of alerts and false alarms.\r4.2.2.4.1.4 Assessment\rThe focus of these papers is the reduction in nuisance and other non-critical alerts through smart technology. As such, these papers will be of more interest to those who will be responsible for the development of the technology and algorithms that will determine which data anomaly conditions in the RMP will merit being brought to the attention of operators., They do not apply directly to the current project where the focus is on interface design approaches to minimise alert intrusiveness.\r4.2.2.5 Summary\rThe following table summarizes some of the key recommendations and design principles from the literature on alert warnings or indicators that are potentially relevant to the design and implementation of initial alert warnings for the RJOC operators.\rTable 12: Summary\rForm of literature\rIssue\rRecommendations or design principles\rModels\rAlert display\rDefine alerts by colour and shape\rLow interruption\rAlerts that are low in interruption are the secondary display and ambient media\rDesign Guidelines\rOperator workload\rKeep set points at a level to avoid nuisance and false alarms\rOperators should be able to suppress or shelve certain alerts\rThe rate at which alert lists are populated must not exceed the users’ information processing capabilities\rOperators should be able to turn off non-critical alerts without erasing information\rAlert display\rDo not present status indications through an alert system display\rAlert acceptance should be reflected by a change on the visual display\rAlert presentation should use specific sounds/colours/etc. to identify the category of risk and priority\rThe auditory portion of an alert should shut-off automatically when it no longer provides useful information\rEmpirical Research\rAlert display\rSlow fade text\rVoice warning\rHigh urgency=sharp sound, medium urgency=soft sound\rPredictive system\rA system which predicts the operator’s actions to interrupt when appropriate\rHumansystems® Non-Intrusive Alert System Page 45\r\nForm of literature\rIssue\rRecommendations or design principles\rDesign Concepts\rOperator workload\rAllow operators the ability to control alert responses through negotiated interruption or surfacing the alert\rAlert display\rUse alert pop ups near cursor position\rColour code alerts by length of time on the screen with alerts getting darker in colour the longer they are on the screen (note: this assumes certain display photometric properties that may or may not be applicable in the RJOCs)\rUse visual distortion techniques to draw operator attention to critical information\rShow only active alerts\r4.2.3 Comprehend alert information content\rWhereas the previous section dealt with issues relating to bringing an alert to the attention of an operator, this section examines relevant research on how to represent the semantic information associated with the conditions that gave rise to the alert. That is, how information should be structured to provide immediate situation awareness of the cause and conditions of the alert. Research relating to alert information content was in the form of design guidelines, empirical research and design concepts.\r4.2.3.1 Design guidelines\rMiller (2005) examined the role of trust and etiquette in adaptive automation. “Etiquette” embodies unwritten codes that define the roles and acceptable behaviour of participants in a social setting. Etiquette rules define the expectations and interpretations of other people’s behaviour. Therefore, etiquette can serve a role in human-computer interaction. For instance, a system using familiar domain jargon indicates that it is experienced with the domain and should be accorded the trust reserved for those in that domain. Etiquette can also define polite and rude behaviours. Humans are prone to interpret computer behaviour similarly to human behaviour (Reeves & Nass, 1996; as cited in Miller, 2005), thus, systems are likely to elicit the same negative or positive reactions if they hold to or defy established etiquette. Therefore, the information content of an alert should comply with system characteristics and jargon with which operators are familiar.\rMiller (2005) defined the following guidelines for adaptive automation. The system needs to:\r• Infer tasks;\r• Allow operator’s to over rule the system;\r• Adapt automation behaviours; and\r• Adapt information presentation.\rAdapting information presentation can be done by:\r• • •\rCommunicating about ongoing and future tasks; Using similar domain jargon;\rUsing text format to report context and tasks; and\rPage 46\rNon-Intrusive Alert System\rHumansystems®\r\n• Overall, maintaining similar human-to-human etiquette. Assessment\rMiller’s (2005) guidelines are based on a study which used etiquette and gained a favourable response from operators. The four bullet points relating to adaptive automation support the notion of having a function in an alerting system that allows local user configuration. The second set of bullets deal more with ensuring that the mode of the communication is consistent with existing operational concepts, language and procedures. In this case, we believe these principles should also be extended to cover the maintenance of existing “human to system to human etiquette”.\r4.2.3.2 Empirical research\rThe literature review uncovered one article describing empirical research relating to alert information content. Steefkerk, Esch-Bussemakers and Neerincx (2007) designed a context-aware alert system and described the system’s implementation. Alert systems should direct the user to important and clear information to allow for quick interpretation and reaction. A prototype was designed on a personal digital assistant (PDA) using visual, auditory and information condensing effects. The alert information included in the prototype varied according to the user’s workload. In low workload situations, the full message (i.e., providing all the information) was presented to the user. Conversely when workload was high, only a summary message was presented to the user, with the exception of high urgency messages. Further, in order to prevent interruption of the user while attending to the PDA, an icon appeared when a new message was received, rather than the full alert. This context-aware prototype in which only a summary message is presented in high workload conditions was compared to a non-adaptive prototype which alerted the user with full messages regardless of workload.\rResults showed that more targets were remembered in the high workload context for those using the adaptive prototype than those who used the non-adaptive prototype. No significant differences were found in terms of the number of correctly remembered messages. Participants who used the adaptive system reported the messages to be less intrusive than those who used the non-adaptive system, especially during high workload conditions. Adaptive systems were also rated as less interruptive and irritating than non-adaptive systems and were generally preferred. Overall, results showed that a system which presented all the information related to the alert during periods of low workload and a summary of the information during high workload was preferred.\rAssessment\rThis study has limited applicability to the present project in the sense that it is based on an assumption that there will be appropriate technology available to assess ongoing operator workload. However, the design concept of providing summary, rather than detailed, alert information in certain conditions may be applicable and will be pursued in the design of a maritime anomaly alerting system for RJOC operators.\r4.2.3.3 Design concepts\rAs described previously, Dicken (1999) discussed the problems that could arise when switching from hard to soft-desks. Dicken also reported how the information related to the alert should be displayed. There are features available to the operator to limit the overload of information. The operator has the option of retrieving more information regarding the alert. This comes in the form of a point and click picklist. The picklist has a number of options that give operators the ability to retrieve more information relevant to the alert, at a time that is convenient to them. The difference\rHumansystems® Non-Intrusive Alert System Page 47\r\nbetween a picklist and an alert list is that the former allows the operator to progressively select further layers of information concerning the alerting state as the situation and time available merit.\rAssessment\rThe merit of this approach is that it would minimize workload by allowing operators to have control over what information is needed at certain times. Thus, the general concept that may be applied to an alert information window is to structure information hierarchically, thereby allowing users progressively more detailed information at lower levels, which can be simply accessed by point and click approaches.\r4.2.4 Maintain primary task\rThis section focuses on the operator’s ability to perform his or her primary task while being presented with alerts. Research suggests that there are three features of an alerting system that will support an operator’s ability to main their primary task: assessing the interruptibility of the operator, preparing an operator for an upcoming alert, and facilitating the operator’s return to the primary task. This section is therefore further subdivided according to these three features.\r4.2.4.1 Assessment of interruptibility\rThis section describes literature relating to human factors and the assessment of interruptibility in the design of an alert system. Of the four categories of literature, we were only able to find relevant papers relating to generic design guidelines.\rDesign guidelines\rThe assessment of interruptibility relates to the impact an alert has on the performance of the operator. The goal is to minimize interruption to maintain performance and situation awareness. The literature provides some evidence that emotional states have a major impact on perception, cognition, and motor processes, which in turn, impact performance. Belief states (i.e., assessment of the current situation) have also been found to impact decision making and response selection. Given the impact that affect and beliefs have on performance, the following authors have looked at how to adapt information presentation to the individual belief states of operators.\rThe Affective and Belief Adaptive Interface System (ABAIS) developed by Hudlicka and McNeese (2002), was designed to address individual differences. Specifically, ABAIS uses an adaptive methodology framework capable of adapting the user interface and information to an operator’s current affective state, key personality traits, situation specific beliefs, and preferences. In particular, the ABAIS system architecture uses an adaptive methodology consisting of the following four modules:\r1.\r2.\r3. Page 48\rUser State Assessment: identifies the user’s affective state and task relevant beliefs (e.g., level of anxiety). The User State Assessment module receives information about the operator and task context to identify the operator’s predominant affective state and situation-relevant beliefs.\rImpact Prediction: identifies the effect of operator state on performance (e.g., focus on threatening stimuli). The Impact Prediction module inputs the identified affective state and operator beliefs to determine, using rule-based reasoning, the impact on task performance.\rStrategy Selection: selects a compensatory strategy (e.g., presentation of additional information to reduce ambiguity). The Strategy Selection module receives the predicted\rNon-Intrusive Alert System Humansystems®\r\nimpact as input and selects a compensatory strategy to counteract resulting performance biases.\r4. GUI/Decision Support System (DSS) Adaptation: modifies the user interface content and format to improve detection, recognition, and assimilation of incoming data to enhance situation awareness. The GUI/DSS module implements a selected compensatory strategy in terms of specific GUI modifications. GUI modifications are based on individual user preferences for information presentation (e.g., blinking, colour change, size change of relevant display or icon).\rPrior to using ABAIS, each operator must provide background information (e.g., individual history information, baseline physiological or diagnostic data). Categories of information include personality, skill, individual history and adaptation preferences. The information is then used to perform a Cognitive, Affective, Personality Task Analysis (CAPTA) to produce a comprehensive description of possible behaviours and behaviour states associated with specific user states, traits and beliefs.\rOnce the user affective and belief states are identified and their likely impact is predicated, ABAIS identifies a compensatory strategy and selects a means of implementing this strategy in terms of specific user interface modifications. This strategy is defined based on stable performance biases and the current task context. Once the compensatory strategy has been identified, ABAIS implements this strategy in terms of specific modifications to the user’s interface. Modifications of the interface to present the information can be made by:\r• Modifying the GUI icons in terms of attributes (e.g., changing colour or size) or modify the icon appearance itself.\r• Modifying the display as a whole by changing size, location, appearance, or contents.\r• Implementing changes to the GUI as a whole or insert additional display elements designed to focus attention on particular areas. For example, reconfigure entire set of instruments to reflect a different system model and insert attention-capturing and attention-directing elements designed to direct the user’s attention to a particular icon or display.\r• Inserting new or modifying existing alert and alert notifications, or adding an icon to a display to represent new information. An example of a notification level adaptation includes adding text regarding desired focus of attention, or adding an icon to a display to represent new information.\rThe ABAIS technology could prove very useful in reducing alert intrusiveness. Prior to presenting information to an operator, the system takes into consideration the operator’s stable beliefs, current affective state, and information presentation preferences. Combining this information potentially provides users with information in the most effective and least intrusive manner. We use the term “potentially” because the ABAIS has not yet been empirically tested with operators.\r4.2.4.1.1.1 Assessment\rThe fundamental assumption of this paper is that affective states can be reliably and accurately assessed and modelled to provide clear parameters for selecting appropriate alert configurations and parameters. This approach is therefore highly speculative and subject to potentially serious operational consequences, if not implemented with a very high level of accuracy and reliability.\rHumansystems® Non-Intrusive Alert System Page 49\r\nOther than gross examples, the paper does not provide any insight or concepts on exactly how different emotional or other states would be mapped onto specific alert designs.\r4.2.4.2 Warning of impending alert\rThis section describes the literature related to how to prepare operators for an impending alert by providing an advanced warning. Of the four categories of papers, only empirical research was found.\rEmpirical research\rHwang, Lin, Liang, Yenn and Hsu (2008) developed a pre-alert system that would reduce the frequency of alerts and examined whether these pre-alerts should be a text or a graphic. Reducing the numbers of alerts is done by aiding the operator in identifying faults before the alert is activated. This method has been applied to many industries and has shown positive results in power plant control rooms. Hwang et al. (2008) designed the pre-alert system based on the following 5 rules in deciding whether or not a system is out of control:\r1. “Any one point falls outside the upper control limits (UCL) or lower control limits (LCL);\r2. Seven points in a row are continually increasing (or decreasing);\r3. Cyclical patterns of points occur;\r4. Two out of three consecutive points fall beyond the two sigma limit;\r5. A run of five points falls beyond the one-sigma limit (Aft, 1998)” (as cited in Hwang et al., 2008, p. 2).\rThe pre-alerts were in the form of a text or graphic. The text alert turned black to yellow and was accompanied by a “ding” sound when the value changed 7 times (dropped or rose consecutively; Rule 2). This text changed from yellow to red and was accompanied with an alert when the value was outside the parameters (Rule 1). The other type of pre-alert was graphical which was designed to provide trend information of the situation. The original alert did not include the pre-alert function.\rTwenty-six graduate students and staff of National Tsing Hua University were randomly assigned to 13 groups which included a reactor operator (RO), assistant reactor operator (ARO) and a supervisor. The primary task of the RO and ARO was to monitor 6 critical parameters (vessel pressure, level, reactor feedwater pump turbine vibration, discharge pressure, turbine vibration or generator vibration). The RO monitored the parameters of the core flow and power during the 12 minute shutdown (normal state). During shutdown, the ARO closed and opened valves and pumps. During the load rejection (abnormal state), 5 alerts went off. The RO had to search for 10 items and find solutions in 10 minutes. The alerted task was deciding if the presented values were greater than or less than each other. Participants then filled out a questionnaire assessing their perceived mental workload.\rOverall results showed that the pre-alert type significantly reduced the number of alerts and the mental workload of the operator and maintained situation awareness. There were no significant differences found between the text and graphic pre-alert types for reducing the number of alerts. However, for the alerted task, operators had significantly more correct answers when deciding if the presented numbers were greater or less than each other when the alert was graphic compared to textual. The operator had significantly more correct answers during shutdown (normal state) than\rPage 50 Non-Intrusive Alert System Humansystems®\r\nin load reduction (abnormal state). Also, the ARO had significantly more correct answers than the RO during load reduction (abnormal state).\rIn summary, Hwang et al. (2008) discovered that a pre-alert system would reduce the number of alerts, thus reducing the intrusiveness experienced by the operators. Although both types of pre- alert systems would be a benefit to the operator, Hwang et al. (2008) recommend the text type pre- alert to be implemented in control rooms.\r4.2.4.2.1.1 Assessment\rThe generalisability of this paper to the operations room remains unknown, since the major thrust is to reduce the number of alerts (presumably in a context where this is a serious problem for operators). If the alert frequency is high, the use of an auditory alert may still result in nuisance alerts for the operator. In addition, the use of trend data to cue the alert may not be applicable to the maritime anomaly context.\rNote that the paper by Mitchell (1998) reviewed in section 4.2.2.3 also has some relevance to the issue of interruptibility.\r4.2.4.3 Facilitate return to primary task\rThis section examines relevant research on how to best support an operator in returning to his/her primary task after attending to an alert. That is, what should be considered in the design of an alert system (which may draw the operator’s attention away from the primary task) so that the operator can easily and quickly return to his/her primary task? We were only able to find research in the form of design concepts rather than models, design guidelines or empirical research.\rDesign concepts\rAfter attending to an alert, operators must typically return to their primary task. This can be problematic for operators as interruptions to deal with alerts are associated with increased errors, reduced efficiency, and increased stress (McFarlane & Latorella, 2002). In addition, interruptions such as attending to an alert can reduce an operator’s situation awareness of the primary task domain. Operators may experience a “resumption lag” once they return to a primary task because they then have to work to re-acquire situation awareness, retrieve suspended task goals, and perform required actions (Monsel, 2003; as cited in St. John, Smallman & Manes, 2005). Returning to a primary task after an interruption can be particularly difficult if the primary task requires the operator to monitor a screen and detect changes. As noted by Smallman and St. John (2005), humans have remarkable difficulty identifying changes, which is particularly true when operators are distracted or interrupted. The longer the disruption, the more problematic it will be for operators to detect changes because their memory of the state prior to the alert will decay. Smallman and St. John (2003) argue that current methods of displaying information only add to an operator’s difficulties when returning to primary tasks because current displays show information in real time, which require operators to remember and mentally integrate previous information with current information. This forces operators to determine for themselves whether or not changes have occurred. In order to address the effects associated with reduced situation awareness (e.g., tunnel vision, resumption lag) and to improve change detection ability, Smallman developed the Change History Explicit (CHEX) human computer interface tool.\rThe CHEX tool augments human attention by detecting significant changes to a situation and logging these changes into a table, which can be sorted and filtered by the operator according to specific variables (e.g., significance, change type, age). Table entries are linked back to objects in\rHumansystems® Non-Intrusive Alert System Page 51\r\nthe geographical display. In order to evaluate the usefulness of the CHEX tool, Smallman and St. John (2003; St. John, Smallman & Manes, 2005) conducted a series of studies comparing the CHEX tool to conventional displays.\rIn the first study, Smallman and St. John (2003) had 80 university students monitor a Geoplot containing a high density of aircrafts (40) or a low density (13). The aircraft slowly moved about an Own-ship signal, changing direction, speed, and turning on and off their fire-control radar (FCR). The participant’s task was to identify aircrafts that turned critically threatening, as quickly as possible. Aircrafts on the Geoplot were assigned an “interest score” to reflect their potential significance to the operator; ten interest points were assigned to aircraft that were 1) flying fast, 2) flying towards own-ship, and 3) had FCR turned on. Once an aircraft had a score of 30, it was defined as a “critical aircraft”. Aircrafts with interest scores of 10 or greater were shown in yellow, whereas those with interest scores of less than 10 were faded. Within each density condition, participants used one of four different change awareness human-computer interaction (HCI) schemes, 1) a baseline of the Geoplot and CRO, 2) baseline plus a static, chronologically sorted Change History Table, 3) baseline plus Change History Table and red circle alerts around all aircraft with changes (circles could be removed by selecting the aircraft), or 4) baseline plus CHEX (a sortable Change History Table linked to the Geoplot). Participants conducted both monitoring and reconstruction tasks. For the monitoring task, participants had to indicate when an aircraft became critical and how many changes the aircraft had made. For the reconstruction task, participants performed mental arithmetic for a minute while the scenario continued to play out of view then returned to the display to indicate when an aircraft became critical and the number of changes. The authors found that adding a Change History Table and change alert circles improved performance in terms of the percent of correctly identified critical aircrafts, but the greatest improvement in performance was seen when the participants were using CHEX in the high density condition. Participants in the CHEX condition had an 80% improvement in change identification speed compared to the baseline condition.\rIn the second study, St. John, Smallman, and Manes (2005) were interested in evaluating the design space of situation awareness recovery tools by comparing CHEX against an alternative tool, Instant Replay. Instant Replay allows operators to return to a monitoring task after an interruption and replay the missed period at high speed to quickly search for any changes to the situation. Participants were allocated into one of five conditions: Baseline (map and the aircraft data display in the lower right corner of the screen), Basic Replay (allowed participants to restart the scenario from the beginning of the last interruption), Explicit Replay (automatically detected and marked significant changes by adding small red triangles to the aircraft symbols and a “pop” sound), Explicit Markers (removed the replay function but kept the red triangles and pop sounds), and CHEX (included a table that logged the time, aircraft identification number, and a short description of the change). For a screenshot of the CHEX display, refer to St. John et al. (2005).\rEach scenario, contained aircrafts moving slowly in the display, interruptions, and changes. Participants response times using the CHEX tool were significantly faster than the other tools, and 57% faster than the Baseline condition. Participants in the CHEX condition also produced fewer misses and fewer errors than participants in any of the other conditions. Participants in the Explicit Markers condition produced few misses, but a high number of errors and moderate response times. Participants in the Baseline condition produced high miss rates, high error rates, and slow response times. Participants in the Basic Replay condition had the slowest response times.\rMcFarlane and Latorella (2002) reviewed the tools that could improve an operators’ ability to return to primary tasks by designing user interfaces to present reminders about the existence and\rPage 52 Non-Intrusive Alert System Humansystems®\r\nstate of interrupted activities. Marlin et al. (1991; as cited in McFarlane & Latorella, 2002) designed a user interface that specifically allowed users to suspend and resume activities. Specifically, this design allows users to explicitly mark when an interruption occurred, which allows the computer to generate appropriate recovery support. Rouncefield (1994; as cited in McFarlane & Latorella, 2002) used a similar method in paper-based offices by having workers mark their work context before leaving to handle an interruption. These markers were found to facilitate recovery of prior work contexts when people returned to their prior tasks. This could be implemented in a computer environment by noting interruptions on an electronic notepad that constantly displays a list of interrupted activities (Cypher, 1986; as cited in McFarlane & Latorella, 2002). Finally, Lee (1992; as cited in McFarlane & Latorella, 2002) found that marking the primary task window with an animated border, instead of a static border, reduced the confusion about which window was active when operators resumed a task after an interruption.\r4.2.4.3.1.1 Assessment\rWhile the problems associated with recovering situation awareness and quickly resuming a primary task that was interrupted by an alert are not the primary focus of the present project, these papers provide some concrete suggestions that could be implemented into a future operator interface.\r• Include a table of significant recent changes.\r• Automatically highlight all changes to a tracked vessel when a change is selected. When a change occurs, a pop sounds and a new row is added to the top of the table. Selecting a row highlights the row in yellow, highlights the aircraft on the map with a yellow circle, and presents the aircraft’s data in the data display.\r• Automatically link and highlight between the “Change Table” and the Geoplot when a vessel in either display is selected. This allows for faster critical vessel identification because operators do not have to search the Geoplot to find the location of the relevant vessel.\r• Including the ability to sort the “change table” as needed by the operator.\rThe following aids, while not suited to the current implementation of GCCS or Concept of Operations in the RJOCs, may also have merit in allowing operators to quickly regain situation awareness of the primary task:\r• Electronically marking primary tasks before attending to an alert;\r• Using an electronic notepad to display interrupted activities; and\r• Mark interrupted task windows with an animated border to allow for quick identification of interrupted tasks.\rFinally, it should be noted that the operational environments that the authors of the above studies had in mind are characterised by multi-tasking on a single or multiple displays, highly dynamic data inputs and the often need for a rapid operator response. For the most part, these are not characteristics that would apply to the RJOCs, except under occasional and special circumstances.\r4.2.5 Manage Alerts\rAlthough not the primary focus of the literature review, several papers were found relating to human factors and the management of alerts in the form of design concepts. Since these could have\rHumansystems® Non-Intrusive Alert System Page 53\r\npotential relevance for the prototype design phase of the project, they have been included in the review.\r4.2.5.1 Design concepts\rIt is becoming increasingly important to introduce intelligent alert handling capability in order to manage alert systems. Liu et al. (2003) developed an Intelligent Alarm Management System (IAMS) for suppressing nuisance alerts and for providing advisory information to help panel operators focus and respond quickly to important alert information. IAMS was developed to incorporate special-purpose algorithms, process knowledge, and control system expertise. It consists of a graphical user interface (GUI), a Data I/O function, an Alarm/Trend/Knowledge Database and six sub-blocks:\r1. Statistical analysis: counts alert numbers in real time for different time periods, tags, message types, and alert statuses.\r2. Nuisance HI/LO analysis: analyzes high or low alerts and suppresses those that are repeating.\r3. IOP (Input Open) analysis: identifies the cause of input open alerts and suppresses those that are nuisance alerts.\r4. Criticality analysis: gives a criticality tag of very important, important, less important, or calculation-related to each alert message.\r5. Standing alert analysis: shows standing alerts, warns of ramping alerts, and resets standing alerts.\r6. Monitor & recover: shows changes to distributed control system (DCS) alert settings and restores alert setting when the nuisance status is cleared.\rIn order to aid operators in making appropriate responses to information, IAMS provides operators with advisory information that:\r• Informs operators which alerts are emergent or critical;\r• Provides operators with early warning for alerts that will lead to violations of high-high or\rlow-low limits;\r• Provides online maintenance reports; and\r• Provides alert statistics.\rAll information is provided to the operator through a GUI. The GUI displays guidance information, criticality, statistics, and an alert management overview, and allows operators to suppress nuisance IOP alerts. The operator controls the alert suppression by clicking the SPR button to enable and the RST button to disable the alert suppression function. The IAMS system also allows operators to obtain a control loop status report over the last work shift (SFT), obtain maintenance information (MTN), monitor alert information such as setting changes (MON), suppress nuisance IOP alerts (IOP), as well as guidance information and alerts that have occurred (GID). Operators also have access to all, calculation-related, ordinary, important, or critical alert messages (CRIT, CAL, ORD, IMP, EMG, respectively). Alert statistics reports can be easily generated for the last 10 seconds, 5 minutes, hour, day, or a special time period (SEC, MIN, HR, DAY, SPE, respectively).\rPage 54 Non-Intrusive Alert System Humansystems®\r\nThe IAMS not only allows operators to suppress nuisance alerts, it allows them to manage the non- nuisance alerts. This alert management system maintains operator situation awareness by providing the necessary information to the operators at their convenience.\rAnother non intrusive alert management method mentioned in the literature is an alert list. Riveiro, Falkman and Ziemke (2008) and Dicken (1999) used an alert list in their design of an alert system.\rAn alert list is an alert management component of an alerting system, designed to manage alerts that have been presented to an operator. Riveiro et al. (2008) designed an anomaly detection system which includes an alert list. The interface display of the anomaly detection system includes a geographical map, controls, detailed information, and the alert list (shown in the bottom left hand corner). This list is shown in detail in Figure 5.\rFigure 5: Alert list (Riveiro et al., 2008, p. 6; © 2008IEEE)\rWhen a vessel is considered anomalous, it is given an identification number and a coloured ellipse, reflecting the probability of the anomaly. A card appears on the alert list containing information such as the object identification (ID), coordinates, probability, main reason, age of alert, and delete/report buttons. By pressing any of these buttons, the operator can obtain more information regarding the alert. The object ID is the object identification number, which identifies each vessel accordingly. The object ID is also highlighted with an urgency colour indicating the probability of the anomaly (red, orange, yellow). The probability of the alert identifies the probability that the alert is anomalous. The position of the ship is given by x- and y-coordinates. Every alert is time- stamped which allows the operator to determine how old the alert is compared to the other alerts. Finally, each alert on the list contains a report and delete button the operator can click to perform the intended action.\rDicken (1999) discussed an alert list in the context of a recent operator desk change, from a hard- to soft-desk. A hard-desk can be described as a horse shoe control desk which includes dials, meters, chart recorder, knobs, and buttons. It consists of a back panel where the indicators can be found along with alerts. A soft-desk is a hard-desk incorporated with Visual Display Unit (VDU) screens. Along with this modernization of the standard hard desk, alert systems have also been changed to include alert management software and user displays.\rAs described by Dicken (1999), the alert interface has two options for soft-desk set up, 1) the standard VDU alert interface, or 2) the alert display and acceptance interface. The latter option is performed by pointing and clicking a mouse on a standard screen. This option also has an alert list as a secondary backup which can be displayed on any screen and is permanently on due to necessity. These lists are limited to a single chronological alert list and are unusable during emergencies. Recommended improvements to the alert list are done by: 1) showing alerts on pages rather than scrolling, 2) keeping the alerts in the same order (no shuffling), 3) adding new alerts to the bottom of the list, 4) removing alerts only by operator action, 5) having a flashing marker,\rHumansystems® Non-Intrusive Alert System Page 55\r\nrather than the alert text flashing, and 6) prioritizing alerts by colour. Dicken (1999) notes that these lists should be filtered, especially during an alert flood. Filters such as priority, category (category rectangles are displayed on the bottom of the list), named list (alerts associated with a task are built into a filter), modes (plant modes, e.g., stable operation, start up), and unaccepted (alerts that have not been accepted) can be useful to prevent operator overload.\rThe alert list also contains a “shelve” option for the inevitable nuisance alerts. Until these are serviced and fixed, operators can choose to ‘shelve’ these alerts to limit their intrusiveness. This shelving option allows operators to ignore false or nuisance alerts that should not be deleted and should not take time away from the primary task. However, unlike ignoring alerts, the shelving option would not remind the operators of the alert because operators are still responsible for the shelved alert list.\rAssessment\rThis paper provides a good description of the issues concerning the management of alerts and potential design solutions. It shows that an alert management system could be one method to ease operator workload and maintain situation awareness. An alert management system can organize alerts into a coherent list which the operator can access at any time to obtain more information. The suggested information contained in these lists are vessel ID number, position, urgency of alert, age of alert, and action buttons (ignore, delete, report, shelve). Alerts on the list should be colour coded according to urgency and this colour should match the actual alert (e.g., red for high urgency, orange for medium urgency, yellow for low urgency). The list should also include a detailed history of all alerts, including those shelved, deleted and reported. Lastly, this list should contain all the information found in the actual alert, including links to pertinent information.\rThere was some lack of detail concerning the implementation of the alert list in both Riveiro et al. (2008) and Dicken (1999) although it should be acknowledged that this was not the main focus of either paper. For example the following issues were not addressed: can an alert list contain a shelved/deleted/reported/ignored list or should they be treated separately? Do the alert lists include alerts that are 1 hour old, 1 day old, 1 week old, etc.? Can an alert system display all the alerts in an appropriate manner without looking cluttered, or are there a maximum number of alerts that can be listed? Further, maritime operators may have a large number of alerts present over the course of a watch, which could pose a problem in terms of where and how the alert list should be displayed. In the example provided, the alert list looks like it could fit 7 alerts across the screen, but the paper does not give any indication of what happens to the alert list when it becomes full.\rIn addition, it is probable that operators would find the alert list more manageable and they would have improved situation awareness if the list were able to be sorted by alert severity. These questions and more need to be examined thoroughly before the appropriate functionality and design requirements for an operational context can be determined.\rFinally, the paper assumes that operators can usefully comprehend categories of alert probability, which remains an assumption for which current empirical experimentation provides no clear guidelines.\rPage 56 Non-Intrusive Alert System Humansystems®\r\n4.3 Conclusions\r4.3.1 State of current knowledge\rThe following summary table shows the number of papers found and reviewed for each of the main functional aspects of an alert system categorised according to paper type (see Table 13).\rTable 13: Literature in the report\rConfigure Alert parameters\rReceive Information on Alert State\rAlert Information Content\rMaintain Primary Task\rAlert Management\rTotal\rModels\r0\r1\r0\r0\r0\r1\rGuidelines\r0\r6\r1\r0\r0\r7\rExperiments\r0\r7\r1\r1\r0\r9\rDesign Concepts\r2\r9\r1\r5\r3\r20\rTotal\r2\r23\r3\r6\r3\r37\rOverall, the largest subset of papers was about design concepts, and the majority of these focused on how to indicate to the user that an alert or alarm had occurred. It was somewhat disappointing to find so few papers that provided conceptual guidance for the design of alert systems and how little empirical research had been done to test the validity or generalisability of design options. One central theme was evident in several papers, namely, the importance of designing alert systems to minimise the operator’s mental workload and to reduce the potential for annoyance.\rWhile we found some empirical research on alerts and their potential impact on operator performance, there is a lack of research explicitly related to non-intrusive alerts. This may be because much of the literature has focused on the traditional problem of how to make an alert intrusive or salient enough to catch the attention of the operator.\rRecent attention has turned to the issue of how a high number of false or nuisance alarms can degrade system and/or operator performance. This has resulted in a body of research attempting to decrease nuisance and false alarms. Within this body of research are design guidelines and concepts that may also be useful in reducing the overall intrusiveness of alerts. To that end, a number of researchers (e.g., Edworthy & Hellier, 2002; Brown et al., 2002; Chyssler et al., 2004; Ahnlund et al., 2003) have recommended ways to reduce the number of alerts an operator is exposed to on a daily basis.\rUsing the ontology outlined earlier in the paper as a reference point, we have extracted from the literature a number of design principles for each of the main alerting system components, as shown in Table 14.\rHumansystems® Non-Intrusive Alert System Page 57\r\nTable 14: Alert design principles\rAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rConfigure alert parameters\rOperator preferences (individual differences)\rInterrupt the operator according to their preferences (i.e., colour, size, time, modality, movement)\rThis guideline should be treated with caution to ensure that operators are not provided with the freedom to create HF inappropriate designs.\rReceive Information on Alert State\rVisual\rColour – differ by priority (e.g., red=high urgency, orange=moderate urgency, green=low urgency) and length of time on screen\rNeed specific guidelines for time on screen or duty cycle for flashing alarms.\rColour coding implemented in design prototypes.\rSize of alert icon – larger icons for higher priority alerts\rNeed specific guidelines on size of alert and how this relates to the display size and the size of windows.\rSolid v. blinking icons\rNeed specific guidelines on blink rates\rChange display when alert has been accepted/actioned (e.g., remove alert from screen, visual marker to note it has been accepted)\rPop-up alert near cursor\rThis may be too intrusive for some tasks.\rNot suitable for RJOC context\rSlow fade v. blast or ticker alerts\rNeed specifications on the dynamics of the fade\rMovement at visual periphery\rMay be too general a recommendation without specifying primary tasks for which this would be appropriate.\rNot suitable for RJOC\rPage 58 Non-Intrusive Alert System Humansystems®\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rcontext\rShift display to foveal region\rThis would have to be implemented with caution because of the potential to impact adversely on the situation awareness of the primary task.\rNot suitable for RJOC context.\rDisplay arrows pointing to alert\rNot suitable for RJOC context\rVisual distortion techniques\rThis would have to be implemented with caution because of the potential to impact adversely on the situation awareness of the primary task.\rNot suitable for RJOC context.\rAuditory (NOTE: none deemed suitable for RJOC context)\rVoice saying “Conflict Conflict”\rLimited application\rHigh urgency=sharp sound, medium urgency=soft sounds\rNeed to define frequency and amplitude characteristics more specifically.\rUse specific sounds to identify category of risk\rPotential impact on increased need for training.\rAutomatically shut-off auditory portion of alert when it no longer provides useful information\rOther modalities\rTactile and olfactory\rTactile may be suitable for warning individual operators who are away from their workstation. Need guidelines and research on the vibrotactile profile and its perceived urgency.\rOlfactory unsuitable\rHumansystems® Non-Intrusive Alert System Page 59\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rfor most environments.\rAlert Information Content\rAdaptable to workload\rHigh workload=summarized information, low workload=full information, high urgency=full information\rAssumes that system is able to assess workload.\rNot suitable for RJOC context.\rDirect operator /constrain information\rUse picklists.\rButtons relating to the schematics, control panel, trends, point information, actions, procedures, and history of the alert\rNot suitable for RJOC context.\rMaintain situation awareness\rAbility to gain information of a vessel by clicking on the vessel on the RMP\rHighly relevant to RJOC. Implemented in design prototypes.\rMaintain Primary Task\rAssess Interruptibility\rThe system accounts for user’s beliefs, affect, preferences, ongoing task priorities\rTechnology not yet available to ensure the level of accuracy required in estimating interruptibility.\rPre-alert\rUse text rather than graph\rNot applicable to RJOC\rReturn to primary task\rInclude a sortable table of recent changes in the situation while operator was away.\rMore suitable for more highly dynamic information environments than the RMP .\rRestore task window to former look\rWhile generally advocating this approach, we caution that depending on the context there is potential for operator disorientation if the picture/RMP suddenly changes focus/range etc without operator input.\rMarking primary task\rElectronically mark task before attending to an alert\rNot applicable to RJOC\rElectronic notepad to record interrupted activities\rNot applicable to RJOC\rMark interrupted task window\rNot applicable to\rPage 60 Non-Intrusive Alert System Humansystems®\r\nAlert system parameters\rConsiderations\rSpecific design recommendations\rComment\rwith an animated border\rRJOC\rAlert Management\rAlert list\rProvide sortable lists of alerts that show: alert priority, alert context, time of alert and relevant information concerning contact details. Functionality to re-order and delete.\rImplemented in design prototypes\rProvide detailed history of all alerts (shelved, deleted and time actioned)\rRelevant to RJOC but not in scope of present work.\rNot to exceed user’s information processing capabilities\rToo general to be useful!\rEtiquette\rCommunicating about ongoing and future tasks\rThis is more applicable to highly dynamic task contexts.\rUsing similar domain jargon\rEnsure that design approaches are consistent with RJOC CONOPS and GCCS style.\rOur general assessment is that the above represents a somewhat piecemeal and haphazard collection of principles and guidelines, as might be expected since they are an agglomeration from many different papers with quite different application environments and goals. Clearly, there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. However, the detailed guidelines found in Shorrock et al (2002) and Han et al (2007) provide a good starting point for an integrated guidance document, which would then need to be extended and made more context relevant to the RJOCs. In addition, there would be a need to make this guidance more specific than statements such as “alarms should signal the need for action”, “alarms should be detected rapidly...”, “alarms should not annoy, startle or distract unnecessarily” etc. Thus, while these recommendations are sound, there is a lack of information on how they are to be implemented as specific design guidelines.\r4.3.2 Gaps in the literature\rIn general, there was a common theme in the literature relating to alert system design, namely appropriate ways to alert an operator of the situation at hand. Capturing an operator’s attention requires shifting attention to the alert and often times away from the primary task, although not necessarily for a significant period of time. One could argue that the very nature of an alert is to be intrusive. Although the literature presented various methods to lessen the intrusiveness of the alert system, the notion of an explicit ‘non-intrusive’ alert was not mentioned. Further, the impetus for designing less intrusive alerts appears to be minimizing operator annoyance rather than cognitive demands.\rHumansystems® Non-Intrusive Alert System Page 61\r\nAs shown in Table 14 in the previous section, literature in the form of models that could be used to inform the design of a maritime anomaly alert system was minimal. It is clear that the potential impact of poorly designed alerts or alarms on operator performance and behaviour (e.g., turning off nuisance alerts) has been recognized and a number of design concepts have resulted. However, normative models that describe the relationship between alert system parameters and human cognition do not appear to exist. Such models would be extremely valuable in providing a theoretical foundation for the design of an alert system. At present, the most suitable models are generic cognitive information processing approaches those that focus on attention sharing and resource competition. While not the focus of the present work, models of alert annoyance have some relevance to the design of auditory alerts, although they also may lack the specificity to inform design approaches.\rThe majority of the literature that was found related to the alerting cue itself with little focus on the other functional components of an alerting system. Although there is a relation between the alert presentation and the alert information content, most research focused on how to present an alert to an operator, rather than the information that should be included in the alert, when to present the alert (interruption), how to configure the alerts and how to manage old and new alerts.\rThe literature does provide, however, some reasonable guidance in reducing the frequency of alarms and configuring alarms to an operator’s local priorities.\rThere is a significant gap in the literature when it comes to concepts relevant to non-intrusive alert design guidelines. That is, recognition of the need to capture the operator’s attention while not interfering with cognitive processing of the primary task was not the impetus behind the majority of the guidelines. There were a few applicable design guidelines that were found relevant to the alert state and the alert information content and these guided some of the design concepts that were developed.\rFew articles investigated and compared different modalities of alerts. Given the pervasiveness of visual interfaces, it is not surprising that visual alerts were the focus of the bulk of the literature. However, there is research to suggest that auditory and tactile alerts may be useful and perhaps even more appropriate than visual alerts in certain contexts. Factors relating to the intrusiveness of auditory and tactile alerts, however, were beyond the scope of this review and would need to be further investigated.\rIn conclusion, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. Nor, did we find a comprehensive body of information that could provide specific answers on how to scale the intrusiveness of alerts. Thus, the conclusions we have reached concerning design for non-intrusiveness are based upon relevant concepts extracted from the more general alert/alarm literature. In doing so, it should be pointed out that many of the recommendations lacked the specificity to inform design approaches.\rThe following table summarises our assessment of the state of the art of the knowledge base, and what future studies may need to be done in order to provide a comprehensive set of guidelines for the implementation of non-intrusive alerts to operational contexts where such an approach is merited.\rPage 62 Non-Intrusive Alert System Humansystems®\r\nTable 15: Summary of literature relevance and need for research\rAlert Function\rComment\rConfigure alert parameters\rThere are some useful generic guidelines available to guide the design of interfaces that would allow operators to configure alert parameters and priorities. Examples of interfaces for rapid selection of rules and configuration are available.\rResearch question - what is the appropriate number of alert priorities?\rReceive information on alert state\rWhile there is abundant information on the various ways an alert can be brought to the attention of the operator, there is little research on how this can be done non-intrusively.\rResearch is needed to determine the relative intrusiveness of a range of design parameters pertaining to the alert size, location and dynamics.\rMore refined information processing models need to be developed to allow a better understanding of how attention and intrusiveness are related.\rComprehend alert condition\rThere is a sold base of information available in the general alarm/alert literature. In an RMP context, there is a need to examine trade-offs between implementing the information within the RMP window and/or providing a separate window.\rAction the alert condition\rBeyond the scope of the present project\rManage Alerts\rSome useful general principles are found in the literature.\rIn the context of the RJOC, analysis needs to be performed of the information requirements for an alert management system for data anomalies. May also need to consider how this would integrate within the overall “alert” system within the centre.\rMaintain primary task\rAssessment of interruptibility\rThe technology for this is not proven and has the potential for creating adverse operator reactions. The human factors literature in this area was not a priority for this project. However, computer scientists have been interested in this problem and have developed some interesting models.7\rWarning of impending alerts\rThis literature is probably of more relevance to more highly dynamic information environments than the RJOC.\rMethods for return to primary task\rGeneral principles for rapid regain of situation awareness are applicable.\rResearch questions: should RMP be refocused/ranged as before the alert was serviced? Does changing RMP focus between primary task and actioning the alert cause loss of situation awareness. Is there a need to alert operators to any change in RMP status on return from alert?\r7 See, for example, Gievska, S. and Sibert, J. Using task context variables for selecting the best timing for interrupting users. ACM International Conference Proceedings, vol 121, 2005.\rHumansystems® Non-Intrusive Alert System Page 63\r\nThis page intentionally left blank.\rPage 64 Non-Intrusive Alert System Humansystems®\r\n5. The Design Development Process 5.1 Development of design concepts\rBased on a review of the literature, and bounded by the primary focus of the project, we concentrated on developing design ideas for two primary functional elements of an alert system:\r1. An alert indicator which appears superimposed upon the GCCS window and is designed to advise of new alerts non-intrusively; and\r2. An alert information window (AIW) which allows operators to obtain details on specific alerts and to manage alert lists.\r5.1.1 The alert indicator\rWith factors outlined in the previous section in mind, we developed design concepts that map onto three levels of importance of the information that triggers the alarm. Our discussions with an SME and our own analysis of potential requirements suggest that three priority levels represent an appropriate balance of information categories.\rFor each information category (i.e., attribute, movement and VOI related) we have provided operational examples of RMP anomalies taken from Davenport (2008) which have been sorted into the three priority levels by an SME. There are four basic information requirements for all alert concepts:\r1. To bring to the attention of the operator that an alert has occurred;\r2. To provide an indication of the alert priority;\r3. To provide a cumulative numerical indication of how many outstanding alerts are in the system (i.e., have not been processed or cleared); and\r4. To update the cumulative alert indicator when an alert has been cleared.\rThere are several coding schemes that could be potentially used to visually indicate urgency level, including factors such as:\r• Colour stereotypes\r• Colour/luminance increments from the background\r• Size of the alerting stimulus\r• Rate of flashing\r• Locus on the display\rConsiderations for the design of a visual alert indicator for each of the three priority levels are outlined in the next section.\rIn terms of other modalities to indicate alerts, auditory alarms were not considered as we concluded that the perceptual deviation from the operator’s primary task (i.e., a visual monitoring task) would\rHumansystems® Non-Intrusive Alert System Page 65\r\nbe too great and they would therefore be too intrusive. Further, it is anticipated that auditory alarms would be disruptive to other operators in the RJOC.\rA tactile interface, on the other hand, may be feasible in that it can be isolated to the individual operator and therefore not disruptive to other operators. Furthermore, the intrusiveness of a tactile alert can be altered to imply a specific level of priority of information. Considerations for the design of a tactile alert indicator for each of the three priority levels are outlined in Section 5.1.3.\r5.1.1.1 Priority 1 alerts\rA priority 1 alert is an anomaly that is of critical significance. It will require operator attention at the earliest opportunity and may require temporary suspension of the primary task. The occurrence of the alert should be readily perceivable while an operator performs a primary task. The alert should clearly signal that it is of high priority. Some examples of priority 1 anomalies are:\r• Grab and dash fishing - a foreign fishing boat moves from the international zone to Canadian waters (where it is forbidden from fishing) for a few hours just before leaving for its home port.\r• Not heading for port - a vessel is heading in a direction where there is no harbour, or is not heading toward its declared destination. Cargo and Ferry vessels always go from one port to another port, and generally by the shortest available route.\r• Changes destination - a cargo ship changes course in mid-journey or possibly even reverses it’s heading and returning to port.\r• Heading into danger - a ship is heading toward a natural obstacle such as ice or non- navigable water.\r• Regulatory infraction - a ship enters, without permission, a regulated zone such as the Northwest Passage, where ships must register their plans and receive permission to proceed.\r• Infringing a closed zone - a ship is in a zone of the ocean that is closed to its type of commercial activity, whether for environmental, wildlife protection, or national security reasons.\r5.1.1.2 Priority 2 alerts\rA priority 2 alert is an anomaly for which the related information is important but can be dealt with as soon as primary task activity permits. The occurrence of the alert should be readily perceivable while an operator performs a primary task. The alert should clearly signal that it is of intermediate priority. Some examples of anomalies are:\r•\r• •\rUnexplained high speed - a ship that is claiming (e.g., in call-ins or on AIS) to be a normal merchant ship suddenly starts travelling at a high speed more typical of a passenger ship or warship.\rSpeed too slow - a Cargo, Passenger, or Ferry is observed going slowly. As these vessels generally go as fast as they safely can, it may be an indicator of a problem.\rLoitering - a cargo ship stops outside of or far from a harbour, or steams very slowly, rather than proceeding directly into port.\rPage 66\rNon-Intrusive Alert System Humansystems®\r\n• •\r• •\r5.1.1.3\rOutside historical route - a ship that historically follows a consistent route, is deviating or slowing down for no apparent reason.\rOutside shipping lane - a ship that should be in a shipping lane is instead travelling outside the lane. Ships approaching port enter a “Vessel Traffic Management” zone and are required to stay within designated shipping routes.\rZone mismatch to activity - a ship’s location does not match its claimed activity, where that activity can only be carried out in specific regions of the sea, due either to regulations or to physical requirements of the activity itself.\rLittoral rendezvous - many small crafts converge on a larger ship, and then the small crafts spread out at high speeds to many different ports.\rPriority 3 alerts\rA priority 3 alert is an anomaly for which the related information is less important and will be attended to as time and resources permit. The occurrence of the alert should not be as readily perceivable as priority 1 and 2 alerts and should not draw attention to its occurrence. It should be perceivable only when the operator needs to check for alerts. The perceptual properties of the alert should clearly indicate that it is of lowest priority. Some examples of priority 3 anomalies are:\r• Track ends - a ship track ends in mid-ocean. A ship track will normally not end, except at a harbour or by the ship leaving Canadian waters.\r• Proximity to infrastructure - a ship approaches or loiters around Canadian infrastructure, such as oil production equipment, sub sea pipelines, communication cables, etc.\rAgain, we want to emphasize that the above does not constitute a definitive set of anomalies that may be of interest, nor is the specific priority classification being recommended for adoption. These assumptions were made simply to facilitate the development of design concepts.\r5.1.2 Visual design concepts\rMany of the concepts to be described have used colour and/or luminance coding as one basic approach to differentiating priority. Based on existing population stereotypes, the priority coding is as follows:\r• Priority 1: red sector of the spectrum\r• Priority 2: yellow-orange sector of the spectrum\r• Priority 3: unsaturated, neutral areas of the spectrum (e.g., grey)\rThe use of red is considered acceptable, even though red is used in GCCS to code VOI, hostile and suspect tracks, since there is unlikely to be any possible confusion because of where the red alert is located and the shape and context in which it appears.\rA second general principle is to locate the alerting stimulus in the periphery of the display towards the right. We did consider locating it on the menu bar at the top of the screen, but this area is already cluttered and we believed that the spatial separation from the menu bar would in fact encourage cognitive separation, so that typical menu intensive tasks would not be compromised by the adjacent proximity of alerts. Similarly, the alerts themselves would be more salient (enough to capture the operator’s attention without being intrusive) by being spatially separated.\rHumansystems® Non-Intrusive Alert System Page 67\r\nFive different designs have been created and will be discussed in details next.\r5.1.2.1 Design 1: Cumulative Total Indicator\rFigure 6: Design 1: Cumulative Total Indicator\rThe alert category is indicated by colour. For priority 1 alerts, the number in the red box increments and the box blinks at rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the Alert Information Window (AIW). When the operator has finished processing the alert, either one of two conditions exist. One, the alert has been dealt with and is no longer of interest, in which case the counter resets to n (number of current outstanding alerts) -1. Or two, the alert remains in the system, and the indicator no longer flashes and stays at the current value, in which case the next alert would increment this value and flash until attended to by the operator.\rFor priority 2 alerts, the number in the yellow box increments and the box blinks at an approximate rate of .25-.5 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. The remaining functionality is the same as a Priority 1 alert.\rOn a new priority 3 alert, the number in the grey box simply increments. When the operator has processed the alert, the number decreases to n-1.\r000\rPage 68 Non-Intrusive Alert System Humansystems®\r\n5.1.2.2 Design 2: Vertical Cumulative Indicator\rFigure 7: Design 2: Vertical Cumulative Indicator\rThis design concept is similar to design 1, except the count of outstanding alerts is indicated by a vertical progress bar. The above example show several outstanding alerts in all three priority categories. A vertical scale provides an indication of the number of alerts.\rA priority 1 alert is shown in this design by the number 1 blinking red at a rate of 2 Hz and the associated red vertical bar incrementing in height. The flashing of the number 1 to red continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. When the operator has finished processing the alert, one of two conditions will exist. The alert has been dealt with and is no longer of interest, in which case the vertical bar decrements by one unit, or the alert remains in the system, and the indicator bar stays at the current height; in either case the box background reverts to grey. The next alert would again increment the height of the bar and the box would flash red again until attended to.\rA priority 2 alert is shown in this design by the number 2 in the second box blinking yellow at an approximate rate of .25-.5 Hz. and the vertical bar incrementing in height. The flashing continues until the operator acknowledges the alert by clicking on it. The remaining functionality is the same as a Priority 1 alert.\rOn a new priority 3 alert, the vertical bar simply increments. When the operator has processed the alert, the number decreases to the current outstanding number minus 1, or remains the same if the alert is not deleted.\r10 5 0\rHumansystems® Non-Intrusive Alert System Page 69\r\n5.1.2.3 Design 3: Horizontal Indicator Bar\rFigure 8: Design 3: Horizontal Indicator Bar\rThe three alert categories are indicated along the bottom of the display, segregated by position and colour coding. It was assumed that on the east coast contacts on the left hand side of the screen are generally considered higher priority than those on the right side of the screen simply because they are closer to land. For this reason we suggest that priority 1 alerts are situated on the left hand side of the screen for east coast RJOC operators while, for operators on the west coast, high priority alerts are positioned on the right side. That is, the design will be coast dependent. This assumption should be validated with both east and west coast operators.\rA priority 1 alert is indicated in this design by the first empty rectangle in the left most group turning red and blinking at a rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. Again, when the operator has finished processing the alert, one of two conditions will exist. The alert has been dealt with and is of no longer interest, in which case the box is no longer filled with red, or the alert remains in the system, and the box no longer flashes and stays filled. In which case, a new alert would result in the next horizontal box blinking red until attended to by the operator.\rA priority 2 alert is indicated in this design by the first empty rectangle in the middle group turning light yellow and blinking at a rate of 2 Hz. The flashing continues until the operator acknowledges the alert by clicking on it. This takes the operator to the AIW. The remaining functionality is the same as a Priority 1 alert. As the number of outstanding alerts in this category increases (seven boxes are filled), the colour changes from light yellow to orange.\rOn a new priority 3 alert, a grey box in the right most group is filled. When the operator has processed the alert, the fill is removed from the box, or remains the same if the alert is not deleted.\rPage 70 Non-Intrusive Alert System Humansystems®\r\nAs the number of outstanding alerts in this category increases (seven boxes are filled), the colour changes from light grey to dark grey.\r5.1.2.4 Design 4: Ticker and Fading Bar\rFigure 9: Design 4: Ticker & Fading Bar\rThis design is similar to Design 2 in that the count of outstanding alerts is indicated by a vertical progress bar with a scale to provide an indication of the number of alerts in the system (i.e., unaddressed). In addition, individual incoming priority 1 and 2 alerts are indicated by a bar appearing at the bottom of the screen.\rOn a new priority 1 alert, a red bar appears across the bottom of the screen with a message scrolling from right to left indicating that there is a new priority 1 alert. A brief description of the type of anomaly is also provided (e.g., contact veering off-course). An unacknowledged priority 1 alert would also increment the height of the red bar in the counter on the bottom right of the screen, showing an increase in the number of active priority 1 alerts in the system. The red bar is present and the scrolling continues until the operator acknowledges the alert by clicking it. The number of active priority 1 alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\rOn a new priority 2 alert, an orange bar with text indicating that there is a priority 2 alert fades in and out of the bottom of the screen at a rate of 5 Hz. The bar and message remain on the screen for approximately 2 seconds before fading away and then returning again until the alert is acknowledged by the operator (by clicking on it). An unacknowledged priority 2 alert would also increment the height of the orange bar in the counter on the bottom right of the screen, showing an\rPriority 2 Alert\r10 5 0\rHumansystems® Non-Intrusive Alert System Page 71\r\nincrease in the number of active priority 1 alerts in the system. The number of active priority 2 alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\rOn a new priority 3 alert, the height of the grey bar in the counter at the bottom right of the screen would increment by one indicating an increase in the number of active priority 3 alerts. The operator would only notice this increment if he/she was looking directly at the counter at the moment it increments. Therefore, the operator would be required to intentionally seek out active priority 3 alerts rather than directly being made aware of new alerts. The number of active low priority alerts, as indicated by the counter, would remain unchanged until individual alerts are processed.\r5.1.2.5 Design 5: Polygon\rFigure 10: Design 5: Polygon\rThis design is based on principles of ecological interface design in that it is intended to represent both the desired state of the system (i.e., no active alerts in the system) as well as the current state of the system (i.e., the presence of active alerts) in a way that is easily and quickly perceived by the operator. The solid green triangle signifies the desired state (i.e., no active alerts) while the dotted triangle represents the actual state (i.e., if there are active alerts and if so, what type priority of alert). As alerts accumulate, the dotted triangle moves along the axes for which there are alerts. The X-axis (red in colour and labelled with a “1”) represents priority 1 alerts; the Y-axis (orange in colour and labelled with a “2”) represents priority 2 alerts; and the Z-axis (coloured grey and labelled with a “3”) signifies priority 3 alerts. If there are an equal number of active priority 1, 2 and 3 alerts, the dotted triangle is an isosceles triangle; if there are unequal numbers, the dotted triangle becomes skewed toward the priority level for which there is the most active alerts.\r2\r13\rPage 72 Non-Intrusive Alert System Humansystems®\r\nOn a new priority 1 alert, the border of the dotted triangle skews toward the priority 1 axis showing an increase in the number of high priority alerts. The dotted triangle also turns red and flashes at a rate of approximately 5 Hz. The triangle remains red and flashing until the operator acknowledges the alarm by clicking it. The number of active priority 1 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\rUpon an incoming priority 2 alert, the border of the dotted triangle skews toward the priority 2 axis showing an increase in the number of medium priority alerts. The dotted triangle also turns orange and flashes at a rate of approximately 0.5 Hz. The triangle remains orange and flashing until the operator acknowledges the alert by clicking it. The number of active priority 2 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\rOn a new priority 3 alert, the border of the dotted triangle skews toward the priority 3 axis showing an increase in the number of low priority alerts. The dotted triangle also turns grey but does not flash. The operator would only notice this increment if he/she was looking directly at the display at moment it changes shape and colour. Therefore, the operator would be required to intentionally seek out active priority 3 alerts rather than directly being made aware of new alerts. The number of active priority 3 alerts, as indicated by the size and shape of the dotted triangle, would remain unchanged until individual alerts are processed.\r5.1.3 Tactile design concept\rThis design concept assumes that the RJOC operator could carry a pager-type device that would transmit the alerts. The design is based on a brief review of literature on the use of tactons for mobile phone alerts to imply priority (Brown & Kaaresoja, 2006). Generally, priority can be implied by the number, duration and intensity of pulses. That is, higher priority alerts would be indicated by more, longer and more intense pulses.\rOn a new priority 1 alert, the pager would receive two pulses of approximately 30 milliseconds in duration. The intensity of the pulse would be approximately 1.38 V. The pulses would repeat until the operator acknowledges the alarm by clicking it. Interrogating and processing the alarm would have to be done on the operator’s computer workstation.\rOn a new priority 2 alert the pager would receive one pulse of approximately 10 milliseconds in duration. The intensity of the pulse would be approximately 0.98 V. The pulse would repeat until the operator acknowledges the alarm by clicking it. Interrogating and processing the alarm would have to be done on the operator’s computer workstation. The operator would not receive priority 3 alerts via the pager. He/she would therefore have to be at their workstation looking at the screen and intentionally seeking out priority 3 alerts.\r5.1.4 The alert information window\rThe purpose of the Alert Information Window (AIW) window is to provide the operator with specific information about the alert details and to manage alert lists. It is not a window for problem solving or analysis which we assume will take place using existing functionality in the RJOCs.\rFor the AIW, only visual designs were considered. An initial design for an AIW window is shown in Figure 11.\rHumansystems® Non-Intrusive Alert System Page 73\r\nPriority 1\rRMP RMP RMP\r3\r+ Priority2 8 + Priority3 9\r#Alerts Track#\rName\r4689 CP SPIRIT\r3193 EAGLE BOSTON\r4745 OVERSEAS SILVAMAR\rAlert\rHeading into danger\rChanges destination Grab and dash fishing\rClear\rTime 09:48\r10:17 14:21\rDone\rFigure 11: Alert Information Window\rThe window comprises separate areas for each alert priority with the priority level colour coded. Within each category, there is a field for the number of alerts in the system. The information provided includes track number, name of the contact, the specific nature of the anomaly, and time of the report that gave rise to the anomaly.\rThe above example shows what would happen if the operator had selected a priority 1 alert in the RMP window. The priority 1 section is expanded to show all alerts and the most recent alert, that caused the alert indicator to flash, is highlighted. It is important to note that the operator could select another alert in the priority 1 list or another alert category (in which case that section would be automatically expanded to show all of the tracks within that category). At this point the operator may choose to refer back to the RMP to see the location of the alert and the context by clicking the RMP button on the track line. When this happens, the GCCS RMP window is brought back showing the contact in question highlighted, as shown in Figure 12 (see concentric broken circle around the contact of interest in the upper part of the RMP).8\r8 In the PowerPoint presentation for the SMEs, this concentric circle shrinks and expands dynamically around the track symbol to better enable the operator to locate the track in question.\rPage 74 Non-Intrusive Alert System Humansystems®\r\n10 5 0\rFigure 12: RMP track highlight\rWhen the operator has finished analysis on the track of interest, she/he can return back to the AIW (by clicking on the AI indicator) and decide to either leave the track in the system or to clear the alert by way of the CLEAR button. This would then result in the track being deleted from the alert list, as shown in Figure 13.\rFigure 13: Alert Information Window after a track is deleted\rThe operator may choose to process other alerts in the system, or return to the RMP via the button DONE at the bottom of the window.\rHumansystems® Non-Intrusive Alert System Page 75\r\n5.1.5 The RMP pop up box\rThe purpose of this box is to provide the operator with a shortened version of the information about the alert. The AIW provides full information regarding each alert and allows operators to manage all the alerts in the system. The RMP pop up box on the other hand, allows operators a quick and easy method to clear an alert or leave it in the system. The pop up box appears when the operator acknowledges (i.e. clicks on) the indicator for an incoming alert. This was designed as a second method to present operators with relevant information. The research team did not come across any literature regarding this concept. As shown in Figure 14, a pop up box was designed to included the name of the vessel (e.g., Eagle Boston), reason for the alert (e.g., Not heading to port), and time of alert (e.g., 21:15), as well as action buttons (i.e., Clear or Leave).\rs\rFigure 14: RMP pop up box\rThe box’s border would be outlined in the same colour as the priority of the alert, in this case, red for priority 1. If the operator chose to CLEAR the alert, the system would delete the alert and the counter would return back to the previous number, in this case, back to 1. If the operator chose to leave the alert in the system, the counter would remain the same, in this case, stay at 2.\rEAGLE BOSTON: NOT HDNG TO PORT:21:15\rCLEAR\rLEAVE\r10\r5\r0\rPriority 1 Alert – Ves\rPage 76 Non-Intrusive Alert System Humansystems®\r\n6. Evaluation and Review of Design Concepts by Subject Matter Experts\r6.1 Preparation for Design Evaluation and Review\rFollowing the development of the design concepts, a consultative process was begun with the Scientific Authority to determine which concepts should be taken forward for review by Subject Matter Experts (SMEs). As a result, it was decided that the polygon design (because it was thought to non-intuitive) nor the tactile design (because it was considered impractical) would be explored further.\rIn preparation for the evaluation, the individual design elements (alert indicator, alert information window, alert track highlighter and RMP pop up box) were incorporated into a PowerPoint demonstration concept that would simulate the functionality of an alerting system. Each demonstration comprised a new alert initiation (all three priority levels considered sequentially), acknowledgment of the alert, getting information on the alert and clearing the alert.\rTo evaluate the designs, questionnaires were constructed to address issues such as the usability and utility of the design, ease of comprehension and overall preferences.\rAn ethical protocol was submitted to and approved by the Human Research Ethics Committee at DRDC Toronto. The approved ethics protocol is included in Annex A.\r6.2 Method\rThe following section outlines the methodology used in reviewing and evaluating the anomaly alert system design concepts with SMEs.\r6.2.1 Date and Location of SME Evaluation\rSME evaluations were conducted in the MAPLE lab at DRDC Atlantic from February 23-26, 2009.\r6.2.2 Participants\rSeven Subject Matter Experts (SMEs) were recruited to participate in an assessment of the non- intrusive alerting designs. There were 2 Lt(N), 2 retired CF personnel, 2 operators, and a civilian. Combined related experience included a Common Operational Picture Officer, Watch Officer, Surveillance Officers, Bridge Watch Keeper, and Surveillance Database Operators. There were 6 men and 1 woman.\r6.2.3 Materials\rTrial participants were presented with PowerPoint representations of the various design concepts, which demonstrated with animation how the basic functionality would work. The concepts were presented in the following order:\r1. Cumulative total indicator with pop-up window (Figure 6)\rHumansystems® Non-Intrusive Alert System Page 77\r\n2. Cumulative total indicator with AIW (Figure 6)\r3. Vertical cumulative indicator with pop-up window (Figure 7)\r4. Vertical cumulative indicator with AIW (Figure 7)\r5. Horizontal indicator bar with pop-up window (Figure 8)\r6. Horizontal indicator bar with AIW (Figure 8)\r7. Ticker and fading bar with pop-up window (Figure 14)\r8. Ticker and fading bar with AIW (Figure 9)\rAll participants reviewed the design concepts in the same order which means that a potential order effect could not be determined. However, the presence or absence of an order effect was believed to be inconsequential for this exploratory research.\rPart of the evaluation of the designs was accomplished by a five part questionnaire. The first section included demographic questions such as name, rank, number of years experience, etc. The second part of the questionnaire included 15 questions related to the usability and usefulness of a non-intrusive alerting system in general. SMEs were asked to select the rating (on a 5-point scale) they felt most appropriate. Example questions included “These alerts would enhance our knowledge of anomalies” and “This alerting system would be difficult to use.”\rThe third section of the questionnaire assessed participant’s attitudes towards each non-intrusive alert design. Again, SMEs were asked to select the rating (on a 7-point scale) they felt most appropriate. Example questions included “The number of alerts was easy to comprehend” and “The priorities of the alerts were easy to comprehend.”\rThe fourth part of the questionnaire assessed the level of preference for each alert design across three dimensions; overall effectiveness in bringing alerts to the operator’s attention, the method in which different alert priorities are presented, and the degree to which all of the required information about an alert is presented. For each dimension, participants were asked to rank each of the four designs; with ‘1’ indicating the most preferred and ‘4’ the least preferred.\rThe final section of the questionnaire included open-ended questions related to each alert design. For each alert design, participants were asked if they thought the design should be implemented (Yes=1, No=2). They were then asked to list their likes and dislikes for each design. For the final question in this section, participants were asked to rate the intrusiveness of the design based on a 7- point scale (where 1=Not at all Intrusive, 7=Extremely Intrusive). Details of the questionnaires and interview questions can be found Annexes B and C.\r6.2.4 Procedure\rEach SME participated individually in a walkthrough of each of the designs using the PowerPoint presentation on a computer screen, and then completed the usability questionnaire and answered a number of follow up questions in an interview. Sessions lasted on average one hour.\rTwo members of the research team lead the walkthroughs, one leading the process and the other taking notes and audio recording participant responses. Participants were first asked to read a pre- experiment information sheet and then read and sign a voluntary consent form (see Annex A). A member of the research team then described the goals of the project as well as goals of an alert system in general. She then presented a definition of non-intrusive alerts, an overview of the alert\rPage 78 Non-Intrusive Alert System Humansystems®\r\ndesigns, and a basic description of the method to be used for the walkthrough. After the introduction, participants were asked if they had any questions before they were shown each alert design in detail. A member of the research team then walked participants through the four alert designs, each with the RMP pop-up box and then with the AIW. At different points during the walkthrough, participants were questioned relating to the topics such as priority levels, setting alert trip points and parameters for alerts. Participants were allowed to ask questions and make comments throughout the walkthroughs.\rAfter the walkthroughs, participants were asked to complete the five part questionnaire. Participants were then asked some general follow up questions that were not previously addressed during the walkthroughs. If desired, participants were able to revisit the different design concepts during the questionnaire and general discussion phases.\rThe interviewers used the following points to guide the discussion, asking questions when necessary (depending on what had already been discussed during the presentation of the design concepts):\r• Operator’s attendance at their desk\r• Frequency of alerts (by priority)\r• Priority 1 alerts being intrusive\r• Ignoring alerts\r• Priority indications\r• Alert colour scheme (e.g., red, orange, yellow, grey)\r• Font (e.g., size, colour)\r• Terminology\r• Alert Information Window\r• Intuitive versus not intuitive\r• Intrusiveness of alerts\r• Auditory/tactile alerts\r• Flexibility in the location of ticker and horizontal indicator bar\r• RMP centred on contact\r• Information in the RMP pop up box\r• Ability to return to primary task\r• Shift change over problems\rA summary of individual participant responses are provided in detail in Section 6.2.3.\r6.3 Results\r6.3.1 Questionnaire data\rThe questionnaire data are divided into two sections: quantitative and qualitative. With the exception of the intrusiveness ratings, the majority of questionnaire responses were ratings on a 5- point scale. A 7-point scale was used for intrusiveness ratings as the perception of intrusiveness was of primary concern for this project and so it was thought that detecting finer distinctions between people would be desirable. Results suggest, however, that a 5-point scale would likely have been suitable. In addition to rating scales, participants were also asked to rank the designs, and provide their likes and dislikes of each design in an open ended format.\rHumansystems® Non-Intrusive Alert System Page 79\r\n6.3.2 Quantitative data\r6.3.2.1 General Non-Intrusive Design Questions\rThe Usability and Usefulness Questionnaire assessed participants attitudes on the usefulness of a general non-intrusive alerting system (see Table 16).\rTable 16: Mean ratings for Usability and Usefulness of Non-Intrusive Alerting System\rStatement\rAnswer9\rMean\rSt. Dev.\rRange (scale = 1-5)\rThese alerts would enhance our knowledge of anomalies\rStrongly agree\r4.6\r.53\r4-5\rThis alerting system would be used on a daily basis\rStrongly agree\r4.9\r.38\r4-5\rTasks can be performed in a straightforward manner using this alerting system\rStrongly agree\r4.6\r.79\r3-5\rThe thinking required to use this alerting system requires significant effort\rDisagree\r1.9\r.38\r1-2\rThis alerting system would be difficult to use\rStrongly Disagree/Disagree\r2.0\r1.41\r1-5\rThis alerting system will improve my situation awareness\rAgree\r4.4\r.53\r4-5\rThis alerting system would make it easier to identify anomalies\rStrongly agree\r4.7\r.49\r4-5\rI would find this alert system useful\rDisagree\r1.7\r.49\r1-2\rI would not ignore alerts while using this technology\rAgree\r4.0\r1.00\r2-5\rThe Alert Information Window (AIW) was confusing\rDisagree\r2.1\r.38\r2-3\rIt was easy to learn how the AIW was represented\rStrongly agree\r4.7\r.49\r4-5\rThe AIW had all the necessary information\rDisagree\r2.9\r1.07\r2-4\rIt was easy navigating between the RMP and the AIW\rAgree\r4.1\r.38\r4-5\rI prefer clearing and deferring alerts directly from the RMP\rUndecided\r3.1\r1.07\r2-5\rI prefer using the AIW to clear or defer alerts\rUndecided\r3.1\r1.07\r2-5\rAs shown in Table 16, participants showed generally favourable attitudes towards a non-intrusive alerting system. Specifically, participants strongly agreed that the non-intrusive alerting system would enhance their knowledge of maritime anomalies (Mean = 4.6), be used on a daily basis (Mean = 4.9), perform tasks in a straightforward manner (Mean = 4.6), and make it easier to\r9 Scale descriptor is based on the most frequent rating (mode).\rPage 80 Non-Intrusive Alert System Humansystems®\r\nidentify anomalies (Mean = 4.7). Despite these positive answers, participants reported that they would not find the alert system useful (Mean = 1.7). This rating is somewhat surprising and participant comments shed no light on the reasons that may have led to this rating. Therefore, the way in which operators consider the potential usefulness of an alerting system for maritime anomalies clearly needs to be addressed in future work. Participants were also undecided about whether the Alert Information Window (AIW) had all the necessary information and whether they preferred clearing alerts directly from the RMP or the AIW. Follow up interview questions covered these issues and will be discussed in Section 6.2.4.\r6.3.2.2 Specific Design Questions\rThe rest of the questions assessed the participant’s attitudes regarding each specific alerting design. The following table shows the means and standard deviations for each question.\rTable 17: Mean Ratings for the Alert Design Questions\rStatement & Design\rAnswer10\rMean\rSt. Dev.\rRange (scale = 1-5)\rThe number of alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\rStrongly agree\r4.7\r.49\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.69\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.9\r1.07\r2-5\rDesign 4: Ticker & Fading Bar\rAgree\r4.1\r.06\r3-5\rThe presence of an alert was easy to recognize\rDesign 1: Cumulative Total Indicator\rAgree\r4.3\r.49\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.90\r2-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.7\r1.25\r2-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree\r4.4\r1.13\r2-5\rThe priorities of the alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\rStrongly agree\r4.6\r.53\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.4\r.53\r4-5\rDesign 3: Horizontal Indicator Bar\rAgree\r3.9\r1.07\r2-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree\r4.4\r.79\r3-5\rIt was easy to find the relevant anomaly\rDesign 1: Cumulative Total Indicator\rAgree\r4.1\r.69\r3-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.0\r.58\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r4.1\r.69\r3-5\rDesign 4: Ticker & Fading Bar\rStrongly Agree/Agree\r4.3\r.76\r3-5\rIt was easy to find information on anomalies\rDesign 1: Cumulative Total Indicator\rAgree\r4.3\r.49\r4-5\r10 Scale descriptor is based on the most frequent rating (mode).\rHumansystems® Non-Intrusive Alert System Page 81\r\nStatement & Design\rAnswer10\rMean\rSt. Dev.\rRange (scale = 1-5)\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.9\r.38\r3-4\rDesign 3: Horizontal Indicator Bar\rAgree\r4.0\r.58\r3-5\rDesign 4: Ticker & Fading Bar\rAgree\r4.1\r.69\r3-5\rThe alerting design enhanced my situation awareness of maritime anomalies\rDesign 1: Cumulative Total Indicator\rAgree\r4.1\r.38\r4-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r4.0\r.58\r3-5\rDesign 3: Horizontal Indicator Bar\rAgree\r4.0\r.58\r3-5\rDesign 4: Ticker & Fading Bar\rStrongly agree\r4.6\r.53\r4-5\rThe appearance of the alerts is compatible with my current interface\rDesign 1: Cumulative Total Indicator\rStrongly Agree\r4.0\r1.15\r2-5\rDesign 2: Vertical Cumulative Indicator\rAgree\r3.6\r.98\r2-5\rDesign 3: Horizontal Indicator Bar\rUndecided/Agree\r3.3\r.76\r2-4\rDesign 4: Ticker & Fading Bar\rAgree\r4.0\r1.00\r2-5\rA parametric one-way Analysis of Variance (ANOVAs) and non-parametric tests were conducted and revealed no significant differences in ratings between the four alert designs for any of the above questions. As a result, this section presents only observations on the trends in the data, which need to be validated through further experimentation.\rParticipants rated the Cumulative Total Indicator as the easiest to comprehend, while the Vertical Cumulative Indicator and the Horizontal Indicator Bar were equally more difficult to comprehend. The presence of new alerts was easiest to recognize in the Ticker and Fading Bar, followed by the Cumulative Total Indicator, the Vertical Cumulative Indicator, and then the Horizontal Indicator Bar. Alert priorities in the Cumulative Total Indicator were easiest to comprehend, and most difficult in the Horizontal Indicator Bar. The results also suggest that participants found that the Ticker and Fading Bar was the easiest design in which to find the anomaly, while the Vertical Cumulative Indicator was the most difficult. The Cumulative Total Indicator was rated the easiest to find the relevant information pertaining to the anomaly, while the Vertical Cumulative Indicator was rated the hardest to find the information. The Ticker and Fading Bar enhanced the operator’s situation awareness of maritime anomalies (i.e., incoming alerts and total number of active alerts in the system), while the Vertical Cumulative Indicator and the Horizontal Indicator Bar were tied for least likely to enhance situation awareness. Lastly, the Cumulative Total Indicator and the Ticker and Fading Bar were rated the most compatible with the current interface, while the Horizontal Indicator Bar was rated the least compatible.\rParticipants were also asked to rank the designs in order of preference (i.e., 1, 2, 3 and 4) from the perspective of alert priorities (Figure 15), the degree to which the design grabbed the attention of the operator (Figure 16), and providing relevant information without distraction (Figure 17). All three figures show a frequency count of the specific rank for each of the four alert design concepts.\rPage 82 Non-Intrusive Alert System Humansystems®\r\n6 5 4 3 2 1 0\r1234\rEffectiveness Ranking\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 15: Subject rankings for effectiveness in bringing alerts to my attention\r6 5 4 3 2 1 0\r1234\rPriority Rankings\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 16: Subject rankings for methods for representing the priorities\rHumansystems® Non-Intrusive Alert System Page 83\rFrequency Frequency\r\n4.5 4 3.5 3 2.5 2 1.5 1 0.5 0\r1234\rInformation Rankings\rCumulative Total Indicator Vertical Cumulative Indicator Horizontal Indicator Bar Ticker & Fading Bar\rFigure 17: Subject rankings for providing required information without distraction\rKruskal-Wallis ANOVA and Chi-Square analyses revealed no significant differences between the four alert designs in terms of effectiveness in bringing alerts to the operator’s attention, methods of representing priorities and providing required information without distraction. Though not statistically significant, a trend analysis suggests that the Ticker and Fading Bar was the preferred design for all of these three features while the Vertical Cumulative Indicator was the least preferred. Further empirical research is needed to verify this trend.\r6.3.2.3 Implementation and intrusiveness\rParticipants were asked if each design should be implemented and they were also asked to rate the intrusiveness, as shown in Table 18.\rPage 84 Non-Intrusive Alert System Humansystems®\rFrequency\r\nTable 18: Mean ratings for design implementation and intrusiveness\rStatement & Design\rAnswer11\rMeans\rSt. Dev.\rRange\rShould the design be implemented\rDesign 1: Cumulative Total Indicator\rYes\r1.3\r.49\r1-2\rDesign 2: Vertical Cumulative Indicator\rNo\r1.9\r.38\r1-2\rDesign 3: Horizontal Indicator Bar\rNo\r1.6\r.53\r1-2\rDesign 4: Ticker & Fading Bar\rYes\r1.0\r.00\r1\rIntrusiveness12 (scale = 1-7)\rDesign 1: Cumulative Total Indicator\rNot at all intrusive\r2.1\r.90\r1-4\rDesign 2: Vertical Cumulative Indicator\rSomewhat intrusive\r2.9\r1.07\r1-4\rDesign 3: Horizontal Indicator Bar\rSomewhat intrusive\r3.7\r1.60\r1-5\rDesign 4: Ticker & Fading Bar\rSomewhat intrusive\r3.1\r1.46\r1-5\rA one-way parametric ANOVA and non-parametric tests revealed no significant differences between the four alert designs in terms of which design should be implemented or the intrusiveness of each design. Though not statistically significant, all participants felt that the Ticker and Fading Bar (Mean = 1.0) should be implemented and all but two participants felt that the Cumulative Total Indicator (Mean = 1.3) should be implemented. On the other hand, only one participant indicated that the Vertical Cumulative Indicator (Mean = 1.9) should be implemented and three felt that the Horizontal Indicator Bar (Mean = 1.6) should be implemented13.\rAlthough not statistically significant, participants rated all four designs as not at all intrusive or somewhat intrusive. The horizontal indicator bar was rated the most intrusive and the cumulative total indicator the least intrusive. Interestingly, as noted above, all participants felt that the Ticker and Fading Bar should be implemented even though it did not receive the least intrusive rating.\rIn summary, the Cumulative Total Indicator and the Ticker and Fading Bar were rated more favourably than the Vertical Cumulative Indicator and the Horizontal Indicator Bar. Specifically, in comparing the rank ordering of the designs, the Ticker and Fading Bar was preferred over all other designs, while the Vertical Cumulative Indicator was the least preferred. What is interesting to note is the Ticker and Fading Bar includes the Vertical Cumulative Indicator scale with the only difference being priority colour in the boxes. Based on discussions, participants preferred the actual ticker/fading bar that appeared at the bottom of the screen, rather than the scale, which will be discussed later.\r6.3.3 Qualitative data\rThe following table shows the participants’ qualitative assessments of each alert design.\r11 Scale descriptor is based on the most frequent rating (mode).\r12 The intrusiveness was on a 7-point scale (1 = Not at all intrusive, 4 = Somewhat intrusive, 7 = Extremely intrusive)\r13 Yes was considered a 1, while No was considered a 2.\rHumansystems® Non-Intrusive Alert System Page 85\r\nTable 19: Likes and Dislikes of Alert Designs\rDesign\rLikes\rDislikes\rComments\r1: Cumulative Total Indicator\rSimple and clear; uses less screen; numeric total subtle but effective\rNo hover feature for listing alerts without leaving RMP\rThis feature did not exist for any of the design concepts, but could be readily implemented.\rUnobtrusive; not overbearing but available as a quick visual reference\rThis is not good for operators that sit for hours in front of this system\rComments made during the discussion suggest that this design may be more easily ignored by operators especially over long periods\rExact priority of each alert; unobtrusive display\rNot as immediately visible as the ticker/bar\rCompact, clear number\rSmall numbers could create a change in priority for an operator, contrary to command's need\rNumber boxes could easily be made larger\rDead simple\rNo immediate visual of number of each type of alert without focusing on small numbers. Also, location should be able to be moved at will\rThe size and location of alert could easily be changed in future versions. Also numbers could easily be made larger.\rDoes not fill the RMP with unnecessary info\r2: Vertical Cumulative Indicator\rVisual and numerical representation of alert number and type; small display\rNumber scale could be confused with alert totals or other data\rUnsure what is meant by this comment, especially “other data”\rNot too intrusive\rLocation should be able to be moved at will\rFairly simple\rDifficult to determine number of alerts in bar\rNot annoying\rScale would have to change as alerts increase\rCan become complacent\rDoes not give accurate info on alerts\rThis is not good for operators that sit for hours in front of this system\rComments made during the discussion suggest that this design may be more easily ignored by operators especially over long periods\r3: Horizontal Indicator Bar\rEasy to interpret; unsure if the colour saturation is prominent enough when alerts increase\rTakes up too much screen; scale changes with number s alerts; don’t like gradual colour change\rBut occupies about the same amount of screen as the ticker design.\rPage 86 Non-Intrusive Alert System Humansystems®\r\nDesign\rLikes\rDislikes\rComments\rCatches your eye allows for quick scan without refocusing attention from priority tasking\rNo number available for alerts; colour saturation can be hard to detect; takes up a lot of bottom screen space\rClear; it would be at the forefront of the operator’s mind\rTakes more space\rToo wide; won't notice overflow of alerts easily\rTakes up too much space on RMP screen\r4: Ticker & Fading Bar\rThis would also keep the operator on the ball\rStill has the vertical scale that could be confusing\rThis could easily be removed\rTicker for high priority alerts and ability for operator to see information on cause\rRecommended fading grey bar in and out (like priority 2 ticker) when priority 3 alert arrives\rBut this would produce some potential confusion between different alert priorities. In addition, priority 3 alerts are not defined as events that should immediately capture attention.\rDecreasing intrusiveness; wide area only used for incoming alerts\rDon't like ticker; fading bar should be option; some operators will like and some won't\rAllows immediate connection with track in RMP; less steps to view\rFade bar quite distracting; no real info contained\rHowever, this is no different from having a flashing indicator, as in the cumulative counter design.\rWith information in the bar, the operator does not have to change what they are doing\rThis is not really the case, as the operator will have to shift attention momentarily to process the information in the bar.\rMost prominent; easy to distinguish levels\rProminence may become a negative factor in a high frequency alert context.\rAs shown in Table 19, some of the dislikes were implementation issues and the designs could be easily changed or modified to accommodate the suggestions. There was a comment for the Vertical Cumulative Indicator that needs further explanation. The participant who said, “This is not good for operators that sit for hours in front of this system” and noted “.... the design is not stimulating enough for the mind”. The participant believed that the operator needs to be “stimulated” and their mind “needs to be active”, and believed that with Design 2, this would not happen.\rOverall, the Cumulative Total Indicator was evaluated as being non-intrusive and very simple to use and interpret. However, some participants felt that it was too small and could therefore be easily ignored by operators. It appears that there needs to be a compromise relating to the size of the alert such that it is large and salient enough to be noticed yet not too salient as to distract the operator from his/her primary task or become an annoyance.\rHumansystems® Non-Intrusive Alert System Page 87\r\nThe Vertical Cumulative Indicator was also rated as non-intrusive yet participants found the scale more confusing to interpret than the digital indicator in the Cumulative Total Indicator.\rOnly three participants liked some aspects of the Horizontal Indicator Bar, while most participants felt the display obscured too much of the screen. Although the Ticker and Fading Bar used up as much, if not more space than the Horizontal Indicator Bar, participants did not mention this issue for the former. It is clear that such inconsistencies may have biased or influenced the results. Hence, further research is required to explain these inconsistencies.\rThere were two components to the Ticker and Fading Bar design that did not exist with the other designs. First, the ticker and fading bar itself alerted operators to individual incoming alerts. Second, a counter similar to that of the Vertical Cumulative Indicator was used to depict the total number of active alerts in the system (i.e., alerts that hadn’t been addressed). It is not surprising then that participants generally did not like the scale (which is consistent with the comments on the Vertical Cumulative Indicator). A number of participants recommended that the ticker or fading bar be used for both priority 1 and 2 alerts (i.e., there was no need to have the ticker for only priority 1 alerts and the fading bar for priority 2 alerts). Most participants preferred the Ticker and Fading Bar because of the intrusiveness (i.e., it was easily noticed) and the information contained in the display bar. This result may be different, of course, if there are frequent alerts. Further research in an environment with a representative number of alerts should be conducted to validate these findings.\rAfter participants completed the questionnaires, they were asked some additional follow-up questions. These questions related generally to alerting parameters, information presentation in the RMP and potential challenges of an alerting system in general and of our visual designs.\r6.3.4 General discussion with participants\rThe following points were discussed with participants following the design walkthrough and administration of the questionnaire.\r6.3.4.1 Alert parameters\rFor the purpose of this study, we assumed that the alert parameters operators would want to set included area (co-ordinates), vessel type (e.g., fishing, warship, merchant, etc), speed, and alert priority (1, 2 or 3). Participants stated that they would also like to set alert parameters according to the activity of the vessel, vessel name, threat level (e.g., friendly, neutral, suspect), age of alert, type of anomaly (e.g., not heading to port), flag (i.e., country), course direction, estimated time of arrival, AIS number14, and next port of call. They emphasized that changing alert parameters should be done quickly and easily, and they should have the ability to change these parameters geographically depending on their area of interest at a given time. The example given was that if you were monitoring Europe’s coast, operators would not want alerts relating to all vessels in that area. Thus, the operators require the ability to quickly and easily set these alerts according to their current area of interest.\r14 AIS number is used to identify ship transponder to radio receiving station while MMSI number does the same for satellite receiving stations\rPage 88 Non-Intrusive Alert System Humansystems®\r\n6.3.4.2 Intrusiveness and priority\rParticipants agreed that the level of intrusiveness of an alert must vary according to the priority of the alert. For the purpose of this study, we assumed three priority levels would be appropriate and for the most part, participants agreed. A few participants suggested having 4 or 2 alert priorities instead of 3. Furthermore, several participants added the caveat that priority 3 alerts will be ignored 90% of the time.\r6.3.4.3 Inability to see visual alerts\rWhen asked how often operators could be away from their desks and the RMP, in turn, unable to be notified of alerts, participants reported that there are manning issues on a regular basis and therefore operators are needed elsewhere. Despite this challenge, participants did not like the idea of an auditory or tactile alert (e.g., vibrating pager) because, although the operator would be notified of the alert, he or she would not be able to get back to their workstation to deal with the alert. Furthermore, most participants felt that operators would elect not to use such a pager-type device. Rather, they felt that the visual alert should continue to be displayed on the RMP until the operator acknowledges it (as is the case with the design concepts presented)\r6.3.4.4 Workload\rIn terms of workload, participants admitted that this type of alert design would increase the operator’s workload; however, all believed the anomaly alerting system would be worthwhile because currently most such anomalies are missed.\r6.3.4.5 Colour coding\rAll the participants liked the colour scheme that was used to depict priority levels in the alert designs (i.e., red for priority 1, gold for priority 2 and grey for priority 3). In the current RMP, red, yellow and green are used, but participants felt that green would be inappropriate for priority 3 alerts. As one participant stated, “Green doesn’t communicate the right message. Green means go.”\r6.3.4.6 Readability\rParticipants felt that the size of the displays and fonts were generally acceptable, with the exception of two participants who felt that the numbers in the Cumulative Indicator were too small. Furthermore, one participant suggested that the font in the pop-up windows should be slightly larger.\r6.3.4.7 Location on the screen\rAll participants also liked the idea of having the ability to click and drag the anomaly alert display anywhere on the screen. The alert designs presented to the participants in the trial had the alert display either in the bottom right hand corner or covering the entire width of the bottom of the screen. Participants thought this may impair their situation awareness if they had to focus on the bottom of the RMP for any length of time. Similarly, the current RMP has the ability to centre around a vessel of interest or area of interest. Participants indicated that they would like this to be an option for the non-intrusive design as well. That is, they would like the ability to centre the picture on a contact for which there is anomalous information. This would allow an operator to understand the immediate area and then the surroundings around the contact of interest. One participant mentioned that the option to centre the RMP on the contact of interest could be\rHumansystems® Non-Intrusive Alert System Page 89\r\nimplemented as a button in the AIW. For example, clicking on the name of the ship could centre the RMP on the vessel of interest.\r6.3.4.8 Retrieving information on alert\rAs previously described, participants were shown two options to retrieve information about an alert. In one case, the RMP had a pop up box coming from the vessel of interest which included the name of the vessel, reason for alert (e.g., grab and dash fishing), the time of the alert, and action buttons (i.e., Clear or Leave). While most of the participants thought the RMP pop up box included enough information, one participant recommended that it include course, speed, latitude, longitude, Maritime Mobile Service Identity (MMSI) number (or the AIS #). The second way to get information about an alert was to go to the AIW which was on a separate screen. As designed, the AIW also functions as a tool to manage all alerts as it shows all the alerts in the system according to priority level, total number of alerts by priority, track number, name of vessel, reason for alert, time of alert, and action buttons (i.e., Clear, Done and RMP, which is a button that takes you back to the RMP). Participants recommended that the AIW also include who reported the alert (i.e., source), vessel type, flag, age of alert (instead of time of alert), time of last report (as well as the ability to go to that report), age of track, course, speed, and the history of the alert (e.g., if the vessel has had other alerts before such as speeding up when it should not). Participants mentioned that track number is not useful and should be exchanged for AIS number. It was also recommended that a smaller version of the AIW pop up in the RMP (similar to the pop-up box) rather than taking the operator to a separate screen as navigating away from the RMP can adversely impact their situation awareness.\rAdditionally, the alert designs included a flashing circle around the contact of interest, but only in the designs using the AIW (the pop-up window points to the contact of interest so there is no need for a circle). The circle was red for priority 1 alerts, gold for priority 2 alerts and grey for priority 3 alerts. Two options for this design were shown to participants. First, the circle appeared around the contact of interest as soon as the operator clicked on an incoming alert (i.e., before going to the AIW). The other option was to navigate to the AIW first then click on the RMP button to come back to the RMP on which the flashing circle would appear around the contact. All participants preferred when this circle appeared as soon as the alert was clicked (i.e., before going to the AIW). However, participants indicated that they generally preferred the pop-up window over the AIW as the source for anomaly information which means that the flashing circle would not be required.\rIn summary, for the seven participants there were eleven overall favourite designs, due to participants preferring two of the designs equally. Five of the seven participants preferred the Ticker and Fading Bar, four participants preferred the Cumulative Total Indicator, and one participant preferred the Horizontal Indicator Bar. There was only one participant who favoured the Vertical Cumulative Indicator; however, with the caveat that it had to be in conjunction with the Ticker and Fading Bar.\rThus, overall the Ticker and Fading Bar was favoured because it was the most prominent and participants liked the information provided in the ticker. The participants who favoured the Cumulative Total Indicator did so because it was the least intrusive and they could see the exact number of alerts in the system. The Vertical Cumulative Indicator was disliked because of the scale and the fact that it was difficult to see the exact number of alerts in the system. Further, they were concerned about the amount of space it would take up on the screen if there were a lot of alerts and the scale needed to be increased (a 0-10 scale was used for the design). The Horizontal Indicator Bar was disliked because it was difficult to determine the actual number of alerts and the colour\rPage 90 Non-Intrusive Alert System Humansystems®\r\nsaturation would be difficult to learn. However, one participant believed that this design requires some cognitive effort to interpret which could be positive in that it would “keep the operator’s mind busy” (i.e., maintaining or improving attention and vigilance). On the other hand, this participant felt that the Cumulative Total Indicator and the Vertical Cumulative Indicator could be easily ignored as they do not require as much mental effort to interpret. These comments suggest that the appropriate level of intrusiveness for a maritime alert system for RJOC operators is not established and must be further researched and determined experimentally, particularly under operational contexts of medium to high alert frequency.\r6.4 Conclusions and recommendations based on evaluation\rThis section presents conclusions and a number of recommendations for future anomaly alert system designs based on the SME review and evaluation.\r6.4.1 Conclusions\rIn terms of presentation of incoming alerts, participants favoured both the Ticker and Fading Bar, which was rated as fairly intrusive and the Cumulative Total Indicator, which was rated as relatively non-intrusive. Further research is therefore required to determine the appropriate level of intrusiveness especially once the potential number alerts that may be present in the system is better understood.\rIn terms of presentation of the number of active alerts in a system, participants favoured a numerical display rather than a scale as it was easier to interpret at a quick glance.\rWith regards to getting information about an incoming alert, participants preferred the pop up window in the RMP compared to the AIW. This was primarily because the way in which the AIW was implemented in the prototype; it required the user to navigate to a separate window. Participants felt that this adversely impacted their situation awareness as a result. This suggests that managing alerts should be implemented in such a way that operators do not have to leave the RMP. For example, a separate window or sidebar could pop up in the RMP thereby allowing the operators to still have the RMP in view while managing alerts.\rFinally, participants felt that the AIW would be appropriate for managing alerts, although they suggested a number of additional pieces of information that it should include.\rAlthough the SME feedback from the design review was valuable, the results must be interpreted with caution given the small sample size and inconsistency in the comments. Further research is required to understand these inconsistencies that may have biased or influenced the results.\rAlthough beyond the scope of the current project, it should be noted that an anomaly alerting system will undoubtedly require revision of the current Standard Operating Procedures (SOPs). For example, information relating to anomalies will have to be passed on to incoming watch keepers during watch handover. That is, operators will at least be required to pass on the current alert parameter preferences (i.e., the alert trip points) as well as a summary of the number and types of alerts that have emerged over the course of the shift. Also, the operator may be required to brief the Watch Officer on any Priority 1 alerts immediately and perhaps keep a log of all priority 2 and 3 alerts. Furthermore, which alerts have been briefed to the Watch Officer and the time of the briefing may be an additional piece of information that should be included in the AIW.\rHumansystems® Non-Intrusive Alert System Page 91\r\n6.4.2 Limitations\rThere are three primary issues concerning the validity of the data obtained from the study.\r(i) Reliability: the small sample size means that the data obtained should be treated with caution and should be used to indicate trends in attitudes towards a non-intrusive alert system. A more extensive evaluation should be conducted in the future to verify these trends (particularly as they pertain to specific design elements). This evaluation should also involve operators from the West Coast.\r(ii) Context validity: the evaluation was limited in scope by showing designs as single event representations. However, in considering a future operational environment, there could be potentially a continuing volume of alerts that occur during a watch. Therefore, the validity of the judgments obtained in the walkthrough concerning the appropriate level of intrusiveness of the design alternatives must be treated with caution. A design option that appears to offer the right level of intrusiveness in alerting the operator, when viewed in isolation, may, over time and with a high frequency of alerts, become too distracting.\r(iii) Halo effects: it is possible that if a trial participant favoured a particular design, then all detailed evaluation questions concerning the design could be tainted by this bias. For example, if the design were seen as being insufficiently intrusive to “keep operators on their toes”, then its usefulness and utility may also have been judged lower.\r6.4.3 Design recommendations for anomaly alert system\rConsidering the participant feedback, the results suggest that an anomaly alert design which combines the counter from the Cumulative Total Indicator, to indicate the number of active alerts in the system, with the Ticker and Fading Bar, to notify the operator of incoming alerts (with an option for either one), would be the next logical iteration of a non-intrusive anomaly alerting system design.\rWhile participant feedback from the design walkthroughs can be used to point the way toward future iterations of an alert system design, caution must be used in interpreting this feedback especially given the small sample size and inconsistency in the comments. Hence, design guidelines must also be based on human factors principles and further experimentation in a realistic context (i.e., in the RJOC using the RMP). Table 20 shows a number of recommendations for future alert system designs based on participant feedback as well as human factors principles.\rTable 20: Design recommendations for future iterations of alert system interface\rDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rAlert\rAlert indicator should be visual\rAlert indicator should be visual\rAlert indicator should only disappear after acknowledgement\rAlert indicator should only disappear after acknowledgement\rAbility to click and drag the alert display to a different location on the RMP\rAbility to click and drag the alert display to a different location on the RMP so as not to obscure the RMP (i.e. primary task)\rFont in the pop-up boxes should be a bit\rFont in the pop-up boxes should be\rPage 92 Non-Intrusive Alert System Humansystems®\r\nDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rbigger\rappropriate for distance that operator is sitting from screen and size of fonts used in primary task and ambient illumination\rTime should be in Zulu time\rTime should be consistent with that used for current tasks\rTicker and fading bar should also include the vessel name\rThe required level of information content to be contained with in alert indicator should be further investigated\rOption to centre the screen around a vessel of interest or area of interest\rOption to centre the screen around a vessel of interest or area of interest to support SA\rPriority\r• Priority 1 alerts will be highlighted in red\r• Priority 2 alerts will be highlighted gold\r• Priority 3 alerts will be highlighted in grey\rPriority\r• Priority 1 alerts will be highlighted in red.\r• Priority 2 alerts will be highlighted gold.\r• Priority 3 alerts will be highlighted in grey.\rIncoming vs. Active Alerts\r• Incoming alerts should be indicated in a ticker or fading bar (priority 1 and 2) and/ or as a cumulative number in the count box (priority 1, 2 and 3)\r• Active alerts left in the system will only be indicated as a cumulative number in the count box (priority 1, 2 and 3)\rIncoming vs. Active Alerts\r• Presentation of incoming alerts should be further investigated in a context that is representative of RJOC operator’s actual work environment.\r• Active alerts left in the system will be indicated as a cumulative number in the count box (priority 1, 2 and 3)\rRetrieving Information\rThe RMP pop up box, rather than the AIW, should be used to retrieve information related to an anomaly\rThis solution represents a trade-off between the amount of space required for the appropriate information content concerning the anomaly (which has a potential for obscuring the RMP) and obtaining the information from a separate window, taking the operator’s attention away from the RMP. The appropriate design option will require further investigation before this recommendation can be stated definitively.\rThe RMP pop up box will show AIS number, name of the vessel, reason for alert, age of alert, and action buttons\rThe anomaly information content of the RMP pop up box should be further investigated\rManaging Alerts\rThe AIW should be used to manage alerts\rThe AIW should be used to manage alerts\rThe AIW will show priority level, a button to return to the RMP, number of alerts, age of alerts, AIS number, name of vessel, reason for alert, action buttons, source of alert,\rThe content and functionality of the AIW should be further investigated by determining the specific operational requirements.\rHumansystems® Non-Intrusive Alert System Page 93\r\nDesign feature\rRecommendations based on participant feedback\rRecommendations considering HF principles\rvessel type, flag, age of alert, course, speed, alert history and age of track\rThe operator should not have to navigate to a different screen to see the AIW\rThis cannot be supported without further investigation. For example, by the end of the watch, and during a high frequency alerting context, there may be numerous alerts in the system. These would either have to be represented in a small window on the RMP, though which the operator would have to continually screen (i.e., not functionally efficient, or usable) or a large window, which would then obscure a significant portion of the RMP .\rThe AIW may appear as a pop up window over the RMP that can be increased or decreased in size as desired\rThe AIW may appear as a pop up window over the RMP that can be appropriately sized for the information content up to a certain maximum.\rAcknowledging Alerts\rAcknowledgment of an alert will come in the form of LEAVE or CLEAR\rThe most appropriate terms for acknowledging an alert should be intuitive to all users and be consistent with similar functions in the current system, and should therefore be further investigated\rAbility to action alerts through the RMP pop up box\rAbility to action alerts through the RMP pop up box\rAbility to action alerts through the AIW\rAbility to action alerts through the AIW\rPage 94 Non-Intrusive Alert System Humansystems®\r\n7. Overall conclusions and recommendations\rThis section presents a number of overall conclusions based on the literature review, design development process and SME review and evaluation of the alert system design concepts.\r7.1 Conclusions\rOur general assessment of the literature relating to non-intrusive alert system design is that it is a somewhat piecemeal and haphazard collection of principles and guidelines from various application environments and serving a number of different goals. Clearly, there is a lack of a unified design approach and associated recommendations that would be applicable for non- intrusive alerting contexts. However, the detailed guidelines found in Shorrock et al (2002) and Han et al (2007) provide a good starting point for an integrated guidance document, which would then need to be extended and made more context relevant to the RJOCs. In addition, there would be a need to make this guidance more specific than statements such as “alarms should signal the need for action”, “alarms should be detected rapidly...”, “alarms should not annoy, startle or distract unnecessarily” etc. Thus, while these recommendations are sound, there is a lack of information on how they are to be implemented as specific design guidelines.\rThe review of the literature gave rise to some general principles for four alert system design concepts that were the presented to and evaluated by SMEs. The actual translation of these principles into specific designs was very much based on Humansystems’ prior experience with HF design implementation, rather than specific recommendations from the literature reviewed. The design evaluation, combined with consideration of general human factors principles, resulted in a list of design requirements for the best way to:\r• Alert RMP operator to a new incoming alert\r• Provide operator with awareness of the number of active alerts in the system\r• Provide operator with information specific to an incoming alert\r• Provide operator with information on all active alerts in the system\r• Provide operator with a means to acknowledge the occurrence of an alert\r• Enable operator to manage (i.e., action) any active alerts in the system\rFurther research, however, is needed to better clarify design options that would support these design requirements, particularly under more realistic operational conditions of multiple alerts within a watch.\r7.2 Future Work\rThe literature review and SME feedback from the design review were valuable in providing a direction for both future iterations of an anomaly alert system design as well as future research. Specifically, future design efforts should work toward developing an alert system interface design\rHumansystems® Non-Intrusive Alert System Page 95\r\nin accordance with the design principles listed in section 6.3.3 once these design requirements have been validated through further research.\rFuture research efforts should focus on both experimentally evaluating anomaly alert system designs in the context of the RMP (i.e., representative of user’s work environment including the potential number of alerts) as well as broader research relating to intrusiveness and attention. The following list provides a number of research questions that have yet to be resolved:\r• •\r•\r• •\r• • •\rWhat is the appropriate number of alert priorities?\rWhat is the most appropriate level of intrusiveness for different priorities of alerts and how is this influenced by alert frequency?\rWhat is the relative intrusiveness of a range of design parameters such as alert size, location and dynamics?\rHow are attention and intrusiveness related?\rIn an RMP context, what are the trade-offs between implementing the information within the RMP window and/or providing a separate window?\rIn the context of the RJOC, what are the information requirements for an alert management system for data anomalies?\rHow should the anomaly alert system be integrated within the overall “alert” system currently used in GCCS-M?\rGiven the characteristics of the GCCS-M, what are the most appropriate design characteristics (e.g., colour, font size, etc.) for an anomaly alert system?\rPage 96\rNon-Intrusive Alert System Humansystems®\r\nReferences\rAhlstrom, V. (2003). An Initial Survey of National Airspace System Auditory Alarm Issues in Terminal Air Traffic Control. In USDO. Transportation (Ed.).\rAhnlund, J., Bergquist, T., & Spaanenburg, L. (2003). Rule-Based Reduction of Alarm Signals in Industrial Control. Journal of Intelligent and Fuzzy Systems, 14, 73-84.\rAiken, D.S., Green, G.E., Arntz, S.J. and Meliza, L.L. (2005). Real time decision alert, aid and after action review system for combat and training (ARI Technical Report 1165). Alexandria, VA: U.S. Army Research Institute for the Behavioral and Social Sciences.\rBrown, L.M. & Kaaresoja, T. (2006). Feel who's talking: Using tactons for mobile phone alerts. CHI Extended Abstracts, 604-609.\rBrown, W. S., O'Hara, J. M., & Higgins, J. C. (2000). Advanced Alarm Systems: Revision of Guidance and Its Technical Basis. In U. S. N. R. Commission (Ed.). Upton, NY: Brookhaven National Library.\rChyssler, T., Burschka, S., Semling, M., Lingvall, T., & Burbeck, K. (2004). Alarm Reduction and Correlaion in Intrusion Detection Systems. In U. F. a. M. Meier (Ed.), Proceedings from the International GI Workshop on Detection of Intrusions and Malware & Vulnerability Assessment (pp. 9-24). Dortmund, Germany.\rColcombe, A., & Wickens, C. D. (2006). Cockpit Display of Traffic Information Automated Conflict Alerting: Parameters to Maximize Effectiveness and Minimize Disruption in Multi-Task Environments. In N. A. R. Center (Ed.). Moffett Field, CA.\rDavenport, M. (2008). Kinematic behaviour anomaly detection (KBAD) Final Report. DRDC CORA Reference No. KBAD-RP-52-6615.\rDavenport, M., Rafuse, J. Lt(N) & Widdis, E. (2005). RMP baseline update study. Volume 1: Main Report. DRDC Atlantic No. TR-2004-293.\rDe Muer, T., Botteldooren, D., De Coensel, B., Berglund, B., Nilsson, M. & Lercher, P. (2005). A model for noise annoyance based on notice-events. In Proceedings of the 34th International Congress on Noise Control Engineering (Internoise), Rio de Janeiro, Brazil.\rDicken, C. R. (1999). \"Soft\" Control Desk and Alarm Display. Computing & Control Engineering Journal, February 1999, 11-16.\rEdworthy, J., & Hellier, E. (2008). Auditory Warnings in Noisy Environments. Noise and Health, 6, 27-39.\rGrootjen, M., Bierman, E. P. B., & Neerincx, M. A. (2006). Optimizing Cognitive Task Load in Naval Ship Control Centres: Design of an Adaptive Interface, IEA 2006: 16th World Congress on Ergonomics.\rGruber, T. R. (1995). Toward principles for the design of ontologies used for knowledge sharing. International Journal Human-Computer Studies, 43(5-6), 907-928\rHan, S. H., Yang, H., & Im, D.-G. (2007). Designing a Human-Computer Interface for a Process Control Room: A Case Study of a Steel Manufacturing Company. International Journal of Industrial Ergonomics, 37, 383-393.\rHumansystems® Non-Intrusive Alert System Page 97\r\nHautamaki, B. S., Bagnall, T., & Small, R. L. (2006). Human Interface Evaluation Methods for Submarine Combat Systems, Undersea Human Systems Integration Symposium. Mystic, Conneticut.\rHudlicka, E., & McNeese, M. D. (2002). Assessment of User Affective and Belief States for Interface Adaptation: Application to an Air Force Pilot Task. User Modeling and User- Adapted Interaction, 12, 1-47.\rHwang, S.-L., Lin, J.-T., Liang, G.-F., Yau, Y.-J., Yenn, T.-C., & Hsu, C.-C. (2008). Application Control Chart Concepts of Designing a Pre-Alarm System in the Nuclear Power Plant Control Room. Nuclear Engineering and Design, in press.\rKrausman, A. S., Elliott, L. R., & Pettitt, R. A. (2005). Effects of Visual, Auditory, and Tactile Alerts on Platoon Leader Performance and Decision Making. In A. R. Laboratory (Ed.).\rLiu, J., Lim, K. W., Ho, W. K., Tan, K. C., Srinivasan, B., & Tay, A. (2003). The Intelligent Alarm Management System. IEEE Software, 66-71.\rMatthews, M., Bruyn, L., Keeble, R. & Rafuse, J. (2004). Maritime Operation Centre: Function Analysis and Development of Measures of Performance to Evaluate Future Multi-Sensor Integration within a Common Operating Environment. DRDC Contract Report TR-2004- 296.\rMcCrickard, D. S., Catrambone, R., Chewar, C. M., & Stasko, J. T. (2003b). Establishing Tradeoffs that Leverage Attention for Utility: Empirically Evaluating Information Display in Notification Systems. International Journal of Human-Computer Studies, 58, 547-582.\rMcCrickard, D. S., Chewar, C. M., Somervell, J. P., & Ndiwalana, A. (2003a). A Model for Notification Systems Evaluation - Assessing User Goals for Multitasking Activity. ACM Transactions on Computer-Human Interaction, 10(4), 312-338.\rMcFarlane, D. C., & Berger, P. H. (2005). The HSI Implications of a New Naval Combat System Alerting Technology Based on Negotiation-Based Coordination: The Hail Technology, Human Systems Integration Symposium, 2005, American Society of Naval Engineers. Arlington, VA.\rMcFarlane, D. C., & Latorella, K. A. (2002). The scope and importance of human interruption in human-computer interaction design. Human-Computer Interaction, 17, 1-61.\rMiller, C. A. (2005). Trust in Adaptive Automation: The Role of Etiquette in Tuning Trust via Analogic and Affective Methods. Paper presented at the Proceedings of the 1st International Conference on Augmented Cognition, Las Vegas, NV.\rMitchell, C. M. (1998). Model-Based Design of Human Interaction with Complex Systems. In A. P. S. a. W. B. Rouse (Ed.), Handbook of Systems Engineering and Management. New York, NY: John Wilev & Sons.\rRhodes (2007). Biologically-inspired Approcahes to Higher-Level Information Fusion. 10th Interbational Conference on Information Fusion.\rRiveiro, M., Falkman, G., & Ziemke, T. (2008). Improving Maritime Anomaly Detection and Situation Awareness Through Interactive Visualization. Paper presented at the Proceedings of the 11th International Conference on Information Fusion.\rPage 98 Non-Intrusive Alert System Humansystems®\r\nShorrock, S. T., Scaife, R., & Cousins, A. (2002). Model-Based Principles for Human-Centred Alarm Systems from Theory and Practice. Paper presented at the 21st European Conference on Human Decision Making and Control.\rSmallman, H. S., & John, M. S. (2003). CHEX (Change History EXplicit): New HCI Concepts for Change Awareness. Paper presented at the Proceedings of the 46th Annual Meeting of the Human Factors and Ergonomics Society, Santa Monica, CA.\rSmallman, H. S., & John, M. S. (2005). Improving Recovery from Multi-Task Interruptions Using an Intelligent Change Awareness Tool, Proceedings from the 1st International Conference on Augmented Cognition. Las Vegas, NV.\rSorkin, R. D. (1988). Why are People Turning Off Our Alarms? Journal of the Acoustic Society of America, 84(3), 1107-1108.\rSt.John, M., Smallman, H. S., & Manes, D. I. (2005). Recovery from Interruptions to a Dynamic Monitoring Task: The Beguiling Utility of Instant Replay, Proceedings of HFES 2005: 49th Annual Meeting of the Human Factors and Ergonomics Society (pp. 473-477). Santa Monica, CA: Human Factors and Ergonomics Society.\rStreefkerk, J. W., Esch-Bussemakers, M. v., & Neerincx, M. (2007). Context-Aware Notification for Mobile Police Officers. In D. Harris (Ed.), Engineering Psychology and Cognitive Ergonomics 7th International Conference. Beijing, China.\rToet, A. (2006). Gaze Directed Displays as an Enabling Technology for Attention Aware Systems. Computers in Human Behavior, 22, 615-647.\rWickens, C. (1984). Processing resources and attention. In R. Parasuraman & D.R. Davies (Eds.), Varieties of Attention. New York: Academic Press.\rHumansystems® Non-Intrusive Alert System Page 99\r\nThis page intentionally left blank.\rPage 100 Non-Intrusive Alert System Humansystems®\r\nAnnex A: Ethics Protocol\rEXECUTIVE SUMMARY\rProtocol # L676\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study\rPrincipal Investigators: Dr. Michael Matthews, Lora Bruyn Martin, Humansystems® Incorporated (HSI®), Guelph, Ontario. Tel: 519-836-5911\rDefence Research and Development Canada (DRDC) Co-Investigators:\rMs. Sharon McFadden, DRDC Toronto, Tel (416) 635-2189 Ms. Liesa Lapinski, DRDC Atlantic. Tel: (902) 426-3100 x180 Thrust: 11he Maritime Domain Awareness\rObjectives:\rThe goal of this experiment is to explore, in the context of the Recognized Maritime Picture (RMP), non-intrusive ways of presenting to the operator information concerning anomalous behavior of vessels. Anomalous behavior can take many forms, e.g. a sudden increase in speed, a vessel in transit that suddenly stops, a ship that changes its port of destination or a ship heading into regulated waters without appropriate permissions. Because of the large number of vessels and associated track data, it is not currently possible for operators to readily determine when, or where, such anomalies occur.\rOverview:\rSix volunteer participants (no age or gender restrictions) will be required to review a number of different design options for a non-intrusive, anomaly alerting system. The participants will individually do a walk through of the options which will be presented as a PowerPoint mock up of the screen interface. A questionnaire will be administered at the end of each session, to record the volunteers’ subjective evaluation of the different design options in terms of their utility and usability. The experiment will be undertaken at the MAPLE laboratory DRDC and will require one walkthrough session. Each session will comprise approximately 15 minutes of background briefing and a 60-90 minute walkthrough session, which will involve interface walkthroughs and the completion of a questionnaire. This work will be conducted by Humansystems® Incorporated.\rParticipants:\rMale or female Navy or ex-Navy operators will be recruited and paid for their participation. There is no restriction on age or gender. The two ex-Navy operators are both males.\rRisks:\rThis experiment offers minimal risk to the participant’s health and well-being. There is a low risk of eye fatigue or eyestrain, as is associated with doing any visually intensive task on a computer display.\rProtocol # L676\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study\rHumansystems® Non-Intrusive Alert System Page A-1\r\nPrincipal Investigators:\rDr. Michael Matthews, Lora Bruyn Martin, Humansystems Incorporated® (HSI®), Guelph, Ontario. Tel: 519-836-5911\rDRDC Co-Investigators:\rMs. Sharon McFadden, DRDC Toronto, Tel (416) 635-2189 Ms. Liesa Lapinski, DRDC Atlantic, Tel: (902) 426-3100 Thrust: 11he Maritime Domain Awareness\rList of Acronyms:\rDRDC Defence Research and Development Canada HSI® Humansystems Incorporated\rMDA Maritime Domain Awareness\rRMP Recognized Maritime Picture\rRJOCs Regional Joint Operations Centres\rBackground:\rThis applied research project in the Maritime Domain Awareness (MDA) Thrust is studying information visualization and management for enhanced domain awareness in maritime security. The DRDC/HSI® team wants to investigate the best way to provide operators with non-intrusive alerts when certain forms of anomalous vessel behavior occur to see if the information provided by way of the alert can help improve understanding of the Recognized Maritime Picture (RMP), decision making based on the RMP and the efficiency of the RMP operators’ duties.\rThe RMP is a product produced by the Regional Joint Operation Centres (RJOCs). In its common form, it is a map of the Canadian coastal waters, with contacts, typically ships, marked on the map. Each contact has a set of metadata associated with it which can include (but is not exclusive to) position, speed, heading, ship name, hull number, threat, flag, destination, origin, type, cargo and a digital image. At worst, the metadata only consist of a position (i.e., there is something out there). At best, the metadata consist of all of the above. The different degrees of metadata are due to the multiple sources of information that feed the RMP. These sources include everything from radar to surveillance flights to self reporting systems to voluntary reports, each providing its own subset of data.\rAnomalies in the movement and behavior of vessels may be of many types and include, for example:\r• Unexplained high speed: a ship that is claiming to be a normal merchant ship suddenly starts travelling at a high speed more typical of a passenger ship or warship.\r• Speed too slow: a Cargo, Passenger, or Ferry is observed going slowly. As these vessels generally go as fast as they safely can, it may be an indicator of a problem.\r• Loitering: a cargo ship stops outside of or far from a harbour, or steams very slowly, rather than proceeding directly into port.\rPage A-2 Non-Intrusive Alert System Humansystems®\r\n• Grab and dash fishing: a foreign fishing boat moves from the international zone to Canadian waters (where it is forbidden from fishing) for a few hours just before leaving for its home port.\r• Not heading to port: a vessel is heading in a direction where there is no harbour, or is not heading toward its declared destination. Cargo and Ferry vessels always go from one port to another port, and generally by the shortest available route.\rThe purpose of the proposed study is to explore the best ways of alerting operators to such anomalies in a non-intrusive manner. Given the potential for many such anomalies during a normal watch, it is imperative that discipline be used in the design of an alerting system to ensure that operators are not hindered in the performance of their primary task by nuisance alerts. Also, it is important that functionality is provided to allow operators to define their own criteria for different alert priorities in different contexts, and that the interface provides good situation awareness of the different alert types. The outcome of this work will serve both the maritime operations communities, as well the scientific communities in the advancement of new methods for the design of non-intrusive alerting systems. This work will be conducted by Humansystems® Incorporated.\rObjectives:\rThe goal of this experiment is to explore, in the context of the RMP, non-intrusive ways of presenting to the operator information concerning anomalous behavior of vessels. Because of the large number of vessel and associated track data, it is not currently possible for operators to readily determine when or where such anomalies occur.\rOverview:\rSix volunteer participants (no restriction on age or gender) will be recruited to review a number of different design options for a non-intrusive, anomaly alerting system. The participants will individually do a walk through of the options which will be presented as a PowerPoint mock up of the screen interface. A questionnaire will be administered at the end of each session, to record the volunteers’ subjective evaluation of the different design options in terms of their utility and usability. The experiment will be undertaken at the MAPLE laboratory DRDC Atlantic and will require one walkthrough session. Each session will comprise approximately 15 minutes of background briefing and a 60-90 minute walkthrough session, which will involve interface walkthroughs and the completion of a questionnaire. This work will be conducted by Humansystems® Incorporated.\rProcedures:\rBackground Briefing\rImmediately prior to the walkthrough session, participants will be given an orientation briefing on the overall study, its objectives and what they will be asked to do. At this stage they will be asked to complete a consent form and an information sheet about the study.\rWalkthrough session: Review of design options\rThe goals of a non-intrusive alerting system will be described to the participant and the major functional components of the system will be described at a high level. Participants will then be guided through a PowerPoint presentation of how a non-intrusive alerting system interface would look and work. For each functional component of the system, participants will interact with an animated PowerPoint slide to simulate the actions of an interface. As participants proceed through\rHumansystems® Non-Intrusive Alert System Page A-3\r\nthe design they will be engaged in discussion concerning how intuitive and easy the interface is to use, how it serves their information needs, what information requirements are not being met and what additional functions they would like to see. It is anticipated that the participant will be presented with 4-5 design options to review in this manner.\rAt the completion of the walkthrough the participants will complete a subjective questionnaire documenting their evaluation of the different design options.\rParticipants:\rApproximately six Navy, or ex-Navy operators, will participate Navy operators will be recruited through a formal request through the DRDC Atlantic Navy Liaison Officer. There will be no restriction on age or gender. Ex-Navy participants will be recruited from a list maintained by HSI of ex-Navy personnel who have indicated a prior willingness to be contacted as potential study participants. . The ex-navy operators are males, Participants will be required to self identify that they have normal colour vision.\rEquipment and Facilities:\rThe apparatus comprises a standard “Windows” workstation with 19” colour screen, a mouse and keyboard input, a work surface to record notes, and an ergonomically designed operator’s chair.\rData collected:\rThe following information will be collected during each walkthrough session:\r- Responses to the questionnaires concerning the participant’s evaluation of the design alternatives\r- Summary of the participants’ comments during free discussion with the walkthrough facilitator\rExperimental Design/Statistical Analysis:\rThe small sample size (limited by the availability of operational personnel) will likely have insufficient power to warrant the use of analytical statistical procedures for estimation of probabilities. However, it should be noted that the present study is designed to be an exploratory approach to defining a preliminary set of good design alternatives, which, at some future date, could be evaluated more completely in a more rigorous experiment.\rRisks and Safety Recommendations:\rThis experiment offers minimal risk to the participant’s health and well-being. There is a low risk of eye fatigue or eyestrain, as would be associated with doing any visually intensive task (e.g. web searching, word processing) on computer display for the period of time used in the walkthrough sessions. This may manifest itself as eye discomfort, dry or itchy eyes, or mild headache. However, the duration of exposure to the presentation will be short. Participants will be encouraged to inform experimenters if they experience any discomfort or eyestrain, or if they have any problems during the investigation. They may be told to stop their activities until problems or conditions are resolved. The risks from participation in this experiment are generally the same as those associated with the performance of normal monitoring of a visual display that a person might do while word processing, surfing the web or playing video games.\rPage A-4 Non-Intrusive Alert System Humansystems®\r\nBenefits of Study:\rThrough their involvement in the study participants will be able to contribute to the validation, development and design of new methods for providing information on vessel anomalies, which in turn provides important human factors data for the re-design and automation of future systems to represent the maritime picture.\rInformed Consent:\rParticipants will be fully briefed on the relevant aspects of the experimental protocol and will be given a copy of this protocol to review. They will be required to sign a voluntary consent form, indicating their willing informed consent, before being allowed to participate in the experiment. No deception is involved.\rConfidentiality:\rAny personal or performance data collected for each participant will be available only to the experimenters and will be held in the strictest confidence. Participants will not be identified by name in the data records; group statistics will be used in future presentations or publications. Individual participant data will be coded anonymously and maintained in a computer file. The file may be accessed only by the project team.\rParticipant Debriefing:\rParticipants will be permitted to ask any questions they wish about the study after they have completed the experiment.\rParticipant Stress Remuneration:\rParticipants will be paid participant pay in the amount of $26.93. This assumes that the total time required will be no more than 3 hours per participant for participation in this study (in accordance with DRDC guidelines and as authorized by DND policies15). The remuneration is calculated as follows:\r3 hours x 2 (stress level) x $2.50 +$11.93 x 1 day = $26.93\rApproximate Time Involvement:\rAll participants will be required to participate in one session of approximately 3 hours with a 15 minute break in the middle.\rMedical screening:\rNo screening is required for this study\rPhysician supervision:\rNo physician supervision is required for this study.\r15 Guide to Stress Compensation for Human Subjects, R. Pigeau. DRDC Toronto.\rHumansystems® Non-Intrusive Alert System Page A-5\r\nThis page intentionally left blank.\rPage A-6 Non-Intrusive Alert System Humansystems®\r\nVoluntary Consent Form\rProtocol Number: L676\rResearch Project Title: Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study Principal Investigators:\rDr. Michael Matthews, Humansystems Incorporated, Guelph, Ontario.\rLora Bruyn Martin, Humansystems Incorporated, Guelph, Ontario\rDRDC Co-Investigators:\rMs. Sharon McFadden, DRDC (Toronto)\rMs. Liesa Lapinski. DRDC (Atlantic)\r1. I,__________________________________________________________\r________________________________________________________________\r(Name, Address, Phone number) hereby volunteer to participate as a participant in the experiment entitled “Evaluation of Interface Designs for Non-Intrusive Alerts: Pilot Study”, the aim of which is to explore the best ways of providing operators with information on anomalous behavior of vessels in a maritime context. I understand that I am required to read the attached protocol in its entirety. I have had the opportunity to study and discuss the attached protocol with the investigators and I have been informed to my satisfaction about the possible discomforts associated with these tests.\r2. I am aware that before starting I will receive a briefing on the aims and procedures for the experiment. I will have the opportunity to ask and receive answers to any questions I may have. I understand that I am free to refuse to participate and may withdraw my consent without prejudice or hard feelings at any time. Should I withdraw my consent, my participation as a participant will cease immediately. I understand that the entire session will last no more than 3 hours.\r3. I have been told that the principal risks associated with this experiment are the possible development of eyestrain or visual fatigue. This may manifest itself as eye discomfort, dry or itchy eyes, or mild headache. I understand that the limited duration of each experiment and rest periods between experiments will mitigate this risk. I understand and accept this risk. I am aware that there are inherent, unknown and currently unforeseen risks by DRDCs Atlantic and Toronto and the Project Investigators that are associated with any scientific research and that all known risks have been explained to my satisfaction.. I have been given examples of potential minor and remote risks associated with the experiment and consider these risks acceptable as well.\r4. I agree to provide responses to questions that are to the best of my knowledge truthful and complete. I have been advised that the experimental data concerning me will be treated as confidential and not revealed to anyone other than the investigators without my consent except as data unidentified as to source. I understand that my name will not be identified or attached in any manner to any publication arising from this study.\r5. I understand that for my participation in this research project, I am entitled to stress remuneration in the form of participant payment of $ 26.93. Stress remuneration is taxable. However, T4A slips are issued only for amounts in excess of $500.00 remuneration per year.\rHumansystems® Non-Intrusive Alert System Page A-7\r\n6. I acknowledge that I have read this form and I understand that my consent is voluntary and has been given under circumstances in which I can exercise free power of choice. I have been informed that I may, at any time, revoke my consent and withdraw from the experiment, and that the investigators may terminate my involvement in the experiment, regardless of my wishes.\r7. I understand that by signing this consent form I have not waived any legal rights I may have as a result of any harm to me occasioned by my participation in this research project beyond all risks I have assumed.\r8. [For Canadian Forces (CF) members only] – I understand that I am considered to be on duty for disciplinary, administrative and Pension Act purposes during my participation in this study. With that said, this duty status has no effect on my right to withdraw for the experiment at any time I wish and it is clear that no action will be taken against me for exercising this right. As well, in the unlikely event that my participation in this study results in a medical condition rendering me unfit for service, I may be released from the CF and my military benefits apply.\rVolunteer’s Name: _________________________________________________\rSignature: _____________________________ Date: _____________________\rName of Witness to Signature: _______________________________________\rSignature: _____________________________ Date: _____________________\rFamily Member or Contact Person (name, address, daytime phone number & relationship):_____________________________________________________________\rPrincipal Investigator: ______________________________________________\rSignature: _____________________________ Date: _____________________\rFOR PARTICIPANT ENQUIRY IF REQUIRED:\rShould I have any questions or concerns regarding this project before, during, or after participation, I understand that I am encouraged to contact any of the contacts below by phone or e-mail, to\rPrincipal Investigators:\rDr. Michael Matthews, Humansystems® Incorporated (HSI®), Guelph, Ontario Tel: (519) 836- 5911, email: mmatthews@humansys.com\rLora Bruyn Martin, Humansystems® Incorporated (HSI®), Guelph, Ontario Tel: (519) 836-5911 Ex. 303, email: lbruyn@humansys.com\rCo-Investigator:\rPage A-8 Non-Intrusive Alert System Humansystems®\r\nMs. Sharon McFadden, DRDC Toronto, Tel (416-635-2189), email: sharon.mcfadden@drdc- rddc.gc.ca\rChair, DRDC Human Research Ethics Committee (HREC): Dr. Jack P. Landolt, phone: 416- 635-2120, email: jack.landolt@drdc-rddc.gc.ca\rI understand that I will be given a copy of this consent form so that I may contact any of the above-mentioned individuals at some time in the future should that be required.\rSecondary Use of Data: I consent/do not consent (delete as appropriate) to the use of this study’s experimental data involving me in unidentified form in future related studies provided review and approval have been given by DRDC HREC.\rVolunteer’s Signature_____________________ Date ____________________\rHumansystems® Non-Intrusive Alert System Page A-9\r\nINFORMATION FOR PARTICIPANTS\rTitle: Evaluation of Interface Designs for Non-Intrusive Alerts\rProtocol #L676\rPrincipal Investigators: Dr. Michael Matthews & Lora Bruyn Martin (HSI®), Guelph, Ontario Defence Research and Development Canada (DRDC) Co-Investigators:\rMs. Sharon McFadden (DRDC Toronto) & Ms. Liesa Lapinski (DRDC Atlantic)\rBackground & Purpose of Study:\rThis applied research project in the Maritime Domain Awareness Thrust is studying information visualization and management for enhanced domain awareness in maritime security. The DRDC/HSI® team wants to investigate the best way to provide operators with non-intrusive alerts when certain forms of anomalous vessel behavior occur to see if the information provided by way of the alert can help improve understanding of the Recognized Maritime Picture (RMP), decision making based on the RMP and the efficiency of the RMP operators’ duties. Given the potential for many such anomalies during a normal watch, it is imperative that the design of an alerting system ensures that operators are not hindered in the performance of their primary task by nuisance alerts. Also, it is important that functionality is provided to allow operators to define their own criteria for different alert priorities in different contexts, and that the interface provides good situation awareness of the different alert types. The outcome of this work will serve in the advancement of new methods for the design of non-intrusive alerting systems.\rProcedure:\rImmediately prior to the walkthrough session, you will be given an orientation briefing on the overall study, its objectives and what you will be asked to do. At this stage you will be asked to complete a consent form.\rThe goals of a non-intrusive alerting system will be described to you and the major functional components of the system will be described at a high level. You will then be guided through a PowerPoint presentation of how a non-intrusive alerting system interface would look and work. For each functional component of the system, you will interact with an animated PowerPoint slide to simulate the actions of an interface. As you proceed through the design you will be engaged in discussion concerning the interface, how it serves your information needs, what information requirements are not being met and what additional functions you would like to see. You will be presented with 4-5 design options to review in this manner. You will then complete a subjective questionnaire documenting your evaluation of the different design options.\rThe session will be approximately 2-3 hours with a 15 minute break in the middle.\rPage A-10 Non-Intrusive Alert System Humansystems®\r\nAnnex B: Questionnaires for SME Evaluation\rDemographic Questionnaire\rInstructions: Please read each question below and write the appropriate response in the answer\rsection.\rQuestion\rAnswer\r1. Name.\r2. What is your current rank?\r3. What is your current position?\r4. How long have you been in that position?\r5. Please list any other related positions you have had.\rHumansystems® Non-Intrusive Alert System Page B-1\r\nUsability and Usefulness Questionnaire\rInstructions: Please read each statement below and circle the response you feel is most\rappropriate concerning the usefulness of a general non-intrusive alerting system.\rStatement\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\r1. These alerts would enhance our knowledge of anomalies\r1\r2\r3\r4\r5\r2. This alerting system would be used on a daily basis\r1\r2\r3\r4\r5\r3. Tasks can be performed in a straightforward manner using this alerting system\r1\r2\r3\r4\r5\r4. The thinking required to use this alerting system requires significant effort\r1\r2\r3\r4\r5\r5. This alerting system would be difficult to use\r1\r2\r3\r4\r5\r6. This alerting system will improve my situation awareness\r1\r2\r3\r4\r5\r7. This alerting system would make it easier to identify anomalies\r1\r2\r3\r4\r5\r8. I would find this alert system useful\r1\r2\r3\r4\r5\r9. I would not ignore alerts while using this technology\r1\r2\r3\r4\r5\r10. The Alert Information Window (AIW) was confusing\r1\r2\r3\r4\r5\r11. It was easy to learn how the AIW was represented\r1\r2\r3\r4\r5\r12. The AIW had all the necessary information\r1\r2\r3\r4\r5\r13. It was easy navigating between the RMP and the AIW\r1\r2\r3\r4\r5\r14. I prefer clearing and deferring alerts directly from the RMP\r1\r2\r3\r4\r5\r15. I prefer using the AIW to clear or defer alerts\r1\r2\r3\r4\r5\rPage B-2 Non-Intrusive Alert System Humansystems®\r\nAlert Design Questionnaire\rInstructions: Please read each statement below and circle the response you feel is most\rappropriate concerning the usability of each non intrusive alert design.\rStatement & Design\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\r16. The number of alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r17. The presence of an alert was easy to recognize\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r18. The priorities of the alerts were easy to comprehend\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r19. It was easy to find the relevant anomaly\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r20. It was easy to find information on anomalies\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r21. The alerting design enhanced my situation awareness of maritime anomalies\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rHumansystems® Non-Intrusive Alert System Page B-3\r\nStatement & Design\rStrongly Disagree\rDisagree\rUndecided\rAgree\rStrongly Agree\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\r22. The appearance of the alerts is compatible with my current interface\rDesign 1: Cumulative Total Indicator\r1\r2\r3\r4\r5\rDesign 2: Vertical Cumulative Indicator\r1\r2\r3\r4\r5\rDesign 3: Horizontal Indicator Bar\r1\r2\r3\r4\r5\rDesign 4: Ticker & Fading Bar\r1\r2\r3\r4\r5\rPage B-4 Non-Intrusive Alert System Humansystems®\r\nRanking Questionnaire\rInstructions: For the following questions, please rank each of the alert designs in order of\rpreference (where 1 = most preferred, 4 = least preferred).\rCumulative Total Indicator Vertical Cumulative Horizontal Indicator Bar Indicator\r23. In terms of overall effectiveness in bringing alerts to my attention\rDesign 1: Cumulative Total Indicator Design 2: Vertical Cumulative Indicator Design 3: Horizontal Indicator Bar Design 4: Ticker & Fading Bar\r24. In comparing the methods for representing the different alert priorities\rDesign 1: Cumulative Total Indicator ____ Design 2: Vertical Cumulative Indicator ____ Design 3: Horizontal Indicator Bar ____ Design 4: Ticker & Fading Bar ____ 25. In terms of providing all of the required information about alerts, without distracting\rme from my primary task\rDesign 1: Cumulative Total Indicator Design 2: Vertical Cumulative Indicator Design 3: Horizontal Indicator Bar Design 4: Ticker & Fading Bar\r____\r____\r____\r____\rTicker and Fading Bar\r____\r____\r____\r____\rHumansystems®\rNon-Intrusive Alert System\rPage B-5\r\nLikes and Dislikes Questionnaire\rInstructions: Please respond to each alert design below by indicating whether or not the\rdesign should be implemented, your likes and dislikes, and the design’s intrusiveness.\r26. Should Design 1: Cumulative Total Indicator be implemented? Yes No\rLikes:________________________________ ____________________________________ ____________________________________\rDislikes:______________________________ ____________________________________ ___________________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\r27. Should Design 2: Vertical Cumulative Indicator be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ __________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\rPage B-6 Non-Intrusive Alert System Humansystems®\r\n28. Should Design 3: Horizontal Indicator Bar be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ __________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\r29. Should Design 4: Ticker & Fading Bar be implemented? Yes No\rLikes:________________________________ ____________________________________ __________________________\rDislikes:______________________________ ____________________________________ _________________________\r1234567\rNot at all intrusive Somewhat intrusive Extremely intrusive\rHumansystems® Non-Intrusive Alert System Page B-7\r\nThis page intentionally left blank.\rPage B-8 Non-Intrusive Alert System Humansystems®\r\nAnnex C: General Discussion Interview Questions\rSME Interview Questions: Evaluating Interface Designs for Non-Intrusive Alerts Things to emphasize:\r• Operators will define priorities and select rules\r• Show 3 priority levels Rory created\r• Show rule selection window\rGeneral:\r• At desk vs. away from desk\r• Looking at screen vs. not looking at screen\r• Frequency of alerts (by priority)\r• Should priority 1 alerts be intrusive?\r• Ignoring alerts\r• Did you like the way the priorities were indicated?\rInterface design:\r• Alert colour scheme (e.g., red, orange, yellow, grey)\r• Font (e.g., size, colour)\r• Was the terminology used familiar, clear and understandable?\r• Alert Information Window – useful and comprehensiveness?\r• Navigation – can you find the relevant information?\r• Intuitive versus not intuitive\r• Intrusiveness of alerts\r• Should there be auditory/tactile alerts in tandem or separately?\r• Flexibility in the location of ticker and horizontal indicator bar\r• Would operators want RMP centred on contact?\r• Would you want more information in the pop up box in the RMP?\rContext of use:\r• Ability to return to primary task\r• Potential problems from using the non-intrusive alert designs in practice?\ro During an increased workload?\ro When you are not at your computer screen?\ro At shift change over? Beginning or end of shift\rHumansystems® Non-Intrusive Alert System Page C-1\r\nThis page intentionally left blank.\rPage C-2 Non-Intrusive Alert System Humansystems®\r\nAcronyms\rABAIS\rAffect and Belief Adaptive Interface System\rACT\rAlarm Cleanup Toolbox\rAIS\rAutomatic Identification System\rAIW\rAlert Information Window\rANOVA\rAnalysis of Variance\rARO\rAssistant Reactor Operator\rATCS\rAir Traffic Control Specialists\rATM\rAir Traffic Management\rAVMS\rAIS Vessel Monitoring System\rCAPTA\rCognitive, Affective, Personality Task Analysis\rCCS\rCombat Control System\rCDTI\rCockpit Displays of Traffic Information\rCHDB\rContact History Database\rCHEX\rChange History Explicit\rCISTI\rCanada Institute for Scientific and Technical Information\rCONOPS\rConcept of Operations\rCOTS\rCommercial-Off-The-Shelf\rCS\rCombat System\rDCS\rDistributed Control System\rDRDC\rDefence Research and Development Canada\rDSS\rDecision Support System\rELINT\rElectronic Intelligence\rETA\rEstimated Time of Arrival\rFCR\rFire-Control Radar\rFFC\rBio-Fuelled District Heating Plant\rGCCS-M\rGlobal Command and Control System-Maritime\rGSFC\rGoddard Space Flight Center\rGUI\rGraphic User Interface\rHAIL\rHuman Alerting and Interruption Logistics\rHCI\rHuman Computer Interaction\rHF\rHuman Factors\rHFSWR\rHigh Frequency Surface Wave Radar\rHMIAS\rHazard Monitor and Intelligent Alerting System\rIAMS\rIntelligent Alarm Management System\rID\rIdentification\rHumansystems® Non-Intrusive Alert System Page D-1\r\nIDS\rIntrusion Detection Systems\rIOP\rInput Open\rIP\rInternet Protocol\rIRC\rInterruption, Reaction and Comprehension\rIS\rIdentification Supervisor\rLCCI\rLarge Complex Critical Infrastructure\rLCL\rLower Control Limits\rMARLANT\rMaritime Forces Atlantic\rMISR\rMaritime Intelligence, Surveillance, and Reconnaissance\rMMI\rMan-Machine Interface\rMMSI\rMaritime Mobile Service Identity\rMPA\rMaritime Patrol Aircraft\rMSOCC\rMultisatellite Operations Control Center\rNAS\rNational Airspace System\rNASA\rNational Aeronautics and Space Administration\rNATO\rNorth Atlantic Treaty Organization\rNRC\rNuclear Regulatory Commission\rOFMspert\rOperator Function Model Expert System\rPAL\rProvincial Airlines\rPDA\rPersonal Digital Assistant\rPL\rPlatoon Leaders\rR&D\rResearch and Development\rRJOC\rRegional Joint Operations Center\rRMP\rRecognized Maritime Picture\rRO\rReactor Operator\rRSVP\rRapid Serial Visual Presentation\rSHIELD\rSystem to Help Identify and Empower Leader Decisions\rSME\rSubject Matter Expert\rSOP\rStandard Operating Procedure\rTA\rTechnical Authority\rVDU\rVisual Display Unit\rVOI\rVessel of Interest\rUAV\rUnmanned Aerial Vehicle\rUCL\rUpper Control Limits\rPage D-2 Non-Intrusive Alert System Humansystems®\r\nDistribution list\rDocument No.: DRDC Toronto CR 2009-042\rLIST PART 1: Internal Distribution by Centre:\r2 DRDC Toronto Library file copies 1 Sharon McFadden\r1 Dr. Justin Hollands\r1 Dr. Ming Ho\r5 TOTAL LIST PART 1\rLIST PART 2: External Distribution by DRDKIM\r1 DRDKIM\rDRDC Atlantic\r9 Grove St, Dartmouth NS B2Y3Z7 3 Attn: Dr. L. Lapinski\rDr. J. Crebolder\rDr. Bruce Chalmers\rDRDC Ottawa\r1 3701 Carling Avenue, Ottawa, Ontario, K1A 0Z4\rAttn: Chris Helleur\rDefence R&D Canada - Valcartier\r2459 Pie Xi Blvd North 3 Val-Belair, QC G3J 1X5\rAttn: Dennis Gouin Alain Bouchard\rAndrew Wind, DRDC CORA\rMARLANT HQ\r3rd Floor, Room 311\rPO Box 99000 Stn Forces, Halifax, NS, B3K 5X5\rSteven Horn, DRDC CORA\rJTFP J02 OR-1, PO Box 17000 Station Forces, Victoria, BC, B9A 7N2\rCommanding Officer\rTRINITY JOSIC, PO Box 99000 Stn Forces, Halifax, NS, B3K 5X5 Capt(N) K. Greenwood\rJ3, Canada COM HQ, NDHQ - 101 Col. By Dr., Ottawa, ON, K1A 0K2\r1\r1\r1 1\rDRDC Toronto TR 2009-042 1\r\nLCdr A. Gyorkos\r1 OIC Maritime Operations Centre, JTFP HQ, MARPAC/JTFP, PO Box 17000 Station\rForces, Victoria, BC, B9A 7N2 13 TOTAL LIST PART 2\r18 TOTAL COPIES REQUIRED\rDRDC Toronto TR 2009-042 2\r\nDOCUMENT CONTROL DATA\r(Security classification of title, body of abstract and indexing annotation must be entered when the overall document is classified)\r1. ORIGINATOR (The name and address of the organization preparing the document. Organizations for whom the document was prepared, e.g. Centre sponsoring a contractor's report, or tasking agency, are entered in section 8.)\rHumansystems Inc.\r111 Farquhar St., 2nd floor Guelph, Ontario, N1H 3N4\r2. SECURITY CLASSIFICATION\r(Overall security classification of the document including special warning terms if applicable.)\rUNCLASSIFIED\r3. TITLE (The complete document title as indicated on the title page. Its classification should be indicated by the appropriate abbreviation (S, C or U) in parentheses after the title.)\rA non−intrusive alert system for maritime anomalies: literature review and the development and assessment of interface design concepts (U)\rSystème d’alerte non intrusive en cas d’anomalies maritimes: examen de la documentation et élaboration/évaluation de concepts d’interface (U)\r4. AUTHORS (last name, followed by initials – ranks, titles, etc. not to be used)\rMichael Matthews; Lora Bruyn Martin; Courtney D. Tario; Andrea L. Brown\r5. DATE OF PUBLICATION\r(Month and year of publication of document.)\rMarch 2009\r6a. NO. OF PAGES\r(Total containing information, including Annexes, Appendices, etc.)\r140\r6b. NO. OF REFS\r(Total cited in document.)\r37\r7. DESCRIPTIVE NOTES (The category of the document, e.g. technical report, technical note or memorandum. If appropriate, enter the type of report, e.g. interim, progress, summary, annual or final. Give the inclusive dates when a specific reporting period is covered.)\rTechnical Report\r8. SPONSORING ACTIVITY (The name of the department project office or laboratory sponsoring the research and development – include address.)\rDefence R&D Canada – Toronto 1133 Sheppard Avenue West P.O. Box 2000\rToronto, Ontario M3M 3B9\r9a. PROJECT OR GRANT NO. (If appropriate, the applicable research and development project or grant number under which the document\rwas written. Please specify whether project or grant.)\r11he04\r9b. CONTRACT NO. (If appropriate, the applicable number under which the document was written.)\rW7711-088124/001/TOR\r10a. ORIGINATOR'S DOCUMENT NUMBER (The official document number by which the document is identified by the originating activity. This number must be unique to this document.)\rDRDC Toronto CR 2009-042\r10b. OTHERDOCUMENTNO(s).(Anyothernumberswhichmaybe assigned this document either by the originator or by the sponsor.)\r11. DOCUMENT AVAILABILITY (Any limitations on further dissemination of the document, other than those imposed by security classification.) Unlimited\r12. DOCUMENT ANNOUNCEMENT (Any limitation to the bibliographic announcement of this document. This will normally correspond to the Document Availability (11). However, where further distribution (beyond the audience specified in (11) is possible, a wider announcement audience may be selected.))\rUnlimited\rDRDC Toronto TR 2009-042 3\r\n13. ABSTRACT (A brief and factual summary of the document. It may also appear elsewhere in the body of the document itself. It is highly desirable that the abstract of classified documents be unclassified. Each paragraph of the abstract shall begin with an indication of the security classification of the information in the paragraph (unless the document itself is unclassified) represented as (S), (C), (R), or (U). It is not necessary to include here abstracts in both official languages unless the text is bilingual.)\rThis project involves the investigation of best practices for the development of design concepts for a visualization aid, specifically an alerting system, which would increase the RMP operators’ awareness and understanding of maritime anomalies in the RMP (e.g. vessel not heading to port, grab and dash fishing, etc.). Such an alerting system, however, must make operators aware of anomalies that may be present without impacting on the performance of their primary tasks.\rThe objectives of this project were (i) to identify and analyse available literature relevant to non-intrusive alert systems, (ii) develop design concepts for a non-intrusive alerting interface to be used in GCCS-M and (iii) obtain feedback from Navy Subject Matter Experts (SMEs) on the suitability of the design options.\rThe results of the literature review suggest that there is a lack of a unified design approach and associated recommendations for non-intrusive alerting contexts. Furthermore, there was no single paper that definitively addressed the issue of how to design a non-intrusive alerting system. However, we were able to extract relevant concepts from the literature relating to alert/alarm design in general. These concepts, combined with general human factors principles, provided direction for a number of design concepts which were then reviewed and evaluated by subject matter experts.\rFuture design efforts should work toward developing an alert system interface design in accordance with the design principles listed above, once these design requirements have been validated.\rLe projet comprend l’étude des meilleures pratiques applicables à la définition de concepts pour un système d’aide à la visualisation, en l’occurrence un système d’alerte, qui aiderait les opérateurs du TSM à mieux connaître et comprendre les anomalies maritimes indiquées dans le TSM (déroutement d’un navire, braconnage maritime, etc.). Un tel système d’alerte doit toutefois permettre aux opérateurs d’être informés des anomalies éventuelles sans pour autant entraver l’exécution de leurs tâches principales.\rLe projet visait les objectifs suivants : (i) identifier et analyser la documentation disponible sur les systèmes d’alerte non intrusive, (ii) élaborer des concepts pour une interface d’alerte non intrusive à utiliser dans le GCCS-M et (iii) obtenir la rétroaction des experts de la Marine sur la valeur des options de conception.\rL’examen de la documentation a révélé l’absence d’une approche de conception unifiée et de recommandations associées dans le contexte d’alertes non intrusives. En outre, aucun document n’offrait de solution définitive au problème de la conception d’un système d’alerte non intrusive. Toutefois, on a pu extraire de la documentation des concepts pertinents pour la conception de systèmes d’alerte et d’alarme en général. Ces concepts, associés à des principes généraux touchant les facteurs humains, ont fourni des orientations pour la définition d’un certain nombre de concepts qui ont ensuite été examinés et évalués.\rLes recherches futures devraient viser à définir une conception d’interface de système d’alerte conformément aux principes de conception présentés ci-dessus, une fois que ces exigences de conception auront été validées.\r14. KEYWORDS, DESCRIPTORS or IDENTIFIERS (Technically meaningful terms or short phrases that characterize a document and could be helpful in cataloguing the document. They should be selected so that no security classification is required. Identifiers, such as equipment model designation, trade name, military project code name, geographic location may also be included. If possible keywords should be selected from a published thesaurus, e.g. Thesaurus of Engineering and Scientific Terms (TEST) and that thesaurus identified. If it is not possible to select indexing terms which are Unclassified, the classification of each should be indicated as with the title.)\ranomalies; visualization; alerts; non−intrusive; recognized maritime picture\rDRDC Toronto TR 2009-042 4","title":"Microsoft Word - Design Alert Final Report Apr 2 Final to DRDC.doc","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"intrus","time":1461915875398,"auto":true,"weight":1.0},{"@type":"Tag","text":"ticker","time":1461915875523,"auto":true,"weight":1.0},{"@type":"Tag","text":"anomali","time":1461915875476,"auto":true,"weight":1.0},{"@type":"Tag","text":"humansystem","time":1461915875445,"auto":true,"weight":1.0},{"@type":"Tag","text":"rmp","time":1461915875463,"auto":true,"weight":1.0},{"@type":"Tag","text":"alert","time":1461915875378,"auto":true,"weight":1.0},{"@type":"Tag","text":"interrupt","time":1461915875494,"auto":true,"weight":1.0},{"@type":"Tag","text":"alarm","time":1461915875510,"auto":true,"weight":1.0},{"@type":"Tag","text":"oper","time":1461915875430,"auto":true,"weight":1.0},{"@type":"Tag","text":"prioriti","time":1461915875413,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":2,"appId":"34f445a03877f7f2e16d28ce67b9d72f882715c2","timeCreated":1461915874360,"timeModified":1461915874485,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.031554088,"uri":"file:///Users/cheny13/Documents/2015vis/TVCG/papers/230-wang.pdf","plainTextContent":"230\rIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rThe Visual Causality Analyst:\rAn Interactive Interface for Causal Reasoning Jun Wang and Klaus Mueller, Senior Member, IEEE\rFig. 1. An overview of the Visual Causality Analyst framework running on the auto MPG dataset [29].\rAbstract—Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst – a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.\rIndex Terms—Visual knowledge discovery, Causality, Hypothesis testing, Visual evidence, High-dimensional data\r1 INTRODUCTION\rRecovering the causal relations from purely observational data is one of the ultimate goals for data analysts and a fundamental problem in science. After decades of efforts by many, causality research gained particularly strong attention when Judea Pearl, a long time pioneer of the field, won the Turing award in 2011 for the underlying mathematical framework of causal inference. The advantage of knowing the causal relations rather than just statistical associations, e. g., correlations, is that the former enables explicit guidance in predicting the effects of actions perturbing the observed system.\r  Jun Wang and Klaus Mueller are with the Visual Analytics and Imaging Lab at the Computer Science Department, Stony Brook University, Stony Brook, NY. E-mail: {junwang2, mueller}@cs.stonybrook.edu.\r  Klaus Mueller is also with the Computer Science Dept. at SUNY Korea.\rThe most reliable way to determine causation is by controlled experiments. However, controlled experiments are often either impossible or associated with high cost and thus impractical in real world. A loose detour usually taken is trying to express causation by correlations calculated from observational data. One typical example of this is the website Google Correlate [1] which can provide visitors with endless hours of entertainment by entering any search term and then browsing a long list of spurious correlations the term has with either time or US states. But while users of Google can easily tolerate the many irrelevant links the search engine typically generates, for other applications, such as healthcare diagnosis and financial prediction, blindly inferring causation from spurious correlations can have severe consequences.\rTo infer a precise model describing and measuring causal relations embedded in observational data, the theory of causal inference and analysis, started with the work of Pearl [2, 3], Spirts [4, 5], and others, has become a hot topic in recent years. While many\rManuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of\rManuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication x2x0 xAxuxg2. 021051;5d; adtaeteofocf ucrurrernetnvt evresrisoinonxx25xxOx c2t0. 1250.1F5.or information oFnoroibntfaoirnminagtiroenporintosbotaf itnhinsgarteipcrlein, tssenodf teh-ims ariltitcol:e,tvpclgea@secosmenpduter.org. e-mail to: tvcg@computer.org.\rDigital Object Identifier no. 10.1109/TVCG.2015.2467931\r1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\r\nWANG AND MUELLER: THE VISUAL CAUSALITY ANALYST: AN INTERACTIVE INTERFACE FOR CAUSAL REASONING 231\rmodern causal discovery algorithms claim that they can generate causal models with enough accuracy, they usually hold very strong assumptions on data distributions (e.g. Multinomial, or continuous with Gaussian sample error) that are hard to keep in practice, and make algorithms unstable when error relations are generated in early stages. Thus none can guarantee an answer that is accurate in the sense of being completely consistent with the real world. Even with the emergence of big data the automated derivation of a consistent causal model remains challenging because it requires a fundamental theory of how and why the observed phenomena occur. This in turn requires creativity with the human expert in the inference process. This is feasible when the model is sufficiently small, that is, the number of variables in the model is manageable. However, big data not only increases the number of observations, it also typically gives access to a greatly increased number of variables. These can help users build a consistent theory of the real world phenomena but the process is difficult to manage without visual support.\rThe system we describe in this paper, the Visual Causality Analyst, is a first step into creating such a visual support system. It offers various interactive and automatic tools for visual causal discovery. Following previous work on correlation maps [6] and Pearl’s depicting of the causality structure as a directed acyclic graph (DAG) [2, 3, 7] our framework visualizes the causal relations as an interactive spatial 2D layout, in which each edge connecting two variables implies a causal relation and the direction of an edge identifies the effect from the cause. Our interface also offers real time visual interactions where users are allowed to arbitrarily change the relations between variables and the impact of each modification is visualized simultaneously on the graph. Mathematical measurements of causal relations in the form of either linear regression analysis (targeting numeric variables) or logistic regression analysis (targeting categorical variables) are calculated and fed back along with the interactive operations, enabling users to explore potential causalities with statistical proof. Subsequently, these measurements are then also visualized in the spatial layout in terms of edge colors and opacities.\rThe main utility for computational causal inference lies in the conditional independence (CI) test, which is usually conducted via G-test or partial correlation. The former applies to discrete (categorical) data, while the latter applies to continuous (numerical) variables. None can handle both. We choose the partial correlation approach since our motivating domain application has mostly numerical variables and discretizing numerical variables into bins causes loss of detail [8, 9] which is undesirable. To go the other way, we are inspired by recent work of Zhang et al. [6] which for each pair of categorical and numerical variables reorders and repositions the levels of the categorical variable such that Pearson’s correlation between the pair is optimized. To accommodate the computational causal inference process we extend Zhang’s level reordering and repositioning mechanism from a single numerical variable to sets of numerical variables. This global optimization mechanism enables the causal inference algorithms to return plausible results on datasets containing both continuous and discrete data.\rOur paper is structured as follows. Section 2 discusses related work. Section 3 introduces theoretical background and contributions. Section 4 introduces our novel Visual Causality Analyst interface. Case studies on multiple datasets are given in Section 5, and Section 6 ends with conclusions and an outlook on future work.\r2 RELATED WORK\rAs mentioned, causality has been an active research topic and research on the visualization of causal networks has also emerged. In the following we shall briefly review this work.\r2.1 Causality Visualization\rA number of methods have been developed for the visualization of causality. The Hasse diagram is one of the earliest systems that have the ability to represent causal relations. It was originally introduced\rin order theory and has been adopted for demonstrating distributed systems [10], parallel processes [11], and many other information structures that contain causal events. However, since Hasse diagrams typically produce layouts with a large amount of intersecting edges and lack the ability to represent causal semantics, it can be difficult to comprehend them, especially when the number of variables is large and causal relationships are complex.\rGrowing-squares [12] and its enhancement growing-polygons [13] are both animated techniques that focus on visualizing sets of connected causal events called processes. The latter uses n-sided polygons to represent n processes and the gradual change of processes is visualized by animating the polygon’s change of size. However, the growing-polygons can only illustrate causality at the process level with very limited abilities of signifying causal relation strengths. Kadaba et al. [14] address these problems by depicting causal relations by node-link arrows and glyphs, leveraging simple animation of node sizes to indicate interactions between the factors and the target. While such a graph design can be effective for causality visualization, it only feeds back brief semantics of a causal relation, e.g. positive or negative. However, when explicit causal measurements need to be demonstrated on the graph, no existing causal visualization approach can give a plausible result. Wongsuphasawat and Gotz [15] describe a system that visualizes alternative pathway chains of temporal event sequences. While these chains suggest causal effects they are not casual networks. Also, their system focuses mainly on event flow visualization and has no support for interactive statistical causal reasoning.\rAccording to Pearl’s DAG patterns of causal structures [2], a 2D spatial graph layout of the network is a natural fit. Spatial graph layouts have been widely used in information visualization in various contexts. A related example is the visualization of Bayesian belief networks [16], in which the layout is guided by a temporal order, and multiple visual variables like color, node size, and proximity are used to represent network semantics. More recently, Zhang et al. [6] demonstrated an interactive correlation map with spatial representations. By ways of slider bars, users can filter edges corresponding to weak relations. Our work is inspired by these methods and we extend them for the visualization of causal relations, providing a suite of interactive utilities to manipulate the graph.\r2.2 Causality Representation and Inference\rOur framework provides automatic discovery of causalities in the data, thus causality representation and inference algorithms are closely related to our work. The causality system is often represented as Bayesian Networks (BN) [17, 18], in which causal relations are represented as dependencies measured by conditional probabilities. Algorithms recognizing BN structures usually require knowledge of the data distributions, which is difficult to achieve in practice especially with continuous data. For this type of data, Structural Causal Models [3, 2, 7, 19] which assume effects are linear functions of their causes plus Gaussian noise are better suited. The structure of this model is typically built via a multi-phase process involving a number of CI tests using partial correlations [20, 21, 22, 4]. Unfortunately, these algorithms often fail when categorical variables are included in the data. Introducing dummy variables is a standard technique in statistics [23], but the resulting significant increase in the number of variables may lead to an exponential increase in the number of CI tests needed, and also the mutual exclusions among dummies from the same variable can be very difficult to guarantee.\rFor the purpose of handling both categorical and numerical variables in a correlation network, Zhang et al. [6] recently proposed an algorithm that uses a pairwise optimization approach to reorder and reposition the levels of each categorical variable with respect to each numerical variable. The new levels are computed by maximizing Pearson’s correlation with the pair’s numerical variable. This approach is superior to other encoding methods like [24, 25] in that it provides both reordered and optimized distances between categories. However, in contrast to correlation networks, causal inference requires a global frame and so the algorithm proposed by\r2\r\n232 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rZhang et al. is not directly applicable. But it served as an inspiration for the global optimization approach we developed which computes the new level values of a categorical variable with respect to all numerical variables in the system.\r3 THEORETICAL BACKGROUND AND CONTRIBUTIONS\rOur causality analysis framework comprises the following three steps: (1) find all true CI relations embedded in the observational data (see Section 3.1), (2) build a DAG that is consistent (termed faithful) to all of these conditions (see Section 3.2), and (3) compute the causal strengths of the relations coded by the DAG (see Section 3.4). Steps 1 and 2 make use of correlation analysis where we require a novel transformation of categorical to numerical variables which we introduce in Section 3.3. Conversely, step 3 uses dedicated regression analyses where no such transformation is needed. Our treatment of steps 1-3 is necessarily brief and the reader is referred to the tutorials by Pearl [7] and Spirtes [26] for more detail.\r3.1 Causality Analysis and CI Test\rThe idea of causality analysis is based on counterfactual theory, which can be explained in terms of the form “If A had not occurred, B would not have occurred”. Although counter facts cannot be treated equally as causation on a philosophical level [27], causality analysis serves uniquely in telling us how a distribution would differ if external conditions were changed by treatments or interventions [7]. To achieve such functionality, CI tests are used as core instruments. The goal of a CI test is to find out whether two variables are related when the rest of the system is controlled, i.e., test the dependency of two variables while eliminating the impact of all other variables or at least a subset of them. This can be interpreted as a simulation of a controlled experiment on observed data.\rIn statistics, for some variables   and   in a numerical dataset, a CI test is equivalent to a test for zero partial correlations between   and   given a set of other variables   in the dataset. This is called conditioning on   [2, 3]. The partial correlation between   and   given   is defined as the correlation of the residuals from regressions of   on   and of   on  . In a dataset, the partial correlations from each pair of variables conditioned on all remaining variables form a partial correlation matrix. Such a matrix can be efficiently computed based on the correlation matrix  , so that with             , we have\r                                                                                                       \rwhere    and    are two variables, and                is the partial correlation of    and    given all other variables in the dataset. Then with the partial correlation matrix, we are able to find all potential causal relations. This process as a whole is often called feature extraction, which is the first step in many causal discovery algorithms [22, 20, 28].\rHowever, variables actually independent to each other may still be found causally related when conditioned on certain variables. Suppose a graduate school admits students only by the sum of one’s GPA and personal statement score. We may find these two scores negatively correlated within those who are admitted, as high GPA with low statement score or low GPA with high statement score is just enough for being admitted. But there is no apparent causal relation between the two scores in the real world. This means some variables (admission status in this example) cannot be conditioned on in finding the true CI relations between two variables. Such variables are called colliders and their descendants, and conditioning on them will generate false causations and introduce triangle patterns. The right set of variables to be conditioned on so that two variables can be deemed having a CI relation is called d-separating set. If no such set can be found in the dataset for a pair of variables, we can infer there is direct causation between them. All terms refer to [2].\r3.2 Causal Graph and Inference\rThe goal of causality analysis is to build a causal graph that is faithful to all the CI relations embedded in the observational data. A causal graph            is a DAG that consists of a set of vertices   denoting the variables and a set of directed edges   denoting the causal relations between two variables. Assuming there is no latent variable, the basic graph pattern of the causal relations among any three observed variables related to each other are: (1) a chain of causal influences (Fig. 2a), (2) a common cause influencing multiple variables (confounding, Fig. 2b), or (3) a common effect caused by multiple variables (selection bias, Fig. 2c). The first two patterns imply the same conditional independency which is “A is independent of B conditioning on C”. But the third pattern, also called the V- structure, is different as C is just the collider of A and B as mentioned in the previous subsection, thus the true independency of A and B can be recognized only when C is NOT conditioned on.\rHowever, in feature extraction we simply conditioned on all other variables and no causal relations are oriented, thus patterns as Fig. 2c would become an undirected triangle, and patterns in Fig. 2a and b would look the same. The resulting undirected graph is often called a Markov field or a moral graph depending on the author.\rHow to remove false links and orient the edges correctly is one of the major issues in modern causal inference researches. The usual procedure is to look at each pair of connected variables and conduct a subset search for colliders in variables forming triangles with them. If colliders are found then the two variables are disconnected and V- structures are recognized. This process costs a number of CI tests exponential to the number of variables forming triangles with each variable pair in the undirected graph. This is where the main computation cost lies. After all triangles have been processed a constraint propagation algorithm is run and a maximally but often partially oriented DAG is obtained [2, 4].\rIt is worth noting that partially oriented graphs returned by such a causal inference process only represent observationally equivalent classes [2] of true causal graphs, as there may be multiple DAG corresponding to the same set of CI relations. Expressed formally, for the generated   and some variables            in  ,\r                           or       in rea                                                                                        \rThis means that we will always need further verification to obtain the perfect causal graph in practice. Our system is purposed to help analysts in this verification task, using visualization to allow them to maintain their bearings on all levels of scale.\r3.3 Correlations of Categorical & Numerical Variables\rTo efficiently compute the partial correlation matrix for the CI tests, we need to calculate a correlation matrix first. While correlations between pairs of numerical variables and pairs of categorical variables can be achieved accordingly with Pearson’s correlation coefficient and Cramer’s V, traditional methods applicable for correlations between numerical and categorical variables, e.g.t-test, ANOVA, and MANOVA, have the problem that they are not normalized, so that one must consult significance tables to measure\rFig. 2. Three basic patterns of causal relations among any tree observed variables related to each other: (a) a chain of causal relations from A to B via C; (b) a common cause C influencing both A and B; (c) a common effect caused by both A and B.\r3\r\nWANG AND MUELLER: THE VISUAL CAUSALITY ANALYST: AN INTERACTIVE INTERFACE FOR CAUSAL REASONING 233\rVariable pair categorical/numerical origin/horsepower origin/weight origin/displacement origin/mpg origin/timeTo60mph\rPair-Opt Correlation 0.488 0.595 0.656 0.576 0.272\r(a)\rGlobal-Opt Correlation 0.476 0.561 0.637 -0.530 -0.272\rFig. 3. Effects of global optimization on the categorical variable origin from the auto MPG dataset [29]. (a) Comparisons between correlations of origin and several numerical variables under pairwise optimization and global optimization. The correlation value are similar in scale under two value mapping approaches. The different signs of correlations in the last two rows mean the level ordering after global value mapping is just the opposite to that after pairwise value mapping. (b) The parallel coordinate view of mpg, origin, and weight before global value mapping. (c) The parallel coordinate view of mpg, origin, and weight after global value mapping. We can see that both ordering and distances between categories are optimized, so that correlation is more visible than that in (b).\rthe association between two variables. However, the ability to handle mixed types of variables is often required in practical applications. Our solution to this problem makes use of Zhang et al.’s approach [6] which returns a maximized Pearson’s correlation between a pair of numerical and categorical variables using the following equation:\r                                                                                                             \rHere,       is the value assigned to level   of categorical variable    regarding to numerical variable    and          is the average of    correspondingtolevel  of   .Thiswillbringanoptimizedordering and distances of   ’s levels regarding   .\rThis method, however, is only partially useful for casual analysis because the level ordering and adjustment for a given categorical variable will be different for each numerical variable. This is fine for correlation analysis but causal reasoning requires global consistence of variable values in all CI tests. In the following we describe a novel generalization of the scheme of Zhang et al. that can achieve this.\rAn ideal globally consistent value mapping should be such that correlations between the categorical and all numerical variables are simultaneously maximized. A naïve idea would be to simply mediate all pairwise optimized values mapped from each numerical variable, setting up the target equation as,\r  \r                                                                                         \r            \rthe one with largest pairwise correlation with    , call it      . Let             be the decision function representing the process and         be the correlation function, then,\r                                                                                     \rWe are now ready to put together the final target equation, using            and     as weights in the outer summation of equation (4):\r                                                                               \r            \rHere we use   to denote           for convenience        \rin which we suppose there is a categorical variable   with   levels      \r                                                                                                   \r   \rWith equation (8), we can now assign numerical values to   ’s levels, which can be used consistently in causal inference processes.\rFig. 3a illustrates how this global optimization performs for the auto MPG dataset [29] (                      ). In the table, the first column gives the variable pairs, in which origin is a categorical variable and all others are numerical variables. The second column shows the correlation using Zhang et al.’s pairwise optimized assignment for each level of origin. The third column shows the (similar) correlations obtained with our global optimization method. For the last two variable pairs, the different sign of global from pairwise correlation means that the level ordering is just the opposite. Fig. 3b and c show two parallel coordinate tiles before and after the transformation, respectively. We observe that after the transformation, (1) categories (levels) that behave similarly are put close to each other; and (2) the correlation is more visible in the parallel coordinate plots.\r3.4 Regression Analysis\rAfter the structure of the causal graph model has been recovered, we need tools to model, measure, and test the causal relations statistically. In Pearl’s theory of Structural Causal Models [3, 2, 7], linear regressions are used as such tools. Linear regression measures the linear relationships between a dependent variable   and one or more explanatory variables                       , taking the form\rand   numerical variables                        in the dataset.        is the global optimized value we require for level   of   , and        is the pairwise optimized value for level   with regards to numeric variable   .\rHowever, with the empirical knowledge that strong causal relations typically lead to strong correlations (although this is not true reversely), the values of    ’s levels should more depend on numerical variables that are strongly correlated with it, but less on those are weakly correlated with it. This can be easily implemented by weighting the inner summation of equation (4) with the pairwise optimized correlation between    and   , namely   .\rIf two orderings of   ’s levels regarding two different numerical variables    and    are just opposite to each other, solving equation (4) will result in that all   ’s levels have similar values. The solution is to reverse one of two orderings so that the two become identical. This is equivalent to changing the sign of its pairwise optimized correlation weighting the inner summation. The mechanism to decide whether a level ordering should be reversed can be achieved by testing the sign of a correlation of orderings measurement, in which we consider    ’s level ordering      regarding to    as a standard, then reverse the ordering      with regards to    when the correlation of      and      is negative. The selection of    can be\r4\rWe found that satisfactory results can be achieved when      . Then by making (6) equal to 0 and differentiating on         , we can solve the optimization problem and obtain a closed formula,\r                                          \ras combining equation (3) we obtain,\r \r      As            only serves\ra normalization factor, also\r              \r                                             \r       \r\n234 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\r                                                                                 \rIn this equation, the subscript   indicates the  -th observation and    represents the intercept, which is interpreted as causal effects from latent factors (e.g. unobserved variables, sampling noise) in causality theory;    is the regression coefficient for   , which is also taken as the main measurement of causal strength. If    is a   level categorical variable, it is turned into       binary dummy variables, each standing for a level of    . Linear regression analysis can test the statistical significance of each explanatory variable via student’s t- test, as well as test the goodness of fit of the whole model via F-test, R-squared coefficients, and many other statistical utilities.\rOur framework uses logistic regression analysis to measure causal relations targeting categorical variables. Logistic regression analysis, although named “regression”, is actually a model of classification probabilities, i.e. the probability of the categorical variable taking a certain level. It is a better fit than linear regression analysis in models targeting categorical variables. It takes the form of a logistic function as:\r                                                           \r                                                                     \rin which    is the structural coefficient for    and measurement of causal strength, with error term    representing the disturbance from latent factors. If    is a categorical variable with   levels, it is turned into       dummy variables. Logistic regression analysis can also test for variable significances (via Wald statistics) and for goodness of fit (via deviance, likelihood ratio tests, and so on). Note here the optimized values of categorical variables are not used in either regression analyses, but only in causal structure inference processes.\r4 THE VISUAL CAUSALITY ANALYST\rIn the following we use the Auto MPG dataset [29] to illustrate our interface and the interactions we defined on it. This dataset has eight variables – one of them categorical (origin) – and 392 instances. Fig. 1 shows all elements of our interface. The main window contains the 2D spatial layout of the causal graph in the center and the regression analysis view on the right. The variable type window on the left opens when a new dataset is read in, allowing the user to indicate which of the variables are numerical and which are categorical.\r4.1 The Causal Graph Display\rThe causal graph is generated by the causal inference algorithm described in Section 3, using our global value mapping scheme for the categorical variables. The causal graph display provides an overview of all data dimensions in terms of their causal relations in variable space. In this display the vertices correspond to variables, laid out via a Fruchterman-Reingold force-directed model [30]. We set all edges to have the same natural strength so that vertices are uniformly spread on the canvas. The color of a vertex encodes the type of variable, blue for numerical and yellow for categorical. We use a different color for categorical variables since they will usually turn into dummy variables for the regression analysis. As such, each yellow vertex in the graph may correspond to multiple variables used in several regression analyses.\rThe edges of the graph link two variables in terms of their causal relationships. The direction icon on an edge encodes the direction of the causal relation, going from cause to effect. The colors of the direction icon encode the type of the causal relation. Green arrows encode positive relations, red arrows encode negative relations, and a yellow arrow emanating from a categorical variable corresponds to multiple relations between the target and dummies of the categorical variable. If the target variable is a categorical variable, the arrow will be yellow too. The reason to use yellow arrows is that complex causal relation involving categorical variables cannot be simply described as negative or positive.\rThe opacity of the edge encodes the amount of change that is exerted by the cause onto the effect, which is measured by regression coefficients. A more visible edge has a stronger effect. However, edges with too low opacities are often difficult to observe on the graph. Thus gamma correction is introduced such that for an edge connecting variable     and     with regression coefficient      , its opacity     is\r                                                                                                            \r \rwhere   is the normalizer to make all opacities lie in the range of\r     ,   is the gamma value, and   is the offset to guarantee minimum opacity. Usually we set         and         to avoid an edge to be rendered too weak to be observed, and at the same time rendering a strong edge evidently darker than a weak edge.\rBelow the graph in Fig. 1 is a control panel that allows users to run the causal inference algorithm – via the causal layout button – as well as add edges, give them cause-effect directions, and test these in the regression analysis after which a re-layout of the casual graph might be run. Slider bars allow the user to either filter away or enhance the opacity of weak edges. There is also a button to load new data which pops up the specification window on the left.\r4.2 The Regression Analysis View\rWhen a variable is selected the system computes the regression model for all variables with incoming casual edges to it. In statistics, the former variable is often called the response variable, while the latter are the predictor variables. The regression analysis view shows the linear regression coefficient for each predictor variable as well as the p-value to give an indication of. The fit of the overall model can be gauged by the R-square metric. It is 1.0 when the regression model fit perfectly. The R-square metric gains in meaning especially when it is used to compare regression models. If R-square decreases significantly when a predictor variable is dropped from the model then there is a good chance that this variable was required. Another test statistics our system reports is the F-value gauged by the F- statistics. The F-statistics is also particularly useful for comparing two competing models. We can write\r                                                                                            \rwhere     is the residual sum-of-squares (RSS) of a model and    is its degrees of freedom which is the number of observations minus the number of predictors minus 1. Let’s assume that      is the RSS for the model with fewer predictor variables. Assume      is higher than      which is the RSS of the model with more predictors, and     is higher than     since there are fewer predictors. Now, If the more complicated model is correct, we can expect the relative increase in RSS (going from the complicated to the simple model) to be greater than the relative increase in the degrees of freedom, or                                      . The significance of this increase can be tested via the F-statistics, but even informally, when   is large when a predictor variable is included in the model, we know that this predictor was valuable.\r4.3 Illustrative Example #1\rIn the graph of Fig. 1 the user has selected a few edges to highlight the causal flow they are part of. For example, the user marked all edges that link the miles per gallon (mpg) rating of a car to the factors that might cause this rating (and the physical process behind it). These factors are weight, origin, and model_year. The regression analysis window gives the statistical measurements and proofs for the identified causal models by means of linear regression and logistic regression analyses. Here we learn that weight has a strong negative effect (the regression coefficient is -0.647), while the effects of the other factors are rather mild. All but one effect is statistically significant – their p-values are less than 0.05.\r5\r\nWANG AND MUELLER: THE VISUAL CAUSALITY ANALYST: AN INTERACTIVE INTERFACE FOR CAUSAL REASONING 235\r4.4 Interaction with the Causal Graph Interface\rIt is often the case that true causalities may be missing or are wrong in a causal graph built from observational data. For this reason further verifications (hypothesis tests) on identified causal relationships are always needed. These verifications usually require modification of the causal graph by means of connecting two vertices and assigning the causal direction, reversing the direction of an edge, deleting an edge, or marking an edge as unknown (of direction). Furthermore, especially in the presence of large causal graphs, users will wish to focus on certain variables and their casual relationships while hiding all others.\rOur causal inference interface provides interactive utilities capable to perform all of the above functions with visual feedback. That is to say, whenever the causal graph is modified, the impact of the modification on the rest of the graph, e.g. direction icon colors and edge opacities, will be updated immediately. Vertices of variables of interest can be selected either in the graph or by marking them in the variable list. Edge selection is achieved by either clicking on them in the graph view or choosing them in the control panel. We note, however, that any deselected (hidden) variable should still be taken into consideration in the causal structure learning process as we need to condition on them in CI tests. If hidden variables are not considered then erroneous causal relationships might be inferred. This is similar to the case when important variables have not even been observed. In both cases our visual interface provides a helpful medium for human experts to recognize these false relationships and seek their resolution.\rCausality is subtle, and to test and measure it, we make use of the statistics and regression analyses tools described in Sections 3.4 and 4.2. We show the results of these analyses, such as coefficients and others in the regression analysis view whenever a causal graph is generated. The analysis view also provides automatic update on the analysis results whenever the graph is modified by the user. Finally, if an edge on the graph is selected, all regression analysis results involving it will be highlighted to made salient for the user.\r4.5 Illustrative Example #2\rFig. 4a shows another example for the Auto MPG dataset. Here, the user has decided to focus on the causal graph of weight, horsepower, and timeTo60mph, hiding all other variables and relationships. The graph implies that a light car with high horsepower tends to have short acceleration time, which is consistent with real world knowledge. However, we also see in the graph that high horsepower increases weight which would bring down timeTo60mph. To research this conflict we delete the edge from horsepower to timeTo60mph and observe (in Fig. 4b) that weight and timeTo60mph are now negatively related (the visualization updated accordingly). A visual indicator is that the edge opacity dropped compared to the opacity in Fig. 4a. This new relationship is inconsistent with common knowledge and it likely means that only considering weight cannot explain acceleration well. To explore this argument more deeply, a detailed statistical proof is needed. This proof can be provided by the regression analysis view of our framework.\rFig. 4c and d are two screen shots of the regression analysis view showing linear models of timeTo60mph, corresponding to the graph models in Figs. 4a and b, generated and updated automatically. We observe from Fig. 4c that when taking both horsepower and weight as causes, horsepower plays a much greater role (with regression coefficient -1.049) in effecting timeTo60mph than weight (with regression coefficient 0.632). When only regressing on weight, its regression coefficient is indeed negative (-0.343, Fig. 4d). However, the R-square coefficient in Fig. 4d is only 0.161, which is much lower than that in Fig. 4c where it was 0.622. This means that the linear model described in Fig. 4d is much worse than that in Fig. 4c, and we verified our previous guess that only considering weight will not explain acceleration well. Likewise the F-value drops by a large amount which also indicates that horsepower is a significant casual variable should not be dismissed.\rWe learn from this investigation that horsepower is indeed a good predictor for acceleration and that the apparent conflict due to the positive causal link between horsepower and weight might be related to the weight variable and not timeTo60mph. So we would continue our investigation there (see Section 5.1).\r5 CASE STUDIES\rWe demonstrate our framework with the following three datasets, the first of which we have already used in the previous example.\rAuto MPG dataset: This dataset contains 392 complete records of cars with 8 attributes: mpg, cylinders, displacement, horsepower, weight, timeTo60mph, model_ year, and origin, in which origin is a three-category nominal variable and all other variables are continuous. All car models in the dataset use gasoline and were built before 1983. This dataset was retrieved from the UCI Machine Learning Repository [29].\rSales campaign dataset: The dataset has been synthesized based on actual data describing the sales marketing and its effects on a company’s financials. There are 600 data samples each representing one salesperson, and 10 numeric variables: %Completed, #Leads, LeadsWon, #Opportunity, PipeRevn (pipeline revenue), ExpectROI (Return on Investment), Cost, Cost/WonLead, PlanRev (planned revenue), and planROI. This data set was previously adopted for demonstrating the correlation map by Zhang et al. in [6]. We can now make more explicit decisions by ways of causality analysis with our new interface.\rHeart disease Dataset: This is a realistic dataset on heart disease diagnosis, retrieved from the UCI Machine Learning Repository [29]. The dataset has 270 diagnosis records, each per person, with 7 categorical variables: sex, chestPainType, fastBloodSuger, restECD (electrocardiographic), angina, thalassemia, and disease; and 6 numeric attributes: bloodPressure, serumChol (Cholestoral), maxHeartRate, exerST(ST depression induced by exercise), slopeExerST, and numVessels (colored by flourosopy).\rFig. 4. The visual feedback and statistical analysis provided by the Visual Causality Analyst. (a) A visualized causal graph structure of variable weight, horsepower, and timeTo60mpg from the auto MPG dataset, generated by our framework. (b) The visual feedback after deleting the edge from horsepower to timeTo60mpg. Here weight becomes a negative cause of timeTo60mpg, which is inconsistent with common knowledge. (c) A screen shot of linear regression analysis targeting timeTo60mpg in line with the causal model of (a). (d) A screen shot of linear regression analysis targeting timeTo60mpg in line with the causal model of (b). Comparing (c) and (d) we can see that only regressing on weight cannot explain timeTo60mpg well, due to the dropping of R-and F-T est value from (c) to (d).\r6\r\n236 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rFig. 5. Causal reasoning on auto MPG dataset with the Visual Causality Analyst. (a) The causal graph generated by randomly assigning values to origin’s categories, which introduces several error edges. (b) The causal graph generated by assigning globally optimized values to origin’s categories. (c) The graph with regression coefficient threshold of 0.4. Weak causal relations are filtered away. (d) The graph relevant to mpg, which is a chain of causal relationships from cylinders to mpg. (e) The causal graph in which an edge from displacement to mpg is added and highlighted. (f) A screen shot of linear regression analysis on mpg without displacement. (g) A screen shot of linear regression analysis on mpg with displacement. We see that displacement has a large p-value in (g), also the F-Test in (g) is decreased from that in (f), so displacement should not be considered as a direct cause of mpg. (h) The parallel coordinate view of the variables related to mpg, in the order consistent to the causal relationships represented in (d). A clear flow of data variable relations can be observed.\r5.1 Causality Analysis: Auto MPG Dataset\rWe firstly present the basic concepts of our Visual Causality Analyst interface with the auto MPG dataset. Fig. 5a gives an initial casual graph generated by randomly assigning values to levels of origin. We see here that horsepower is mistakenly drawn as the positive cause of cylinders and displacement. It is common knowledge that these two edges should at least be reversed. The reason for such errors is typical. The feature extraction found an undirected edge between horsepower and origin with origin’s random level values, then cancelled it in the d-separating set search and directed the causal relation as horsepowe displacement and origin displacement. This error then spread in later processes and affected the direction between horsepower-cylinders.\rA better causal graph is shown in Fig. 5b, which is generated by using globally optimized level values of origin. Now we can see that all causal relations between horsepower, cylinders and displacement are correct. We also found that the categorical variable origin plays a weak (low edge opacity) cause of displacement and mpg. As origin will turn into dummy variables, it is represented by a yellow vertex. Also the arrows on edges leaving from origin are colored yellow as each denotes multiple coefficients. Fig. 5c shows an enhanced causal graph after setting the regression coefficient threshold to 0.4. All weak causal relations are filtered away. We now observe that origin and model_year are independent of all other variables, and the direct relation between horsepower and weight has also been eliminated.\rOur original purpose for this dataset was to predict car mpg and find the direct and indirect causes for it. Fig. 5c suggests that timeTo60mph and horsepower are not related to mpg since there is no causal edge pointing to it. Thus we may unselect them and only lay out those variables that have strong direct or indirect causal relations with mpg. Having done this we obtain the causal graph of\rFig. 5d, which is a chain of causal relations with four variables. Fig. 5h shows the parallel coordinates plot of these variables in the order of the causal chain. We can clearly observe a flow of associations from cylinders to mpg.\rThis chain is consistent with the mechanics of cars, at least when it comes to cars captured in this dataset. Adding cylinders to such a car increases its displacement, but not the other way around since we can also increase displacement by adding volume to the current set of cylinders. More displacement (and the power it affords) requires a heavier car for stability. But moving the extra weight around requires more gasoline, decreasing mpg.\r5.1.1 Interactive exploration of causal relationships\rOne may want to further explore the potential causal relationships that are not suggested by the graph in Fig. 5b. This can also be easily achieved with the interactive tools provided by our framework.\rFor example, the causal graph did not draw direct edges between displacement and mpg. However, we might wish to test the hypothesis if this causal relation actually exists. To do this, we can simply select the pair of variables as cause and effect, respectively, in the control panel and assign the edge. The resulting causal graph is shown in Fig. 5e, with the added edge highlighted. Colors and opacities of other edges on the graph may change accordingly if the causal relations they represent are affected by such operation.\rTo determine whether this causal relation holds, we need to refer to statistical analyses. Two screen shots of the linear regression analyses results before and after adding the edge are shown in Fig. 5f and g. Since the p-value of displacement from the student t-test is too large (p = 0.339) in Fig. 5g, together with the dropping of F-value, the direct causal relation between displacement and mpg should not be considered as existing. Hence there is no direct relationship between displacement and mpg. Raising the displacement of a car\r7\r\nWANG AND MUELLER: THE VISUAL CAUSALITY ANALYST: AN INTERACTIVE INTERFACE FOR CAUSAL REASONING 237\rFig. 6. Strategizing with the Visual Causality Analyst for the sales campaign dataset. (a) The causal graph generated from the dataset showing how the sales system works. (b) All the routes pointing to PipeRevn from some variable, indicating possible strategies to increase pipeline revenue. Here #Leads and Cost/WL are two variables that all routes start from. (c) Screen shots of linear regression analyses on PipeRevn and Cost. Here the purpose is to investigate the effect of Cost/WL on PipeRevn. As the scale of the direct effect of Cost/WL on PipeRevn is larger than the indirect effect via Cost, the total effect of rising Cost/WL will be the reduction of PipeRevn. (d) A correlation map view browsing only variables in a similar strategizing scenario. However, variables correlated to each other may not necessarily imply any causal relationship.\rusually will not directly lead to the decrease of its mpg. But when displacement increases, usually the car will be heavier (as mentioned before), which causes the mpg to reduce, since weight negatively changes mpg. In this case, weight serves as a mediator variable completing the chain of displacement and mpg.\rMany more conclusions can be drawn and many more explorations can be done from this single causality visualization. Therefore we believe our Visual Causality Analyst is powerful and effective for casual reasoning explorations, and the graphs in Fig. 5 may potentially be helpful for consumers to select cars, as well as for car manufacturers to balance their offering of models.\r5.2 Strategizing: Sales Campaign Dataset\rIn this example, we use the Sales Campaign dataset to show how business executives may analyze sales behaviors and strategize with our Visual Causality Analyst software.\rTo give some background, a sales pipeline typically starts with a lead generator responsible for developing prospective customers called leads with whom salespersons may actually close deals. Leads may become won leads when they give positive feedback and then opportunities when they offer further interests. For each won leads, an increased sales pitch at cost per won lead (cost/WL) will be invested. The goal of the entire sales effort is to increase the expected return on investment (ExpectROI), and ultimately maximize pipeline revenue (PipeRevn). In [6] a correlation map was used on the same dataset to showcase its features. In the following,\rwe will demonstrate that by upgrading to the Visual Causality Analyst, the decision making process becomes even more explicit.\rSuppose a team of sales data analysts in the company are busy analyzing the sales strategy for the following year, basing on last years’ data from their sales teams. Their first step is to build the causal graph of all data attributes to get an overview on how their sales system actually functions. This process is straightforward – import the data with an Excel spreadsheet and lay out the initial causal graph shown in Fig. 6a.\r5.2.1 Strategy development\rAfter drawing the sales system’s causal graph, the analysts proceed in developing effective business strategies using our interface.\rTo achieve the goal of increasing the pipeline revenue, the analysts first filter out weak relations in the graph and select a series of related causal relations, highlighted in Fig. 6b. These relations form several routes starting from some variables and finally pointing to PipeRevn. Clearly on the graph, the variable #Leads is the starting point of multiple routes to the final goal. In all of these routes, #Leads plays a positive factor for PipeRevn, which means increasing the former will lead to an increase of the latter in the end. So the first strategy might be to generate more leads, i.e. reach out to more people to look for potential customers, simply and clearly.\rAnother variable related to the goal of the sales data analysts is Cost/WL which is the sales pitch invested into each won lead. However, the effect of increasing this variable can have both a\rFig. 7. Analyzing heart disease dataset with the Visual Causality Analyst. (a) An original causal graph generated by the new framework. As there are multiple categorical variables, many nodes and arrows are colored yellow. (b) The causal graph targeting disease, in which only variables and relations relevant to disease are selected and shown. (c) Screen shots of logistic regression analyses on each of the three restECD’s categories. Only electrocardiographic type restECD-2.0 is found as a sign of heart disease, due to the positive regression coefficient and small p- value of disease in its logistic model. (d) Screen shots of logistic regression analyses on each of the four chestPainType’s categories. The last type chestPainType-4.0 should be considered positively relevant to heart disease, while other types are either irrelevant or not a sign of heart disease.\r8\r\n238 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rpositive and a negative effect on PipeRevn. The positive effect is through the variable Cost, which is the salesman’s total investment. The negative effect can be both direct and indirect through multiple routes. To resolve this conflict, we can refer to the coefficients analysis view of the software. Here we present two screen shots in Fig. 6c. They show that the direct effect of Cost/WL in the linear regression model of PipeRevn is larger than that of the indirect relation (Cost/WL   Cost) × (Cost   PipeRevn). In addition, the coefficient of ExpectROI on PipeRevn is more than twice that of Cost, and Cost/WL negatively impacts ExpectROI. Thus, to increase PipeRevn we are advised to not increase Cost/WL – in fact, we might rather decrease it.\rThese two strategies essentially imply that, to increase the revenue, each of the company’s salespersons should put more effort in exploring new customers. Further, the model indicates that once a potential customer has already shown interest, there is no need to invest extra promotions. It may even have some negative impact on closing the deal.\rThe strategic guidance our Visual Causality Analyst can provide is explicit and assuring, partly due to its visualization of the casual relationships and partly due to its interactive response rates. When users see the causal graph, they can visually think and form hypotheses that a certain action might potentially lead to a certain outcome. Further, via the regression analysis the size of the effect can also be measured and communicated. This is a significant improvement over the correlation map proposed in prior work (Fig. 6d), by browsing which, users may learn how two variables are correlated in past data (e.g. Cost/WL and ExpectROI, #Leads and ExpectROI etc.), but variables strongly correlated to each other may not necessarily imply any causal relationship. And thus, adjusting a variable just based on the correlation map alone will not necessarily lead to the expected change in another variable in the real world.\r5.3 Analyzing Categories: Heart Disease Dataset\rIn this final example we will demonstrate how the Visual Causality Analyst can also be used to visually analyze the causal relationships in medical data that include mixed types of variables.\rSuppose an expert on cardiology has been keeping a collection of medical records on his past patients and wishes to identify the most effective methods for diagnosing heart diseases. The expert opens our software and imports his data, then generates the initial causal graph shown in Fig. 7a. Since there are multiple variables that are categorical, we observe many nodes and arrows on edges that are colored yellow.\rAny edges on the graph directly pointing to and from disease indicate either diagnostics (the outgoing edges) or causes (the incoming edges) of heart disease. These edges and the causal relationships they represent are of greatest interest to the expert. From Fig. 7a he learns that restECD, numVessels, maxHeartRate, serumChol, chestPainType, and thalassemia are all variables directly linked to disease. Thus he unselects all other variables and re-lays out the graph, which yields Fig. 7b.\rIn Fig. 7b, the categorical variable restECD has three levels where each represents a type of electrocardiographic test result. To test which type of electrocardiographic result is caused by heart disease, we need to consult the logistic regression analysis. Fig. 7c is a screen shot of the analysis result targeting each of restECD’s level. In the first model, disease has a negative coefficient and a small p- value, which means restECD-0.0 is not a sign of heart disease, or even a sign of a healthy heart. In the second model, although disease has a positive coefficient, its p-value is too large. The values of other statistical metrics, such as low Chi-Squared value, high model p- value, low deviance, etc. all indicate that restECD-1.0 is likely irrelevant to heart disease diagnosis. The last logistic model, restECD-2.0, disease shows both a positive coefficient and a small p-value, and therefore this test seems to be a valuable means to\rdiagnose an impending heart disease. The expert is satisfied having succeeded in finding an effective means for heart disease diagnostics from his treasure trove of data.\rA similar process can be conducted on the variable chestPainType. The logistic regression analysis targeting each of its four categories is shown in Fig. 7d. Here we observe that only chestPainType-4.0 has both a positive coefficient and a zero p-value. Other statistical features of this model, e.g. high Chi-Square value and high deviance also indicate that chestPainType-4.0 should be considered a sign of heart disease. Other types of chest pains are either irrelevant (chestPainType-1.0) or not a sign of disease (chestPainType-2.0 and 3.0).\rThere are many more hypotheses that the expert might discover, test and prove or disprove given his data and using our software. We cannot list them all here. But the case study presented shows that the Visual Causality Analyst is well applicable for health sciences data, as well as all other scientific dataset with mixed types of variables.\r6 CONCLUSION AND FUTURE WORK\rWe have presented the Visual Causal Analyst – the first interactive framework for visual causal reasoning and visual causal discovery for high-dimensional data. An added novelty of our framework is that it supports both numerical and categorical variables, which is important for real-world applications. Our interface can serve both as a causality exploration environment and as a platform to visually demonstrate, explain, and justify causal relations that exist in the data with statistical proof provided by linear regression analyses and logistic regression analyses. Our framework is general and applicable to a wide set of real cases, as demonstrated by our case studies.\rA present limitation of our framework is that causal relations may exist and vary in different data clusters. Therefore a prior visualization and possibly clustering of the data might be advised. Interactive clustering algorithms, such as ClusterSculptor [31], would allow users to first isolate an independent data cluster and then deduce causalities only on this partial data.\rFuture work will also focus on visualizing the test statistics, such as R-squared and F-value directly in the visualization. Since the comparison of models (that is, configurations with certain causal edges missing or added) is a frequent task, we might add an information visualization widget that would allow users to compare the values of the test statistics for these alternative models and convey the statistical relevance of the different values.\rAnother frontier is the ability to perform visual causal reasoning with time series data. This is of great interest to scientists, policy makers, economists, etc. Although we can deal with time series data by simply treating time as a data variable, a dedicated visual analytic approach will be better, possibly using Granger causality.\rFinally, we should note that causality can be affected by outliers, non-linear relationships, heteroskedasticity, and multicollinearity. To achieve more statistical robustness, techniques for outlier detection and removal, non-linear causality need to be added to our system.\rACKNOWLEDGEMENTS\rThis research was partially supported by NSF grant IIS 1117132 and the MSIP (Ministry of Science, ICT and Future Planning), Korea, under the “ICT Consilience Creative Program” supervised by the IITP (Institute for Information & communications Technology Promotion)\". Partial support was also provided by the US Department of Energy (DOE) Office of Basic Energy Sciences, Division of Chemical Sciences, Geosciences, and Biosciences. Some of this research was performed in the Environmental Molecular Sciences Laboratory, a national scientific user facility sponsored by the DOE’s OBER at Pacific Northwest National Laboratory (PNNL). PNNL is operated by the US DOE by Battelle Memorial Institute under contract No. DE-AC06-76RL0.\r9\r\nWANG AND MUELLER: THE VISUAL CAUSALITY ANALYST: AN INTERACTIVE INTERFACE FOR CAUSAL REASONING 239\rREFERENCES\r[1] \"Google Correlate,\" [Online]. Available: http://www.google.com/trends/correlate/. [Accessed 20 March 2014].\r[2] J. Pearl, Causality: Models, Reasoning and Inference, Cambridge University Press, 2000.\r[3] J. Pearl, \"Causal diagrams for empirical research,\" Biometrika, vol. 82, no. 4, pp. 669-688, 1995.\r[4] P. Spirtes, C. N. Glymour and R. Scheines, Causation, Prediction, and Search, Berlin: Springer Verlag, 1993.\r[5] P. Spirtes, C. Glymour and R. Scheines, \"Causality from probability,\" Philosophical Studies, vol. 64, no. 1, pp. 1-36, 1991.\r[6] Z. Zhang, K. T. McDonnell, E. Zadok and K. Mueller, \"Visual Correlation Analysis of Numerical and Categorical Data on the Correlation Map,\" IEEE Transactions on Visualization and Computer Graphics, vol. 21, no. 2, pp. 289-303, 2015.\r[7] J. Pearl, \"An Introduction to Causal Inference,\" The International Journal of Biostatistics, vol. 6, no. 2, pp. 1557-4679, 2010.\r[8] P. Royston, D. G. Altman and W. Sauerbrei, \"Dichotomizing continuous predictors in multiple regression: a bad idea,\" Statistics in medicine, vol. 25, no. 1, pp. 127-141, 2006.\r[9] H. Wainera, M. Gessarolib and M. Verdib, \"Finding what is not there through the unfortunate binning of results: The Mendel effect,\" Chance, vol. 19, no. 1, pp. 49-52, 2006.\r[10] C. Rehn, \"A Definition of Data Consistency Using Event Lattices,\" in\rProceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, 2004.\r[11] L. Viennot, \"Parallel N-free Order Recognition,\" Theoretical Computer Science, vol. 175, no. 2, pp. 393-406, 1997.\r[12] N. Elmqvist and P. Tsigas, \"Animated visualization of causal relations through growing 2D geometry,\" Information Visualization, vol. 3, no. 3, pp. 154-172, 2004.\r[13] N. Elmqvist and P. Tsigas, \"Causality visualization using animated growing polygons,\" in IEEE Symposium on Information Visualization, 2003.\r[14] N. R. Kadaba, P. P. Irani and J. Leboe, \"Visualizing Causal Semantics using Animations,\" IEEE Transactions on Visualization and Computer Graphics, vol. 13, no. 6, pp. 1254-1260, 2007.\r[15] K. Wongsuphasawat and D. Gotz, \"Exploring Flow, Factors, and Outcomes of Temporal Event Sequences with the Outflow Visualization,\" IEEE Transactions on Visualization and Computer Graphics, vol. 18, no. 12, pp. 2659-2668, 2012.\r[16] J.-D. Zapata-Rivera, E. Neufeld and J. E. Greer, \"Visualization of Bayesian belief networks,\" in Proceedings of IEEE Visualization’99, Late Breaking Hot Topics, 1999.\r[17] N. Friedman and D. Koller, \"Being Bayesian About Network Structure. A Bayesian Approach to Structure Discovery,\" Machine Learning, vol. 20, no. 1-2, pp. 95-125, 2003.\r[18] S. Nadkarni and P. P. Shenoy, \"A Bayesian network approach to making inferences in causal maps,\" European Journal of Operational Research, vol. 128, no. 3, pp. 479-498, 2001.\r[19] J. Peters, J. M. Mooij, D. Janzing and B. Schölkopf, \"Causal Discovery with Continuous Additive Noise Models,\" The Journal of Machine Learning Research, vol. 15, no. 1, pp. 2009-2053, 2014.\r[20] Z. Wang and L. Chan, \"An Efficient Causal Discovery Algorithm for Linear Models,\" in Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, 2010.\r[21] K. Baba, R. Shibata and M. Sibuya, \"Partial correlation and conditional correlation as measures of conditional independence,\" Australian & New Zealand Journal of Statistics, vol. 46, no. 4, pp. 647-664, 2004.\r[22] J.-P. Pellet and A. Elisseeff, \"Using Markov Blankets for Causal Structure Learning,\" Journal of Machine Learning Research, vol. 9, pp. 1295-1342, 2008.\r[23] B. G. Tabachnick and L. S. Fidell, Using multivariate statistics, New York: Harper & Row, 2001.\r[24] S. Ma and J. Hellerstein, \"Ordering categorical data to improve\rvisualization,\" in IEEE symposium on information visualization, 1999.\r[25] J. Cohen, P. Cohen, S. G. West and L. S. Aiken, Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.), Routledge, 2002.\r[26] P. Spirtes, \"Introduction to Causal Inference,\" Journal of Machine Learning Research, vol. 11, pp. 1643-1662, 2010.\r[27] S. L. Morgan and C. Winship, Counterfactuals and causal inference, Cambridge: Cambridge University Press, 2007.\r[28] T. S. Verma and J. Pear, \"Equivalence and synthesis of causal models,\" in The Sixth Annual Conference on Uncertainty in Artificial Intelligence, Mountain View, 1990.\r[29] M. Lichman, {UCI} Machine Learning Repository, University of California, Irvine, School of Information and Computer Sciences, 2013.\r[30] T. M. Fruchterman and E. M. Reingold, \"Graph drawing by force- directed placement,\" Software: Practice and experience, vol. 21, no. 11, pp. 1129-1164, 1991.\r[31] E. J. Nam, Y. Han, K. Mueller, A. Zelenyuk and D. Imre, \"ClusterSculptor: A Visual Analytics Tool for High-Dimensional Data,\" in Visual Analytics Science and Technology, 2007.\r10","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"horsepow","time":1461915874466,"auto":true,"weight":1.0},{"@type":"Tag","text":"variabl","time":1461915874373,"auto":true,"weight":1.0},{"@type":"Tag","text":"mpg","time":1461915874387,"auto":true,"weight":1.0},{"@type":"Tag","text":"regress","time":1461915874399,"auto":true,"weight":1.0},{"@type":"Tag","text":"coeffici","time":1461915874436,"auto":true,"weight":1.0},{"@type":"Tag","text":"edg","time":1461915874409,"auto":true,"weight":1.0},{"@type":"Tag","text":"displac","time":1461915874422,"auto":true,"weight":1.0},{"@type":"Tag","text":"causal","time":1461915874360,"auto":true,"weight":1.0},{"@type":"Tag","text":"diseas","time":1461915874485,"auto":true,"weight":1.0},{"@type":"Tag","text":"dataset","time":1461915874449,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":19,"appId":"ca3c84166d69932bee47026dc3233228d88ac648","timeCreated":1456673494065,"timeModified":1461915876930,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.031554088,"uri":"file:///Users/cheny13/Downloads/visual_text_mining_2011_camera_ready.pdf","plainTextContent":"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/262329119 Seeing beyond reading: A survey on visual text\ranalytics\rARTICLE in WILEY INTERDISCIPLINARY REVIEWS: DATA MINING AND KNOWLEDGE DISCOVERY · NOVEMBER 2012 Impact Factor: 1.59 · DOI: 10.1002/widm.1071\rCITATIONS READS\r7 123\r3 AUTHORS, INCLUDING:\rMaria Cristina F. Oliveira\rUniversity of São Paulo\r117 PUBLICATIONS 1,207 CITATIONS SEE PROFILE\rAvailable from: Maria Cristina F. Oliveira Retrieved on: 01 February 2016\r\nArticle type: Overview\rSeeing beyond reading: a survey on vi-\rsual text analytics DMKD-00112\rAretha B. Alencar, Maria Cristina F. de Oliveira, and Fernando V. Paulovich\rInstituto de Ciências Matemáticas e de Computação (ICMC) Universidade de São Paulo (USP)\rSão Carlos/SP, Brazil\rAbstract\rWe review recent visualization techniques aimed at supporting tasks that re- quire the analysis of text documents, from approaches targeted at visually sum- marizing the relevant content of a single document to those aimed at assisting exploratory investigation of whole collections of documents. Techniques are organized considering their target input material – either single texts or collec- tions of texts – and their focus, which may be at displaying content, emphasiz- ing relevant relationships, highlighting the temporal evolution of a document or collection, or helping users to handle results from a query posed to a search engine. We describe the approaches adopted by distinct techniques and briefly review the strategies they employ to obtain meaningful text models, how they extract the information required to produce representative visualizations, the tasks they intend to support and the interaction issues involved, as well as strengths and limitations. Finally, we show a summary of techniques, highlight- ing their goals and distinguishing characteristics. We also briefly discuss some open problems and research directions in the fields of visual text mining and text analytics.\rKeywords\rInformation visualization, Text mining, Visual analytics, Text analytics.\rTextual documents are widely available in digital format and provide a rich source of data and information. Nevertheless, accessing and interpreting such information poses a major challenge to human analysts working in a variety of domains and situations. Even the lay person faces difficulties in identifying, handling and selecting relevant material from the many sources available. This scenario motivates a rising number of text analytic applications that embed vi- sual representations to assist humans in tasks that require inspection of textual material. In this article we review recent visualization techniques being applied in this context. We provide an overview of visualizations aimed at supporting a variety of tasks, from approaches targeted at displaying the relevant con- tent information in a single document to those aimed at displaying document\r1\r\ncollections. We briefly discuss issues involved in obtaining representative visu- alizations, as well as the strengths and limitations of specific approaches.\rVisualization techniques vary in how they pre-process and represent text. Many techniques adopt the standard “bag-of-words representation” from information retrieval [1], which models text content as a set of words (or terms), each with an associated frequency count. For single documents and simple tasks, this straightforward vector representation suffices to create appealing visual- izations. It is also adopted in many techniques that display document collec- tions, as it allows inferring document dissimilarity based on comparing shared word frequencies. Other techniques extract topics or other entities with se- mantic meaning, which typically requires more elaborate and computationally expensive pre-processing. Moreover, a text also embeds structural organiza- tion at multiple levels and has associated attributes, or metadata, describing additional properties. Structural information is sometimes employed on visual- izations that attempt to convey semantically richer information, whereas many visualizations focusing on document relationships or their evolution along time usually consider metadata such as authorship, citations or publication date.\rVisualizing Documents\rMany simple visualizations of a single document simply show relevant words, or terms, considering frequency of occurrence as a relevance measure. Tag-clouds are currently a very popular visual metaphor. It presents a list of frequent terms in alphabetical order, with term frequency mapped to font size – as exemplified, e.g., by the TagCrowd web application [2]. An improved visual representation, Wordle [3, 4], adopts a heuristic to optimize usage of the available visual area. Seifert et al. [5] also introduce an approach to render compact visualizations, in this case constrained to the interior of convex poly- gons of arbitrary shapes. Figure 1 shows visualizations obtained employing TagCrowd and Wordle on the text of the testimony of William Jefferson “Bill” Clinton, former President of the United States, on his impeachment trial in 1999.\rThis simple approach does not guarantee, however, that sequences of related words will be placed close or sequentially in the visual representation. In ManiWordle [6] users are given flexible control of the layout produced by Wordle by supporting custom manipulations. Alternatively, a clustering algorithm has been employed to identify groups of similar terms, given by their co-occurrence in the text, and then create a visual representation that shows these clusters explicitly [7].\rOn a different line, Oelke and Keim [8] propose a strategy suitable to explore extracted or calculated features that characterize documents, such as vocabulary richness or sen- tence length. These features represent documents at multiple levels of detail, from words to sentences and chapters. The visual representation is very simple: parts of the text (e.g., words or sentences) are mapped to screen pixels, with pixel color indicating the value of their associated features. Tests have shown that such simple visualiza-\r2\r\ntions result in text “fingerprints” that are very useful to characterize texts and identify authorship.\rApproaches based on term frequency, albeit appealing, cannot convey semantic rela- tionships amongst terms. Several alternative visualizations attempt to overcome this limitation, e.g., representing a text as a tree that is rendered so as to enable fast content exploration. This is the underlying rationale of Word Tree [9], which creates a tree with nodes representing terms and branches linking sequential terms, called a “suffix tree”. Users can navigate on a text by selecting a word or groups of words, and checking all sentences that include them, enabling rapid exploratory queries. Figure 2(a) presents a Word Tree representation of the contexts including the word ‘sexual’ in Clinton’s speech. Similarly, DocuBurst [10] adopts a radial space-filling layout to show seman- tic relations amongst terms, additionally mapping term frequency to font size.\rAimed at supporting more detailed analyses Phrase Nets [11] builds a graph where nodes represent the words and edges represent some user-specified relationship be- tween them, defined either at the syntactic or the lexical levels. Figure 2(b) presents the visual outcome of Clinton’s speech considering the clause “is” as the target rela- tionship between words. Font size is proportional to the number of word occurrences in a match; the thickness of an arrow between two words is proportional to how many times they occur in the same phrase. Darker font colors indicate a word more likely to be found in the first slot of a pattern. Rusu et al. [12] rely on natural language process- ing tools to create a directed graph that embeds semantic information, thus extending the tree representation. This solution shows existing relationships between words at a more refined level.\rAnother focus for text analysis is on detecting changes in the narrative flow. Miller et al. [13] address this issue considering a textual document as a signal defined by its terms. A wavelet transform is applied to this signal, and the visual outcome is a wave layout that can support the identification of thematic changes. Mao et al.[14] also represent documents as curves that summarize sequential trends. Abrupt changes within documents may be identified inspecting their curvatures, thus overcoming the lack of sequential information incurred when representing documents as simple word histograms.\rThere are also contributions concerned with conveying document modifications along time. Information on when and how documents are created and edited is important to understand collaborative dynamics within communities, as in Wikipedia – the largest public wiki meant as a free online encyclopedia. Viégas et al. [15] proposed the tech- nique history flow to highlight editions in a page, emphasizing what survives (or not) along time. A particular version of a document is represented by a vertical “revision line” with length proportional to the text length. Authors are identified by colors, with the “revision line” formed by sections colored to reflect their original authors. Text sections that have been preserved across consecutive versions are visually linked. In the visualization shown in Figure 3 a user has selected part of a “revision line”, and the linked text panel at the right side shows the text of the corresponding version, high- lighting the contributions by the author.\r3\r\nVisualizing Document Collections\rWhen visualizing collections of documents, rather than individual pieces of text, doc- ument maps are a popular metaphor. Document maps are visualizations that spa- tially reflect some relationship among documents, providing a navigation interface useful to access information and improve human capability of solving real knowledge- management problems [16]. Map-based metaphors are appealing because they some- how mimic cartographic maps, intuitive to most users.\rTwo solutions for displaying document collections that rely on the familiar map metaphor are the Cartographic Maps [17] and Galaxies [18] systems. The former generates a vi- sualization similar to a geographic map, whereas the later incorporates visualizations that resemble a night sky, for a global view. Both metaphors are available in the IN- SPIRETM [19]documentvisualizationsystem.\rFrom the existing methods to create documents maps, multidimensional projection techniques are possibly the most common [20]. Projection maps represent documents by graphical markers arranged in the visual space so that their proximity reflects the content similarity of their corresponding documents: close markers indicate similar documents, distant ones indicate content-wise uncorrelated documents. Projection techniques usually take as input a frequency-based vector space model of the collection, or a vector describing topics or other extracted features. Alternatively, some techniques only require a matrix describing dissimilarities, or distances, among all document pairs.\rMany issues must be considered when deriving low-dimensional spatial layouts to dis- play document collections, handled in different ways by numerous techniques avail- able. The Least Square Projection (LSP) [20] adopts a strategy of seeking to preserve local data neighborhoods instead of pure dissimilarity between documents. LSP builds and solves a Laplacian system to place each document within the convex hull of its nearest neighbors. For high-dimensional sparse spaces, which is typically the case in vector space (bag-of-words) representations, LSP has been shown to be more ef- fective in revealing groups of similar documents, as compared to distance-preserving approaches.\rFigure 4(a) shows a map, obtained with LSP, of a collection of scientific papers – con- tent considered includes title, authors, abstract and references – in four different areas (indicated by the colors), namely Case-Based Reasoning, Inductive Logic Program- ming, Information Retrieval and Sonification. The map has been annotated with in- formative labels obtained with an automatic topic extraction technique based on term covariance [21].\rAutomatic topic extraction is, indeed, a critical problem, as text visualizations must display informative labels. This may be addressed with data mining algorithms, e.g., Lopes et al. [22] propose an association rule mining strategy to identify meaningful term associations indicative of relevant topics. The strategy is a good example of cou- pling text mining with visualization: in a similarity-based map, users brush the visual- ization to delimit a group of documents. These are input to the rule mining algorithm, which in turn outputs meaningful term associations for labeling the selection.\r4\r\nAlthough document maps can speed-up tasks that require interpreting document collec- tions, they face some critical problems, such as the overlapping of graphical elements and the cognitive overload faced by users in layouts that show many documents at once. Hierarchical strategies have been developed to handle such limitations, allowing users to view maps at multiple levels of detail, departing from large clusters of similar documents and gradually drilling down and navigating until reaching small groups and individual documents.\rInfoSky [23] offers an interesting approach for hierarchically organized document col- lections. A recursive Voronoi subdivision of the visual space is used to display the hierarchy and users can zoom in or out at certain areas of the projection, analogous to operating a telescope. For collections with no hierarchical structure, the Hierarchi- cal Point Placement (HiPP) [24] projection employs a recursive partitioning process to automatically infer a cluster tree from the data. Tree nodes are projected to create a multilevel visualization of groups and sub-groups of documents depicted as circles within circles. Placement of circles in the visualization reflects the overall similarity of their containing documents. Figure 4(b) shows a document map created with HiPP for the same scientific paper collection depicted in Figure 4(a).\rIn point of fact, collections of scientific papers provide an ever growing body of data for visualization and pose challenges of their own. The body of techniques suitable to visualize the domain structure of scientific disciplines is generally known as “knowl- edge domain visualizations” [25]. Visual representations range from the already men- tioned content-based document maps to graph layouts depicting authorship or citation networks, enriched with domain specific interaction functionalities, as discussed later on.\rOther visualizations to support exploratory analysis of document collections focus on properties and attributes other than those considered in creating document maps. Doc- ument Cards [26] presents a quick overview of either collections or single documents aimed at enhancing browsing capability on display devices of different sizes. The technique adopts the rationale of top trumps game cards, which use expressive images and facts to provide a combined overview of an object. With a similar intent, Document Cards visualizations highlight important key terms and representative images extracted from a document. This solution is suitable to provide a compact visualization of a large document collection, nonetheless it fails to show inter-document relationships.\rVisualizing Document Collections over Time\rTime-related attributes also establish relevant relationships in collections such as news corpora, email archives or scientific articles, which have an associated time stamp in- forming the date/time a news piece was reported, an email sent, or an article was pub- lished. Although often ignored, the temporal component is critical for understanding and analyzing topical changes in such time-stamped document collections. This is a difficult problem that has been attracting increased attention over the last years.\r5\r\nSeveral contributions attempt to adapt existing document visualizations to handle time- stamped collections, e.g., time-oriented variations of tag clouds: SparkClouds [27] dis- play a sparkline (a minimal simplified line chart) under each term to show its frequency variation over time; Cui et al. [28] introduced a visualization method that couples a trend chart with tag clouds at each time point, trying to preserve semantic coherence and spatial stability of the terms. The trend chart shows the significance of each tag cloud along time, which is higher when the tag cloud conveys more information by itself with less information shared by surrounding tag clouds.\rThe “river” metaphor is often applied in time-oriented visualizations with information flowing from left to right through time. The ThemeRiver [29] is a visualization that indicates temporal variation adopting this metaphor. It is intended to display temporal thematic changes in a document collection by highlighting selected topics represented by single words. Individual topics are visually represented as colored “streams” within the river. Flow width indicates topic strength, and the width of the river at a specific time instant depicts the collective strength of the selected topics.\rFigure 5 shows a visualization created with ThemeRiver from documents related to the Cuban Missile Crisis. This visual representation includes a river of topics (words), a timeline below the river, and markers manually added by the authors along the top to identify related historical events. The TIARA system (Text Insight via Automated Re- sponsive Analytics) [30] adopts a similar metaphor to depict temporal evolution of the topical content in collections of news or emails. TIARA relies on a more sophisticated strategy to identify topics, based on Latent Dirichlet Allocation (LDA) [31], a statis- tical approach applicable for summarizing texts into topics (represented as vectors of weighted words), and deriving time-sensitive keywords.\rThe same metaphor is employed in TextFlow [32], now in a more complex scenario in which topics events – like topic birth, split, merge and death – are detected and visualized. In this technique, first a set of topics, along with merge and splitting rela- tionships, are automatically extracted with an incremental hierarchical Dirichlet pro- cess [33]. Given this first output, topic events such as birth, death, splitting and merging are detected. To help users to better identify topic content and understand the major reasons behind critical events, the system also detects keyword correlations by extract- ing terms from each document, counting their co-occurrences and displaying the top frequent keywords. This information is then visually presented in a river flow layout, formed by three layers: flows that represent the topics; glyphs that represent critical events, overlaid at the time points where they occur; and threads (blue lines) that rep- resent the detected keywords. The choice of meaningful threads is tricky: a lot of information may be hidden if just a few keyword threads are included; on the other hand, showing many keywords results in a cluttered visualization.\rFigure 6 shows TextFlow applied to scientific articles published in the IEEE Informa- tion Visualization (InfoVis) conference from 2001 to 2010. Keyword pairs pointing to flows were labeled manually by the authors. For instance, the critical event d indicates that the topic “document/temporal” (characterized by keywords explore and document) has become a major topic in InfoVis around year 2009.\r6\r\nThe “river” metaphor has also been employed to assist analysis of news corpora. News pieces are typically a consequence of relevant events occurring, and the underlying rationale in EventRiver [34] is to identify clusters of news and map them to real-life events. A temporal-locality clustering technique is applied to group news that are both similar in content and adjacent in time. Each cluster is assumed to represent an event, and events are semantically represented by extracted keywords. The proposed visual layout resembles a river of events flowing over time. Each event is shown as a bubble, for which the vertical dimension maps the number of its documents and the horizontal dimensional maps its duration. Events with the same color and place adjacent to each other are closely related and construct a long-term story, i.e. a group of events with close content. The visualization is enhanced by different interaction techniques that allow, for example, to search for events by keywords.\rAn alternative strategy is to create new multidimensional projection techniques, or adapt existing ones, to handle the time attribute explicitly so as to convey temporal changes in the similarity relationships among documents from a collection. This re- lates to the problem of computing layouts that evolve over time to reflect changes in the data set. For instance, the Visone tool [35] has been employed to generate several time-based networks from collections of scientific articles, including journal citation networks and heterogeneous networks that have title words, authors, and journals as nodes. The authors claim these time-based networks are good indicators of structural changes on the underlying data. Visone relies on an MDS (Multidimensional Scaling) algorithm to dynamically layout a sequence of networks by optimizing a stress measure over the current, previous and subsequent maps. This modified stress function penal- izes drastic movements of a node from a map to the next. In this manner, stability and consistence are preserved along a sequence of layouts, thus avoiding user confusion due to sudden unexpected layout changes. However, stability is not dictated solely by the data, but by a parameter in the stress function.\rThe Time-based Least Square Projection (T-LSP) [36] adapts the already introduced LSP multidimensional projection to show temporal evolution. Given a collection of time-stamped documents split into a list of batches according to some temporal prop- erty, it operates backwards to generate a temporal sequence of similarity-based maps. First, the entire collection is projected using LSP generating a final layout. Then, start- ing from the last, each batch is processed: the documents in this batch are removed from the subsequent layout. As a result, documents in the high-dimensional neighbor- hoods of the removed documents must be reprojected in order to update the map. The documents that were not in the neighborhood of removed documents will remain at their current position – these stable documents are taken as “control points” for LSP in reprojecting the others. An intermediate layout is thus generated for each batch, and finally a smooth animation is created to display the series of layouts so obtained, in the correct order. The technique seeks to maintain local accuracy and global spatial coher- ence throughout the sequence of maps, and unlike in Visone the degree of stability is dictated by the data.\rA major drawback of the time-oriented techniques mentioned so far is their inability to handle document streams, such as newswires and blogs. Such techniques are not truly incremental, since layouts, once obtained, can not be rearranged to accommodate\r7\r\nnew incoming elements. The Incremental Board (incBoard) [37] handles this problem by placing a sequence of data elements (e.g., documents or images) over a 2D grid of visual cells: data elements are placed incrementally and dynamically rearranged in the grid so as to reflect their relative similarity rankings, rather than a similarity metric. The solution adopted is inherently incremental, as the grid maintains a coherent disposition of elements along time while it is dynamically rearranged as elements are added or removed. Authors also extend the underlying principle to an Incremental Space (incSpace) that eliminates the grid.\rWith a different strategy, Streamit [38] visualizes text streams employing a dynamic force-directed projection. Force-directed projection techniques iteratively rearrange the data points approximating those projected too far away and repelling those pro- jected closer than expected. This iterative process accounts for the dynamic behavior of the technique. A similarity grid over the current layout is employed to determine the initial placement of a new document: it is inserted in the center of the cell with more documents similar to the incoming one. Force-directed placement is not suit- able for handling high-dimensional data, due to its quadratic complexity. Thus, the topic modeling technique Latent Dirichlet Allocation (LDA) is employed to obtain a low-dimensional vector representation of the collection. However, since this system handles text streams, LDA is actually applied to a very similar document collection to extract the topics, represented as feature vectors of terms probabilities. Each incoming document must be matched, according to its terms, to the topics extracted by LDA and then represented by a vector of the probable weights of its topics. The system also contains a dynamic clustering that automatically discovers clusters from the evolving instances and the corresponding merge and split events along time.\rFigure 7 shows a Streamit visualization of 1,000 abstracts of projects funded by the US National Science Foundation Information and Intelligent Systems (NSF IIS) between March 2000 and August 2003. Figures 7(a) and 7(b) show the stream for the same month in two subsequent years. The size of circles representing the documents is proportional to the project’s funding amount. The largest clusters identified are shown with background colored halos. Given the LDA topics, documents that match specific user selected topics may be presented as pie charts with slice sizes indicating the weight of a topic in the document.\rNetworks for Visualizing Document Relations\rThe techniques presented so far are mainly concerned with visualizing document con- tent. However, documents commonly have associated properties represented as meta- data. Scientific manuscripts, for instance, have properties such as title, authors and their affiliations, abstract, keywords, journal or conference name, references and pub- lication year. Some visualization techniques and tools have been proposed specifically to assist exploratory analysis and visualization based on such metadata. Most of these rely on network analysis [39, 40], with the units of interest – which may be papers, authors or institutions – represented as network nodes and their relationships as edges.\r8\r\nOne example are article citation networks [41] that model how articles cite others via references – articles are depicted as nodes and references as directed edges from the citing article to the cited article, indicating the information flow. Citation networks may also be built for authors, journals, etc. The body of knowledge in complex net- work analysis provides a rich set of tools to characterize and understand the behavior of such networks.\rThe Action Science Explorer (ASE) environment [42] (see Figure 8) incorporates net- work analysis as part of its framework. This tool has been designed for exploration of collections of scientific articles through network visualization, statistics, citation con- text extraction and natural language summarization. ASE partly integrates two existing tools: the SocialAction [43] network analysis tool and JabRef [44], an open source bib- liography reference manager. The reference manager view includes the list of articles under analysis, whereas the network view includes a force-directed citation network visualization, plus functions for ranking and filtering papers by statistical measures, scatterplots of paper attributes and statistics, and automatic cluster detection on the network. A multi-document summarization view shows automatically generated sum- maries of user selected articles. These views are linked and coordinated to help users finding unexpected trends, clusters, gaps and outliers on the information flow.\rThe CiteSpace II tool [41] relies on article citation networks to visualize two related concepts: research fronts, a sub-set of highly cited articles that characterize the state of the art in a research field; and intellectual bases, articles heavily cited by research front articles. The tool builds a visual representation aimed at depicting the temporal evolution of research fronts and intellectual bases and their transient patterns, for a given research field.\rHealth-related document collections also share relationships, in this case based on facets. For example, documents in Google Health describe diseases and include in- formation on a number of facets: symptom, treatment, cause, diagnosis, prognosis, and prevention. A relationship occurs when two described diseases share, e.g. a symptom or a prevention. FacetAtlas [45] is an interactive visualization proposed to help users analyzing large multifaceted document collections with complex cross-documents rela- tionships. Applied to health-related document collections, FacetAtlas helps answering complex questions such as “Which diseases can lead to this set of symptoms?”. The technique employs a multifaceted graph to visualize local relations and a density map to convey a global context.\rVisualizing Query Results\rMany users are familiar with handling a collection of document summaries, known as snippets, retrieved by a search engine in response to a query posed as a set of terms. Typically, the snippets are presented as a list ordered by their relevance to the query, as computed by the search algorithm. The list-based snippet metaphor is simple and intuitive, but recognized as limited in many situations. Users feel overwhelmed, for instance, when too many hits are returned, or when they try to grasp a global view\r9\r\nof the retrieved documents and how their content relates with the query and among themselves.\rSeveral visualization techniques and systems have attempted to provide more flexi- ble alternatives to users inspecting and navigating the result of textual queries. Up to 1995, most information retrieval systems focused on retrieving only document titles and abstracts. Hearst [46] argued in favour of full text document search, stressing that it should indicate, in addition to the strength of the match, the frequency and distribu- tion of relevant (e.g., query) terms in the retrieved texts. The TitleBars visualization has been proposed to supply this information to users performing full text searches. It visually represents each document as a rectangle icon composed by colored bars – each bar represents a set of related query terms. The bars are visually displayed as squares spatially placed to indicate a text segment that addresses a particular topic (detected au- tomatically by the TextTiling algorithm [47]). Squares shown in darker colors indicate higher frequencies of a particular query term set in that specific text segment.\rFigure 9 shows the results of a query by a user interested in documents addressing computer aided techniques for medical diagnosis. The query has been formulated as a conjunction of three term sets: (patient medicine medical) AND (test scan cure diagno- sis) AND (software program), thus each rectangle is represented as three colored bars. The rectangles length indicates the document length, while the the colored bars simul- taneously indicates the frequency of the term sets in the document and their distribution relative to the document and to each other.\rLater visual techniques contributed variations to the TileBars strategy of enriching the basic ranked list metaphor by adding term frequency and/or term distribution informa- tion, e.g., the work by Heimonen and Jhaveri [48]; the HotMap [49] and WordBars [50] visualizations, or the PubCloud [51] system that creates tag cloud visualizations of ab- stracts returned from searching the PubMed database.\rOther techniques for visualizing query hits replace the textual snippets by summary thumbnails [52, 53, 54, 55]. As the visual summaries may include representative text and/or figures, those techniques must access the full document content. There are also visualizations that replace or complement the snippet list with alternative metaphors, e.g. a solar system [56], a spiral shape [57] or scatter plots [58]. A comprehensive review of these and other techniques for visualizing query results is beyond the scope of this paper. The interested reader is referred to Yao et al. [59] and to Hearst [60] for further information on visual interfaces to support general and textual search tasks.\rSummary, Critical Analysis and Discussion\rTables 1, 2 and 3 show a summary overview of the main visualization techniques con- sidered in this article, highlighting their underlying layout type, properties, choice of representation, tasks supported, publication year and main reference work. They reveal a lively field with many techniques and systems proposed, that rely on a rich variety of choices of visual representations and tasks to support, as well as of pre-processing and\r10\r\ninteraction strategies. One observes, however, that certain visual representations afford certain types of tasks – e.g., document maps are often employed to support tasks that require identifying content correlation among documents, and river flow metaphors are popular to convey temporal behavior. Indeed, it is remarkable the increasing number of visualization techniques aimed at supporting analysis of temporal behavior of doc- uments and document collections introduced over the past few years. This scenario signals the great potential of visual techniques as valuable aids in text analytics tasks. Still, it is apparent that most solutions are being validated with case studies in limited scenarios, rather than with real users handling real tasks. In fact, user needs are inferred from the lack of proper support to certain tasks, and visual aids provided to fill these gaps, but little is known about how such tasks fit into the global analysis and decision making processes and the real needs of users from this wider perspective.\rConsidering that text analytics spans a wide variety of domains and goals, approach- ing the problem from a general perspective is hardly effective, and it is not surprising that many solutions reviewed focus on domain-specific tasks or on particular types of documents, as one observes from the Tables. Even for visualizations targeted at a spe- cific domain, providing the right support requires knowing user intentions and goals, which typically vary. For instance, for users searching for a specific information, the traditional ranked list of snippets is a very appropriate viewing metaphor. A similarity- based document map might be preferable if the user wants to browse and correlate the results of an ill-posed query, for example. But then, these different needs should be detected so that a browser would switch between alternative representations, according to user convenience.\rThese inherent difficulties, added to the lack of more in-depth knowledge about the target audience, likely contribute for the low deployment of existing solutions to end users outside the visualization community. As such, user studies and further system- atic investigation on evaluation and validation procedures are very welcome in the text analytics field. Also still lacking is a careful analysis of the low- and high-level cogni- tive aspects and perceptual processes involved in interpreting document visualizations, to guide developers into producing more effective visual solutions. For the body of techniques reviewed, no systematic studies or analyses have been reported, aimed at verifying how users perceive the relevant information and how they incorporate it into their overall decision making processes. Moreover, we identified no studies on how the short- or long-term memories are involved in handling text visualizations, nor to which extent perception of such visualizations is uncontrolled (pre-attentive) or controlled (attentive).\rConclusion\rThis survey has shown an overview of the lively field of visual text analytics. The variety of tasks and situations addressed introduce a demand for many domain-specific and/or task-oriented solutions. Nonetheless, despite the im- pressive number of contributions and wide variety of approaches identified in the literature, the field is still in its infancy. Deployment of existing and novel techniques to a wider audience of users performing real-life tasks remains a challenge that requires tackling multiple issues.\r11\r\n12\rName\rLayout Type\rProperties Representation\rTasks Supported\rYear Reference\rTagCrowd\rWord cloud\rSimple tag cloud\rBag-of-words\rVisualize frequent terms\r2011 [2]\rWordle ManiWordle Oelke and Keim\rWord cloud Word cloud\rHeuristic to optimize area usage\rBag-of-words\rVisualize frequent terms\r2009 [3] 2010 [6] 2007 [8]\rWord Tree\rSuffix tree\rUser specifies the term to search for contexts\rOccurrences of a term, along with its following phrases\rVisualize phrases (contexts) including a term\r2008 [9]\rDocuBurst Phrase Nets Miller et al. Mao et al.\rRadial space- filling layout\rVisual summaries at varying granularity levels\rLexical tree structure\rShow semantic relations among terms\r2009 [10] 2009 [11] 1998 [13] 2007 [14]\rHistory Flow\rLayout based on “revision lines”\rTargeted at versioned docu- ments\rDifferences among version pairs\rView history of versions of a document\r2004 [15]\rText “fingerprints”\rels at multiple levels of detail\rFeature values that characterize documents\rCharacterize texts according to the features; identify authorship\rNetwork layout\rUser must specify the patterns\rPattern matches\rVisualize patterns (relation- ships) between words\rWave layout Curve layout\rDocument as signal, wavelet transform to identify changes\rNarrative order of the words\rDetect thematic changes in nar- rative flow\rTable 1: Summary of Visualization Techniques\rBased on Wordle\rFeatures mapped to screen pix-\rBag-of-words\rVisualize frequent terms; cus- tom manipulations\rDrastic curve movements indi- cate thematic changes\rLocally weighted bag of words rep- resentation\rDetect thematic changes in nar- rative flow\rTechniques for Single Documents\r\n13\rName\rLayout Type\rProperties Representation\rTasks Supported\rYear Reference\rCartographic Map\rDocument map\rGeographic map metaphor\rBag-of-words\rView global document relation- ships; visually identify groups\r2002 [17]\rGalaxies\rDocument map Document map\rNight sky metaphor\rBag-of-words Bag-of-words\rView global document relation- ships; visually identify groups\r1999 [18] 2008 [20]\rLeast Square Projection (LSP)\rSeeks to preserve local data neighborhoods\rView global document relation- ships; visually identify groups; topic identification\rInfoSky\rLayout based on recursive Voronoi subdi- vision\rHierarchical; targeted at hierar- chical document collections\rBag-of-words\rView global document relation- ships; visually identify groups\r2002 [23]\rHierarchical Point Placement (HiPP)\rDocument map\rHierarchical; infers cluster tree to create hierarchical document map\rBag-of-words\rView global document relation- ships; visually identify groups at multiple levels; topic identi- fication\r2008 [24]\rDocument Cards\rLayout based on game cards\rSuitable for large and small dis- play sizes; does not show inter- document properties\rKey term and im- age extraction\rHighlight important key terms and representative images\r2009 [26]\rSparkClouds\rTemporal tag cloud\rSparkline under each term shows temporal frequency variation\rTerm frequencies along time\rVisualize trends across multiple tag clouds\r2010 [27]\rCui et al.\rTrend chart co- ordinated with tag clouds\rTries to preserve term semantic coherence and spatial stability along time\rTerm\rfrequencies along time\rVisualize trends between multi- ple tag clouds\r2010 [28]\rThemeRiver TIARA\rRiver layout River layout\rTopics represented by single words\rTerm frequencies along time\rView temporal thematic changes\r2002 [29] 2010 [30]\rTable 2: Summary of Visualization Techniques (cont.)\rTechniques for Document Collections\rTechniques for Document Collections over Time\rTopics represented as vectors of weighted words\rLatent Dirichlet Allocation (LDA)\rDepict temporal content evolu- tion of topics\r\n14\rName\rLayout Type\rProperties Representation Tasks Supported\rYear Reference\rTextFlow\rRiver layout\rScenario with topic events: birth, split, merge and death\rIncremental Hier- archical Dirichlet Process\rVisualize topic evolution (events); view keywords correlated with each topic\r2011 [32]\rEventRiver\rRiver layout\rTargeted at collections of news\rKeyword vectors\rIdentify clusters of news that can be mapped to real life events\r2012 [34]\rVisone Temporal-LSP\rDynamic network\rUses animation; based on Mul- tidimensional scaling (MDS)\rCitation matrices Bag-of-words\rHighlight structural changes on data\r2008 [35] 2012 [36]\rIncremental Board (incBoard)\rDynamic grid layout\rStreaming; uses animation; seeks to maintain local ac- curacy and global spatial coherence; Uses relative similarity\rBag-of-words\rHighlight temporal changes in similarity patterns\r2010 [37]\rStreamit\rTemporal document map\rStreaming; animation; based on force-directed placement\rLatent Dirichlet Allocation (LDA)\rHighlight temporal changes in similarity patterns; dynamic clustering\r2012 [38]\rAction Science Ex- plorer (ASE)\rNetwork layout with coordi- nated views\rTargeted at collections of scien- tific articles\rCitation matrices\rMulti-document summariza- tion; cluster detection\r2011 [42]\rFacetAtlas\rGraph and den- sity map\rDeveloped for health-related documents with facets\rMultifaceted en- tity relational data model\rReveal multifaceted relation- ships within documents or cross the document clusters\r2010 [45]\rTileBars\rRectangles formed colored bars\rTargeted at for full text docu- ment search\rTerm frequencies within TileBars [47]\rVisualize relative document length, query terms frequency and distribution\r1995 [46]\rTemporal document map\rUses animation; based on Least Square Projection (LSP); seeks to maintain local accuracy and global spatial coherence\rHighlight temporal changes in the similarity patterns\rby\rTable 3: Summary of Visualization Techniques (cont.)\rTechniques for Document Collections over Time (cont.)\rTechniques for Documents Relations\rTechniques for Query Results\r\nOne issue is to foster tighter integration with traditional text mining tasks and algorithms. Various contributions are found in the literature reporting usage of visual interfaces or visualizations to support interpretation of the output of tradi- tional text mining algorithms. Still, visualization has the potential to give users a much more active role in text mining tasks and related activities, and concrete examples of such usage are still scarce. Many rich possibilities remain open to further exploration. Better visual text analytics will also likely require more sophisticated text models, possibly integrating results and tools from research on natural language processing. Finally, providing usable tools also requires addressing several issues related to scalability, i.e., the capability of effectively handling very large text documents and textual collections.\rReferences\r[1] Salton G, Wong A, and Yang CS. A vector space model for automatic indexing. ACM Communications, 18(11):613–620, 1975.\r[2] Steinbock D. Tag Crowd home page. http://tagcrowd.com/, 2011.\r[3] Viegas FB, Wattenberg M, and Feinberg J. Participatory visualization with wor- dle. IEEE Transactions on Visualization and Computer Graphics, 15(6):1137 –1144, 2009.\r[4] Feinberg J. Wordle home page. http://www.wordle.net/, 2011.\r[5] Seifert C, Kump B, Kienreich W, Granitzer G, and Granitzer M. On the beauty and usability of tag clouds. In International Conference Information Visualisa- tion, pages 17–25, Washington, DC, USA, 2008. IEEE Computer Society.\r[6] Kyle K, Bongshin L, Bohyoung K, and Jinwook S. ManiWordle: Providing flex- ible control over wordle. IEEE Transactions on Visualization and Computer Graphics, 16:1190–1197, 2010.\r[7] Hassan-Montero Y and Herrero-Solana V. Improving tag-clouds as visual in- formation retrieval interfaces. In International Conference on Multidisciplinary Information Sciences and Technologies, 2006.\r[8] Keim DA and Oelke D. Literature fingerprinting: A new method for visual literary analysis. In IEEE Symposium on Visual Analytics Science and Technology, pages 115–122, Washington, DC, USA, 2007. IEEE Computer Society.\r[9] Wattenberg M and Viégas FB. The Word Tree, an interactive visual concor- dance. IEEE Transactions on Visualization and Computer Graphics, 14:1221– 1228, 2008.\r[10] Collins C, Carpendale S, and Penn G. DocuBurst: Visualizing document con- tent using language structure. Computer Graphics Forum, 28(3):1039–1046, 2009.\r15\r\n[11] van Ham F, Wattenberg M, and Viegas FB. Mapping text with Phrase Nets. IEEE Transactions on Visualization and Computer Graphics, 15:1169–1176, 2009.\r[12] Rusu D, Fortuna B, Mladenic D, Grobelnik M, and Sipos R. Document visu- alization based on semantic graphs. In International Conference Information Visualisation, pages 292–297. IEEE Computer Society, 2009.\r[13] Miller NE, Chung Wong P, Brewster M, and Foote H. Topic Islands - a wavelet- based text visualization system. In IEEE Conference on Visualization, pages 189–196, Los Alamitos, CA, USA, 1998. IEEE Computer Society.\r[14] Mao Y, Dillon J, and Lebanon G. Sequential document visualization. IEEE Transactions on Visualization and Computer Graphics, 13:1208–1215, 2007.\r[15] Viégas FB, Wattenberg M, and Dave K. Studying cooperation and conflict between authors with history flow visualizations. In Conference on Human factors in Computing Systems, pages 575–582, New York, NY, USA, 2004. ACM.\r[16] Becks A. Benefits of document maps for text access in knowledge manage- ment: A comparative study. In Proceedings of the ACM Symposium on Applied Computing, pages 621–626. ACM, 2002.\r[17] Skupin A. A cartographic approach to visualizing conference abstracts. IEEE Computer Graphics and Applications, 22:50–58, 2002.\r[18] Wise JA. The ecological approach to text visualization. Journal of the American Society for Information Science, 50:1224–1233, November 1999.\r[19] PNNL. IN-SPIRETM Visual document analysis. Pacific Northwest Na- tional Laboratory (PNNL). http://in-spire.pnl.gov/ (accessed em 10/10/2011), 2011.\r[20] Paulovich FV, Nonato LG, Minghim R, and Levkowitz H. Least square projec- tion: A fast high-precision multidimensional projection technique and its appli- cation to document mapping. IEEE Transactions on Visualization and Com- puter Graphics, 14:564–575, 2008.\r[21] Eler DM, Paulovich FV, de Oliveira MCF, and Minghim R. Topic-based coor- dination for visual analysis of evolving document collections. In International Conference on Information Visualisation, pages 149–155. IEEE Computer So- ciety, july 2009.\r[22] Lopes AA, Pinho R, Paulovich FV, and Minghim R. Visual text mining using association rules. Computer Graphics, 31:316–326, 2007.\r[23] Andrews K, Kienreich W, Sabol V, Becker J, Droschl G, Kappe F, Granitzer M, Auer P, and Tochtermann K. The InfoSky visual explorer: exploiting hierarchi- cal structure and document similarities. Information Visualization, 1:166–181, 2002.\r16\r\n[24] Paulovich FV and R Minghim. HiPP: A novel hierarchical point placement strat- egy and its application to the exploration of document collections. IEEE Trans- actions on Visualization and Computer Graphics, 14(6):1229 –1236, 2008.\r[25] Börner K, Chen C, and Boyack KW. Visualizing knowledge domains. Annual Review of Information Science and Technology, 37(1):179–255, 2003.\r[26] Strobelt H, Oelke D, Rohrdantz C, Stoffel A, Keim DA, and Deussen O. Docu- ment Cards: A top trumps visualization for documents. IEEE Transactions on Visualization and Computer Graphics, 15:1145–1152, 2009.\r[27] Lee B, Riche NH, Karlson AK, and Carpendale S. SparkClouds: Visualizing trends in tag clouds. IEEE Transactions on Visualization and Computer Graph- ics, 16(6):1182 –1189, 2010.\r[28] Cui W, Wu Y, Liu S, Wei F, Zhou MX, and Qu H. Context-preserving, dynamic word cloud visualization. IEEE Computer Graphics and Applications, 30(6):42– 53, 2010.\r[29] Havre S, Hetzler E, Whitney P, and Nowell L. ThemeRiver: Visualizing thematic changes in large document collections. IEEE Transactions on Visualization and Computer Graphics, 8:9–20, 2002.\r[30] Wei F, Liu S, Song Y, Pan S, Zhou MX, Qian W, Shi L, Tan L, and Zhang Q. TIARA: a visual exploratory text analytic system. In ACM International Conference on Knowledge Discovery and Data Mining, pages 153–162, New York, NY, USA, 2010. ACM.\r[31] Blei DM, Ng AY, and Jordan MI. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.\r[32] Cui W, Liu S, Tan L, Shi C, Song Y, Gao Z, Qu H, and Tong X. TextFlow: Towards better understanding of evolving topics in text. IEEE Transactions on Visualization and Computer Graphics, 17:2412–2421, 2011.\r[33] Teh YW, Jordan MI, Beal MJ, and Blei DM. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581, 2004.\r[34] Luo D, Yang J, Krstajic M, Ribarsky William, and Keim DA. EventRiver: Visu- ally exploring text collections with temporal references. IEEE Transactions on Visualization and Computer Graphics, 18(1), 2012.\r[35] Leydesdorff L and Schank T. Dynamic animations of journal maps: Indicators of structural changes and interdisciplinary developments. Journal of the American Society for Information Science and Technology, 59:1810–1818, 2008.\r[36] Alencar AB, Paulovich FV, Börner K, and Oliveira MCF. Time-aware visual- ization of document collections. In ACM Symposium on Applied Computing - Multimedia and Visualization Track, pages 997–1004, Riva del Garda, Italy, 2012. ACM.\r17\r\n[37] Pinho R de, Oliveira MCF, and Lopes AA. An incremental space to visualize dynamic data sets. Multimedia Tools and Applications, 50(3):533–562, 2010.\r[38] Alsakran J, Chen Y, Luo D, Zhao Y, Yang J, Dou W, and Liu S. Real-time visu- alization of streaming text with a force-based dynamic system. IEEE Computer Graphics and Applications, 32(1):34–45, 2012.\r[39] Sci2 Team. Science of Science (Sci2) Tool. Indiana University and SciTech Strategies. http://sci2.cns.iu.edu, 2009.\r[40] HerrBW,DuhonRJ,BörnerK,HardyEF,andPenumarthyS.113yearsofphys- ical review: Using flow maps to show temporal and topical citation patterns. In International Conference on Information Visualisation, pages 421–426, Los Alamitos, CA, USA, 2008. IEEE Computer Society.\r[41] Chen C. CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature. Journal of the American Society for Information Science and Technology, 57:359–377, 2006.\r[42] DunneC,ShneidermanB,GoveR,KlavansJ,andDorrB.Rapidunderstanding of scientific paper collections: Integrating statistics, text analytics, and visual- ization. JASIST: Journal of the American Society for Information Science and Technology, 2012.\r[43] Perer A and Shneiderman B. Balancing systematic and flexible exploration of social networks. IEEE Transactions on Visualization and Computer Graphics, 12(5):693–700, 2006.\r[44] JabRef Development Team. JabRef. JabRef Development Team, 2010.\r[45] Cao N, Sun J, Lin Y-R, Gotz D, Liu S, and Qu H. FacetAtlas: Multifaceted visual- ization for rich text corpora. IEEE Transactions on Visualization and Computer Graphics, 16(6):1172–1181, 2010.\r[46] Hearst MA. TileBars: Visualization of term distribution information in full text information access. In Conference on Human factors in Computing Systems, Denver, CO, 1995. ACM.\r[47] Hearst MA. Multi-paragraph segmentation of expository text. In Proceedingsof the 32nd Meeting of the Association for Computational Linguistics, pages 9–16, Stroudsburg, PA, USA, 1994. Association for Computational Linguistics.\r[48] Heimonen T and Jhaveri N. Visualizing query occurrence in search result lists. In International Conference on Information Visualisation, pages 877–882. IEEE Computer Society, 2005.\r[49] Hoeber O and Yang XD. The visual exploration of web search results using HotMap. In International Conference on Information Visualization, pages 157– 165. IEEE Computer Society, 2006.\r18\r\n[50] Hoeber O and Yang XD. Interactive web information retrieval using wordbars. In ACM Conference on Web Inteligence. ACM, 2006.\r[51] KuoBY-L,HentrichT,GoodBM,andWilkinsonMD.Tagcloudsforsummarizing web search results. In International Conference on World Wide Web, pages 1203–1204. ACM, 2007.\r[52] Lam H and Baudisch P. Summary thumbnails: readable overviews for small screen web browsers. In Conference on Human Factors in Computing Sys- tems, pages 681–690. ACM, 2005.\r[53] Li Z, Shi S, and Zhang L. Improving relevance judgment of web search results with image excerpts. In International Conference on World Wide Web, pages 21–30. ACM, 2008.\r[54] Teevan J, Cutrell E, Fisher D, Drucker SM, Ramos G, Andre P, and Hu C. Vi- sual snippets: summarizing web pages for search and revisitation. In Inter- national Conference on Human Factors in Computing Systems, pages 2023– 2032. ACM, 2009.\r[55] Jiao B, Yang L, Xu J, and Wu F. Visual summarization of web pages. In ACM Conference on Research and Development in Information Retrieval, pages 499–506. ACM, 2010.\r[56] Nguyen TN and Zhang J. A novel visualization model for web search results. IEEE Transactions on Visualization and Computer Graphics, 12(5):981 –988, 2006.\r[57] Spoerri A. Rankspiral: Toward enhancing search results visualization. In In- ternational Conference Information Visualisation, pages 208–214. IEEE Com- puter Society, 2004.\r[58] Nizamee MR and Shojib MA. Visualizing the web search results with web search visualization using scatter plot. In IEEE Symposium on Web Society, pages 5 –10. IEEE Computer Society, 2010.\r[59] Jing Tao Yao, Orland Hoeber, and Xue Dong Yang. Supporting Web Search with Visualization, pages 183–214. Springer London, 2010.\r[60] Hearst MA. Search User Interfaces. Cambridge University Press, 2009.\r19\r\nFigure 1: Tag-cloud visual metaphor for the testimony of William Jefferson “Bill” Clinton on his impeachment trial. (a) TagCrowd visual representation. (b) Wor- dle visual representation. The size of the font maps the frequency of the corre- sponding term occurring in the testimony, with larger fonts indicating more fre- quent terms. Images generated with the IBM Many Eyes visualization system (http://www-958.ibm.com).\r20\r\nFigure 2: Different visualizations that convey semantic relationships amongst terms oc- curring in the testimony of William Jefferson “Bill” Clinton on his impeachment trial. (a) Word Tree representation. (b) Phrase Net representation. In the Word Tree, sequen- tial terms in the text are linked, enabling users to navigate in the text by selecting words and checking all sentences in which they occur. The Phrase Nets creates a graph where nodes correspond to terms and edges correspond to user-specified relationships. In this example the clause “is” defines the relationship connecting the terms. Images generated with the IBM Many Eyes visualization system (http://www-958.ibm.com).\r21\r\nFigure 3: History flow: this visualization highlights the temporal patterns of editions made by different authors in the Wikipedia entry about Microsoft. It shows each ver- sion of the target document as a vertical “revision line”, formed by several colored sections and with length proportional to the length of the corresponding text. Each au- thor has been assigned a different color, and the sections of each revision line are col- ored according to their original author. Text sections that have been preserved across consecutive versions are visually linked. Source: [15] (reproduced with permission).\r22\r\nFigure 4: Document maps of a collection of scientific papers obtained with multidi- mensional projection techniques. (a) Least Square Projection (LSP) representation. (b) Hierarchical Point Placement (HiPP) representation. On LSP, circles represent docu- ments and are placed so that circle proximity is proportional to the similarity amongst the corresponding documents. On HiPP, the circles represent groups of similar docu- ments and proximity maps the similarity between the groups. Both maps are annotated with automatically extracted topics, and the colors reflect an existing classification of the documents. Sources: [20, 24] (reproduced with permission).\r23\r\nFigure 5: ThemeRiver: visualization showing documents about the Cuban Missile Cri- sis, from December 1959 through June 1961. In this representation the major topics addressed in the document collection are shown as colored “streams”, with stream width indicating the topic’s strength at a certain moment. Source: [29] (reproduced with permission).\r24\r\nFigure 6: TextFlow: topic flows for scientific articles published in IEEE InfoVis from 2001 to 2010. Similarly to Theme River, TextFlow employs a metaphor based on river “streams” to represent the strength of different topics varying over time within a docu- ment collection. It adds extra visual marks to represent events associated with topics, such as topic birth, split, merge and death. In this example, the event marked as d indi- cates that the topic “document/temporal” has turned into a major topic in this collection around year 2009. Source: [32] (reproduced with permission).\r25\r\nFigure 7: Streamit: dynamic document map for a collection of abstracts describing projects funded by the US NSF IIS award between March 2000 and August 2003, generated with a dynamic force-directed projection. Given LDA topics extracted in a pre-processing step, documents that match specific user-selected topics are presented as pie charts, with slice sizes indicating the topic’s weight in the corresponding docu- ment. Circle sizes represent the amount of funding to the project. Topical events are discovered with a dynamic clustering approach: (a) September 2000 – red pie slice rep- resents topic 16 (Query, Database, Data, XML, Stream, Edu) and green slices represent topic 19 (Data, Workflow, Privacy, Management, Web, Metadata); (b) September 2001 – clusters 1 and 2 from Figure 7(a) have merged into cluster 3. Clusters 4 and 5 are new. Source: [38] (reproduced with permission).\r26\r\nFigure 8: Action Science Explorer (ASE): tool presenting multiple views of research papers on a particular field – tables of papers, full texts, text summaries, and visualiza- tions of the citation network and its groups are shown. All data views are coordinated. Source: [42] (reproduced with permission).\r27\r\nFigure 9: TileBars: visualization of the results of a search on medical documents. Each document appears as a rectangular icon composed by colored bars spatially placed to indicate the frequencies and distribution of the query terms in the document. Squares in darker colors indicate higher frequencies of a particular query term set. Source: [46] (reproduced with permission).\r28","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"layout","time":1461915876892,"auto":true,"weight":1.0},{"@type":"Tag","text":"cloud","time":1461915876922,"auto":true,"weight":1.0},{"@type":"Tag","text":"document","time":1461915876913,"auto":true,"weight":1.0},{"@type":"Tag","text":"lsp","time":1461915876882,"auto":true,"weight":1.0},{"@type":"Tag","text":"visual","time":1461915876809,"auto":true,"weight":1.0},{"@type":"Tag","text":"bag","time":1461915876930,"auto":true,"weight":1.0},{"@type":"Tag","text":"text","time":1461915876844,"auto":true,"weight":1.0},{"@type":"Tag","text":"river","time":1461915876856,"auto":true,"weight":1.0},{"@type":"Tag","text":"ieee","time":1461915876829,"auto":true,"weight":1.0},{"@type":"Tag","text":"techniqu","time":1461915876905,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":37,"appId":"PeyeDF_f9c51ff3150292f81302dea2e4b48a4469a51877","timeCreated":1462192950186,"timeModified":1468599605947,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.031554088,"uri":"file:///Users/cheny13/Downloads/morris_cscw2013_collab_search_revisited.pdf","plainTextContent":"Collaborative Search Revisited\rMeredith Ringel Morris\rMicrosoft Research Redmond, WA, USA\rmerrie@microsoft.com\rABSTRACT\rDespite recent innovations in technologies supporting collaborative web search [11, 13, 25, 34, 35, 37], the features of the primary tools for digital information seeking (web browsers and search engines) continue to reflect a presumption that search is a single-user activity. In this paper, we present the findings of a survey of 167 diverse users’ collaborative web search practices, including the prevalence and frequency of such activities, the information needs motivating collaboration, the methods and tools employed in such tasks, and users’ satisfaction with the status quo. We find an increased prevalence and frequency of collaborative search, particularly by younger users, and an appropriation of “old” technologies like e-mail as well as “new” technologies like smartphones and social networking sites, rather than the use of dedicated collaborative search tools. We reflect on how and why collaborative search practices have changed in the six years since the first survey detailing this phenomenon was conducted [22], and synthesize our findings to offer suggestions for the design of future collaborative search technologies.\rAuthor Keywords\rWeb search, collaborative search, social search, CSCW.\rACM Classification Keywords\rH.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous.\rINTRODUCTION\rOur 20061 survey of 204 Microsoft employees [22] provided data regarding the prevalence of collaborative web search, the tasks motivating such practices, and the methods used to enable such collaborations. In the six years since we conducted that survey, the technology landscape has undergone significant changes, particularly the rise of social networking sites (only 16% of Americans had social networking profiles in 2006, compared with 66% in 2012 [4]) and the growing ubiquity of smartphones (46% of American adults owned smartphones as of February 2012 [36]) and other powerful portable technologies (e.g., tablet computers). The intervening six years have also seen a flurry of research and commercial technologies for collaborative search support, though none have had mainstream success.\rIn light of this changed landscape, we reassess status quo collaborative search practices through a survey of 167 American adults. Our results provide insight into the evolution of collaborative and social search practices; for instance, we find an increase in the prevalence and frequency of collaborative web search, as well as appropriation of new technologies like social networking sites and smartphones to support this phenomenon. We also find that users continue to piece together general purpose technologies to facilitate collaborative information seeking, rather than taking advantage of systems designed specifically for such experiences. In light of these findings, we reflect on barriers to adoption of collaborative information seeking tools, and identify key research directions moving forward.\rRELATED WORK\rThe term social search is used to refer to a broad spectrum of information seeking behaviors, ranging from behaviors which are implicitly social (e.g., search over socially- generated data sets) to those that are explicitly social (e.g., interacting with other people during various stages of the search process) [13]. Evans and Chi [7] described the types\r1 Though published in 2008, the survey data was collected in November 2006. We refer to it (and other surveys discussed) by the year of data-gathering rather than the year of publication, when known, since the former more accurately represents the socio-technical context of the findings given the rapid evolution of technologies and practices.\rInformation-seeking\rtechnologies are the two most popular online tools; a 2011 Pew Research survey [32] found that 92% of online American adults use search engines, and a similar proportion use email. However, this compartmentalization of practices as either search or collaboration is tenuous. Although web search is often considered a de facto solo activity, and nearly all mainstream search technologies are designed for single-user scenarios, a growing body of research suggests that active collaboration on search tasks among users with shared information needs is relatively commonplace [13, 22, 25].\rtechnologies and\rcollaboration\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rCSCW ’13, February 23–27, 2013, San Antonio, Texas, USA. Copyright 2013 ACM 978-1-4503-1331-5/13/02...$15.00.\r\nof social engagements possible at different stages of the Web search process.\rCollaborative search [11, 13, 25, 34] is a subset of social search in which participants work together to satisfy an information need. The collaborative nature of search tasks in pre-web scenarios (e.g., in libraries and paper-driven offices) has a long history of scholarship (e.g., [9, 38]). Academic investigation of the challenges and practices associated with collaborative web search is a more recent phenomenon, usually associated with the 2007 introduction of the SearchTogether [24] system, whose design was informed by a survey of collaborative web search practices conducted in 2006 (but not published until 2008) [22]. Collaborative search has many benefits, including enabling participants to achieve synergic effects such as greater recall [31, 35], offering the potential to improve search skills through exposure to others’ behavior [20, 21], and providing an opportunity to strengthen social connections [27, 28]. Note that the investigation of collaborative web search differs from prior work on collaborative web browsers (e.g., [12]) in that its focus is not on general- purpose web browsing, but specifically on the use of the web for information-seeking tasks.\rIn [22], we reported the results of a November 2006 survey of 204 Microsoft employees’ collaborative web search practices. It is unclear how representative this demographic’s behavior was, as respondents differed in many ways from the general population, primarily on the basis of their technical expertise, as well as along other demographic dimensions that were also non-typical (e.g., 80.4% of respondents were male). However, [22] presents the most complete picture available of the prevalence and characteristics of the collaborative search phenomenon; it found that, despite the lack of any tools designed to explicitly support collaborative web search, 53.4% of those surveyed had engaged in such activities by using the bottom-up [41] approach of appropriating existing technologies (e.g., email, instant messaging, etc.) to supplement the web browser. A smaller, diary-based study of 20 Microsoft employees [2] conducted in 20072 provided additional insights into the specific scenario of co-located collaborative search. More recently, several studies have characterized the asymmetric collaborative search scenario (in which participants have different roles and/or motivations [25]) of using social networking sites to engage contacts in various stages of an information-seeking task [6, 17, 27, 29].\rIn the intervening six years since our survey [22], a number of research prototypes supporting collaborative search have been introduced. Some innovations have proposed algorithmic techniques to enhance the collaborative search experience, including role-specific weightings of input [31],\r2 Data gathered in 2007; published in 2009.\rgroup personalization of results [26], expertise-matching of potential collaborators [16], and agents that use context from social network Q&A exchanges to suggest relevant links [15]. Others have proposed user interface enhancements, such as enhancing collaborators’ awareness of each other’s search process [20, 24], enabling distribution of control in co-located settings [1], supporting collaborative search among users with asymmetric access to devices [42], and supporting collaborative search on emerging technologies such as large touch surfaces [23].\rSome commercial technologies have also included features supportive of specific subsets of collaborative search activities. Examples include Aardvark (a service to match users with experts to support their information-seeking; 2007), Flock (a web browser that incorporated social networking as a first-class feature in its design; 2008), HeyStaks (which uses feedback from communities of users with common interests to re-rank search results; 2008), Pinterest (a shared bulletin board for collections of web imagery; 2010), SearchTeam (a tool whose features are highly reminiscent of SearchTogether [24], 2011), and So.cl (a social network based around sharing collections of search results [8]; 2011). None of these commercial tools has achieved mainstream adoption – some, like Aardvark and Flock, have already become defunct; others, like Pinterest and So.cl, are still in limited, invitation-only beta stages.\rThis paper adds to this body of work on collaborative web search by presenting survey results that give an updated and more complete view of the current state of practice. Our survey reports on the collaborative search practices of a more representative sample of the general public (as opposed to highly technical knowledge workers, e.g., [22]), and reflects recent changes in the technological landscape (such as the growing prevalence of social networking [4] and smartphone use [36]).\rSURVEY\rWe conducted an online survey over a one week period in March 2012 to assess current practices regarding collaborative information seeking. The survey consisted of both open-ended and multiple-choice questions. Respondents were asked whether they had ever collaborated with other people to search the Web; if they answered affirmatively, they were asked to describe their most recent collaborative search experience and to answer a series of questions about that specific search incident (a critical-incident approach [7, 10]). Additional survey questions addressed demographics and use of specific search and collaboration technologies.\rThe survey was advertised as a questionnaire on “Information Seeking Practices” via the Survey Monkey Audience recruiting service3 to 1,025 adult American\r3 http://www.surveymonkey.com/mp/audience/\r\n2006\r2012\rDaily\r0.9%\r11.0%\rWeekly\r25.7%\r38.5%\rMonthly\r48.6%\r15.6%\r24.8%\rLess Often\rTable 1. Percent of respondents reporting collaboratively searching at various frequencies. 2006 numbers are taken from Table 2 of [22].\r34.8%\rparticipants; 167 completed the entire survey, yielding a 16% response rate.\rRESULTS\rWe first characterize the demographic details of our 167 respondents. We then report our findings regarding the prevalence of collaborative search and the nature of such searches. We also report findings on the use of specific technologies for collaborative information seeking, including smartphones, social networks, and Q&A tools.\rNote that we use non-parametric statistical tests when analyzing Likert responses, due to the subjective and potentially non-linear interpretations of the “spacing” between adjacent items on such scales.\rDemographics\rOur 167 survey respondents were all residents of the United States; 40 of the 50 states were covered by our sample. 56% of respondents were female. 38% of respondents were aged 18 – 29; 24% were aged 30 – 44; 27% were aged 45 – 60; and 11% were older than 60 years. 5% had a high school diploma or less, 57% had completed some college, 29% had a college degree, and 9% had a graduate degree.\rOccupations were varied, with students making up the largest single group at 25% of respondents. An additional 8% of the group was comprised of retirees. The remaining 67% of respondents had diverse vocations including sales person, customer service representative, teacher, nurse, school counselor, homemaker, mortgage broker, physician, stock analyst, insurance adjuster, cosmetologist, accountant, software engineer, dentist, paralegal, copy editor, and heavy equipment operator.\rRespondents used search engines frequently; most (79.9%) reported using a major search engine (Ask, Bing, Google, or Yahoo!) several times per day, and nearly all (94.1%) reported doing so at least once per day.\rCollaborative Search\rAll respondents were asked “Have you ever collaborated with other people to search the Web?” If they answered negatively, they skipped ahead to the questions about specific technologies (see the “Beyond ‘Traditional’ Search Engines” section) and demographics. However, the 109 respondents (65.3%) who answered affirmatively were asked several follow-up questions about their experiences with collaborative search.\rFinding that 65.3% of respondents had engaged in collaborative search indicates an increased prevalence of collaborative search behavior – our 2006 survey [22] found the prevalence of collaborative search to be only 53.4% (and that was with a more “tech-savvy” audience, Microsoft employees, whom one would assume might be more likely to appropriate technologies in novel ways than the more diverse audience of the current survey). The prevalence of collaboration we found differs significantly from the\rhypothesized proportions based on the 2006 survey, χ2(1, N = 167) = 9.62, p = .002).\rAge was significantly negatively correlated with the likelihood of engaging in collaborative search (e.g., younger respondents were more likely to engage in this behavior), r = -.26, p = .001.\rNote that percentages given in the remainder of this section (“Collaborative Search”) and its sub-sections are out of the 109 people who indicated that they had searched collaboratively rather than out of the full 167 survey respondents.\r11% of respondents who had searched collaboratively reported doing so on a daily basis, and an additional 38.5% report searching collaboratively at least once per week. This is a marked increase over the self-reported frequency of collaborative searching in 2006 [22], as illustrated in Table 1, χ2(3, N = 108) = 155.26, p < .001).\rAfter indicating whether they had ever searched the Web collaboratively and how often they did so, participants who had searched collaboratively were asked to engage in a recent critical-incident self-report [7, 10]. They were asked in a free-text question to “think about the most recent time you collaborated with others to search the web,” and then to “describe the nature of the information need that prompted this incident.” They were then asked several follow-up questions about that specific incident. The following four sub-sections (“Topics,” “Group Configurations,” “Methods and Tools,” and “Satisfaction”) are based on the follow-up questions about the respondents’ most recent collaborative search incident.\rTopics\rIn addition to describing the information need prompting their most recent collaborative search in a free-response question, we also asked respondents to classify the nature of the information need they investigated collaboratively by selecting one or more topics from a list. The list of topic choices was created by combining the topics reported as prompting collaborative searches in [22] and the list of topics reported as likely and unlikely to prompt requests for search help from members of one’s social network in [27].\r\nTopic\r# of Respondents\rExample Task Description\rprofessional\r26\r“we split up research for software development, searching individually for coding issues, gui design, and what would be appealing to our audience”\rhealth/medicine\r21\r“we needed to find information about iron deficiency and hypokalemia”\rnews/current events\r19\r“looking for reference footage and images for a school project”\rtechnology\r18\r“looking for printer parts for our business operation”\rtravel\r16\r“we were planning a trip to Alaska and all the details that go into it for a group of 10 of us”\rshopping\r15\r“looking for a used car”\rentertainment\r14\r“searching for music on YouTube and lyrics”\rhome/family\r11\r“genealogy”\rfinance\r11\r“we were researching different kinds of e-portfolios”\rrestaurants\r10\r“find local restaurants”\rsocial events\r10\r“planning a wedding”\rTable 2. The most common topics motivating respondents’ recent collaborative Web searches.\rTable 2 shows the most popular topics (those which 10 or more respondents said described their most recent collaboratively-investigated information need), and gives examples from respondents’ free-form descriptions of their most recent search incident.\rGroup Configurations\rSmall-group collaboration was more common than larger groups. Pairs were the most common configuration (31.2%). Triads were also fairly common (22.9%), as were quartets (23.9%). Groups larger than four members were infrequent – 9.2% reported working in groups of five, 4.6% in groups of six, and 8.3% in groups having seven or more members.\rOur 2006 survey [22] also found that smaller group sizes were more common than larger ones, though that survey found much smaller group sizes, reporting that 80.7% collaborated in pairs and 19.3% in groups of three or four, with no larger groups at all. Comparing our frequencies of pairs, groups of three or four, and larger groups to this earlier finding shows a significant change in group sizes, χ2(2, N = 109) = 610, p < .001. Our 2007 diary study of co- located collaborative search [2] also found smaller group sizes than our current study, with 85.7% collaborating in pairs and 9.5% in groups of three or four.\rWe also asked respondents to characterize their relationship with their collaborators on their search task. 55.0% reported collaborating with colleagues or classmates. Family was the next most common type of collaborator relationship, at 25.7%, followed by close friends (19.3%), and then casual\racquaintances (11.0%). Collaboration with strangers or professionals (e.g., librarians) was rare, at 5.5%. Note that these values total to greater than 100%, as some respondents indicated that they worked with a group comprised of multiple relationship types.\rSynchronous collaboration was more common than asynchronous, comprising roughly two-thirds of the incidents (64.2%). Remote collaboration was more common than co-located, characterizing 61.5% of the described searches. This is in contrast to the 2006 survey [22], which found a slight prevalence of co-located search configurations, although their question was phrased differently (asking respondents which of the following behaviors they had ever engaged in, versus asking them to describe a single recent critical incident as we do here), making direct comparisons difficult.\rMethods and Tools\rParticipants used a checklist to indicate what tools they employed in their most recent collaborative search incident (they could check as many items as applied). The use of search engines was common (67.9% of respondents used them in their most recent collaborative search task), but other methods of online information-seeking were also employed, such as using a social networking site (19.3%) or Q&A site (6.4%). We explore the use of these latter two technologies in further detail in the “Beyond ‘Traditional’ Search Engines” section.\rDevices: “Traditional” devices like laptops (61.5%) and PCs (39.4%) were the most common devices used in the\r\nVery Dissatisfied\rDissatisfied\rNeutral\rSatisfied\rVery Satisfied\rquality of answer found\r2.8%\r1.8%\r12.8%\r55.0%\r27.5%\rease of working with others\rTable 3. Respondents’ satisfaction with the informational and social aspects of their most recent collaborative search incident.\r1.8%\r2.8%\r16.5%\r42.1%\r35.8%\rDaily\rWeekly\r% of smartphone owners\r% of co- located searchers\rTable 4. The reported frequency of engaging co-located multi- party smartphone searches among all smartphone owners in our sample (97 participants), and among those who reported engaging in this behavior at least occasionally (90 participants).\rMonthly\r21.6%\rLess than once per month\r10.3%\rNever\r36.1%\r24.7%\r7.2%\r38.9%\r26.7%\r23.3%\r11.1%\rN/A\rcourse of collaborative information seeking. Newer device types were also common, with 30.3% of the searches involving a smartphone and 11% involving a tablet. Technologies that might facilitate public sharing such as TVs and projectors were rarely employed, in only 1.8% and 0.9% of the searches, respectively. Non-digital tools were also an important part of collaborative search processes; for instance, 11% of respondents reported using paper to support their collaborative search task.\rCommunication: Since mainstream web browsers and search engines don’t incorporate communication tools (which are important for facilitating remote collaborative search [24]), respondents often employed out-of-band communication channels. Email was the most common communication tool, involved in 46.8% of the searches. Other communication channels used were talking on the phone (27.5%), text messaging/SMS (30.3%), and instant messaging (12.8%). Videoconferencing was rare; only one participant reported employing it as a communications channel during a collaborative search.\rSatisfaction\rResponents used a five-point Likert scale to rate their level of satisfaction with both the informational (quality of answer found) and social (ease of working collaboratively) aspects of their most recent collaborative search. Table 3 summarizes those results. 82.5% reported satisfaction with the informational outcome and 77.9% reported satisfaction with the ease of collaboration. Though positive overall, these figures still indicate that there is room for improvement of both the informational and social aspects of the collaborative search experience.\rRespondents were also given space to compose a free-form response regarding suggestions for how their collaborative search experience could have been improved. The most common response (7 people) was for facilities to make it easier to share the products of search with group members and increase group awareness of mutual activities (two issues that systems like SearchTogether [24], Coagmento [35], and WeSearch [23] sought to address). For example, one respondent said, “It might have helped if we had some sort of online bulletin board on which to post our findings...” Another noted that he would have liked “an\reasier way for the rest of the group members to access the information each of us found separately,” and one person desired the ability to have “real-time comparisons between mine and my colleagues’ information.” Redundant work [22] remained problematic, with one respondent noting that “if there was a better way to communicate where one person had already looked it would have prevented overlap\rof seeking for information,” and another observing that it would be helpful to have “a database that tracks collaborators’ searches in a private group, so that you can see which papers others have already found.”\rBeyond “Traditional” Search Engines\rAlthough the questions surrounding respondents’ most recent collaborative search incident were asked only to the 109 respondents who indicated that they had engaged in collaborative Web search, all 167 respondents were asked a series of questions about their use of several specific technologies, independent of the critical incident inquiry. In particular, we were interested in investigating the collaborative use of information-seeking technologies other than the traditional “using a search engine in a PC web browser.” Our questions focused on three kinds of tools that have experienced significant changes or growth since 2006 – smartphones, social networking sites, and Q&A sites.\rSmartphones\rThough smartphones (which permit browsing the web and running third-party applications) existed in 2006, the capabilities and adoption of smartphones changed dramatically in 2007 with the introduction of Apple’s iPhone, whose Safari browser provided the ability to view and interact with “real” web pages (rather than special mobile versions). By early 2012, 46% of American adults owned a smartphone [36]. Recent work suggests that mobile local searches (i.e., searching for businesses or services near a user’s current geo-location) are often undertaken in a social setting, but does not offer detailed insight into multi-phone collaborative search practices [39].\rIn our survey sample, 58.1% of respondents reported owning a smartphone. Of these 97 smartphone-owning respondents, 48.5% had Android devices, 34.0% had iOS (Apple) devices, and the remainder had devices running the\r\nFigure 1. Self-reported frequency of posting a question as a status update, a form of asymmetric collaborative search, by account-holders on each social network.\rFacebook\rTwitter\rGoogle+\rLinkedIn\r139 (83.2%)\r49 (29.3%)\r42 (25.1%)\r50 (29.9%)\rhave accounts\rhave ever asked a question\r50.0%\r33.3%\r24.6%\r24.6%\rask questions at least once per week\r15.4%\r9.5%\r9.8%\r4.6%\rlurk (read content but never post)\rTable 5. The first row reports how many of the 167 respondents had accounts on each social networking site. Additional rows report the percentage of those account- holders engaging in specific behaviors.\r4.3%\r8.8%\r12.5%\r15.9%\rPalm, RIM (Blackberry), or Windows operating systems. Smartphone ownership was not significantly correlated with any demographic factors.\rAlthough they may not have previously self-identified as having engaged in collaborative search, nearly all of the smartphone owners (92.8%) reported using their phones to engage in co-located collaborative searches in which several people simultaneously used their smartphones to look up information (Table 4). This behavior was surprisingly frequent – of the 90 respondents who reported engaging in this behavior, 38.9% reported doing so at least once per day, and 65.6% at least a few times per week. Younger respondents engaged in co-located multi-phone searches more frequently than older respondents (r = -.26, p = .01). These initial findings suggest that studying co- located collaborative smartphone search may be a rich area for further investigation to answer questions beyond the scope of our current survey, such as exploring what role specialized “apps” might play in such scenarios.\rSocial Networking Sites\rSocial networking sites were used by only 16% of Americans in 2006 (the year in which Facebook opened enrollment to the general public rather than merely to students at selected universities); by 2012, 66% had an account [4]. Social networking sites have taken on an increasingly prominent role in asymmetric collaborative information seeking [25], through mechanisms such as posting search results directly to social network feeds (e.g., Bing and Ping [5] or So.cl [8]), using socially embedded search engines (e.g., SearchBuddies [15]), or asking questions via status messages [6, 17, 27, 29]. For instance, a 20094 survey of Microsoft employees [27] found that 50.6% had posted questions to Facebook or Twitter.\r4 Survey conducted in 2009; published in 2010.\r87.4% of our survey respondents reported having social networking accounts, with Facebook being by far the most popular, distantly followed by Twitter, LinkedIn, and Google+ (Table 5). Other networks like MySpace, Orkut, Tumblr, and Yammer had negligible representation. Younger respondents were more likely to have social networking accounts than older ones (r = -.27, p < .001).\rAsking questions on these social networking sites was common. 50.0% of those with Facebook accounts reported having used that network to ask a question, as did 33.3% of those with Twitter accounts, and 24.6% of those with LinkedIn and Google+ accounts. Our finding that half of Facebook users have engaged in status-message question asking is similar to the findings of a survey we conducted in 2009 [27], despite the fact that the 2009 survey audience was comprised of Microsoft employees while our current survey draws from a more diverse demographic. However, it conflicts with the findings of a survey by Lampe et al. conducted in 20115 [17] that found that most Facebook users in their sample did not view Facebook as an appropriate venue for information-seeking (i.e., via status message Q&A). The Lampe survey’s audience consisted of employees at a U.S. university, who were less diverse (e.g., more educated, more female, older) than our sample population, which may explain this difference.\rWhile prevalent, this behavior appears to be relatively infrequent (frequency of social network Q&A was not reported in prior surveys such as [27], which focused primarily on prevalence, motivations, and topics associated with this phenomenon). Only 15.4% of the Facebook users, 9.8% of Google+ users, 9.5% of Twitter users, and 4.6% of LinkedIn users reported asking questions at least once per week. The low use of LinkedIn for question-asking may reflect competition from other professional forums, such as internal enterprise SNS sites [40]. Figure 1 shows the reported frequency of question-asking by respondents holding accounts on each of those social networking sites.\r5 Survey conducted in 2011; published in 2012.\r\nFigure 2. Frequencies of viewing content, posting content, and posting questions by account-holders on four social networks. The higher frequency of lurking (users who view but never post content) on Google+ and LinkedIn may contribute to their being viewed as less useful venues for getting questions answered.\rOn all of these social networks, viewing content is more common than posting content, which is more common than posting questions. “Lurking” (having an account and logging in to view content, but never posting any content yourself) was relatively uncommon. Only 4.3% of respondents with Facebook accounts were lurkers. Lurking on Twitter was more common, at 8.8% (note that this figure only includes respondents with Twitter accounts; many additional people likely read Twitter without having accounts at all). The lurking rates for Google+ and LinkedIn were higher still, at 12.5% and 15.9%, respectively. The lurking rate is strongly negatively correlated (r = -.94) with the rate of question-asking on each service, perhaps because it relates to the likelihood that someone who views a question will chime in with an answer. Figure 2 shows the frequency of different interactions on each social network.\rThe frequency of question asking on the three less popular social networks was significantly correlated (p < .01) (Twitter/Google+: r = .36; Twitter/LinkedIn, r = .34, Google+/LinkedIn: r = .44), perhaps representing a clique of “hard-core” askers who try many social venues in pursuit of an information need. In contrast, the frequency of asking on Facebook (a more popular activity overall), was not correlated significantly with asking on LinkedIn (the least popular venue for question asking), and had relatively weak\rcorrelations with asking frequency on Google+ and Twitter (r=.19,p=.03).\rDemographic factors correlated weakly with the frequency of question asking on certain social networks. Younger respondents were more likely to ask questions on LinkedIn frequently (r = -.24, p < .01), as were respondents with lower education levels (r = -.18, p = .03). Among Facebook users, women reported asking questions more often than men (r = -.18, p - .03).\rQ&A Sites\rQ&A sites provide an alternative method of online information seeking than traditional Web search. These forums allow users to post questions for answering by either the general Web population (e.g., Yahoo! Answers, Mahalo Answers, Ask MetaFilter [14]), paid staffers (e.g., ChaCha, kgb), or self-identified topical experts (e.g., Quora [30]). Such sites typically archive past questions and answers, which are browseable and/or searchable by other users.\rThe past few years have seen an increase in the prominence of tools that form social structure around non-anonymous Q&A exchanges (e.g., Quora [33], founded in 2009) and of those that operate on a paid-staffer model, which is a modern-day analog of the reference librarian, (e.g., ChaCha, founded in 2006, whose answer volume surpassed\r\nthat of Yahoo! Answers in 2011 [18]). Use of such “next- generation” Q&A sites could be construed as a form of asymmetric collaborative search [25].\rWe asked all respondents how often they posted questions to a variety of Q&A sites. Most respondents reported that they had never posted a question to Ask MetaFilter (97.3%), ChaCha (93.2%), kgb (98.6%), Mahalo Answers (100%), or Quora (99.3%). The only Q&A site in our survey that was occasionally used was Yahoo! Answers – 24% of respondents had posted a question at least once, though this behavior was infrequent (only 4.8% reported posting a question at least once a week).\rThe frequency of posting questions to Yahoo! Answers was not significantly correlated with demographic factors (age, gender, or education). There was, however, a significant negative correlation between the frequency of posting to Yahoo! Answers and the frequency of posting questions to some of the less popular social networking sites (Google+: r = -.30, p < .01; LinkedIn: r = -.17, p = .04). This might indicate that users employ two distinct “backup” strategies for seeking answers to difficult questions – either posting to a Q&A site or posting to a “secondary” social network.\rWe also asked whether respondents perused the archives of these Q&A sites for answers, even if they did not post a question themselves. 48.6% of respondents reported using Yahoo! Answers in this manner at least once, and 11% reported using ChaCha in this manner. The use of archived answers from Ask Metafilter, kgb, Mahalo, and Quora was negligible. Reusing answers was a more frequent behavior than posting new questions; 1.4% of respondents used the ChaCha archives at least once per week, and 11.7% did so for Yahoo! Answers (a Wilcoxon test comparing the frequency of posting vs. perusing Yahoo! Answers found that the latter was significantly more frequent, z = -5.41, p < .001). Reusing existing answers on Yahoo and ChaCha was significantly inversely correlated with age – younger users were more likely to engage in this behavior frequently (Yahoo! Answers: r = -.41, p < .01; ChaCha: r = -.23, p < .01).\rDISCUSSION\rIn this section, we reflect on our survey findings. First, we compare and contrast our results with those of prior studies of collaborative search behavior, and discuss possible causes of differences. We then discuss the implications of our findings for the design of technical solutions supporting collaborative search.\rComparison with Prior Findings\rCompared to six years ago [22], we found that more people are engaging in collaborative web search, and that they are doing so with greater frequency. 65.3% of our respondents reported having collaborated on web search, with 49.5% of those collaborative searchers engaging in such activities at least once per week. When asked about specific behaviors (such as engaging in co-located smartphone searches), even\rhigher prevalence and frequencies were reported, suggesting that the 65.3% number may be an underestimate, perhaps due to the generic nature of the question (“have you ever collaborated with other people to search the web?”).\rThe typical group size involved in such collaborations has increased, as well, likely due to the adoption of technologies that facilitate simultaneous interaction of larger groups of users for remote collaboration (e.g., social networking sites), and technologies that support larger group engagement in co-located collaboration by providing each group member with their own input device (e.g., smartphones). Despite the considerable press attention some emerging social information seeking solutions have received (e.g., Quora [33]), community Q&A sites do not appear to be part of a typical user’s collaborative search repertoire.\rWe found that younger users were more likely to engage in collaborative searches. It is unclear whether the observed increase in collaborative search activities is due primarily to the coming-of-age of a new generation of technology users who are more comfortable pushing the bounds of a tool’s intended use and/or have differing attitudes toward collaboration, or whether it is due to the invention and\radoption of new technologies (smartphones, networking, etc.). It is likely that both of these played a role in shaping our findings.\rThe tendency of respondents to appropriate communications technologies to create de collaborative search solutions (rather than increasingly available dedicated collaborative remains similar to six years ago. Consequently, respondents reported many of the same frustrations with collaborative search (lack of awareness, wasted duplication of effort) as in our earlier survey [22].\rOur findings suggest that the meaning of the term “collaborative search” has evolved (or should evolve!). Our initial conception of collaborative search in our survey [22] and SearchTogether prototype [24] involved the synchronous or asynchronous use of search engines by multiple parties with a shared information need. However, our survey findings indicate that collaborative search now occurs beyond the search engine (e.g., in apps on smartphones, in questions on social networking sites, etc.).\rLimitations\rThe reader should note that differences between our findings and prior work (particularly [22]) are difficult to attribute to a single cause. Differences may be due to social and technological changes occurring between 2006 and 2012, which is our primary hypothesis. Other sources of differential findings may be in the survey audience (highly technical respondents in [22] versus the more general population we reached with this survey), or in the nature of the questions themselves (the “have you ever” approach\rsocial factors\rexisting facto using tools)\r\nemployed in [22] versus the recent critical-incident approach [7, 10] employed in the current work).\rTo explore whether our findings were due to audience background rather than sociotechnical changes occurring between 2006 and 2012, we issued the same survey to 250 randomly selected U.S.-based Microsoft employees in July 2012; 63 completed the survey (25% response rate). The results from this group were very similar to the results of the more diverse 2012 audience discussed in this paper – for instance, 61.9% of the 2012 Microsoft employee respondents reported having engaged in collaborative Web search, which is not significantly different than the 65.3% figure for the diverse group (χ2(1, N=63) = .279, p = .597). Similarly, of the 2012 Microsoft employees who searched collaboratively, 49.5% reported doing so at least once per week, which is quite similar to the 47.3% of our more diverse sample that collaboratively searched at least weekly. The similarity between the diverse and tech audiences’ responses increased our confidence that differences in audience background between our survey and the 2006 survey are not the primary source of the differences in our findings.\rAdditionally, the reader should bear in mind the inherent limits of all self-report studies, such as potential inaccuracies in participants’ memory or biases in what they choose to report (for more detail on the pros and cons of retrospective self-report methods, see [19]). It is also unclear whether these findings extend to other demographics, such as children or people outside the United States (e.g., staffed Q&A sites are reportedly a popular form of asymmetric collaborative search amongst Korean teenagers [18]). Combining our survey findings with other approaches, such as interview or observational methods, would provide a richer understanding of this phenomenon, and is a suggested direction for further study.\rChallenges for Collaborative Search Solutions\rDespite the increasing availability of tools designed specifically to support collaborative web search (e.g., free online tools including Coagmento [35], HeyStaks [37], SearchTogether [24], and So.cl [8]), none of our respondents utilized such technologies. General-purpose tools that could provide rich collaborative experiences, such as videoconferencing or projection technologies, were also rarely used. Instead, respondents repurposed simpler communications technologies that were part of their everyday routines (e-mail, texting, instant messaging, phone calls, and social networking) as a way to supplement status quo web browser and search engine technologies and enable collaborative information seeking. This suggests that technologies for collaborative web search must be sufficiently lightweight compared with status quo ad hoc solutions. One of our survey respondents articulated this well, observing, “It might have helped if we had some sort of online bulletin board on which to post our findings – but only if posting something to the bulletin board was faster\rand required fewer mouse clicks than copying a link into an e-mail message.” The tension between dedicated, “top down” solutions versus ad hoc “bottom up” solutions is not unique to collaborative search; lessons learned about similar issues in areas like cyberinfrastructure development [41] may be applicable. Reflecting on our survey findings in light of this related work suggests that rather than creating dedicated tools for collaborative search, creating “glue” systems that offer integration, tighter coupling, and symbiotic functionality between existing social and information-seeking technologies might be a more promising approach.\rDespite the challenge of striking a proper balance between having a low barrier to entry and offering rich collaboration support, there appears to be an unmet need for technologies supporting collaborative web search, as evidenced by the increasing prevalence and frequency of such activities. Our finding that collaborative search is more common among younger demographics suggests that its prevalence might continue to increase as a new generation of users with different attitudes about collaboration and technology emerges into the marketplace.\rOur results suggest that systems that address users’ frustrations regarding lack of awareness of collaborators’ activities and the resulting redundant work that occurs would be particularly valued; these findings reinforce similar findings from prior work [22, 24], indicating a need that has continued to go unmet by technical advances. Solutions that can enhance common scenarios, such as the use of social networks for Q&A activities or the use of several smartphones for synchronous co-located searching, may be a particularly promising direction for research and development.\rShortly after the completion of our survey, Microsoft introduced collaborative search support into its Bing search engine with the “sidebar” feature [3], which enables a user to start a conversation with social network contacts around a query and a set of curated search results. The introduction of collaborative features into a mainstream search engine could potentially significantly alter the status quo reliance on bottom-up solutions. Revisiting the state of collaborative search practice in a few years seems prudent given the rapid evolution of technologies and attitudes in the social search space.\rCONCLUSION\rIn this paper, we added to the growing body of knowledge about collaborative web search by presenting survey data about 167 diverse users’ status quo collaborative search practices. We found that collaborative search has become an increasingly common type of information-seeking experience (and that the notion of what constitutes a “collaborative search” has evolved to include technologies beyond search engines, such as smartphones and social networking sites). We also found that ad hoc combinations of everyday technologies are used to support such\r\ncollaborations, rather than dedicated solutions designed specifically for collaborative information seeking.\rBy contrasting our findings with earlier work, we identified changes in the prevalence of this practice and in the technologies employed. We also identified important challenges that remain to be addressed by designers of collaborative web search technologies. Our results indicate that there is great potential for technological innovation to enhance the surprisingly commonplace practice of collaborative information seeking in the digital era.\rREFERENCES\r1. Amershi, S. and Morris, M.R. CoSearch: A System for Co-located Collaborative Web Search. Proceedings of CHI 2008, 1647-1656.\r2. Amershi,S.andMorris,M.R.Co-locatedCollaborative Web Search: Understanding Status Quo Practices. CHI 2009 Extended Abstracts, 3637-3642.\r3. Bing Team. Introducing the New Bing: Spend Less Time Searching, More Time Doing. Bing Search Blog, May 10, 2012.\r4. Brenner, J. Social Networking. Pew Internet and American Life Project, March 29, 2012.\r5. Dybwad,B.BingandPing:ShareSearchResultson Facebook and Twitter. Mashable, Sept. 3, 2009.\r6. Efron,M.andWinget,M.QuestionsareContent:A Taxonomy of Questions in a Microblogging Environment. Proceedings of ASIS&T 2010.\r7. Evans,B.andChi,E.TowardsaModelof Understanding Social Search. Proceedings of CSCW 2008, 485-494.\r8. Farnham,S.D.,Lahav,M.,Raskino,D.,Cheng,L., Ickman, T., and Laird-McConnell, T. So.cl: An Interest Network for Informal Learning. Proceedings of ICWSM 2012.\r9. Fidel,R.,Bruce,H.,Pejtersen,A.M.,Dumais,S.T., Grudin, J., and Poltrock, S. Collaborative Information Retrieval. The New Review of Information Behaviour Research, 2000, volume 1, 235-247.\r10.Flanagan, J. The critical incident technique. Psychological Bulletin, 51:327-358, 1954.\r11.Foster, J. Collaborative Information Behavior: User Engagement and Communication Sharing. IGI Global, June 2010.\r12.Greenberg, S. and Roseman, M. GroupWeb: A WWW Browser as Real Time Groupware. Proceedings of the CHI 1996 Conference Companion, 271-272.\r13.Golovchinksy, G., Morris, M.R., and Pickens, J. Introduction to the special issue. Information Processing & Management, Special Issue on Collaborative Information Seeking, 46(6), November 2010.\r14.Harper, F., Moy, D., and Konstan, J. Facts or friends? Distinguishing informational and conversational questions in social Q&A sites. Proceedings of CHI 2009, 759-768.\r15.Hecht, B., Teevan, J., Morris, M.R., and Liebling, D. SearchBuddies: Bringing Search Engines into the Conversation. Proceedings of ICWSM 2012.\r16.Horowitz, D. and Kamvar, S.D. The Anatomy of a Large-Scale Social Search Engine. Proceedings of WWW 2010, 431-440.\r17.Lampe, C., Vitak, J., Gray, R., & Ellison, N. Perceptions of Facebook’s Value as an Information Source. Proceedings of CHI 2012, 3195-3204.\r18.Lee, U. Kang, H. Yi, E., Yi, M.Y., Kantola, J. Understanding Mobile Q&A Usage: An Exploratory Study. Proceedings of CHI 2012, 3215-3224.\r19.Metts, S., Sprecher, S., and Cupach, W.R. Retrospective Self-Reports. In Studying Interpersonal Interaction (Montgomery, B.M. and Duck, S., eds.). The Guilford Press: 1991.\r20.Moraveji, N., Morris, M.R., Morris, D., Czerwinski, M., and Riche, N. ClassSearch: Facilitating the Development of Web Search Skills through Social Learning. Proceedings of CHI 2011, 1797-1806.\r21.Morris, M.R. Interfaces for Collaborative Exploratory Web Search: Motivations and Directions for Multi-User Designs. CHI 2007 Workshop on Exploratory Search and HCI.\r22.Morris, M.R. A Survey of Collaborative Web Search Practices. Proceedings of CHI 2008, 1657-1660.\r23.Morris, M.R., Fisher, D., and Wigdor, D. Search on Surfaces: Exploring the Potential of Interactive Tabletops for Collaborative Search Tasks. Information Processing and Management, 46(6), November 2010.\r24.Morris, M.R. and Horvitz, E. SearchTogether; An Interface for Collaborative Web Search. Proceedings of UIST 2007, 3-12.\r25.Morris, M.R. and Teevan, J. Collaborative Web Search: Who, What, Where, When, and Why? Morgan & Claypool, 2010.\r26.Morris, M.R., Teevan, J., and Bush, S. Enhancing Collaborative Web Search with Personalization: Groupization, Smart Splitting, and Group Hit- Highlighting. Proceedings of CSCW 2008, 481-484.\r27.Morris, M.R., Teevan, J., and Panovich, K. What Do People Ask Their Social Networks, and Why? A Survey Study of Status Message Q&A Behavior. Proceedings of CHI 2010, 1739-1748.\r28.Morris, M.R., Teevan, J., and Panovich, K. A Comparison of Information Seeking Using Search Engines and Social Networks. Proceedings of ICWSM 2010.\r\n29.Paul, S.A., Hong, L., and Chi, E.H. Is Twitter a Good Place for Asking Questions? A Characterization Study. Proceedings of ICWSM 2011.\r30.Paul, S.A., Hong, L., and Chi, E.H. Who is Authoritative? Understanding Reputation Mechanisms in Quora. Proceedings of Collective Intelligence 2012.\r31.Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., and Back, M. Algorithmic Mediation for Collaborative Exploratory Search. Proceedings of SIGIR 2008, 315- 322.\r32.Purcell, K. Search and email still top the list of most popular online activities. Pew Internet &American Life Project, August 9, 2011.\r33.Rivlin, G. Does Quora Really Have All the Answers? Wired, May 2011.\r34.Shah, C. Collaborative Information Seeking: The Art and Science of Making the Whole Greater than the Sum of All. The Information Retrieval Series, Springer, 2012.\r35.Shah, C. and Gonzalez-Ibanez, R. Evaluating the Synergic Effect of Collaboration in Information Seeking. Proceedings of SIGIR 2011, 913-922.\r36.Smith, A. Nearly half of American adults are smartphone owners. Pew Internet & American Life Project, March 1, 2012.\r37.Smyth, B., Briggs, P., Coyle, M., and O’Mahoney, M. Google Shared: A Case Study in Social Search. Proceedings of UMAP 2009.\r38.Taylor, R.S. Question-Negotiation and Information- Seeking in Libraries. College & Research Libraries, 29(3), 1968, 178-194.\r39.Teevan, J., Karlson, A., Amini, S., Brush, A.J.B., and Krumm, J. Understanding the Importance of Location, Time, and People in Mobile Local Search Behavior. Proceedings of Mobile HCI 2011.\r40.Thom, J., Helsley, S.Y., Matthews, T.L., Daly, E.M., Millen, D.R. What Are You Working On? Status Message Q&A within an Enterprise SNS. Proceedings of ECSCW 2011.\r41.Twidale, M.B. and Floyd, I.R. Infrastructures from the Bottom-Up and Top-Down: Can They Meet in the Middle? Proceedings of PDC 2008, 238-241.\r42.Wiltse, H. and Nichols, J. PlayByPlay: Collaborative Web Browsing for Desktop and Mobile Devices. Proceedings of CHI 2009, 1781-1790.","title":"Collaborative Search Revisited\r","isStoredAs":"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo/#LocalFileDataObject","contentHash":"f9c51ff3150292f81302dea2e4b48a4469a51877","mimeType":"application/pdf","authors":[],"keywords":["Guides","Guides","instructions","Author's kit","Conference Publications"],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"m.r","time":1468599605880,"auto":true,"weight":1.0},{"@type":"Tag","text":"q","time":1468599605947,"auto":true,"weight":1.0},{"@type":"Tag","text":"collabor","time":1468599605680,"auto":true,"weight":1.0},{"@type":"Tag","text":"social","time":1468599605851,"auto":true,"weight":1.0},{"@type":"Tag","text":"2012","time":1468599605932,"auto":true,"weight":1.0},{"@type":"Tag","text":"survei","time":1468599605866,"auto":true,"weight":1.0},{"@type":"Tag","text":"smartphon","time":1468599605801,"auto":true,"weight":1.0},{"@type":"Tag","text":"linkedin","time":1468599605912,"auto":true,"weight":1.0},{"@type":"Tag","text":"respond","time":1468599605829,"auto":true,"weight":1.0},{"@type":"Tag","text":"network","time":1468599605897,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":36,"appId":"PeyeDF_f655b68381c09877b3ffd266515161763bd2af98","timeCreated":1458654848839,"timeModified":1461915878385,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.029103315,"uri":"file:///Users/cheny13/Downloads/10.1.1.221.1557.pdf","plainTextContent":"HCIR 2009\rProceedings of the Third\rWorkshop on Human-Computer Interaction and Information Retrieval\rhttp://cuaslis.org/hcir2009\rThe Catholic University of America, Washington DC, USA October 23, 2009\rWorkshop Chairs:\rBill Kules, The Catholic University of America Daniel Tunkelang, Endeca\rRyen White, Microsoft Research\rSupporters:\r\nThird Workshop on\rHuman-Computer Interaction and Information Retrieval\rWhen we held the first HCIR workshop in 2007, the idea of uniting the fields of Human- Computer Interaction (HCI) and Information Retrieval (IR) was a battle cry to move this research area from the fringes of computer science into the mainstream. Two years later, as we organize this third HCIR workshop on the heels of a highly successful HCIR 2008, we see some of the fruits of our labor. Topics like interactive information retrieval and exploratory search are receiving increasing attention, among both academic researchers and industry practitioners.\rBut we have only begun this journey. Most of the work in these two fields still stays within their silos, and the efforts to realize more sophisticated models, tools, and evaluation metrics for information seeking are still in their early stages.\rIn this year's one-day workshop, we will continue to explore the advances each domain can bring to the other.\rii\r\nPanel Papers\rTable of Contents\r Usefulness as the Criterion for Evaluation of Interactive\rInformation Retrieval .................................................................................1 Michael Cole, Jingjing Liu, Nicholas J. Belkin, Ralf Bierig, Jacek Gwizdka,\rChang Liu, Jun Zhang and Xiangmin Zhang (Rutgers University)\r Modeling Searcher Frustration ...................................................................5 Henry Feild and James Allan (University of Massachusetts Amherst)\r Query Suggestions as Idea Tactics for Information Search ............................9 Diane Kelly (University of North Carolina at Chapel Hill)\r I Come Not to Bury Cranfield, but to Praise It ..............................................13 Ellen Voorhees (National Institute of Standards and Technology)\r Search Tasks and Their Role in Studies of Search Behaviors ....................... 17 Barbara M. Wildemuth (University of North Carolina at Chapel Hill)\rand Luanne Freund (University of British Columbia)\rPoster Papers\r Visual Interaction for Personalized Information Retrieval ............................ 22\rJae-wook Ahn and Peter Brusilovsky (University of Pittsburgh)\r PuppyIR: Designing an Open Source Framework for Interactive\rInformation Services for Children ............................................................ 26 Leif Azzopardi (University of Glasgow), Richard Glassey (University of Glasgow), Mounia Lalmas (University of Glasgow), Tamara Polajnar (University of Glasgow) and Ian Ruthven (University of Strathclyde)\r Designing an Interactive Automatic Document Classification System ...........30 Kirk Baker (Collexis), Archna Bhandari (National Institutes of Health) and\rRao Thotakura (National Institutes of Health)\riii\r\n A Graphic User Interface for Content and Structure Queries in\rXML Retrieval ........................................................................................ 34 Luis M. de Campos, Juan M. Fernández-Luna, Juan F. Huete and\rCarlos J. Martín-Dancausa (University of Granada)\r The HCI Browser Tool for Studying Web Search Behavior ...........................38 Robert Capra (University of North Carolina at Chapel Hill)\r Improving Search-Driven Development with Collaborative Information Retrieval Techniques ..............................................................................42 Juan M. Fernández-Luna (University of Granada), Juan F. Huete\r(University of Granada), Ramiro Pérez-Vázquez (Universidad Central de Las Villas) and Julio C. Rodríguez-Cano (Universidad de Holguín)\r A Visualization Interface for Interactive Search Refinement ........................ 46 Fernando Figueira Filho (State University of Campinas),\rJoão Porto de Albuquerque (University of Sao Paulo), André Resende\r(State University of Campinas), Paulo Lício de Geus (State University of Campinas) and Gary Olson (University of California at Irvine)\r Cognitive Dimensions Analysis of Interfaces for Information Seeking .......... 50 Gene Golovchinsky (FX Palo Alto Laboratory, Inc.)\r Cognitive Load and Web Search Tasks ..................................................... 54 Jacek Gwizdka (Rutgers University)\r Visualising Digital Video Libraries for TV Broadcasting Industry:\rA User-Centered Approach ..................................................................... 58 Mieke Haesen, Jan Meskens and Karin Coninx (Hasselt University)\r Log Based Analysis of How Faceted and Text Based Searching Interact\rin a Library Catalog Interface ...................................................................62 Xi Niu (University of North Carolina at Chapel Hill), Cory Lown\r(North Carolina State Libraries) and Bradley M. Hemminger\r(University of North Carolina at Chapel Hill)\r Freebase Cubed: Text-based Collection Queries for Large, Richly Interconnected Data Sets ........................................................................66 David F. Huynh (Metaweb Technologies, Inc.)\riv\r\n System Controlled Assistance for Improving Search Performance ............... 70 Bernard J. Jansen (Pennsylvania State University)\r Designing for Enterprise Search in a Global Organization ...........................74 Maria Johansson and Lina Westerling (Findwise AB)\r Cultural Differences in Information Behavior .............................................78 Anita Komlodi (University of Maryland Baltimore County) and Karoly Hercegfi (Budapest University of Technology and Economics)\r Adapting an Information Visualization Tool for Mobile\rInformation Retrieval .............................................................................. 82 Sherry Koshman and Jae-wook Ahn (University of Pittsburgh)\r A Theoretical Framework for Subjective Relevance ....................................87 Katrina Muller and Diane Kelly (University of North Carolina)\r Query Reuse in Exploratory Search Tasks .................................................91 Chirag Shah and Gary Marchionini (University of North Carolina at Chapel Hill)\r Towards Timed Predictions of Human Performance for Interactive\rInformation Retrieval Evaluation .............................................................. 95 Mark D. Smucker (University of Waterloo)\r The Information Availability Problem ........................................................ 99 Daniel Tunkelang (Endeca)\r Exploratory Search Over Temporal Event Sequences: Novel\rRequirements, Operations, and a Process Model .....................................102 Taowei Wang, Krist Wongsuphasawat, Catherine Plaisant and Ben Shneiderman (University of Maryland)\r Keyword Search: Quite Exploratory Actually ...........................................106 Max L. Wilson (Swansea University)\r Using Twitter to Assess Information Needs: Early Results ........................109 Max L. Wilson (Swansea University)\rv\r\n Integrating User-generated Content Description to Search\rInterface Design ...................................................................................113 Kyunghye Yoon (State University of New York at Oswego)\r Ambiguity and Context-Aware Query Reformulation .................................120 Hui Zhang (Indiana University)\rvi\r\nUsefulness as the Criterion for Evaluation of Interactive Information Retrieval\rM. Cole, J. Liu, N. J. Belkin, R. Bierig, J. Gwizdka, C. Liu, J. Zhang, X. Zhang\rSchool of Communication and Information\rRutgers University\r4 Huntington Street, New Brunswick, NJ 08901, USA\r{m.cole, belkin, bierig, jacekg}@rutgers.edu, {jingjing, changl, zhangj}@eden.rutgers.edu, xiangminz@gmail.com\rABSTRACT\rThe purpose of an information retrieval (IR) system is to help users accomplish a task. IR system evaluation should consider both task success and the value of support given over the entire information seeking episode. Relevance-based measurements fail to address these requirements. In this paper, usefulness is proposed as a basis for IR evaluation.\rCategories and Subject Descriptors\rH.1.2 [Models and Principles]: User/Machine Systems – human information processing H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval – search process\rGeneral Terms\rMeasurement, Performance, Experimentation, Human Factors\rKeywords\rEvaluation, Information seeking, Interaction, Usefulness\r1 INTRODUCTION\rResearch in information retrieval (IR) has expanded to take a broader perspective of the information seeking process to explicitly include users, tasks, and contexts in a dynamic setting rather than treating information search as static or as a sequence of unrelated events. The traditional Cranfield/TREC IR system evaluation paradigm, using document relevance as a criterion, and evaluating single search results, is not appropriate for interactive information retrieval (IIR). Several alternatives to relevance have been proposed, including utility and satisfaction. We have suggested an evaluation model and methodology grounded in the nature of information seeking and centered on usefulness [1] [2]. We believe this model has broad applicability in current IR research. This paper extends and elaborates the model to provide grounding for practical implementation.\r2 INFORMATION SEEKING\rAs phenomenological sociologists (e.g., [7]) note, people have their life-plans and their knowledge accumulates during the process of accomplishing their plans (or achieving their goals). When personal knowledge is insufficient to deal with a new experience, or to achieve a particular goal, a problematic situation arises for the individual and they seek information to resolve the problem [7]. Simply put, information seeking takes place in the circumstance of having some goal to achieve or task to complete.\rWe can then think of IR as an information seeking episode\rCopyright is held by the author/owner(s).\rHCIR Workshop 2009, Washington, D.C., October 23, 2009.\rconsisting of a sequence of interactions between the user and information objects [4]. Each interaction has an immediate goal, as well as a goal with respect to accomplishing the overall goal/task. Each interaction can itself be construed as a sequence of specific information seeking strategies (ISSs) [8].\rWe believe appropriate evaluation criteria for IR systems are determined by the system goal. The goal of IR systems is to support users in accomplishing the task/achieving the goal that led them to engage in information seeking. Therefore, IR evaluation should be modeled under the goal of information seeking and should measure a system’s performance in fulfilling users’ goals through its support of information seeking.\r3 GOAL, TASK, SUB-GOAL & ISS\rIn accomplishing the general work task and achieving the general goal, a person engaged in information seeking goes through a sequence of information interactions (which are sub-tasks), each having its own short term goal that contributes to achieving the general goal. Figure 1 illustrates the relationships between the task/goal, sub-task/goal, information interaction, and an ISS.\rLet us give an example. Suppose someone in need of a hybrid car wants to choose several car models as candidates for further inspection at local dealers. The problematic situation [7] is that he lacks knowledge on hybrid cars. His general work task is seeking hybrid car information and deciding at which models he should look. He may go through a sequence of steps which have their own short-term goals: 1) locating hybrid car information, 2) learning hybrid car information, 3) comparing several car models, and 4) deciding which local dealers to visit. In each information interaction with a short-term goal, he may go through a sequence of ISSs. For example, searching for hybrid car information may consist of querying, receiving search results, evaluating search results, and saving some of them.\rThere are several general comments. First, Figure 1 shows only the simplest linear relations between the steps along the time line. In fact, the sequence of steps/sub-goals/ISSs could be non-linear. For instance, on the sub-goal level, after learning hybrid car information, the user may go back to an interaction of searching for more information. At the ISS level, after receiving search results, the user may go back to the querying step.\rSecond, the contribution of each sub-goal to the general goal may change over time. For instance, suppose in one information interaction, the user looks at information of car model 1 and decides to choose it as a final candidate. After he learns about car model 2, which outperforms car model 1 in all aspects, he removes model 1 from the candidate list. Therefore, some steps in the sequence (choosing car model 1) may contribute to the\rPage 1 of 122\r\nISS 1\r(querying)\rISS 3\r(evaluating docs)\rISS 1 (opening saved docs)\rISS 1 (comparing docs)\rISS 3 (making notes)\rTime Line\rISS 2 (making notes)\rSub-goal 1\r(locating hybrid car info)\rInformation interaction 1\rISS 2\r(receiving results)\rSub-goal 2\r( learning car info)\rInformation interaction 2\rISS 4 (saving docs)\rSub-goal 3\r(comparing car\rInformation interaction 3\rSub-goal 4\r(deciding dealer(s) to visit)\rInformation interaction 4\rISS 2 (searching dealer info)\rEvaluation based on the following three levels:\rProblematic situation (user lacks knowledge of hybrid cars and does not know what model to buy)\rGeneral Goal/task (seeking car info & deciding candidate models)\r1. The usefulness of the entire information seeking episode with respect to accomplishment of the leading task; 2. The usefulness of each interaction with respect to its contribution to the accomplishment of the leading task; 3. The usefulness of system support toward the goal(s) of each interaction, and of each ISS\rFigure 1. An IIR Evaluation Model\rsub-goal positively, but it contributes to the final and overall goal negatively in that car model 1 is eventually removed.\rThird, the leading goal of this task is, or can be taken to be, relatively stable over the course of the interaction. Different users can and will do different things to achieve similar leading goals. Some of the differences in these sequences may be characteristics of classes of users, for example, high/low domain knowledge, cognitive capacities, and of task types, including task complexity.\r4 AN EVALUATION MODEL\rFundamentally, we are interested in why a person engages an information need and how an interaction session contributes to meeting that need. It follows one must provide a measurement for the session as a whole and for the session constituents.\r4.1 Three levels of evaluation\rA user makes progress towards a goal by virtue of the results of interactions with the system. Support of results and process are two aspects of system performance. Evaluation of a system should center on how well the user is able to achieve their goal, the process of helping the user identify and engage in appropriate interactions, and the relationship of the results of those interactions to the progress toward and accomplishment of the goal. IR evaluation should then be conducted on three levels. First, it should evaluate the information seeking episode as a whole with respect to the accomplishment of the user's task/goal. Second, it should assess each interaction, meaning explicitly the effectiveness of support for each ISS, with respect to its immediate goal. Third, it should assess each interaction with respect to its contribution to the accomplishment of the overall task/goal.\rAn ideal system will support its users’ task accomplishment by presenting resources and user support in an optimally-ordered minimum number of interaction steps (cf. [3]). Resources and user support should address not only search result content, i.e., techniques to rank the most relevant documents at the top, but they should also be manifest in the system interface, including search interface, result display, and various ways to support general task accomplishments. For example, the system could have a function of comparing pages that users have seen for them to better understand or summarize what they have learned about the task topic, so as to help them in solving the task. As another example, the system may have a place for the users to make notes, or create document drafts, which on one hand, is a way of helping users start generating their task-solving documents, and, on the other hand, are helpful for relevance feedback/query reformulation.\r4.2 Criterion: Usefulness\rWe propose usefulness as the criterion for IIR evaluation. Existing measures of IR performance are inadequate for the proposed IIR evaluation model.\rThe sense of usefulness we have in mind is more general than relevance, which has come, for historical reasons [1], to be the received basis for measuring IR systems. Like relevance, people are able to give usefulness judgments as intuitive assessments that do not turn on understanding a technical definition. Usefulness, however, is suited to interaction measurements in ways relevance-based systems cannot address.\rThe problem of measuring IIR has recently received attention in terms of formal models (e.g. [4]) and the relation of local interactions to realization of search session outcomes (e.g. [5]). Usefulness measurements are distinguished from session-level measurements like Järvelin, et al.’s session-based discounted\rISS 2 (reading docs)\rISS 1\r(choosing car)\rISS 3 (deciding dealers)\rPage 2 of 122\r\ncumulative gain (sDCG) [5] in that usefulness explicitly considers the session as a whole. sDCG does not support judgment of relevance to the whole session or how results from an interaction step might be integrated into the whole. It depends on the assumption the only thing that matters is the relevance of the local interaction and the incremental change it makes on the history of relevance judgments to that point.\rUsefulness is specifically distinguishable from relevance in several dimensions. Most strikingly, a usefulness judgment can be explicitly related to the perceived contribution of the judged object or process to progress towards satisfying the leading goal or a goal on the way. In contrast to relevance, a judgment of usefulness can be made of a result or a process, rather than only to the content of an information object. It also applies to all scales of an interaction. Usefulness can be applied to a specific result, to interaction subsequences, and to the session as a whole. Usefulness, then, is more general than relevance, and well-suited to the object of providing a measurement appropriate to the concept of task goal realization.\rThis does not deny the importance of relevance as a specific measurement to be used in appropriate circumstances to determine usefulness. For example, relevance can be used as a usefulness criterion for interaction steps where the immediate goal is to gather topical documents. Here, it is the aboutness of a document that constitutes its usefulness to advancing the task, so relevance is the appropriate usefulness criterion. This example illustrates a larger point tied with the generality of usefulness as a measure. Measuring usefulness relies on adopting appropriate and varied criteria, even within a task session. Examples of such criteria include explicit judgments including relevance and usefulness, and implicit markers, such as decision and dwell times on documents, number of steps to complete a sub-goal, user’s actions to save, revisit, classify and use documents, and issue and reformulate queries. Researchers already use specific criteria such as these for evaluation. One consequence of adopting usefulness is that several measures should be used, and perhaps only for specific segments in the episode. Identifying which measures are important for episode components and for the entire episode must be experimentally determined.\rUsefulness should be applied both for the entire episode against the leading goal/task and, independently, for each sub- task/interaction in the episode. Specifically, 1) How useful is the information seeking episode in accomplishing the leading task/goal? 2) How useful is each interaction in helping accomplish the leading task? 3) How well was the goal of the specific interaction accomplished? From the system perspective, evaluation should focus on: 1) How well does the system support the accomplishment of the overall task/goal? 2) How well does the system support the contribution of each interaction towards the achievement of the overall goal? 3) How well does the system support each interaction?\r4.3 Measurements\rIdentifying specific measures of usefulness and how to obtain them are clearly difficult problems. The most important aspect of this evaluation framework is that it depends crucially upon specification of a leading task or goal whose accomplishment can itself be measured.\rGenerally, operationalization of usefulness at the level of the IR episode will be specific to the user's task/goal; at the level of contribution to the outcome it will be specific to the empirical\rrelationship between each interaction and the search outcome; and finally, at the third level, it will be specific to the goals of each interaction/ISS.\rExamples at each level might be: the perceived usefulness of the located documents in helping accomplish the whole task; task accomplishment itself, in terms of correctness, effort, or time; the extent to which systems suggestions as to what to do are taken up; the extent to which documents seen in an interaction are used in the solution; the degree to which useful documents appear at the top of a results list; and the extent to which suggested query terms are used, and are useful.\rAs an example, consider the hybrid car information seeking episode and focus on just the leading goal/task, sub-goal 1 and the information interaction 1 with its four ISSs. To demonstrate how the criterion of usefulness can be operationalized, the evaluation could be approached from the following aspects (this is not intended as an exclusive list):\r• at the level of the whole episode [leading goal/task] o accomplishment of the task [result]\r  How well did the user successfully select candidate car models? [correctness]\r  How many steps (e.g., interactions, ISSs) did the user go through for the whole task? [effort]\r  How long did the user spend to complete the whole task? [time]\ro support to the information seeking episode [process]  How useful was the system in supporting identification of appropriate sub-goals in selecting\rhybrid car models?\r  Were system suggestions on what to do (e.g., a system\rsuggesting four task steps: locating information,\rlearning, comparing, and deciding) accepted?\r  How well did the system support the user in choosing\ran appropriate sub-task sequence?\r• at the level of the information interaction/sub-goal [sub- goal 1/information interaction 1]\ro accomplishment of the sub-goal [result]\r  How well did the user successfully locate hybrid car\rinformation? [correctness]\r  How many steps (e.g., ISSs) did the user go through in\rlocating car information? [effort]\r  How long did the user spend to locate car information?\r[time]\ro support to information interaction 1[process]\r  How useful was the system in supporting users to identify appropriate ISSs in locating hybrid car\rinformation?\r  Were system suggestions on what to do (e.g.,\rsuggesting a user should now query, view results,\revaluate results or save documents) accepted?\r  How well did the system support the user in choosing\ran appropriate ISS sequence?\r• at the level of the contribution of the sub-goal to the leading goal [sub-goal 1 to the leading goal]\rPage 3 of 122\ro o\raccomplishment of the contribution [result]\r  How much did locating car information contribute to the whole task of selecting candidate car models?\rsupport to this contribution [process]\r  How useful was the system in supporting users to locate car information in order to finally select candidate car models?\r\n• at the level of the ISSs [ISSs 1-4 information interaction 1] o How useful were suggested queries/terms for formulating\rqueries? [ISS1]\ro How much were the suggested queries/terms used? [ISS1] o How well does the system support evaluation of retrieved\rdocuments? [ISS3]\ro How well does the system support saving or retaining the\rretrieved, or useful, documents? [ISS4]\r• at the level of the contribution of each ISS to the sub-goal or leading goal [ISSs 1-4 to sub-goal 1 and leading goal]\ro How useful were the suggested queries/terms (for systems with query formulation assistance) for locating car information? [ISS1 to sub-goal 1]\roHow well did the system rank documents? (using relevance and various other measures: precision, DCG, etc.) [ISS2 to sub-goal 1]\ro How useful was each viewed document in helping users locate hybrid car information? [ISS2 to sub-goal 1]\ro How useful was each viewed document in helping users select the candidate car models? [ISS2 to leading goal]\r5 CONCLUSION\rInformation retrieval is an inherently and unavoidably interactive process, which takes place when a person faces a problematic situation with respect to some goal or task. Thus, evaluating IR systems must mean both evaluating their support with respect to task accomplishment, and evaluating them with respect to the entire information seeking episode. Past, and most current approaches to IR evaluation, as exemplified by TREC, fail to address either of these desiderata, focusing as they do on relevance as the fundamental criterion, and on effectiveness of system response to a single query. In this paper, we propose an alternative evaluation model which attempts to address both of these issues, based on the criterion of usefulness as the basis for IR evaluation. Although our proposed model clearly needs more detailed explication, we believe that it offers a useful basis from which realistic and effective measures and methods of IR evaluation can be developed.\r6 ACKNOWLEDGMENTS\rThese ideas have benefited from discussion at the 2009 Dagstuhl Seminar on Interactive Information Retrieval, especially the contributions of Pertti Vakkari, Kal Järvelin, and Norbert Fuhr, An earlier version of this paper was presented at the SIGIR 2009 Workshop on The Future of IR Evaluation, and discussion there has substantially influenced this version. This work is supported by IMLS grant LG-06-07-0105-07.\r7 REFERENCES\r[1] Belkin, N.J, Cole, M., and Bierig, R. (2008). Is relevance the right criterion for evaluating interactive information retrieval? In Proceedings of the SIGIR 2008 Workshop on Beyond Binary Relevance: Preferences, Diversity, and Set- Level judgments, (Singapore, 2008). Retrieved from: http://research.microsoft.com/en-us/um/people/pauben/bbr- workshop/talks/belkin-bbr-sigir08.pdf on August 24, 2009.\r[2] Belkin, N.J., Cole, M. and Liu, J. (2009). A model for evaluation of interactive information retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation (Boston). IR Publications, Amsterdam. 7-8.\r[3] Belkin, N.J., Cool, C., Stein, A. and Thiel, U. (1995). Cases, scripts, and information-seeking strategies: On the design of interactive information retrieval systems. Expert Systems with Applications, 9(30). 379-395.\r[4] Fuhr, N. (2008). A probability ranking principle for interactive information retrieval. Information Retrieval, 11.251-265.\r[5] Järvelin, K., Price, S.L., Delcambre, L.M.L., and Nielsen, M.L. (2008). Discounted cumulative gain based evaluation of multiple-query IR sessions. In Proceedings of the 30th European Conference on Information Retrieval (Glascow, Scotland, 2008), Springer-Verlag. 4-15.\r[6] Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Edmonton, Canada, 2002) ACM. 133-142.\r[7] Schutz, A. and Luckmann, T. (1973). The structures of the life-world. Northwestern University Press, Evanston, IL.\r[8] Yuan, X.-J. and Belkin, N.J. (2008). Supporting multiple information-seeking strategies in a single system framework. In Proceedings of the 31st ACM SIGIR International Conference on Research and Development in Information Retrieval (Singapore, 2008). ACM. 247-254.\r4.4\rExperimental frameworks for IIR\rsystem measurement\rOne challenge in measuring the performance of IIR systems is to move beyond the Cranfield and TREC relevance-based models. Several experimental frameworks are available to measure system performance over interactive sessions.\rTraditional user-studies can be used by setting a task with a measurable outcome that is related to information seeking activities. Systems are then compared by both outcome and the interaction path taken to task completion. Our proposal addresses how the interaction path can be assessed to measure its contribution to the outcome.\rThe limitations of user-studies are scale-related. One can address only a small number of tasks with a limited number of subjects. User-studies have the virtue of well-specified tasks and the ability to collect many details about users and their interactions.\rAn alternative framework, in the spirit of A-B system comparisons often used in commercial settings, is to make available two versions of a system and compare measures as people make use of the system (e.g. [6]). A big advantage of this approach is the ability to conduct large-scale tests with many users and (implicitly) many tasks. The limitation is that one needs to infer properties of the tasks and also the usefulness of the system response to meeting the needs of the users. One difficult technical issue is the identification of sessions to enable session-level results analysis.\rA third, somewhat intermediate, approach to achieve reasonable scale with enough detail to enable a rich assessment of system performance for user task support, is to build a reference database of session interactions. This might be assembled in a cooperative effort and made available to research groups to generate system performance results. Such a usefulness-based interaction database would presumably include user models to choose interaction outcomes depending on the choices offered by the system and the support provided by the system at each step along the way. Such a database might be generated from uniformly-instrumented user studies and a reference user model(s).\rPage 4 of 122\r\nABSTRACT\rWhen search engine users have trouble finding what they are looking for, they become frustrated. In a pilot study, we found that 36% of queries submitted end with users being moderately to extremely frustrated. By modeling searcher frustration, search engines can predict the current state of user frustration, tailor the search experience to help the user find what they are looking for, and avert them from switch- ing to another search engine. Among other observations, we found that across the fifteen users and six tasks in our study, frustration follows a law of conservation: a frustrated user tends to stay frustrated and a non-frustrated user tends to stay not frustrated. Future work includes extracting fea- tures from the query log data and readings from three phys- ical sensors collected during the study to predict searcher frustration.\rCategories and Subject Descriptors\rH.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—relevance feedback, search process\rGeneral Terms\rHuman Factors, Measurement\rKeywords\rInformation retrieval, human-computer interaction, frustra- tion modeling\r1. INTRODUCTION\rIn this work, we investigate modeling searcher frustra- tion. We consider a user to be frustrated when their search process is impeded, regardless of the reason. A frustration model capable of predicting how frustrated searchers are throughout their search is useful retrospectively to collect statistics about the effectiveness of a search system. More importantly, it allows for real-time system intervention of\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rCopyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.\rfrustrated searchers, hopefully preventing users from leaving for another search engine or just giving up. Evidence from users’ interactions with the search engine during a task can be used to predict a user’s level of frustration. Depending on the level of frustration and some classification of the type of frustration, the system can change the underlying retrieval algorithm or the actual interface. For example, we posit that one common cause or type of frustration is a user’s in- ability to formulate a query for their otherwise well defined information need.\rOne way that a system could adapt to address this kind of frustration is to show the user a conceptual break down of the results; rather than listing all results, group them based on the key concepts that best represent them. So if a user enters ‘java’, they can see the results based on ‘islands’, ‘programming languages’, ‘coffee’, etc. Of course, most search engines already strive to diversify result sets, so documents relating to all of these different facets of ‘java’ are present, but they might not stick out to some users, causing them to become frustrated.\rAn example from the information retrieval (IR) literature of a system that adapts based on a user model is work by White, Jose, and Ruthven [5]. They used implicit relevance feedback to detect the changes in the type of information need of the user and alter the retrieval strategy. In our work, we want to detect frustrated behavior and adapt the system based on the type of frustration.\rWhile automatic frustration modeling has not been specif- ically investigated in the IR literature, it has been explored in the area of intelligent tutoring systems (ITS) research. When a system is tutoring a student, it is helpful to track that student’s affective state, including frustration, in order to adapt the tutoring process to engage the student as much as possible. Our research borrows heavily from the tools used in and insights gleaned from the ITS literature.\rThe goals for our line of research are as follows: first, determine how to detect a user’s level of frustration; second, determine what the key causes or types of frustration are; and third, determine the kinds of system interventions that can counteract each type of frustration. Our current work focuses on the first two, leaving the third for future studies.\r2. RELATED WORK\rOur research is based heavily on two bodies of work: one from the IR literature and the other from the ITS literature. We will first describe work by Fox et al. [3] in IR followed by the work of Cooper et al. [2] and Kapoor, Burleson, and Picard [4] in the field of ITS.\rModeling Searcher Frustration\rHenry Feild and James Allan\rCenter for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst Amherst, MA 01003\r{hfeild, allan}@cs.umass.edu\rPage 5 of 122\r\n2.1 Predicting searcher satisfaction\rFox et al. [3] conducted a study to determine if there is an association between implicit measures of user interest de- rived from query logs and explicit user satisfaction. They collected satisfaction feedback for every non-search engine page visited and for every session (see Section 3 for the def- inition of session).\rFox et al. found there exists an association between query log features and searcher satisfaction, with the most pre- dictive features being click through, the time spent on the search result page, and the manner in which a user ended a search. Using a Bayesian model, they were able to predict the level of satisfaction with 57% accuracy at the results level (with a baseline of 40%) and 70% at the session-level (with a baseline of 56%).\rIn our work, we extend the research of Fox et al. to include a satisfaction feedback prompt for every individual query. We also ask users to rate their frustration with the search process and the degree to which each query’s results meet their expectations. In addition, our scales are finer—five levels for all feedback rather than three—which should allow users to better assess themselves. Our work in part explores if the success of modeling user satisfaction with query log features transfers to modeling frustration.\r2.2 Detecting ITS user emotion\rCooper et al. [2] describe a study in which students using an intelligent tutoring system were outfitted with four sen- sors: a mental state camera that focused on the student’s face, a skin conductance bracelet, a pressure sensitive mouse, and a chair seat capable of detecting posture. The goal of the study was to ascertain if using features drawn from the sensor readings in combination with features extracted from user interaction logs with the ITS could more accurately model the user’s affective state than using the interaction logs alone.\rCooper et al. found that across the three experiments they conducted, the mental state camera was the best stand-alone sensor to use in conjunction with the tutoring interaction logs for determining frustration. However, using features from all sensors and the interaction logs performed best. They used step-wise regression to develop a model for de- scribing each emotion. For frustration, the most significant features where from the interaction logs and the camera, though features from all sensors were considered in the re- gression. The model obtained an accuracy of 89.7%; the baseline—guessing that the emotional state is always low— resulted in an accuracy of 85.29%.\rIn a related study using the same sensors, but different features, Kapoor, Burleson, and Picard [4] created a model to classify ITS user frustration. They achieved a classifica- tion accuracy of 79% with a chance accuracy of 58%. These studies demonstrate the utility of the sensor systems for pre- dicting ITS user frustration. In our research, we will explore how well these sensors predict searcher frustration.\r3. DEFINITIONS\rTo be clear, we will use the following definitions.\rTask. A task is a formal description of an information need. Query. A query is the text submitted to a search engine. We also discuss query level events, which refer to the user interactions with the results returned for the query and any\rsubsequent pages visited until the next query is entered or the session is completed, whichever comes first.\rSession. A session consists of all of the user interactions while searching for information for a particular task. Satisfaction. We define satisfaction as the fulfillment of a need or want; in the case of IR, a user’s information need. For example, a user can be asked the degree to which a Web page, query, or session fulfilled their information need. Frustration. We consider a user frustrated when their search process is impeded, regardless of the reason. To mea- sure frustration, we ask users to rate their level of frustration with the current task up to the current point on a scale of 1 (not frustrated at all) to 5 (extremely frustrated). A user is considered frustrated if they indicate a level of 3 or more. While satisfaction and frustration are closely related, they are distinct. As a consequence, a searcher can ultimately satisfy their information need (i.e., be satisfied), but still have been quite frustrated in the process [1].\r4. USER STUDY\rWe conducted a user study consisting of 15 undergradu- ate and graduate students, each of which was asked to find information for the same six tasks using the Web. Their interactions with the browser were logged along with data from three physical sensors. The subjects were asked to as- sess their level of satisfaction at the result, query, and session levels, their frustration at the query level, and the degree to which the results returned for each query met their expecta- tions. We describe each of the aspects of the study in more detail below.\r4.1 Tasks\rSubjects were asked to search for information to satisfy six tasks. Here we give a brief description of each along with a label in italics at the beginning of the description.\r• [Thailand] Search the Web to make a list of pros and cons of a trip to Thailand.\r• [Anthropology] Search the Web for decent anthropology programs that are as close to Ohio as possible.\r• [GRE] Search the Web to evaluate your chances of get- ting into one of the top 25 computer science PhD pro- grams with a GRE score of 525 Verbal and 650 Math.\r• [Computer Virus] Search for descriptions of the next big computer virus or worm.\r• [MS Word] In MS Word 2008 for Mac, you created a document and set the background to a grid pattern and saved it. When you opened the document later, the background no longer had the grid pattern, but was a solid color. Search the Web to find out how to resolve this.\r• [Hangar Menu] Find the menu for the Hangar Pub and Grill in Amherst, MA.\rAll tasks are meant to be realistic, but are not taken from pre-existing query logs. We chose tasks we anticipated would cause some amount of frustration since our main objective is to understand frustration. Users were asked to spend no more than about ten minutes on a given task, though this was a soft deadline. A timer and reminder pop-up at the ten minute mark were provided in a browser plugin to remind them of the time.\rPage 6 of 122\r\nFive of the tasks are informational, four of which are more research oriented and open ended and one that we catego- rize as a technical debugging task. The five research ori- ented tasks were chosen because of the anticipated time- to-completion; such open ended tasks should involve more queries and more opportunities for the user to become frus- trated. The Thailand and Computer Virus tasks are the most open ended, while the Anthropology and GRE add in some additional constraints that could make the search process more difficult.\rThe task MS Word involves searching for the solution to a bug with Microsoft Word 2008 for Mac. The information need is informational, but what constitutes the task being satisfied depends on whether or not a proposed solution ac- tually remedies the bug. We anticipated that formulating queries for this task would be difficult, making the user frus- trated. The actual problem is real and was encountered by one of the authors.\rThe sixth task, Hangar Menu, is navigational. However, the source is difficult to find for those trying to find it for the first time, making this a good frustration-causing task. The inspiration for this task came from looking at query logs that contained many sessions in which users were clearly trying to navigate to a homepage or Web source that either did not exist or was unavailable.\r4.2 Feedback\rFor each page that was visited for a task, the user was prompted to enter the degree to which the page satisfied the task, with an option that the page was not viewable or not in English. Five satisfaction options were given from “This result in no way satisfied the current task” to “This result completely satisfied the current task”.\rAfter the results for a query were viewed, users were asked to assess the degree to which the results as a whole for that query satisfied their information need. We asked this be- cause each individual result viewed may have only partially satisfied the information need, but taken together, they fully satisfy the information need. Users were also asked to assess how the results returned for the query met their expectations for the query. Five options were given, ranging from much worse to much better. Finally, users were asked to rate their frustration with the search up to the current point on a scale of 1 (not frustrated at all) to 5 (extremely frustrated).\rAt the end of each task, users were asked to indicate the degree to which the task was satisfied over the course of the entire session. They were also given an opportunity to comment about their knowledge of the task before they began searching.\r4.3 Sensors\rWe used the mental state camera, pressure sensitive mouse, pressure sensitive chair, and features used by Cooper et al. [2] (see Section 2.2). The camera reports confidence values for 6 emotions (agreeing, disagreeing, concentrating, thinking, interested, and unsure) in addition to several raw features, such as head tilt and eyebrow movement.\rThe mouse has six pressure sensors that report the amount of pressure exerted on the top, left, and right sides of the mouse. Cooper et al. averaged the pressure across all six sensors to obtain one pressure reading.\rThe chair also has six pressure sensors: three on the seat and three on the back. Cooper et al. derived the features\rnetSeatChange, netBackChange and sitForward from the raw readings.\r4.4 Browser logging\rTo log both the feedback and generate a query log for the sessions, we created a Firefox plugin based on the Lemur Toolbar1. The events logged include the amount of a page scrolled; new tabs being opened and closed; left, right, and middle clicking on links; new windows being opened and closed; the HTML for result pages returned by Google, Ya- hoo!, Bing, and Ask.com; and the current page in focus.\rThis is a client-side query log and is richer than a server- side query log. Both client-side and server-side features can be extracted. We plan to extract features very similar to those used by Fox et al. [3].\r5. DISCUSSION\rOur initial analysis of the data from this first experi- ment have provided several interesting insights into mod- eling searcher frustration. However, we require additional experiments to provide the data necessary to make our find- ings statistically significant.\rAcross the fifteen users, a total of 351 queries were entered— an average of 3.9 per session. Users reported being frus- trated (3–5 on the frustration scale) for 127 or 36% of the queries. The majority of queries (56%) performed worse than expected. Despite unmet expectations, users found their information need at least partially satisfied for 71% of queries. A total of 705 pages were visited (either from the results page or from browsing) for an average of two pages per query. Users at least partially satisfied their information need for 92% of the 90 sessions.\rFigure 1 shows the level of frustration for each individual averaged over the six tasks. The x-axis shows the number of queries that have been entered so far in a session and the y- axis shows the level of frustration on the 1–5 scale. The exact frustration value is smoothed with the user’s overall average frustration, since the number of queries entered for each task is different. The thick line in the middle shows the overall average across all users and tasks. The key observations are that different users are more likely to be frustrated (or not) and frustration tends to increase as session length increases.\rLooking at averages over individual tasks, there appears to be some interaction between query level satisfaction, ex- pectation, and frustration. Tasks where users’ expectations were closer to being met/exceeded and queries at least par- tially satisfied the task also had lower frustration ratings.\rTurning to individual tasks, the research oriented infor- mational tasks shared similar characteristics in the number of queries entered, pages visited, etc. The MS Word task is an outlier in terms of information tasks, however. The task was the most frustration-invoking task, with an average frus- tration rating of 3.0 (moderate frustration). It also had the lowest average satisfaction rating (1.5) and meeting of ex- pectations rating (1.7). It had the most number of queries and the second lowest number of page visitations (the first was the navigational task, Hangar Menu). The average time spent to complete or quit the task was in range of most the other tasks—about ten minutes. The high volume of queries and the low page visitation suggests that formulating queries for the task was difficult, as expected.\r1http://www.lemurproject.org/querylogtoolbar/\rPage 7 of 122\r\nThe Hangar Menu task was not as difficult as anticipated. Users entered fewer queries, visited fewer pages, and spent less time on average for this task. The average frustration rating was low (1.8) while the average satisfaction and ex- pectation ratings were high (2.9 and 2.6, respectively).\rAnother interesting observation we have made is a model of frustration transitions. Aggregating across all users and tasks, we find the probability of becoming (not) frustrated given that a user is (not) frustrated. Again, we consider a user frustrated if their rating is between 3–5. This model shows that the user is frustrated after the first query in 26% of the 90 sessions and is not frustrated in the remaining 74%. Once frustrated, 82% of the time users will be frustrated after their next query and will become not frustrated 18% of the time. Once not frustrated, users will stay not frustrated after the next query 85% of the time and become frustrated the other 15% of the time. This trend is mostly consistent across individual users and tasks.\rThe frustration transition model gives us a key insight into understanding frustration and how to detect it. Namely, frustration is a function not only of the current interaction, but of the previous state of frustration. This suggests that a temporal classifier, such as a Hidden Markov Model, may be a good candidate for detecting frustration.\rFigure 1: The average frustration across tasks for each user after the nth query. Each line represents an individual user; the thick line is the average across all users.\rand did nothing else for that query, is the most frequent se- quence for two tasks and leads to frustration about 60% of the time. This probably indicates a query formulation prob- lem. For the other four tasks, the sequence is the second or third most common sequence, but usually ends in the user not being frustrated. The same sequence leads to no or low satisfaction almost 100% of the time for the first two tasks, demonstrating a complex relationship between frustration and satisfaction.\rIn addition to further analysis with the current data set, we are planning a second experiment. This experiment will involve more people and different tasks. The new tasks will have a larger coverage of the informational and navigational information need types (we will not consider transactional). They will also be narrower in scope and more clearly defined. Some users in the previous experiment found the tasks too open ended. Several bugs in the logging software must also be fixed. After browser crashes, some of the JavaScript was not re-enabled, causes certain events to not be logged.\rThe data from our first experiment is rich and has pro- vided us with many key insights into understanding searcher frustration. Among our observations are: frustration tends to increase with the number of queries submitted for a single task; certain searchers are more predisposed to be frustrated with the search process; a user’s state of frustration is largely conserved, with a small chance of transitioning to the oppo- site state; and frustration appears to have a different shape for different types of tasks, such as informational versus nav- igational. More analysis and data will help us to understand the causes of frustration and how to model them.\r7. ACKNOWLEDGMENTS\rThis work was supported in part by the Center for In- telligent Information Retrieval and in part by UpToDate. Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not nec- essarily reflect those of the sponsor. We would also like to thank Beverly Woolf, David Cooper, and Winslow Burleson for loaning the sensors and logging software.\r8. REFERENCES\r[1] I. Ceaparu, J. Lazar, K. Bessiere, J. Robinson, and B. Shneiderman. Determining Causes and Severity of End-User Frustration. International Journal of Human-Computer Interaction, 17(3):333–356, 2004.\r[2] D. G. Cooper, I. Arroyo, B. P. Woolf, K. Muldner,\rW. Burleson, and R. Christopherson. Sensor model student self concept in the classroom. In First and Seventeenth International Conference on User Modeling, Adaption, and Personalization, Trento, Italy, June 2009.\r[3] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and\rT. White. Evaluating implicit measures to improve web search. ACM Transactions on Information Systems (TOIS), 23(2):147–168, 2005.\r[4] A. Kapoor, W. Burleson, and R. Picard. Automatic prediction of frustration. International Journal of Human-Computer Studies, 65(8):724–736, 2007.\r[5] R. White, J. Jose, and I. Ruthven. An implicit feedback approach for interactive information retrieval. Information Processing and Management, 42(1):166–190, 2006.\r6.\rFUTURE WORK AND CONCLUSIONS\rThere are many avenues of analysis we are looking into currently, including extracting features from the query logs and sensor readings to predict frustration. Our goal is to predict frustration based solely on query logs, i.e., not to rely on sensors. We are exploring gene analysis, a technique reported by Fox et al. [3]. This form of browsing analysis ab- stracts the query log events, assigning a letter or symbol to a few key events. Stringing events together yields a sequence, which we can analyze in a manner similar to genes. In a brief analysis, we found that gene sequences mean different things for different tasks. For instance, the sequence “qL”, meaning the user entered a query, looked at the results page\rPage 8 of 122\r\nQuery Suggestions as Idea Tactics for Information Search\rABSTRACT\rThis paper explores the thesis that query suggestions function as a type of idea tactic. A further thesis of this paper is that query suggestions are most useful for open-ended search tasks that require the searcher to explore and learn about a particular topic, and in situations where topics are difficult. Results are presented from two studies that examined people’s use of query suggestions while searching for open-ended search topics and how usage varied according to topic difficulty.\r1. INTRODUCTION\rIn 1979, Marcia Bates introduced the notion of an idea tactic as a move to help searchers generate new ideas or solutions to information search problems [1]. Bates noted that idea tactics serve a psychological purpose in that they are intended to help improve the searcher’s thinking and creative processes (p. 280). Bates further justifies the importance of idea tactics by observing that new ideas are often “blocked or limited by one’s current thinking” (p. 281). The basic idea is that the searcher’s internal model of the information problem can sometimes block their efforts to think of novel and useful ways to proceed with search.\rBates proposed a number of tactics emphasizing idea generation and pattern-breaking. Idea generation focuses on the stimulation of new ideas by thinking and conducting activities outside of a retrieval system. Pattern-breaking tactics help searchers go beyond their current way of thinking about the problem and suggest moves that can be made while interacting with a retrieval system. Although some pattern-breaking tactics are intended to be used by the searcher introspectively, a number of these focus on search behavior and query generation. However, searchers’ abilities to use these tactics may be limited since often searchers do not have clear understandings of their information needs [2].\rBates’ article was primarily aimed at professional searchers, offering formal guidance and instruction about how to conduct bibliographic searches. Bates did not suggest how information retrieval (IR) systems might support idea tactics and since that time, few (if any) researchers have explored if and how search interfaces can support idea tactics. This paper explores the thesis that query suggestions function as a type of idea tactic by providing support for both idea generation and pattern-breaking. Query suggestions are alternative queries that the system displays to searchers. These suggestions are often identified by examining past searchers’ queries and comparing these to the current searcher’s query and are thus, human-generated, but suggestions can also be machine-generated. Many researchers have studied query suggestion features for information seeking tasks [e.g., 3, 6], but studies have not been conducted to understand how these suggestions support idea generation. Query suggestions can be particularly beneficial because they provide searchers with alternative methods for exploring topics and can potentially help searchers develop better understandings of their topics and richer\rvocabularies with which to pose queries. Furthermore, query suggestions allow searchers to continue to execute searches even when they are unable to formulate their own queries.\rIt is unlikely that query suggestions are useful in all types of search situations and for all types of tasks. A further thesis of this paper is that query suggestions are most useful for open-ended search tasks that require the searcher to explore and learn about a particular topic. This thesis is motivated by the information search models of Kuhlthau [4] and Vakkari [5] that depict the processes that occur while searchers engage in these types of tasks. These models are anchored by different stages, which are associated with different types and sources of desired information, search tactics and mental models. Particularly relevant to this work is Vakkari’s three stages (pre-focus, formulation and post-focus) and the associated types of information sought (general information, faceted background information, specific information) and mental representations of searchers (general or vague, differentiated, integrated). It is proposed that query suggestions can assist in all stages of search by helping the searcher get started during pre- focus, explore various facets during formulation and follow-up with specific questions during post-focus. Following these models, it is further proposed that query suggestions will be most useful as an idea tactic for topics that are difficult to search.\rIn this paper, results are presented from two separate studies that examined people’s use of query suggestions while searching for open-ended search topics. The first of these studies was presented earlier this year at the ACM SIGIR Conference [3], although the results presented here were not published in that paper. The second study has not been published, but is under review. In this paper, key results from each study related to the use of query suggestions as idea tactics, and compares and discusses these results. Details of the study method will not be presented, but the basic setup involved a test collection of newspaper articles (over 1 million) and assigned search tasks that asked subjects to find and save documents related to assigned topics. Subjects used an experimental search system that used Lemur1 for indexing and retrieval. The interfaces were similar and presented subjects with a query box, basic navigation facilities and query suggestions.\r2. STUDY1\rThe major purpose of Study 1 was to evaluate the effectiveness of an experimental technique for generating query suggestions automatically that combined users’ queries with terms generated by classic term relevance feedback techniques. We also compared differences between term and query suggestion interfaces. Each subject completed four topics: two with the term suggestion interface and two with the query suggestion interface. We only report results of subjects’ searches with the query\r1 http://www.lemurproject.org\rDiane Kelly\rSchool of Information and Library Science University of North Carolina at Chapel Hill Chapel Hill, NC 27599-3360 USA\rdianek@email.unc.edu\rPage 9 of 122\r\nsuggestion interface (2 searches per subject). Fifty-five undergraduate students participated in the study. Approximately half of the subjects were presented with query suggestions that had been generated automatically, while about half were presented with query suggestions that had been generated by other subjects. More details about this study can be found in [3].\r2.1 UseofSuggestions\rSubjects submitted a total of 649 queries. Five-hundred and eight (78%) of these queries were queries they entered manually while 141 (22%) were suggestions. On average, subjects entered 5.90 queries per topic (SD=4.26), or about 4.62 (SD=3.30) manually created queries and 1.28 (SD=1.92) query suggestions. The fewest query suggestions taken for a search was 0 and the most was 12. Subjects who received query suggestions that had been created by other people issued a similar number of queries to subjects who received automatically generated suggestions: 323 and 326, respectively. However, those who received user generated query suggestions selected more query suggestions (n=92) than those who received automatically generated suggestions (n=49), and entered fewer queries manually (231 vs. 277, respectively).\r2.2 SuggestionsandTopicDifficulty\rThe assigned search topics were classified according to difficulty, based on subjects’ performances in a previous study. Topics were divided into quartiles: easy, medium, moderate and hard. Each subject completed one topic from each difficulty bin.\rTable 1 displays the mean number of queries issued for topics of each level of difficulty, as well as the mean number of queries subjects generated themselves and the mean number of query suggestions taken. Overall, subjects entered more queries as topic difficulty increased. An average of 4.89 queries were entered for easy topics, while 7.50 were entered for the most difficult topics. This trend is also evident in the number of subject generated queries and the number of query suggestions, although some slight differences exist (for instance, more query suggestions were taken for moderate topics than hard topics). However, for easy and medium topics, subjects selected less than 1 suggestion per topic, while for moderate and hard topics, subjects selected about 1.65 suggestions. These results indicate that query suggestion features are more likely to be useful for difficult topics.\rTable 1. Mean (SD) number of queries entered for topics of various difficulty levels: easy, medium, moderate, and hard.\r3. STUDY2\rThe major purpose of this study was to investigate the extent to which users could be induced to take query suggestions by manipulating usage information associated with each suggestion. The set-up of this study was similar to that of Study 1 except that subjects were only provided with user generated query suggestions. We preselected these suggestions from queries\rentered by subjects in Study 1 for the same search topics (only four topics were used in Study 2). Eight query suggestions were provided for each topic – four query suggestions were good performing queries and four were poor performing queries (we predetermined good and poor queries by examining how many relevant documents were retrieved in the top 20 results). Usage information indicating how many other people used the queries was also displayed next to the query suggestions. For half of the queries this information was high (i.e., many people used the query) and for half the queries this information was low (i.e., very few people used the query). The order of the queries and associated usage information was randomized. Twenty-three subjects participated (22 undergraduates and 1 graduate).\r3.1 Use of Suggestions\rSubjects submitted a total of 722 queries for all topics combined (32 queries on average, or about 8 queries per subject per topic). Four-hundred twenty-five (59%) of these queries were of their own creation while 297 (41%) were suggestions. Each subject was shown a total of 32 suggestions (8 suggestions per topic * 4 topics) and selected an average of 13.70 (SD=7.02). One subject did not select any suggested queries, while another selected 242.\rThe number and proportion of query suggestions taken by subjects in this study (n=297, 41%) was much greater than the number taken by subjects in Study 1 (n=141, 22%). Although many aspects of these studies were similar (subject population, system, collection, topics, basic interface), several main aspects (such as the content of the query suggestions) differed. Thus, we are unable to make any conclusive statements about what impacted the difference in use of suggestions, but note one major difference – subjects’ perceptions of the origins of the suggestions. In Study 1, some query suggestions were created by other users and some were created automatically by the system, but subjects were not provided with any information about the origins of these suggestions. In Study 2, subjects were only presented with user generated suggestions and were told via the interface the origins of the suggestions (that is, subjects were told that certain numbers of other users entered the queries). We did not quiz subjects in the first study to find out their beliefs about the origins of the query suggestions, so we cannot say with certainty that they did or did not believe the queries were created by humans. However, the discrepancy in the amount of suggestions taken between subjects in the two studies suggests that agency attribution is an important factor in determining whether subjects take suggestions and that attributing the suggestions to other humans rather than a machine might increase the use and uptake of suggestions. Our future work will explore this directly by manipulating the actual and communicated origins of the query suggestions. Results of such work might demonstrate if users are biased towards human recommendations and why they appear to make more use of these recommendations than computer generated recommendations.\rSince the set of suggested queries for each search remained the same, we were able to examine the overlap between the queries\r2 Subjects in this study were not influenced by the usage information: subjects selected 148 queries that were associated with low usage information and 165 queries that were associated with high usage information.\rTopic Difficulty\rTotal\rEasy\rMedium\rModerate\rHard\rTotal Queries Issued\r4.89 (3.52)\r5.00 (2.77)\r6.37 (4.36)\r7.50 (5.69)\r5.90 (4.26)\rSubject Generated Queries\r4.04 (2.93)\r4.00 (2.52)\r4.63 (2.95)\r5.92 (4.42)\r4.62 (3.30)\rQuery Suggestions Taken\r0.86 (1.30)\r1.00 (1.28)\r1.74 (2.64)\r1.58 (2.14)\r1.28 (1.92)\rPage 10 of 122\r\nsubjects entered manually and the suggested queries to see how many duplicates occurred. That is, how many times a manually entered query matched exactly a suggested query. We found that 113 of the 425 subject-created queries were exactly the same as one of the suggested queries. Of these 113 queries, only 7 were the first queries typed by subjects (in which case they would not be duplicates since subjects did not receive any suggestions until they entered one query). Although there is no way to ascertain that subjects did not naturally type the 106 duplicate queries on their own, these results suggest that some subjects took suggestions by manually entering the queries rather than clicking on them. Thus, using a simple click metric to measure uptake may not tell the whole story. While suggested queries may provide the subject with ideas for search and a greater understanding of their topic, this usage may not easily measurable by observing behavior. Eye tracking can potentially provide a better indicator of how frequently a subject examines suggestions and can also be correlated with observable behaviors, but this too does not adequately capture the extent to which query suggestions expand the user’s knowledge of the topic and potentially assist with query formulation and identifying search moves. Thus, one challenge to studying the use of query suggestions as idea tactics is identifying more robust methods for observing their impact.\r3.2 IntegrationofSuggestionsintoSearching\rWhen do subjects use query suggestions during their searches? Figure 2 shows the sequence of queries entered by subjects and whether the queries were created by the subject or were suggestions. (Note that subjects had to enter one self-created query at the start of the session before any suggestions were displayed.) This Figure shows that many subjects made use of the suggestion feature early in their searches. For the second and third queries issued, subjects were nearly equally as likely to enter their own query or click on a suggestion.\rFigure 2. Frequency and source of query (self-created or suggested) according to order of submission during search.\rOne remarkable thing about this figure is the number of queries that were submitted: in one case a subject entered 18 queries during a search, which is much more than what is commonly reported in the literature. In Study 1, the maximum number of queries issued by a subject during a search was 22, an even greater number. On average, subjects in Study 1 issued about 5.90 queries per topic, while those in Study 2 issued about 8 per topic. While there are many possible explanations for subjects entering larger numbers of queries than usual, including the fact that they were in a laboratory study, it may be the case that the query\rsuggestions caused the increase. The suggestion may have supported subjects’ query behavior explicitly by providing them with clickable suggestions, or implicitly by providing them with ideas which they could use to create their own new queries.\rIn Figure 3, manually entered queries that duplicated a query suggestion are counted as a user generated query. However, if we consider these as suggestions (Figure 2), we see that subjects integrated the suggestions into their searches even more quickly – about 65% of the second queries issued by subjects were suggestions. For the second to eighth queries entered, subjects were more likely to use a suggestion than enter their own queries, while those who continued past the eighth query were more likely to create their own queries in subsequent iterations.\rFigure 3. Frequency and source of query (self-created or suggested) according to order of submission during search. In this Figure, user generated queries that duplicated query suggestions are counted as query suggestions.\r3.3 Suggestions and Search Stage\rTo understand more about when subjects were more likely to take suggestions, we divided subjects’ searches into three equal parts: beginning, middle and end. This division was based on the length of time each individual subject searched for a specific topic. For example, if one subject spent 12 minutes searching for one topic, then the beginning stage corresponded to the first 4 minutes of the search, the middle stage the next four minutes and the end stage the last four minutes. Of course, such divisions do not correspond temporally to the stages identified by Vakkari [5] who studied searchers for several months. However, we assume that even within a short 15 minute search that subjects will execute similar ‘mini’ stages of pre-focus, formulation and post-focus where they are more likely to be looking for general information in the pre- focus stage, faceted background information in the formulation stage and specific information in the post-focus stage.\rFigure 4 displays the number of user generated queries issued and suggested queries taken for each search stage. In the first Stage, subjects entered more of their own queries, but during Stages 2 and 3, subjects selected more suggested queries. This difference is especially evident at Stage 3. These results suggest that query suggestions might be particular useful during the latter stages of search when subjects are spending more time exploring the various facets of the topic and following-up by looking for specific information. These are likely to be the points at which subjects have exhausted their own ideas for queries and need ideas for alternative queries to continue with their searches.\rPage 11 of 122\r\nFigure 4. Number of user generated queries issued and suggested queries taken for each search stage.\r3.4 SuggestionsandTopicDifficulty\rWe were also interested in seeing if we could replicate the finding from Study 1 with regard to use of suggestions and topic difficulty. Recall that in Study 1, subjects issued more queries and took more suggestions for more difficult topics. Figure 5 displays the number of user generated queries issued and the number of suggested queries taken according to topic difficulty. These difficulty rankings are based on subjects’ post-search evaluations of how difficult it was to search for the topic.\rcomplexity is the number of topic facets. If searchers are unaware of the range and types of facets, then they are likely to have a difficult time identifying queries that represent these different facets. Query suggestions can potentially provide users with an overview of different facets as well as easy entry points into different parts of the collection. Facet and cluster-based browsing is not new, but browsing via query suggestion may provide a more meaningful and effective method of access for users.\r4. CONCLUSION\rThe thesis that query suggestions function as a type of idea tactic was explored using data from two studies. Results showed that subjects used more query suggestions when searching for difficult topics and during the latter stages of search. Results also showed that subjects integrated suggestions into their searches fairly quickly and that they often manually enter suggested queries. These results provide preliminary support for the notion of query suggestions as idea tactics, although further study is necessary.\rBates [1] states that the ultimate measure of success for an idea tactic is whether it improves retrieval performance. Our next step is to investigate search performance. This includes investigating overall performance, individual query performance and cumulative, within-session performance. A direct comparison of searchers’ interactions and tactics with a system that provides query suggestions with one that does not would also help us better understand the potential usefulness of suggestions as idea tactics. Such a study would allow one to explore if searchers using a query suggestion system searched longer, issued more queries (and at different rates, at different stages), achieved better performance, and learned more about their topics than searchers using a system that did not offer suggestions.\rAdditional studies should examine different methods for exploring the usefulness of query suggestions as idea tactics since performance is only one outcome. As Bates [1] described, idea tactics primarily serve a psychological purpose. A better understanding of how to evaluate query suggestions with this goal in mind is an important area for future research.\r5. ACKNOWLEDGEMENT\rMany wonderful students helped with the studies: Karl Gyllstrom, Earl W. Bailey, Xi Niu, Amber Cushing and Maureen Dostert.\r6. REFERENCES\r[1] Bates, M. J. (1979). Idea tactics. Journal of the American Society for Information Science, 30, 280-289.\r[2] Belkin, N. J. (2000). Helping people find what they don’t know. Communications of the ACM, 43(8), 58-61.\r[3] Kelly, D., Gyllstrom, K., & Bailey, E. W. (2009). A comparison of query and term suggestion features for interactive searching. Proceedings of SIGIR ’09, 371-378.\r[4] Kuhlthau, C. C. (1993). Seeking meaning: A process approach to library and information services. NJ: Ablex.\r[5] V akkari, P . (2004). Changes in search tactics and relevance judgments when preparing a research proposal: A summary of the findings of a longitudinal study. Information Retrieval, 4(3-4), 295-310.\r[6] White, R. W., Bilenko, M., & Cucerzan, S. (2007). Studying the use of popular destinations to enhance web search interaction. Proceedings of SIGIR '07, 159-166.\rFigure 5. Relationship between topic difficulty and number of user generated queries and suggested queries.\rThe relationship between search difficulty and use of suggestions was even more pronounced than in Study 1, with subjects taking twice as many suggestions for the most difficult topic than the easiest topic. Similar to Study 1, subjects issued more queries for more difficult topics and the number of queries subjects created on their own was somewhat constant. However, the proportion of query suggestions taken to total queries issued saw greater increases in Study 2 as a result of topic difficulty than in Study 1.\rThe results of both studies suggest that query suggestions might be useful for difficult topics since subjects in both studies issued more queries for difficult topics. For easy topics, subjects may find a sufficient number of documents by issuing a small number of their own queries or may be able to think of a variety of queries based either on their pre-search understanding of the topic or on their search interactions. For difficult topics, subjects may need more assistance both at the beginning and latter search stages because of a lack of pre-search topic knowledge or because initial queries do not yield useful results. One factor that may contribute to search difficulty is the complexity of the topic, where\rPage 12 of 122\r\n2.\rTHE CRANFIELD PARADIGM\rI Come Not to Bury Cranfield, but to Praise It\r1. INTRODUCTION\rMuch information retrieval research is currently per- formed using test collections, a methodology introduced by Cleverdon and his colleagues in the Cranfield tests [4] and further refined in evaluation exercises such as TREC (http: //trec.nist.gov/). A test collection is (purposely) a stark abstraction of real user search tasks that models only a few of the variables that affect search behavior and was explicitly designed to minimize individual searcher effects. Nonethe- less, I argue that Cranfield-style experimentation is critical to the study of interactive (user-in-the-loop) retrieval for at least two reasons. First, research using test collections iden- tifies good retrieval technology, allowing expensive user test- ing to be reserved for the most promising avenues. Second, meta-analysis of the Cranfield methodology can inform the development of new research abstractions that make differ- ent trade-offs among realism, experimental power, and cost.\rThis paper is a condensation of an earlier paper in which I made similar arguments [12]. The arguments stem from my experience with building and validating test collections as the manager of the TREC project, a role that clearly marks me as a Cranfield advocate. The next section provides a brief recap of the Cranfield methodology as currently prac- ticed, the following section examines the immense impact of user variability on retrieval experiments, and the final section describes two prior TREC efforts to develop evalua- tion paradigms in the space between test collections and full interactive experiments. The paper does not contain a pro- posal for a new abstraction for testing interactive retrieval systems; that is very much an open research problem. In- stead, my hope is that the paper clarifies some of the issues that must be addressed by such an abstraction.\rsimplest and most common case, relevance judgments are binary, either a document is relevant to the topic or it is not, and are based on topicality, a document is relevant if its subject matter matches that of the topic.\rIn current practice, a researcher runs a retrieval system on a test collection to produce a ranked list of documents in response to each topic (a “run”). A ranking reflects the system’s idea of which documents are likely to be relevant to the topic; documents it believes more likely to be relevant are ranked ahead of documents it believes are less likely to be relevant. Using the relevance judgments, various evalu- ation metrics can be computed for the ranked list, each of which reflects some aspect of the goal of ranking all relevant documents ahead of all nonrelevant documents. Scores for individual topics are averaged over the set of topics in the test collection. Average scores for one run are compared to other runs using the exact same test collection. Retrieval systems producing runs with better scores are considered more effective retrieval systems.\rThe abstraction that a test collection implements is ad- mittedly impoverished compared to the complexity of a real search task. This is a deliberate design choice to provide more control over experimental variables at the cost of less realism. Performing well on this abstract task is assumed to be a necessary but not sufficient prerequisite for perform- ing well on real search tasks. If a retrieval method can’t at least rank relevant documents before nonrelevant doc- uments (for some reasonable definition of relevance), it is hard to imagine how it can be successful at any real user task. Test collections are thus convenient tools for studying retrieval system performance in the laboratory. As in medi- cal research, laboratory research plays an integral role in the development of new treatments, weeding out less successful approaches early and relatively inexpensively, while reserv- ing much more costly (“clinical”) testing of more realistic functionality for only the most promising approaches.\rOf course laboratory research is only useful if its findings generalize to real-world tasks. There is plenty of historical evidence that Cranfield-style tests are useful. Basic com- ponents of current commercial retrieval systems, including full text indexing, term weighting schemes, and relevance feedback, were first developed using test collections. And while Hersh and Turpin conclude that relevance ranking in the manner of Cranfield is not a trustworthy means to find differences in systems that matter to users [5, 11], I con- tend their studies are better interpreted as examples of the difficulty of gaining sufficient power to distinguish among systems in interactive experiments.\rA Cranfield test collection is a set of documents, a set of information need statements (called “topics” in the remain- der of this paper), and a set of relevance judgments that list which documents should be returned for which topic. In the\rThis paper is authored by an employee(s) of the United States Government and is in the public domain.\rHCIR’09, October 23 2009\rACM X-XXXXX-XX-X/XX/XX.\rEllen M. Voorhees\rNational Institute of Standards and Technology Gaithersburg, MD USA ellen.voorhees@nist.gov\rPage 13 of 122\r\nWhat is the source of this lack of power? Variability. Variability and experimental power are inversely related in any experimental design. An experimental design that pro- vides a given level of power will be increasingly expensive as variability increases since more factors must be controlled for. As described in the next section, users are the domi- nant source of variability in retrieval experiments even in the Cranfield paradigm. Any additional user attributes included in an experimental design can only increase that variability.\r3. USER EFFECT AND VARIABILITY\rThe user is represented in a test collection simply as the combination of a topic and its corresponding set of rele- vance judgments. This ruthless abstraction of the user to a static topic statement is a leading cause of Cranfield critics’ frustration with the methodology. Yet the topic (i.e., the information being sought) is the primary source of varia- tion in Cranfield-style retrieval experiments. An analysis of variance model fitted to the TREC-3 results demonstrated that the topic and system effects, as well as the interaction between topic and system, were all highly significant, with the topic effect as the largest [2]. In other words, even when the user is reduced to simply the question asked, differences between users have a bigger effect on the outcome of an experiment than do differences between systems.\rThe large variability in topic performance for a single re- trieval system is precisely why the Cranfield paradigm uses average scores over a set of topics in comparisons. Sea- soned experimenters have long known that the topic set needs to be relatively large. For example, in the mid-1970’s Sparck Jones and van Rijsbergen suggested 75 topics as a minimum number of topics for an ‘ideal’ test collection[9]. This intuition arose from observing the behavior of indi- vidual topics as demonstrated in Figure 1. The figure shows an interpolated precision-recall graph for an example TREC run. The heavy solid line is the average recall-precision curve over the 50 topics in the test set, while the dotted lines are the curves for 15 individual topics within the test set.\rThe archives of retrieval results from programs like TREC and NTCIR allow hypotheses regarding test collection con- struction and use (such as minimum topic set size) to be experimentally verified. Investigations have established an empirical relationship between the number of topics in a test set, the size of the difference in retrieval scores used to de- cide two runs are different (δ), and the error rate [13, 8], as well as demonstrated that some evaluation measures are more stable than others (including that MAP is much more stable than precision at a given cut-off) [3]. For all measures looked at, the error rate decreases when either the topic set size or δ increases. For the most stable measures and for δ’s of the size commonly observed in the retrieval literature, 25 topics is clearly too few to have confidence in the conclu- sion, and even 50 topics is somewhat iffy—a δ of between 0.03 and 0.04 in MAP scores, equivalent to about a 10% rel- ative difference in MAP scores for current systems, had an error rate of about 11%. Less stable measures have higher error rates for equal topic set sizes.\rThese are sobering conclusions for the prospects of build- ing user-focused abstractions that provide reliable com- parisons among systems. Even in the most controlled environment—the Cranfield task and using MAP as the measure—you still need on the order of 50 topics to control user variability sufficiently to have confidence in a system\r1.0\r0.8\r0.6\r0.4\r0.2\r0.0\rPage 14 of 122\r0.0 0.2 0.4\r0.6 0.8 1.0\rFigure 1: Interpolated precision at standard recall points for selected individual topics (dotted lines) and the overall average for the run (solid line).\rcomparison. Modifications as small as moving from MAP to a more user-focused measure like precision at ten docu- ments retrieved require larger topic sets for a similar level of confidence. More radical departures will require even larger topic sets. Robertson calculated that hundreds of topics per user would be needed to obtain statistical significance in non-matched-pair tests[7].\r4. CRANFIELD ALTERNATIVES\rSo far I have argued that the Cranfield paradigm is suc- cessful because of its carefully calibrated level of abstrac- tion. The document ranking task has sufficient fidelity to real user tasks to be informative, but is sufficiently abstract to be broadly applicable, feasible to implement, and com- paratively inexpensive.\rInteractive studies, currently the main alternative to test collection experiments, generally have substantially more fi- delity to real user tasks than Cranfield tests. Unfortunately, interactive studies that are powerful enough to reach mean- ingful conclusions are also challenging to design and expen- sive to implement. The difficulties arise for a variety of rea- sons. Interactive experiments have a large start-up cost. Regardless of what particular functionality is the actual ob- ject of interest, a good interactive experiment requires com- plete systems that support the functionality in its best light for all alternatives. Large numbers of topics are required to reach statistically significant conclusions. Large numbers of topics imply large numbers of subjects and sophisticated experimental designs to balance subjects across conditions (any given subject can search a given topic no more than once). The more specific the design is to a given opera- tional setting (i.e., the more “real” the experiment) the less\rRecall\rPrecision\r\n0.8\rgeneralizable the findings are to other environments so the\rmore cases that need to be examined. Tague lists a myriad\rof factors that must be considered when designing retrieval 0.6 experiments [10].\rThe quest, then, is to find useful abstractions that make\rdifferent trade-offs among realism, experimental power, and 0.4 cost than test collections on the one hand and interactive\rstudies on the other. TREC has contained two notable ef-\rforts in this regard. The TREC-6 interactive track looked\rto decrease the cost of cross-site interactive tests by using\ra common baseline system, while the TREC HARD track 0.0 augmented a traditional test collection with additional user information.\rUMassMerge UMassBaseSVM chascfrel\rISCAS THUKEGRMRF10 THUKEGbab1 uiucHARDf0 uiucHARDb0\r0.2\rIn the TREC-6 interactive track [6], each participating site ran an experiment comparing their system to a common baseline system. The assumption in the design was that this set-up would allow the effect of performing all n2 compar- isons among the n participating sites for a total cost of only n comparisons (one at each site) since the common system would control for inter-site variance. However, subsequent analysis of the results did not support the assumption. Fur- ther, the design incurred its own costs: participating sites had to obtain, install, and support the common system; and precious human subjects had to be devoted to the common system, a system that was incidental to the main purpose of the experiment being run at that site. As a result of these limitations, the design could not be recommended.\rThe overall goal of the TREC HARD (“High Accuracy Retrieval from Documents”) track was to improve ad hoc retrieval by customizing the search to the particular user. The motivation for the track was that current retrieval sys- tems return results for the “average” user and this necessarily limits their effectiveness for the particular user. The task in the track was an ad hoc document ranking task but with extra information available at search time.\rThe TREC 2004 instantiation of the track [1] provided two different sources of additional information, metadata supplied in the topic statement and information collected from so-called clarification forms. There were five categories of metadata: the (putative) user’s familiarity with the sub- ject area specified as either little or much; the genre of the documents sought specified as either news-report, opinion- editorial, other, or any; the geographical location of the source of the document (US, non-US, or any); a short free- text description of the subject domain; and text from related documents if available. Clarification forms were HTML- based forms that contained a task for the user (in this case, the TREC assessor) to perform. Track participants were free to implement in the form any task they thought would help retrieval, subject to the constraints that the assessor would spend no more than three minutes per topic responding to a form and the form had to be completely self-contained. Examples of clarifying forms were asking the user to resolve the senses of ambiguous topic words or obtaining relevance judgments on terms or document snippets.\rThe track protocol required participants to perform base- line runs using only the traditional topic statement. If de- sired, they could then submit clarification forms to receive the results of the assessor using the forms. Finally they per- formed additional (non-baseline) runs using the metadata from the extended topic statements and/or the clarification form results. This protocol allowed direct comparison be- tween standard and extended information runs for a single\rFigure 2: Effectiveness of top scoring runs submit- ted to the TREC 2004 HARD track.\rparticipant as well as comparisons among participants. Fig- ure 2 gives a precision-recall graph of the top-performing TREC 2004 HARD track runs including one baseline and one additional run per top group. A pair of runs plotted with the same symbol are a pair submitted by a single group. Baseline runs are plotted using a dotted line, and additional runs are plotted using a solid line.\rIn general (though not invariably) the additional infor- mation did improve retrieval effectiveness over the corre- sponding baseline run. But understanding what factors ac- tually contributed to the improvement is difficult. There are too few topics in a single metadata category (for ex- ample, familiarity) to draw reliable conclusions regarding category-specific retrieval techniques. Use of clarification form data by researchers other than the original submitters is complicated by needing to understand the specifics of the forms and the inability to repeat the experiment with slight variations. Little reuse means the costs of creating the re- source is not leveraged over the wider community. Since the construction of the HARD collection was already more ex- pensive than standard Cranfield collections (the metadata categories needed to be decided on and then topics created that populated those categories; researchers needed to cre- ate clarification forms and assessors had to fill them out), the lack of reusability is a double penalty.\rThese were good attempts at defining a new evaluation paradigm for interactive retrieval, though in the end neither approach was as successful as hoped. The HARD track un- derscores how a fairly small change in the amount of user in- formation incorporated into the experimental design mush- rooms into a much larger set of experimental conditions. The number of topics needed to control for user variability to reach reliable conclusions is truly daunting. I believe that any new evaluation paradigm that attempts to encompass all/most/many of the factors that can affect search behavior is doomed to fail. Instead, like Cranfield, a successful new paradigm will need a carefully defined abstract task that models only a minimum of of critical features.\r5. REFERENCES\r[1] James Allan. HARD track overview in TREC 2004: High accuracy retrieval from documents. In Proceedings of the Thirteenth Text REtrieval Conference, TREC 2004, pages 25–35, 2005. NIST Special Publication 500-261.\rPage 15 of 122\r0.0 0.2 0.4\r0.6 0.8\r1.0\rRecall\rPrecision\r\n[2] David Banks, Paul Over, and Nien-Fan Zhang. Blind men and elephants: Six approaches to TREC data. Information Retrieval, 1:7–34, 1999.\r[3] Chris Buckley and Ellen M. Voorhees. Evaluating evaluation measure stability. In N. Belkin,\rP. Ingwersen, and M.K. Leong, editors, Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33–40, 2000.\r[4] C. W. Cleverdon, J. Mills, and E. M. Keen. Factors determining the performance of indexing systems. Two volumes, Cranfield, England, 1968.\r[5] William Hersh, Andrew Turpin, Susan Price, Benjamin Chan, Dale Kraemer, Lynetta Sacherek, and Daniel Olson. Do batch and user evaluations give the same results? In N. Belkin, P. Ingwersen, and M.K. Leong, editors, Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 17–24, 2000.\r[6] Eric Lagergren and Paul Over. Comparing interactive information retrieval systems across sites: The TREC-6 interactive track matrix experiment. In\rW. Bruce Croft, Alistair Moffat, C.J. van Rijsbergen, Ross Wilkinson, and Justin Zobel, editors, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 164–172, Melbourne, Australia, August 1998. ACM Press, New York.\r[7] S. E. Robertson. On sample sizes for non-matched-pair IR experiments. Information Processing and Management, 26(6):739–753, 1990.\r[8] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), pages 525–532, 2006.\r[9] K. Sparck Jones and C.J. van Rijsbergen. Information retrieval test collections. Journal of Documentation, 32(1):59–75, 1976.\r[10] Jean Tague-Sutcliffe. The pragmatics of information retrieval experimentation, revisited. Information Processing and Management, 28(4):467–490, 1992.\r[11] Andrew H. Turpin and William Hersh. Why batch and user evaluations do not give the same results. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 225–231, 2001.\r[12] Ellen M. Voorhees. On test collections for adaptive information retrieval. Information Processing and Management, 44(6):1879–1885, 2008.\r[13] Ellen M. Voorhees and Chris Buckley. The effect of topic set size on retrieval experiment error. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 316–323, 2002.\rPage 16 of 122\r\nSearch Tasks and\rTheir Role in Studies of Search Behaviors\rBarbara M. Wildemuth School of Information & Library Science University of North Carolina at Chapel Hill Chapel Hill, NC 27599-3360, USA\rwildem@ils.unc.edu\rABSTRACT\rIn experimental studies of search behaviors and evaluations of information retrieval systems, researchers generally assign search tasks to the subjects to perform. Since it can be expected that the tasks themselves will influence search behaviors and performance, we need to be able to construct tasks having particular attributes, knowing that our study findings can then be generalized to all search tasks having those attributes. In this paper, we report on an ongoing analysis of the search tasks that have been used in experimental search studies. We review a number of typologies of search tasks currently in use (complex vs. simple, specific vs. general, exploratory vs. lookup, and navigational vs. informational) and make recommendations for designing search tasks for use in future studies.\rCategories and Subject Descriptors\rH3.3. Information search and retrieval: Search process\rGeneral Terms\rExperimentation\rKeywords\rSearch tasks, Research design\r1. INTRODUCTION\rPeople’s search behaviors vary widely. It’s likely that some of this variation is not related to differences in the characteristics of individual searchers (e.g., domain knowledge or search expertise), but is instead due to differences in the goals that they are trying to achieve. In almost all cases, searches for information are under- taken within the context of some other purpose, goal, or activity. In other words, the person’s search behaviors are situated within the context of performing some larger task [30, 33].\rThese embedding tasks may vary along a number of dimensions, including their complexity, structure, and granularity. For example, consider the difference between the complex and amorphous task of completing a dissertation and the simple and well-structured task of locating the address of a local store. While it is important to understand the characteristics of these tasks, by\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR 2009, October 23, 2009, Washington, DC Copyright held by authors.\rLuanne Freund\rSchool of Library, Archival & Information Studies University of British Columbia Vancouver, BC, V6T 1Z1, Canada\rluanne.freund@ubc.ca\rwhich people’s search behaviors are motivated [8, 29], it is also important to focus on the attributes of the search tasks1 themselves, and how those attributes can affect search behaviors.\rIn particular, it is important to understand the potential influence of the search tasks assigned to research subjects when studying search behaviors and evaluating information retrieval (IR) systems. Naturalistic studies are intended to observe real people searching in order to complete real tasks; however, experimental studies are intended to isolate particular effects on user behaviors. Because of this desire for control in experimental studies, researchers usually assign search tasks, either controlling the task effect by assigning the same tasks to all the subjects or manipulating it as an independent variable. Most studies to date have opted for control over manipulation, but in either case, researchers are handicapped by our lack of understanding of the influence of the search task on the study findings. Given that search tasks can vary along many dimensions, findings may be valid for a particular set of tasks, but we do not know to which additional tasks they may be validly applied.\rIn order to make additional progress in experimental studies, we need to gain a better understanding of search tasks and their effects. We need to be able to construct tasks having particular attributes, knowing that our findings can then be generalized to all search tasks having those attributes. To this end, we are collecting and analyzing the search tasks that have been used in experimen- tal search studies.2 This paper is proposed as a starting point for gaining an understanding of these tasks. We will briefly review a selection of studies and compare the ways in which search tasks have been categorized in those studies. We will conclude with suggestions for moving forward on this research agenda.\r2. TYPES OF SEARCH TASKS\rWhen designing a study of search behaviors, the researcher needs to decide how much control to exert over the search tasks. At one end of the spectrum, the study subject is allowed to search on tasks of personal interest (i.e., the tasks are fully self-generated [3] or natural [30]). At the other end, the tasks are fully specified by the researcher. Some studies use a combination of tasks generated\r1 In this paper, the term, search tasks, will be used to designate the goal(s) to be achieved in a specific search situation. They are distinguished from the more general (work-related or other) tasks that have motivated the search. This distinction is explained in more detail by Byström and Hansen (2005).\r2\rPage 17 of 122\rSo far, we have collected over 223 descriptions of tasks from 79 empirical studies and conceptual papers. In addition, we are examining conceptual papers discussing search tasks.\r\nby the researchers and tasks generated or modified by the subjects [28]. In this discussion, we are concerned only with the tasks generated and assigned by researchers.\rIn some past studies, the researchers have not specified the attributes of the assigned search tasks. In other studies, the researchers have described or categorized the tasks in some way (e.g., as complex, simple, known-item, factual, exploratory, navigational, or informational [18]). Attempts to integrate these typologies include Bilal’s [3] integration of open- and closed- ended tasks with complex and simple tasks; Li and Belkin’s [22] faceted classification of tasks; and Jansen, Booth, and Spink’s [16] investigation of Broder’s [6] original typology of Web search tasks as information, navigational, or transactional. This section will discuss some of the ways in which search tasks have been categorized in recent studies of search behaviors, providing specific examples of each. This review of varying typologies of search tasks is meant to be suggestive of future research directions, rather than exhaustive.\r2.1 Task Attributes Not Specified\rIn many studies, assigned search tasks are clearly described and, possibly, the full text of the search tasks is provided, yet the tasks are not categorized as being of a certain type. For example, in Woodruff et al.’s [34] comparison of three types of thumbnails during Web searches, the researchers developed and assigned search tasks that were “much like typical Web search tasks” (p.176). The 12 search tasks covered four areas: picture, homepage, e-commerce, and side effects. These groupings may be interpreted as connoting the topic of the search; for example, the side effects category included the search task, “Find at least three side effects of Halcion” (p.177).\rOther authors might have categorized these search tasks differently. For example, the Halcion example might be classified as a complex search task, since it is likely that the searcher will need to consult several different Web pages to find multiple side effects. It might also be classified as a factual search task, since facts are the end point of the search process.\r2.2 Complex vs. Simple Tasks\rComplexity is the most commonly manipulated attribute of search tasks, although it has been defined and operationalized in many different ways [29]. Unlike other more discrete variables, complexity tends to be treated as an aggregate of one or more of the following task characteristics: structure [21, 26], certainty or a priori determinability [1, 7, 8, 10], number of facets [11, 2], length of the search path [11, 14], cognitive effort [2, 11] and topic familiarity [2, 7].\rAs an example of studies of task complexity, Bell and Ruthven [1] undertook a study that drew upon earlier work by Byström [8] and Campbell [10]. They developed sets of three tasks on the same topics but at differing levels of complexity by manipulating the degree of uncertainty. For the most complex task, “it is unclear what information is being sought, how to obtain relevant information, and how the searcher will know they have found relevant information” (p.61). For example, one of the low complexity task scenarios asks the searcher to “find out how the price of petrol in the UK has changed in recent years,” while the corresponding high complexity task asks the searcher to “find out how and why petrol prices vary worldwide” (p.62).\rThe broad range of conceptualizations of complexity can be seen by comparing Bell and Ruthven’s most complex task example\rwith that of the most complex task assigned by Browne, Pitts and Wetherbe [7]. They also assigned tasks at three levels of complexity, and the most complex was to find a map of a little- known battlefield. While this task proved to be relatively difficulty to perform, by many definitions of complexity, it is simpler than the simple tasks assigned by Bell and Ruthven [1].\r2.3 Specific vs. General Tasks\rThe specificity of the assigned search tasks is another task attribute that researchers have manipulated. Across studies, “specific” tasks tend to have more clearly defined goals than “general” tasks. Specific tasks may be equated with known-item search tasks (e.g., in [19]), factual tasks (e.g., in [14]), or simple lookup tasks (e.g., in [11, 13]).\rRouet [25] focused a study on the effects of task specificity on searching behaviors. The specific search tasks were defined as asking the study subjects to “locate one piece of information” (p.415); an example is, “Which authors have provided the first clinical descriptions of anorexia”?, to be searched in a hypertext document on anorexia. The general search tasks were defined as requiring “the reading and integration of 2-5 separate passages” from the hypertext document; an example is, “What treatments [for anorexia] may be suggested, and what are their effects?” While it is certainly appropriate to describe these two types of search tasks as specific and general, other researchers might have described them as simple and complex.\r2.4 Exploratory vs. Lookup Tasks\rOver the past several years, interest in exploratory search behaviors has increased. Users conducting exploratory searches are likely to “submit a tentative query and take things from there, exploring the retrieved information, selectively seeking and passively obtaining cues about where the next steps lie” [31, p.38]. Exploratory searching is defined as searching that supports learning, investigating, comparison, or discovery [20, 24]. It is contrasted with lookup tasks, which are oriented toward finding particular facts or answering specific questions. There is some evidence that this contrast is perceived by searchers as well as researchers, as found by Kules and Capra [20].\rWhite and Marchionini [32] incorporated both exploratory and lookup tasks in their study of a new approach to query expansion. An example of a lookup task was, “You are doing some research for a term paper you are writing and need to find the name of the first woman to travel in space and her age at the time of her flight”; an example of an exploratory task was, “You are about to depart on a short-tour along the west coast of Italy. The agenda includes a visit to the country’s capital, Rome, during which you hope to find time to pursue your interest in modern art. However, you have recently been told that time in the city is limited and you want information that allows you to choose a gallery to visit” (p.689). Interest in exploratory search behaviors and retrieval systems that can better support exploratory search is likely to continue. While Kules and Capra [20] have made a significant contribution in this direction, additional studies are needed to more clearly conceptualize and define exploratory search tasks.\r2.5 Transactional vs. Navigational vs.\rInformational Tasks\rThe types of tasks already discussed are intended to represent people’s information needs [15, Fig.6.8]; they all assume that a person searches because that person needs to find information for a particular purpose. Broder [6] argues that, with the advent of\rPage 18 of 122\r\nWeb searching, we need to broaden our perspective. In addition to informational tasks, people also may be using the Web to accomplish navigational or transactional tasks. While the purpose of an informational task is to acquire some information, the purpose of a navigational task is “to reach a particular site” and the purpose of a transactional task is “to perform some web- mediated activity” (p.5). Examining query logs from AltaVista, Broder found that about 48% of Web queries were informational, while 20% were navigational and 30% were transactional. Jansen, Booth, and Spink [16] followed up on Broder’s work with an attempt to automatically classify Web queries into Broder’s three types. In addition, they provided a more fine-grained analysis of Broder’s three types of Web queries.\rThis classification of pre-existing Web queries is now beginning to serve as an empirical basis for studies in which search tasks are designed and assigned to study subjects. For example, Lorigo et al. [23] assigned five navigational tasks and five informational tasks to their subjects in a study of subjects’ behaviors during searching and review of results, and Joachims et al. [17] assigned the same 10 tasks to their subjects in a study of the implicit feedback provided by user clicking behavior. An example navigational task was, “Find the homepage for graduate housing at Carnegie Mellon University”; an example informational task was, “What is the name of the researcher who discovered the first modern antibiotic?” (p.6). All of the questions are also depicted as closed-ended, and so did not cover the full range of informational search tasks described in the previous sections.\rThis typology was useful in developing the two example studies described. However, neither study incorporated transactional tasks. In addition, the findings cannot be generalized to all types of informational tasks because only a homogeneous set of straightforward factual search tasks were assigned.\r2.6 What We Can Learn from the Current\rTask Typologies\rThe few typologies discussed in this paper include those that have been repeatedly used in studies of search behaviors over several decades of research. They can be interpreted as the research community’s common-sense understanding of what’s important about search tasks. Thus, we have some sense of the directions that we need to pursue in terms of improving our understanding of search tasks.\rWhile there is some consensus about which attributes of search tasks are most important, there is no consensus about how those attributes should be defined and operationalized. A task that is categorized as simple by one researcher might be categorized as specific by another. A task that is categorized as a lookup task by one researcher might be categorized as an informational task by another. Furthermore, tasks used to operationalize one variable may have additional attributes not accounted for in the study design, which may confound the results. If we ever want to compare results across studies, we must improve our understanding of the search tasks in use as either control variables or independent variables.\rThe task attribute that has garnered the most investigation is task complexity. There have been a number of studies that compare simple tasks with those that are more complex, but there are additional task attributes that warrant consideration. These include the topic, domain or subject area of a search task [27, and the informational goal, whether it be to learn about something, make a decision, solve a problem, etc.\r3. RECOMMENDATIONSFOR\rDEVELOPING SEARCH TASKS\rIdeally, researchers will develop search tasks that are realistic and that appropriately motivate the study subjects to perform a realistic search. Thus, a logical starting point is to elicit real-life situations and task cases from the population of interest [9, 12] or mine existing transaction logs [20]. From these stories of real tasks and the searches they engendered, the researcher can develop search tasks that simulate realistic situations.\rUsing simulated situations to present search tasks, as defined by Borlund [4, 5], increases the validity of the search tasks by decreasing their artificiality. Borlund suggests that simulated situations be made up of two parts: the simulated work task situation and the indicative request. Byström and Hansen [9] go into even more detail, recommending that three levels of description should be used to specify a search task: a contextual description, a situational description, and a topical description and query. Within the context of a study of search behaviors, the simulated situations are “meant to trigger individual information problems in test persons in a controlled manner” [15, p.284]. The recommendations of these scholars are consistent, in that they all encourage the inclusion of contextual information in the specification of a search task. As search task descriptions become increasingly detailed, it is important to validate the effect of search tasks through pre-testing and/or by collecting user feedback during the study. Given that the construction and validation of robust contextualized tasks is challenging, researchers should consider the option of sharing, reusing and/or customizing existing tasks.\rIn addition, in order to effectively evaluate interactive IR systems, it will be necessary to develop more complex search tasks and more exploratory search tasks. Many IR researchers want to design systems that support a broader range of activities – both more complex activities and ongoing activities. Thus, we need to understand how to develop search tasks that can support experi- mental research and system evaluation in more realistic contexts. Two recent articles suggest approaches to this problem. Kules and Capra [20] designed a template for developing exploratory tasks. The situation specified in the template was a search conducted while writing a paper for a class, with the search to be conducted on the university’s library catalog. While specific to their study, it’s easy to imagine that similar templates might be developed for other situations. Taking a different approach, Li and Belkin [22] identified and developed a taxonomy of the attributes of tasks (including both work tasks and search tasks). The taxonomy includes both generic facets of tasks (e.g., source, time-related attributes, and goals) and common attributes of tasks (e.g., complexity, difficulty, and urgency. As researchers develop search tasks, they can use this taxonomy to specify the task attributes of interest for a particular study.\rWhile we need additional research to fully understand which attributes of tasks have an important effect on search behaviors, we can and should improve the research base now by more carefully documenting the attributes of the search tasks assigned to research subjects. In each paper, the researcher should indicate which attributes of the search tasks are most salient. These attributes should then be defined and their operationalization should be described. For example, a study might investigate the search behaviors associated with different levels of task complexity. The researcher’s definition of complexity should be\rPage 19 of 122\r\ndocumented, and how different levels of complexity were incorporate into the search tasks should be clearly explained. Only with such clear specification of our research methods can we gain leverage on understanding the effects of search tasks on search behaviors.\rTo contribute to the improvement of experimental methods for studying search behaviors, we are initiating a project that will compile and analyze studies employing assigned search tasks. From each study, we are capturing the way in which the search tasks are categorized or defined (if they are), as well as the full text of the search tasks themselves. Through this analysis, we hope to be able to more accurately describe the search tasks that have been used in past studies, as well as define the attributes of search tasks which are most likely to influence the outcomes of future studies.\r4. ACKNOWLEDGMENTS\rWe thank Anthony Hughes, Jung Sun Oh, Ashraf Farrag, and Chris Doty, each of whom has made significant contributions to the compilation of a database of assigned search tasks, used as the basis for this analysis.\r5. REFERENCES\r[1] Bell, D. & Ruthven, I. (2004). Searcher's Assessments of Task Complexity for Web Searching. Advances in Information Retrieval, 26th European Conference on IR Research, ECIR 2004. Springer, 57-71.\r[2] Bilal, D. (2001). Children's use of the Yahooligans! Web search engine: II. Cognitive and physical behaviors on research tasks. J. of the Am. Soc. for Info. Sci. & Tech., 52(2), 118-136.\r[3] Bilal, D. (2002). Children’s use of the Yahooligans! Web search engine. III. Cognitive and physical behaviors on fully self-generated search tasks. J. of the Am. Soc. for Info. Sci. & Tech., 53(13), 1170-1183.\r[4] Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Info. Research, 8(3). http://informationr.net/ir/8- 3/paper152.html.\r[5] Borlund, P., & Ingwersen, P. (1997). The development of a method for the evaluation of interactive information retrieval systems. J. of Documentation, 53(3), 225-250.\r[6] Broder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36(2), 3-10.\r[7] Browne, G. J., Pitts, M. G., & Wetherbe, J. C. (2007). Cognitive stopping rules for terminating information search in online tasks. MIS Quarterly, 31(1), 89-104.\r[8] Byström, K. (2002). Information and information sources in tasks of varying complexity. J. of the Am. Soc. for Info. Sci. & Tech., 53(13), 1170-1183.\r[9] Byström, K., & Hansen, P. (2005). Conceptual framework for tasks in information studies. J. of the Am. Soc. for Info. Sci. & Tech., 56(10), 1050-1061.\r[10] Campbell, D. J. (1988). Task complexity; a review and analysis. Acad. of Mgmt. Rev., 13(1), 40-52.\r[11] Capra, R., Marchionini, G., Oh, J. S., Stutzman, F., & Zhang, Y. (2007). Effects of structure and interaction style on\rdistinct search tasks. JCDL 2007: Proc. of the 7th ACM/IEEE Joint Conference on Digital Libraries, 442-451.\r[12] Elsweiler, D., & Ruthven, I. (2007). Toward task-based personal information management evaluations. Proc. of the 30th ACM Conference on Research and Development in Information Retrieval (SIGIR ’07), 22-30.\r[13] Fidel, R., Davies, R.K., Douglass, M.H., Holder, J.K., Hopkins, C.J., Kushner, E.J., Miyagishima, B.K., & Toney, C.D. (1999). A visit to the information mall: Web searching behavior of high school students. J. of the Am. Soc. for Info. Sci., 50(1), 24 –37.\r[14] Gwizdka, J., & Spence, I. (2006). What can searching behavior tell us about the difficulty of information tasks? A study of web navigation. Proc. of the Am. Soc. for Info. Sci. & Tech. doi:10.1002/meet.14504301167.\r[15] Ingwersen, P., & Järvelin, K. (2005). The Turn: Integration of Information Seeking and Retrieval in Context. Springer.\r[16] Jansen, B.J., Booth, D.L., & Spink, A. (2008). Determining the informational, navigational, and transactional intent of Web queries. Info. Proc. & Mgmt., 44(3), 1251-1266.\r[17] Joachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, F., & Gay, G. (2007). Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search. ACM Trans. on Info. Sys., 25(2), Article 7.\r[18] Kelly, D. (2009). Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Info. Retrieval, 3(1-2).\r[19] Kim, K.-S., & Allen, B. (2002). Cognitive and task influences on web searching behavior. J. of the Am. Soc. for Info. Sci. & Tech., 53(2), 109-119.\r[20] Kules, B., & Capra, R. 2008). Creating exploratory tasks for a faceted search interface. HCIR 2008 Workshop Proceedings, 18-21.\r[21] Lazonder, A. W., Biemans, H. J. A., & Wopereis, I. G. J. H. (2000). Differences between novice and experienced users in searching information on the World Wide Web. J. of the Am. Soc. for Info. Sci., 51(6), 576-581.\r[22] Li, Y., & Belkin, N.J. (2008). A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management, 44(6), 1822-1837.\r[23] Lorigo, L., Pan, B., Hembrooke, H., Joachims, T., Granka, L., & Gay, G. (2006). The influence of task and gender on search and evaluation behavior using Google. Info. Proc. & Mgmt., 42(4), 1123-1131.\r[24] Marchionini, G. (2006). Exploratory search: From finding to understanding. Commun. of the ACM, 49(4), 41-46.\r[25] Rouet, J.-F. (2003). What was I looking for? The influence of task specificity and prior knowledge on students' search strategies in hypertext. Interact. with Comp., 15(3), 409-428.\r[26] Sharit, J., Hernandez, M.A., Czaja, S.J., & Pirolli, P. (2008). Investigating the roles of knowledge and cognitive abilities in older adult information seeking on the Web. ACM Trans. on Computer-Human Interaction, 15(1), Article 3.\r[27] Toms, E.G., Freund, L., Kopak, R. & Bartlett, J.C. (2003). The effect of task domain on search. Proc. of CASCON 2003, 1-9.\rPage 20 of 122\r\n[28] Toms, E.G, Kopak, R., Bartlett, J. & Freund, L. (2001). Selecting versus describing: the efficacy of categories in exploring the Web. Proc. of the 10th Text Retrieval Conference, 64-70.\r[29] Vakkari, P. (1999). Task complexity, problem structure and information actions: Integrating studies on information seeking and retrieval. Info. Proc. & Mgmt., 35, 819-837.\r[30] Vakkari, P. (2003). Task-based information searching. Annual Rev. of Info. Sci. & Tech., 37, 413-463.\r[31] White, R.W., Kules, B., Drucker, S.M., & schraefel, m.c. (2006). Introduction [to special issue on Supporting exploratory search]. Commun. of the ACM, 49(4), 36-39.\r[32] White, R. W., & Marchionini, G. (2007). Examining the effectiveness of real-time query expansion. Info. Proc. & Mgmt., 43(3), 685-704.\r[33] Wildemuth, B. M., & Hughes, A. (2005). Perspectives on the tasks in which information behaviors are embedded. In Fisher, K E., Erdelez, S., & McKechnie, L. (Eds.), Theories of Information Behavior. Medford, NJ: Information Today, for ASIST, 275-279.\r[34] Woodruff, A., Rosenholtz, R., Morrison, J. B., Faulring, A., & Pirolli, P. (2002). A comparison of the use of text summaries, plain thumbnails, and enhanced thumbnails for web search tasks. J. of the Am. Soc. for Info. Sci. & Tech., 53(2), 172-185.\rPage 21 of 122\r\nVisual Interaction for Personalized Information Retrieval\rABSTRACT\rJae-wook Ahn School of Information Sciences University of Pittsburgh Pittsburgh, PA 15260 jahn@mail.sis.pitt.edu\rPeter Brusilovsky School of Information Sciences University of Pittsburgh Pittsburgh, PA 15260 peterb@mail.sis.pitt.edu\rthe performance and the transparency of personalized IR systems by providing a more dynamic and flexible interaction model be- tween the system and the users, which is typical for exploratory search interfaces.\rWe propose to incorporate interactive visualization into person- alized search in order to achieve this goal. By combining the per- sonalized search and the interactive visualization, we expect our approach will be able to help users to better explore the informa- tion space and locate relevant information more efficiently. We ex- tended a well-known visualization framework called VIBE (Visual Information Browsing Environment) [6] so that it can visualize the user models and the personalized search results. Beyond the past studies [1, 2] that have found some potentials of this adaptive vi- sualization method, we are planning to conduct a full-scale user study to investigate its strengths and weaknesses. This paper intro- duces the visualization-based personalized IR idea (section 2) and the preliminary experiment results (section 3), and then describes the details of our prototypical personalized search system (section 4) and its interaction model constructed for the future user study.\rThere are two promising answers to the classic information over- load problem: (1) personalized search and (2) exploratory search. Personalized search stresses more on the algorithmic side and the exploratory search pays more attention on the user interface to help users to achieve better search results. We believe that by combining these two approaches, we can provide users with a better solution than the old ranked list based interaction mechanism of personal- ized search. This paper proposes to incorporate interactive visual- ization into personalized search. We extended a well known visu- alization method called VIBE (Visual Information Browsing Envi- ronment) to visualize user models and then incorporated it into the personalized search framework. We expect our approach will be able to help users to better explore the information space and locate relevant information more efficiently. Here, we showed the concept and the potential of this adaptive visualization method. We also introduced a new prototypical personalized search system and its interaction model implementing the adaptive visualization idea.\r1. INTRODUCTION\rThe information overload problem has been one of the major motivations of the information retrieval community. A lot of effec- tive methodologies and algorithms have been designed to help users to locate relevant information from the massive pool of candidate documents. Yet the Web constantly pose new challenges to the re- searchers. Nowadays, when the Web emerged as a universal source of information, a range of problems, which users attempt to solve using the Web expanded as well. More and more frequently people use the Web for exploratory search tasks, which required multiple searches and complex processing of information. Since search en- gines or traditional search tools provided by industry are found not to be adequate to support exploratory search, researchers explored a range of alternative solutions such as (1) personalized search [5] and (2) exploratory search interfaces (or interactive search) [4]. Personalized search applies artificial intelligence to attract user at- tention to most relevant results, while exploratory search interfaces attempt to empower human’s own intelligence by providing more interactive and expressive user interfaces so that they can reach bet- ter search results.\rWhile these approaches are typically considered as alternatives, they are really complementary. We believe that both personal- ized search and exploratory search interfaces can benefit from each other. In particular, most of personalized searching algorithms are still relying on query-ranked list model. Despite the simplicity and straightforwardness of this model, we still see the potential of a more advanced user interface with which users can better under- stand what is happening inside the search engine and eventually contribute to improving the search results. We expect to improve\r2.\r2.1\rVISUALIZING USER MODELS FOR PER- SONALIZED SEARCH\rVIBE – the foundation visualization model\rVIBE (Visual Information Browsing Environment) is a relevance- based visualization method developed at Molde College and the University of Pittsburgh [6]. It displays a number of documents and determines their positions by their similarity ratios to a group of reference points called POIs (Point of Interest). Therefore, if a document has a similarity score 0.2 to POIa and 0.8 to POIb, then it is placed on a position four times closer to POIb due to its four times bigger similarity value. It can visualize more than just two POIs, so that users can investigate the relationships among multiple reference points (POIs) and the documents (See Figure 1 and 2 as examples).\rThe most general idea to apply the VIBE visualization to IR may be defining the query terms as POIs and visualize them with the re- trieved documents. However, all the POIs are treated equally in terms of their dimensions in this idea. This is a shortcoming when we consider multiple groups of POIs that represent different layers of meanings. For example, if we want to define POIs that repre- sent user interests estimated by the system, we need to differentiate them from the queries directly entered by the users. However, tra- ditional VIBE cannot handle this difference. Therefore, we raised a simple idea that can overcome this shortcoming – by spatially and visually separating two POI groups.\r\nFigure 1: Adaptive VIBE layouts – showing three layouts. Yellow (CONVICT and PARDON) and blue (YEAR, POPE, and so on) POIs are query terms and user model keywords respectively. White squares are retrieved documents.\r2.2 Adaptive VIBE – extension of VIBE for user model visualization\rVIBE-based IR visualization approaches usually used a radial layout for their initial placement of POIs (Figure 1a). Like a round table, all POIs are equal in this idea. However, in our application, where we want to visualize the personalized search result, we need to discriminate the documents retrieved by the user model and by the user queries. We wanted to let users know which documents were more affected by the user models than the query terms, or vice versa. We could achieve this discrimination by separating the groups of POIs spatially on the screen. Figure 1 shows an exam- ple. In addition to the traditional layout Radial (a), we defined two new layouts called Hemisphere (b) and Parallel (c). The two latter layouts try to locate two groups of POIs – query and user model POIs – in horizontally separate places and then break up the spaces between two groups. By this POI group separation, the documents under the effect of each group are separated spatially again and the users can easily investigate each cluster of documents to find out relevant information. The difference between the two are whether the separation is bigger (Parallel) or moderate (Hemisphere). In addition to the location, we could use different color codes for two groups. In the figure, the query terms and the user model keywords were painted in yellow and blue respectively.\rThe Radial layout (a) is identical with the non-adaptive VIBE except the color coding. By comparing it with (b) or (c), we can understand the advantage of Adaptive VIBE. The documents are mostly cluttered around the center and it is hard to see the about- ness of the documents to the POIs, especially hard to catch whether they are more about the query or more about the user model. How- ever, in the layout (b) and (c) we can clearly separate the document groups by whether they are closer (in terms of location as well as meaning) to the query (yellow POIs) or the user model (blue POIs). In the Hemisphere layout (b), there still remains a bit of the clutter as in the Radial layout whereas the clutter is almost disappeared in the Parallel layout (c). However, we can still see the advantage of the Hemisphere layout, because it is easier to find the difference within each POI group (user model or query POIs). For example, it is easier to find out documents closer to POPE than RUSSIA in the Hemisphere rather than in the Parallel layout.\r3. PRELIMINARY EXPERIMENTS\rIn order to test if the idea described in the previous section can really benefit the personalized IR, we conducted an experiment [2]. The experiment was feasible with an existing dataset generated from our previous text-based personalized search study [3]. We had built a task-based personalized IR system and conducted a user study, where the subjects were asked to search for a specific infor-\rmation while the system built user models and tried to help users to find relevant information using the user models. Because all the ac- tivities of the users and the system were recorded into the log files, we could extract the core information of the search sessions such as (1) user query (2) retrieved documents (with similarity scores) (3) user model content, and then recreate the Adaptive VIBE visu- alization replacing the textual ranked lists.\rAfter recreating the visualization with the real data extracted from the previous study, we were able to check if the simulated visualization would be able to help the searcher. Figure 2 shows one snapshot of the images generated in the experiment. Because we were equipped with the tasks to be solved by the subjects and the groundtruth of the topics from the user study, we were able to mark the relevant documents to the given topic and examine how their spatial distributions look like. As can be seen in the figure, there tended to appear clusters of relevant (green) and irrelevant (red) documents. What is more interesting is, the relevant docu- ment clusters were closer to the user model side (right hand side according to our layout definitions) than the query side. This trend became stronger as we used the Hemisphere, and then the Parallel layouts. We were able to check this finding numerically by exam- ining the centroid positions of the (ir)relevant document clusters. Table 1 compares the x-coordinates of the cluster centroids by the three layouts. It shows that the x-coordinates of the relevant docu- ments were greater than those of the irrelevant document clusters, and therefore they were closer to the user model because the user models were placed in the right hand side of the screen. The dis- tances between the centroids grew bigger in adaptive visualization layouts (42.40 and 92.94 pixels with Hemisphere and Parallel re- spectively) than the non-adaptive one (20.4 pixels). For more de- tails about the experiment and additional analysis, please check [2].\rTable 1: Comparison of relevant and irrelevant document clus- ter centroid positions on the visualization (averages in pixel)\rRadial Hemisphere Parallel\rRelevant Non-relevant Difference (relevant – irrelevant)\r304.3 337.7 283.9 295.3 20.4 42.40\r300.9 207.96 92.94\r4. 4.1\rA PROTOTYPE SYSTEM Adaptive Visualization\rEncouraged by the preliminary experiment result, we decided to incorporate Adaptive VIBE in a real personalized search system.\r\nFigure 3: Prototype System User Interface – Visual TaskSieve (TaskSieve plus Adaptive VIBE)\rFigure 2: Adaptive VIBE experiment – green and red squares mean relevant/irrelevant documents.\rFigure 3 is a screenshot of a prototypical personalized search sys- tem called TaskSieve [3] empowered by Adaptive VIBE. TaskSieve is a personalized search system with which the users can find out relevant information following the information foraging and sense- making process [7]. The users initiates searching by entering queries but their search process gets enriched by the user models which are constructed from the information foraging and the sensemaking loops. The users can annotate interesting (relevant or informative) evidences (or notes) they found into a special place called shoebox (lower right corner of the screen). Their user models are extracted from the content of the shoebox instantly and automatically, shown in the form of cloud (upper right corner) and applied to the per- sonalized search engine on the fly to update the list of retrieved documents.\rAdding Adaptive VIBE into TaskSieve (Visual TaskSieve), the old textual ranked list was replaced with the Adaptive VIBE visu- alization as shown in Figure 3 (occupying the biggest real estate of\rthe screen). User query and the user model is provided as POIs, and the POI layouts described in section 2 are used to make the visualization adaptive. The interaction models of the text-based TaskSieve (a) and Visual TaskSieve (b) are compared in Figure 4. Two models are almost identical except Visual TaskSieve makes use of the visualization (green box) for representing the search re- sults. In traditional IR models (including old TaskSieve), users ex- amine the list of documents, their ranks, navigate across pages, and then find out relevant documents to improve the initial search re- sults. In the Visual TaskSieve model, however, we present the re- trieved documents visually and users examine them spatially and interactively. The following sections describe the spatial interac- tion model of Visual TaskSieve.\r4.2 Spatial User Interaction on Retrieval Re- sults\rThe evolution from the textual ranked list to the spatial visual- ization changes the way of user interaction too. Users are provided with very limited information about the retrieved documents from the ranked lists, their vertical positions in the list. However, spatial visualizations such as VIBE and Adaptive VIBE added more di- mensions and users can learn the aboutness of the documents to the reference points (POI) and their search context (personalization). As the spatial distribution of the retrieved documents became im- portant, we added a couple of tools to better support the spatial examination of the documents and feedbacks.\rSpatial filtering of retrieved documents – Users can select (or filter) documents spatially on the visualization using a marquee tool. Detailed information such as titles, surrogates, and the links to the fulltext of the selected documents are displayed. The red- colored documents (squares) in Figure 3 are the examples of the selected documents using the marquee tool and their information is shown below the visualization. Because spatial distribution of the documents is tightly connected to the aboutness of the documents\r\nFigure 4: Comparing the interaction models of TaskSieve and Visual TaskSieve 5. CONCLUSIONS\r(more about UM, query, or a specific POI), this selection tool can let users more promptly learn the contexts of the retrieved docu- ments.\rLinking POI and document filtering by a double slider – Users can directly point a POI and filter related documents to the POI (and select/see the detailed information of the documents). They can specify the lower and upper similarity threshold of the docu- ments to the POI using a double slider (Figure 5). This function plays a similar role with the marquee selection, but is expected to help users to select documents more precisely in a reverse direction, from a POI to documents.\rFigure 5: Selecting documents from a POI: users can set the low and high similarity thresholds of the documents to be se- lected using the double slider\rPOI dock – Sometimes, too many POIs at the same time on the screen can make users feel lost and frustrated. However, we cannot just limit the amount of information that users can access. There- fore, we added a special area where some POIs are temporarily moved out so that their effects on the visualization are disabled. We call it as a POI dock, which is shown in Figure 3 next to the user model POIs. Users can drag POIs to/from this area and inter- actively examine the effect of each POI they are interested.\r4.3 Concept Level User Modeling and POI Ex- traction\rIn our previous studies, the elements of the user models were al- ways keywords (or terms). Even though the keyword-based user models were working relatively good, we expect concept-based user models would help users better than the simple keywords. Therefore, we are devising ideas to realize the concept-based user models in our current Visual TaskSieve system. We can extract named-entities from the retrieved (or selected) documents and use them as user model elements, and show them as POIs on the vi- sualization. In Figure 4, the user can examine the NEs from the retrieved documents in the cloud form (below middle), and then they can pick and add some of them as POIs to the visualization.\rThis paper showed our ongoing effort to incorporate adaptive visualization into personalized search. We have extended a well known VIBE visualization algorithm for adaptive visualization and have investigated its effectiveness. We are constructing a full-fledged prototype personalized IR system that supports the adaptive VIBE visualization. We showed the details of the idea and the prototype as well as the preliminary experimental results. We are planning to finish the construction of the system soon and conduct a full-scale user study in order to learn the strengths and the shortcomings of our visual IR idea.\r6. REFERENCES\r[1] J.-w. Ahn and P. Brusilovsky. From user query to user model and back: Adaptive relevance-based visualization for information foraging. In WI ’07: Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence, pages 706–712, 2007.\r[2] J.-w. Ahn and P. Brusilovsky. Adaptive visualization of search results: Bringing user models to visual analytics. Information Visualization, 8(3):167–179, 2009.\r[3] J.-w. Ahn, P. Brusilovsky, D. He, J. Grady, and Q. Li. Personalized web exploration with task models. In WWW ’08: Proceeding of the 17th international conference on World Wide Web, pages 1–10, New York, NY, USA, 2008. ACM.\r[4] G. Marchionini. Exploratory search: from finding to understanding. Commun. ACM, 49(4):41–46, April 2006.\r[5] A. Micarelli, F. Gasparetti, F. Sciarrone, and S. Gauch. Personalized search on the world wide web. pages 195–230. 2007.\r[6] K. A. Olsen, R. R. Korfhage, K. M. Sochats, M. B. Spring, and J. G. Williams. Visualization of a document collection: the vibe system. Inf. Process. Manage., 29(1):69–81, 1993.\r[7] P. Pirolli and S. Card. The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis. In Proceedings of International Conference on Intelligence Analysis, 2005.\r\nPuppyIR: Designing an Open Source Framework for Interactive Information Services for Children\rLeif Azzopardi, Richard Glassey, Mounia Lalmas, Tamara Polajnar Dept. Computing Science\rUniversity of Glasgow\rScotland, U.K.\r{leif, rjg, tamara, mounia}@dcs.gla.ac.uk\rABSTRACT\rOne of the main aims of the PuppyIR project is to provide an open source framework for the development of Interac- tive Information Retrieval Services. The main focus of the project is directed towards developing such services for chil- dren, which introduces a number of novel and challenging issues to address (such as language development, security, moderation, etc).\rIn this poster paper, we outline the preliminary high-level design of the open source framework. The framework uses a layered architecture to minimize dependencies between the user-side concerns of interaction and presentation, and the system-side concerns of aggregating content from multi- ple sources and processing information appropriately. Each layer will consist of a series of interchangeable components, which can be interconnected to form a complete service. To facilitate the construction of diverse information services, a dataflow language is proposed to enable the assembly of the components in an intuitive and visual manner. One of the the design goals of the architecture, and ultimate measures of success, is to provide a “lego” style building block envi- ronment in which researchers and developers of any age can build their own information service.\rThis poster provides the starting point for the design of the framework and aims to seek comments, feedback and suggestions from the community in order to improve and refine the architecture.\r1. INTRODUCTION\rToday children are exploring the Internet from a very early age. However, much of the content and services available on the Internet have been designed for adults. And so while many children actually possess better computing skills than their parents, they are also left vulnerable to the darker side of the net (in the worst case) or overwhelmed and over- loaded when searching for information available online [7]. Furthermore, they also have to interact and engage with\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR ’09 Washington, USA\rCopyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.\rIan Ruthven\rDept. of Computer and Information Sciences University of Strathclyde\rScotland, U.K. ir@cis.strath.ac.uk\rsystems and tools that are not specifically designed to con- sider or develop their cognitive, emotional and intellectual abilities [3].\rConsequently, many children experience difficulties during information seeking, as they have to contend with a number of challenges: non-intuitive search interfaces; unmoderated and unverified content; language and understanding issues; and structured and prescribed interaction styles. In order to help children effectively, efficiently and enjoyably engage with information, entertainment, friends, and so forth on- line, it is important to provide access to such information services in ways that are consistent with their learning, cog- nitive development and curriculum [3]. To support the in- vestigation of such challenges, the PuppyIR project aims to provide the infrastructure to construct and build interactive information services using an open source framework.\rThe remainder of the poster is as follows: In the next section we discuss the aims of the PuppyIR project, then we outline the high-level design of the open source framework, before providing an example of constructing an information service. Finally, we conclude with a brief summary.\r2. PUPPYIR PROJECT\rPuppyIR1 is an European Union project that is funded under the European Community’s Seventh Framework Pro- gramme. The project will run over the next three years and involves seven partners from within European Union. The aims of the project are as follows:\r1. Investigation and promotion of new interaction paradigms for children’s access systems.\r2. A number of interrelated research questions will be ad- dressed that contribute to the interaction of children with information services. The focus will be on presen- tation, mining and language issues for the appropriate development of these information services.\r3. An open source framework will be created to support the development of common interchangeable compo- nents to support the realization of information services for children.\r4. Identification of appropriate evaluation measures and develops an evaluation framework for information ser- vices for children.\r1 http://www.puppyir.eu/\rPage 26 of 122\r\nIn this poster paper, we shall focus on the 3rd aim of this ambitious and exciting project and outline the preliminary design for the high level architecture of the PuppyIR Open Source Framework, as a way to open up the project to the wider community and seek input and feedback on the design and architecture.\r3. OPEN SOURCE FRAMEWORK DESIGN\rOne of the main design goals of this project is to create an open source framework that supports the rapid devel- opment of child-friendly Interactive Information Retrieval (IIR) services. The provision of such a framework enables both system developers and researchers to design and cre- ate different types of IIR systems using a common library of components and configurations, and to do so quickly and cost-effectively.\rWith that in mind, the PuppyIR Open Source Frame- work (PIR-OSF) aims to deliver an open, reconfigurable and reusable software framework. It avoids unnecessary dupli- cation of effort and allows for rapid development of multiple independent IIR services. Finally it provides an effective evaluation environment for ongoing HCI and IR research, based around an open and common toolset.\rTo achieve these aims, the design of PIR-OSF establishes three fundamental abstractions for modeling a generic IIR service:\r1. Layered Architecture: provides the separation of concern between the common aspects of an IIR ser- vice (information sources, information processing, vi- sualization and interaction) that enables multiple en- vironments and application scenarios to be addressed using a single framework.\r2. Rich Information Objects: a flexible abstraction for any type of information content (text, images, au- dio, movie, mixed, etc.) and its accompanying meta- data required by an IIR service.\r3. Dataflow Language for Information Processing: allow both users and developers to reuse, visually re- configure alternative information processing networks to deliver the most appropriate information.\rThe following sections discuss these abstractions and their implications upon the design of the PIR-OSF.\r3.1 Layered Architecture\rIn its most basic form, an IIR service allows users to inter- act by submitting their queries, then presenting them with the static results (e.g. via a search engine) or a stream of information (e.g. a RSS feed) using an appropriate form of presentation. The architecture of the PIR-OSF represents this high-level system view using a series of independent, stacked layers.\rThis design decision reflects the different requirements of building information services for multiple environments and application scenarios. More fundamentally, it recognizes the close relationship that exists between human-computer in- teraction and information retrieval. By providing a com- mon framework, researchers from both fields can conduct research within their appropriate layer(s) and reuse compo- nents from a pre-existing library built by others. Thus, the entry barrier to building a complete system is dramatically\rlowered, reducing the effort for both systems’ designers and the HCIR research communities.\rA concrete illustration of the importance of a layered ar- chitecture is to consider a unified museum guide for children. Results of a query issued at a public static terminal or from their own mobile device could be projected onto a nearby wall-mounted display, taking advantage of the extra screen space and perhaps touch-oriented interaction. Should no display be nearby, the user could still use their mobile de- vice, with the same underlying system, but the presentation of and interaction with the results would be vastly different. To accommodate these differences, a flexible architecture is essential. Figure 1 shows the layered architecture of PIR- OSF. A conceptual divide separates the lower system-side layers from the upper user-side layers.\rInteraction\rPresentation\rLayout\rVisualiztation\ruser\rsystem\rInformation Content Processing\rInformation Services\rContent Space\rPage 27 of 122\rFigure 1: Layered architecture of the PuppyIR Open Source Framework\rSystem-side Layers: At the bottom of the stack, the Content Space layer represents all of the potential informa- tion sources that are indexed and made available to the rest of a system. The Information Services layer encompasses the various interfaces to online services, such as Google, Yahoo, Bing, Youtube, Flickr, Twitter (uncooperative services) and so on, as well as content indexed by offline services such as Lemur, Lucene and Terrier (cooperative services). This layer is responsible for using the respective API of each ser- vice and specifies the interface for creating result wrappers for each service, such that results can be returned as collec- tions or streams of Rich Information Objects (RIO).\rThe Information Content Processing layer manipulates the RIOs by passing them through a network of linked com- ponents called Information Content Processors (ICP), con- figured using a dataflow-oriented approach. For example, a typical ICP may moderate the content of a RIO and return the modified result for further processing. More complex ICPs may act as aggregators and distributors of multiple RIOs, as shown in Fig. 2. Alternatively, two summarizer ICPs could be evaluated by swapping them into the same ICP network, without changing any other aspect of the con- figuration. This networked aspect provides complete flexi- bility with the configuration, making it suitable to develop a wide range of systems.\rUser-side Layers: The upper layers relate to the user-\r\nIO  IO'\rIO' IO  IO' IO'\rIO\rIO  IO' IO\re.g. filter moderator\re.g. multisource cluster\re.g. merger selector\rModifier\rDistributor\rAggregator\rFigure 2: Information Content Processor (ICP) con- figurations\rside concerns; how RIOs are presented, and how the user can interact with the system in terms of query formulation and interactive feedback. The Presentation layer deals with the overall layout of containers of RIOs (e.g. a webpage that has several blocks for separating image and textual re- sults) and the visualization of RIOs within the containers (e.g. a ranked list of results or a carousel of images). The Interaction layer corresponds to the variety of modes and modalities that a user may utilize when using the system, ranging from the familiar textual and graphical interfaces, onwards to more advanced speech, touch and tangible forms of interaction.\r3.2 Rich Information Objects\rThe RIO is an abstraction for all kinds of information that an information service may produce or need to consume. It exposes a public interface that allows the components of PIR-OSF to inspect, manipulate, and exchange it with other components. This abstraction also permits the creation of new content types without needing to modify existing com- ponents of the PIR-OSF. For example, new RIOs could be created to mix multiple types of information together as re- quired by a service.\rBesides representing information, such as text, images and movies (or combinations), each RIO contains metadata, which either has been extracted from the information (e.g. the title of a web page), or added by a component of PIR- OSF as part of its processing (e.g. a readability rating). The range of metadata is not fixed and is fully extensible depending upon the system needs.\rThis approach is crucial to providing a common object that all components can inspect, annotate, transform and exchange. Whilst this type of information wrapping in- evitably creates overhead costs in terms of time and space when processing objects, it is crucial for the final systems abstraction of PIR-OSF: the dataflow oriented approach to processing RIOs.\r3.3 Dataflow Language for Information Con- tent Processing\rEach service built using the PIR-OSF will have different information processing needs. Since there are a lot of dif- ferences between users and their specific needs, as they are children (i.e. lots of variation in age, language skills and lan- guage, knowledge, tastes, moods, etc), then the flexibility to configure the information services is paramount. Example\rinformation content processing units that will be provided to support the information seeking of children will include: moderation components which filter the content ensuring it is suitable and/or appropriate; summarisation components which will re-factor content to suit the child’s reading abil- ity, classification components to aggregate and facet infor- mation depending on their context. Consequently, the Pup- pyIR project will focus on developing information processing components which are designed for children.\rThe ability to chain together different ICPs is therefore critical towards meeting the different IIR service require- ments. One approach might be to treat the chaining of ICPs as a pipe and filter architecture [6], however this re- duces the configuration of ICPs to a single, serial pipeline and does not allow a rich network of ICP components to be assembled. Instead, PIR-OSF adopts a dataflow language oriented-approach.\rThe essence of dataflow programming languages is that a program can be expressed as a directed graph, with nodes acting as primitive instructions (such as arithmetic and com- parative operations) and directed arcs representing the data dependencies between these instructions [5] (see Fig. 3).\rA := X + Y B := Y / 10 C := A * B\r(a)\rX\rY 10\r+/\r*\rC\r(b)\rFigure 3: A simple program (a) and its dataflow equivalent (b)\rHistorically, dataflow languages have been closely linked to visual programming, however the infancy of display and interaction technology hampered their development [4]. With advances in user interfaces, increasingly visual approaches are being adopted for dataflow programming, e.g. the MIT Scratch programming environment2, Apple’s Quartz Com- poser3 for graphics programming and Yahoo! Pipes for mash- ing web data together4. We adopt this approach for the PIR-OSF, where ICPs replace the instructions and RIOs re- place the data. There are three main justifications for using this design choice:\rVisualization: Early in the history of dataflow program- ming, it was recognized that dataflow languages could be easily adopted by novice users in order to communicate and construct programs [1]. Given the intuitiveness of such lan- guages, we envisage that not only could systems designers easily (re)-configure the system using visual tools, but the users themselves (in this case children) could become the de- signers of their own information services; allowing them to better fit their own individual information seeking behavior.\rParallelism: Dataflow programming was proposed as a\r2 http://scratch.mit.edu\r3 http://developer.apple.com/graphicsimaging/quartz 4 http://pipes.yahoo.com/pipes\rPage 28 of 122\r\nsolution to the von Neumann execution model, in which an instruction can only be executed when the program counter reaches it, even if it could be executed earlier. Many scenar- ios exist where multiple sources of information can be ac- cessed in parallel and concurrently processed without affect- ing the final results. Given the importance of responsiveness to the user experience [2], parallelism should be exploited.\rVerifiability: As the development of any concurrent pro- cess tends to be complicated, the use of dataflow program- ming may help reduce this effort. Networks of ICPs, and the flow of RIOs through them, can be formally modeled, simu- lated and verified, before spending time and effort trying to develop, debug and maintain a complex concurrent system.\rcomplex network of ICPs. The query is passed through a moderator ICP and a multi-source ICP, which can be con- figured to distribute its input to multiple recipients (see dis- tributor ICP in Fig. 2). In this case, it submits the same query to both Flickr and Youtube. This returns two re- sult sets, which are processed in parallel (one moderated for adult content, whilst the other is ordered by ratings), then merged together into a single result set for presentation to the user. As the same query can be injected into both ICP networks, output can be presented together to the user, in the most appropriate form for the IIR service.\rIn this example trivial combinations of ICPs were used; more complex networks could be created for specific IIR services. By using a dataflow-oriented approach, IIR ser- vices can more easily be reconfigured and customised by both users and systems developers. Furthermore, this ap- proach permits the evaluation of similar ICPs (e.g. two sum- marizers), by swapping them into an active network, whilst monitoring the user experience and response.\r5. SUMMARY\rIn this poster paper, we have outlined the preliminary high-level design of the PuppyIR Open Source Framework for constructing information services. Key to the design is the layered architecture used to assemble the desired service functionality. This is underpinned by a dataflow language that models the flow of Rich Information Objects through a flexible network of Information Content Processors, from the content space to the presentation layer. It is envisaged that this plug and play architecture will enable the rapid development of information services, whilst also being highly extensible and configurable.\rAcknowledgements: This research is funded by the Eu- ropean Community’s Seventh Framework Programme FP7/ 2007-2013 under grant agreement no. 231507.\r6. REFERENCES\r[1] A. L. Davies. Data-driven nets - a class of maximally parallel, output-functional program schemata. Technical report, Burroughs, San Diego, CA, 1974.\r[2] W. O. Galitz. The essential guide to user interface design: an introduction to GUI design principles and techniques. John Wiley and Sons, 2002.\r[3] H. E. Jochmann-Mannak, T. W. C. Huibers, and\rT. J. M. Sanders. Children’s information retrieval: beyond examining search strategies and interface. In The 2nd BCS-IRSG Symposium: Future Directions in Information Access, 2008.\r[4] W. M. Johnston, J. R. Paul-Hanna, and R. J. Millar. Advances in dataflow programming languages. ACM Computing Surveys, 36(1), March 2004.\r[5] P. Kosinki. A data flow language for operating systems programming. In Proceedings of the ACM SIGPLAN–SIGOPS Interface Meeting, 1973.\r[6] R. N. Taylor, N. Medvidovic, and E. Dashofy. Software Architecture: Foundations, Theory, and Practice. John Wiley and Sons, 2009.\r[7] Y. Zhang. How children find information on the internet: An empirical study and its implications. In The 3rd Annual Research Conference of the Research Center for Educational Technology (RCET), Kent, OH., 2002.\r4.\rDESIGNING AN INTERACTIVE INFOR-\rMATION SERVICE WITH PIR-OSF\rBy way of illustration, we present the high-level design of an IIR service using PIR-OSF. We focus mainly upon the component interaction and the flow of information process- ing for the system-side aspects of the IIR service. The user- side aspects of the system, presentation and interaction, can be assumed to take the form of a web page with aggregated results (a page with a block reserved for web search results, and another for combined image and video results).\rFigure 4 shows the IIR service, with two independent ICP networks configured, and the layered architecture overlaid for reference. On the left-hand side, a simple single-path network has been created. In it, a query is submitted, mod- erated, then sent to the Google search engine using its API. The results returned are wrapped into a set of RIOs at the Information Services layer, before being passed through the moderation and summarizer ICPs. This returns a modified result set (RIO’) that can then be presented to the user.\rInteraction\rQuery\rQuery\rPresentation\rOutput\rOutput\rInformation Content Processing\rmoderator\rmoderator\rAPI\rsummarizer\rmoderator\rmultisource\rmerge\rrater\rmoderator\rInformation Services\rwrapper\rAPI\rAPI\rwrapper\rwrapper\rContent Space\rGoogle Search\rFlickr Youtube\rFigure 4: High-level design of a single IIR ser- vice with two independent network configurations of ICPs\rIn contrast, the right-hand side of Fig. 4 shows a more\rPage 29 of 122\r\nAn Interactive Automatic Document Classification Prototype†\rKirk Baker Collexis\rBethesda, MD 20817 baker@collexis.com\rArchna Bhandari\rOffice of Knowledge Management and Portfolio Analysis\rNational Institutes of Health Bethesda, MD 20892\rRao Thotakura Division of Information Services\rNational Institutes of Health Bethesda, MD 20817\r†Disclaimer: This document was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any of its employees makes any warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial products, process, or service by trade name, trade mark, manufacturer, or otherwise, does not constitute or imply its endorsement, recommendation, or favoring by the United States Government. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government and shall not be used for advertising or product endorsement purposes.\rABSTRACT\rIn this paper we report on a series of completed and ongoing experiments that involve the integration of fully automatic document classification techniques into an existing manually- oriented document retrieval system. We take our primary findings as positions on the design of an interactive document classifier and retrieval tool.\rGeneral Terms\rDesign, Experimentation, Human Factors.\rKeywords\rInformation retrieval, machine learning, user interaction.\r1. INTRODUCTION\rIn this paper we report on a series of completed and ongoing experiments that involve the integration of fully automatic document classification techniques into an existing manually- oriented document retrieval system. We take our primary findings as positions on the design of an interactive document classifier and retrieval tool.\rThe rest of this paper is split into three sections. Section 2 contains background information and a brief technical overview of a document classification system used to report funding allocations across a range of research categories. Section 3 outlines some of the formal system and user requirements that guided our integration of automatic classification techniques into an existing document classification system. In Section 4 we describe our positions on the design of an interactive document classification tool in terms of a functioning prototype that meets these requirements.\r2. TheResearch,ConditionandDisease\rCategorization (RCDC) Initiative\rIn 2006, the United States Congress mandated that the National Institutes of Health (NIH) establish a standardized, automated system for reporting its financial allocations to supported research\rareas, conditions, and disease categories. The RCDC system is the implementation of this mandate.\rIt is implemented atop a vector space document retrieval model. Documents are preprocessed by a natural language processing module that extracts only those variations of term strings that correspond to concepts in the RCDC thesaurus. The RCDC thesaurus is a controlled medical ontology that draws from the Medical Subject Headings (MeSH)1 and CRISP2 databases, the National Cancer Institute thesaurus3 , the UMLS Metathesaurus4, and Jablonski’s Dictionary of Medical Acronyms and Abbreviations [1]. Documents are represented as weighted concept vectors where a frequency-based weighting scheme is applied to concept counts and then normalized such that all concept weights fall between 0 and 1.\rDefinitions of funding areas are also represented as weighted concept vectors along with a similarity threshold, where the set of concepts and weights are determined by subject matter experts and refined in conjunction with ongoing reviews of the set of documents that are retrieved. Concept weights in definitions of funding categories range from -1 to 1, or may be designated as mandatory or excluded. From an information retrieval standpoint, funding area definitions are treated as query vectors and a measure of similarity between each query and each document vector is calculated. When the computed similarity is above threshold for a given query and document, that document is classified as belonging to the given funding category.\rThe impetus for incorporating automatic document classification techniques into the RCDC system originated in response to the following specifications:\r1. a need to alleviate the manual effort required in developing and maintaining category definitions;\r1 http://www.nlm.nih.gov/mesh\r2 http://crisp.cit.nih.gov/crisp/CRISP_Help.help\r3 http://ncit.nci.nih.gov\r4 http://www.nlm.nih.gov/pubs/factsheets/umlsmeta.html\rPage 30 of 122\r\n2. a desire to improve classification accuracy of selected existing categories;\r3. the ability to support ad hoc category definition and retrieval in real-time.\rIn meeting these specifications, we were obligated to adhere to the conditions described in the following section.\r3. DevelopmentofAutomatedClassification\rTechniques\rMaintaining continuity with the existing RCDC system was a required condition for the integration of automated techniques into the grant categorization process. In terms of system requirements, this meant maintaining compatibility with the implemented vector space information retrieval model. Therefore, we restricted ourselves to linear classifiers, primarily linear support vector machines ([2], [3]), although experiments with linear perceptron show similar results ([4]).\rWithin the current RCDC application, the definitions of funding categories (i.e., the set of query terms and their ranking) are as important to subject matter experts as the set of retrieved documents. In practical terms, this means that not only should the output of a trained classifier be interpretable, it must be editable as well. Specifically, users requested the ability to:\r1. limit the number of dimensions in the trained classification function;\r2. delete dimensions that are intuitively irrelevant;\r3. change the value of a dimension to match their intuitions about its relative importance;\r4. add dimensions that were not part of the automatically generated classification function.\rWith regard to the documents retrieved or intended to be retrieved by a given query, users required the ability to indicate retrieval errors (false positives) and to augment the training data with externally labeled documents (false negatives) and incorporate these labels iteratively into the classifier’s training procedure. In light of this requirement, we found that users should be given feedback about the inherent separability of the documents they are trying to classify. We found such feedback useful in tempering expectations of the performance of the automatic classifier and helping users understand why classification accuracy varies for different funding categories.\rIn the next section, we take the requirements above as our positions on the design of a document classification and retrieval tool that incorporates user interaction into the training procedure for an automatic classifier.\r4. PositionsonDesigninganInteractive\rAutomatic Document Classification System\rIn this section, we outline our positions on the design of an interactive document classification and retrieval tool that we prototyped for the RCDC project. Our positions are grounded in a series of experiments that measured classification accuracy for four categories – Lung Cancer, Breast Cancer, Prevention, and Orphan Drug. The data consisted of about 25,000 labeled grants\rthat were funded by NIH in 2008 (a subset of about 80,000 total applications funded for the year).\r4.1 Usersmustbeabletoeditthetrained\rclassification function.\rIn an interactive document classification system, the trained classification function must be interpretable by users. By default, linear classifiers like perceptron or SVM produce as output a weighted list of all (or nearly all, in the case of SVM) of the input dimensions. In our case, this vector typically contained around 20,000 dimensions, which is too many for a user to make sense of. Therefore, the first step we took in producing a human- interpretable classification function was to limit the number of dimensions over which the classifier operated. We evaluated several techniques for restricting the dimensionality of the training data and found that a simple, two-pass strategy worked as good as anything:\r1. Train the classifier on the full-dimensional data set.\r2. Remove all but the top-n ranked dimensions in the output function from each item in the data set.\r3. Retrain the classifier on the n-dimensional data set.\rHappily, the optimal number of dimensions in our classification tasks turned out to be quite small (around 25-100 dimensions) relative to the dimensionality of the original data set. Another simple and effective way to restrict the dimensionality of the trained classification function is to remove low-ranked features from the data set before initially training the classifier (i.e., remove terms from a document vector if their weight is below some threshold).\rFigure 1. Screenshot illustrating functionality to edit the trained classification function by deleting features or changing their weights.\rEven after limiting the number of features in the classification function, some items remain which are unacceptable to users’ intuitions about whether they belong. For example, if a number of Lung Cancer grants originated from the N.C. Cancer Hospital, it is possible that a term like “North Carolina” would be heavily weighted in the resulting classification function. However, a user might feel that this term is inappropriate for a query intended to\rPage 31 of 122\r\ndefine Lung Cancer grants in general. Therefore, our prototype allows users to indicate that particular features should be excluded from the training data.\rFigure 1 shows a screenshot of a prototype application that illustrates this functionality. The primary view in Figure 1 contains a set of weighted features which represent the decision boundary for a sample document category. In this illustration, the two blue highlighted terms have been selected by the user for removal from the classification function. The highlighted features will be removed from the training data prior to retraining the classifier, and in essence become inactive for any future data points that may contain them. We found that allowing users to selectively remove unwanted features from the training data generally has very little impact on classification accuracy and occasionally improves it.\rSometimes users want to override automatically computed weights for a given feature, usually because they feel that it merits a higher weight than the classifier assigned it. The problem with attempting to manually adjust feature weights is that they are likely to change with the next training cycle. Our solution to this requirement has been to store user-modified concepts separately, train the classifier, and overwrite those user-modified concepts before the classification stage. In Figure 1, the weight of the last term shown (“Tumor Tissue”) has been overridden by the user to 1.0 and is depicted in red font.\rAnother way to allow users to interactively modify classifier output is to allow them to apply the classifier to an unlabeled or partially labeled document set. The documents that are returned can be designated by the user as positive or negative examples and incorporated into retraining the classifier (Figure 2).\rFigure 2. Using document relevance feedback to relearn the decision boundary.\r4.2 Usersmustbegivenfeedbackonthe inherent separability of their data set.\rWe sometimes observed users trying to distinguish document classes that are at best poorly separable over the given feature representation. For example, a category like Orphan Drug is defined axiomatically as research pertaining to any pharmaceutical agent used to treat a disease or condition that affects less than 200,000 people in the United States [5]. It is difficult to train a linear classifier to learn this distinction over document term vectors and produce an easily interpretable\rclassification function. In cases like this, users tend to get disgusted at the classifier’s poor performance and blame the computer.\rWe found it useful to guide users’ expectations of system performance by visualizing document similarities with a two- dimensional interactive cluster plot (Figure 3). When users see high overlap between positive and negative training samples, they understand that they’re asking the classifier to do something hard. For example, in the top plot in Figure 3, there is relatively poor separation of the sample document class (green squares labeled “Positives”) from the remainder of the document set (red squares labeled “Negatives).\rAfter the same data set has been restricted to 500 dimensions (from an original feature set of 7814 in this case), we observe less overlap of the two classes (bottom plot in Figure 3). The reduced feature set also corresponds to higher measures of classifier performance on a labeled test set. However, we found the type of visual feedback depicted in Figure 3 more effective than providing traditional evaluation measures like precision, recall, and F1 to users not accustomed to thinking in these terms.\rFigure 3. Document separation and number of features. In the top panel distances between documents are computed over the full set of 7814 features before applying dimensionality reduction necessary for two-dimensional visualization. In the bottom panel only 500 features were retained.\rWe also found it useful to allow users to interact with the classifier via the cluster plot (e.g., using the mouse to zoom or select individual data points). For example, after removing selected features from the training data, a user can examine the impact of this action by re-clustering the data and looking for relatively more or less class separation.\rAlternatively, a user can select a document from the cluster plot to indicate whether it should be used as a positive or negative training point to retrain the classifier. Figure 4 shows a sample window that pops up when a data point is selected in the cluster\rPage 32 of 122\r\nplot. From here the user can see information such as the document title and weighted features that are active in the document, and provide relevance feedback to the classifier prior to any additional training iterations.\rFigure 4. Sample document popup window when a data point is selected in the cluster plot.\r5. Acknowledgments\rWe are indebted to Reinder Verlinde who provided an extremely useful wrapper to libsvm and essentially kick-started this whole prototype.\rREFERENCES\r[1] Jablonski, S. 2008. Jablonski’s Dictionary of Medical Acronyms and Abbreviations. 6th ed. Elsevier Health Sciences.\r[2] C.-W Hsu, C.-C. Chang, and C.-J. Lin. (2003). A practical guide to support vector classification. Technical report, Taipei. http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\r[3] Thorsten Joachims (2002). Learning to Classify Text using Support Vector Machines. Vol. 668 of Kluwer International Series in Engineering and Computer Science. Kluwer.\r[4] Calvin Johnson, William Lau, Archna Bhandari, and Timothy Hays. (2008). A Best-Fit Model for Concept Vectors in Biomedical Research Grants. AMIA 2008 Symposium Proceedings: 93.\r[5] RARE DISEASES ACT OF 2002. 107th Congress.\rhttp://frwebgate.access.gpo.gov/cgi- bin/getdoc.cgi?dbname=107_cong_public_laws&docid=f:pu bl280.107\rPage 33 of 122\r\nA Graphic User Interface for Content and Structure Queries in XML Retrieval\rLuis M. de Campos, J.M. Fernández-Luna, Juan F. Huete, Carlos J. Martín-Dancausa Departamento de Ciencias de la Computación e Inteligencia Artificial\rE.T.S.I. Informática y de Telecomunicación. CITIC–UGR\rUniversidad de Granada, 18071 Granada, Spain (lci,jmfluna,jhg,cmdanca)@decsai.ugr.es\rABSTRACT\rStructured Information Retrieval works with documents in- ternally organised around a well defined structure, typically XML documents. In this research field, documents are not retrieved as a whole, but only those most specific relevant parts of the documents are delivered to the users. Lots of models have been developed to deal with the new dimen- sion of the internal organization. From the point of view of the users, the document structure should be an added value in order to retrieve relevant material, because they are able to specify structural hints, in the form of the types of ele- ments to be retrieved and as restrictions over some elements. There are several ways to query a system specifying content and structure queries (natural and artificial languages), but few of them rely on graphic user interfaces, supporting the users to create queries that fulfil more accurately their in- formation needs. In this paper, we present a graphic user interface with the aim of formulating these types of queries, where the users only have to state what they wish to retrieve and structural restrictions about it.\rCategories and Subject Descriptors\rH.3 [Information Storage and Retrieval]; H.5.2 [User Interfaces]\rGeneral Terms\rHuman Factors, Algorithms\rKeywords\rXML, NEXI, Content and Structure queries, Graphic User Interface, Structured Information Retrieval\r1. INTRODUCTION\rThe increasing importance of the internal structure of doc- uments has caused the evolution of the Information Retrieval (IR) field to a new area where this important feature of the documents has been taken into account for retrieval pur- poses. The Structured IR field [1] deals with models and\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR ’09 Washington, DC USA\rCopyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.\rtools to index, retrieve and present structured documents, i.e. those which are internally organised around a well- defined structure. This means that classic IR models and techniques, which treats documents as if they where atomic, have been extended and adapted to exploit a more elabo- rate content representation, but also new ones have been specifically designed to tackle with this new challenge. By using SGML or XML mark-up metalanguages, structured documents can be easily represented and described.\rThen the atomization of the flat documents is no longer considered, so the new view of such documents is an ag- gregation of interrelated units which need to be indexed, retrieved, and presented both as a whole and separately, in relation to the user’s needs. In other words, an IR system must retrieve a set of document components (units) that are most relevant to this query, not just entire documents.\rThe inclusion of the structure of a document in the index- ing and retrieval process affects the design and implementa- tion of the IR system in many ways. First of all, the index- ing process must consider the structure in an appropriate way so that users can search the collection both by content and structure. Secondly, the retrieval process should use both structure and content when estimating the relevance of documents. Finally, the interface and the whole interac- tion must enable the user to make full use of the document structure. The INEX (INitiative for the Evaluation of XML Retrieval) proceedings is an excellent source of information1.\rFocussing on the query formulation stage, there are several approaches for expressing an information need to a struc- tured IR system: 1. The classical IR approach of providing a set of keywords, known as a content-only (CO) query in INEX terminology. 2. The use of a query language specif- ically designed for querying structured documents. 3. The use of a graphic user interface for formulating the query.\rThe main advantage of having a structured collection is that the structure could be exploited for retrieval purposes, as the user can specify in the query what she/he is looking for, and where this should be located in the required docu- ments. The “what” involves the specification of the content, while the “where” is related to the structure of the docu- ments. This kind of query is known as content and structure (CAS) query following the INEX convention. The output of the system would be a list of relevant XML elements (or ele- ments of the required type) sorted by their relevance degree.\rThen, with the first approach presented above, and from a point of view of a user interacting with a structured IR sys-\r1See http://inex.is.informatik.uni-duisburg.de/ and http://www.inex.otago.ac.nz/ for INEX information.\rPage 34 of 122\r\ntem, the structural restrictions in natural language are very difficult to be captured by the system, so the user is not getting the most of the document structure. In the second approach, a well defined query language allows the creation of CAS queries where structural restrictions are stated. The NEXI language [7] is the main representative of such lan- guages, becoming a de facto standard in the textual facet of XML treatment. In this case, the user is able to declare what type of XML elements have to be returned by the system. The main problems of such languages are two: first, the user has to learn a relatively complex language, and later use it properly, and second, the structure of the collection has to be very well known by the user to take advantage of both the power of the query language and the collection itself. Fi- nally, in the third option, by means of graphic components in a form, the system supports the formulation of a query. The two advantages are that a high knowledge of the struc- ture of the collection is not required and a CAS query could be easily formulated. Also, the possibility of making a mis- take in the formulation of the query is almost null because the process is controlled by the user interface. For expert users, the 2nd option perhaps is the most appropriate, but for common users, the 3rd option is the most suitable.\rSome studies like [6] argue that the addition of users’ structural requirements is not useful at all in XML-IR. One of the reasons is that the users are not able to formulate use- ful structured queries. Then more efforts have to be done in order to overcome this problem. In this line, this paper presents a user interface for formulating CAS queries in or- der to retrieve relevant elements from an XML collection. It is based on a Web form where the user is able to express the content query with its corresponding structural restric- tions very easily and without a deep knowledge of the collec- tion. This ’visual’ query is translated to its corresponding NEXI query and passed to the retrieval system. This GUI has been implemented (and is being used) in Seda, an op- erational structured IR system for the official (structured) documents of the Andalusian Parliament2.\rAfter this introduction, in Section 2, we describe some related works and give basic notions of NEXI in order to understand the rest of the paper. Section 3 presents our pro- posal of user interface for CAS queries. Section 4 presents the algorithm designed to translate the visual query to the corresponding NEXI query. Finally, Section 5 contains our concluding remarks and various proposals for future research.\rcan specify how the queries are combined. A third area is in charge of showing the XIRQL query.\rIn the same line as XIRQL, and in order to allow queries combining content and structure (CAS queries) to be spec- ified, the NEXI language [7] was designed and considered with the time to XML-IR what SQL is to Databases. It is a simplified XPath containing only the descendant operator (//) in a tag path and also an extended XPath containing the about function. NEXI has been used by INEX since 2004, and it is usually the formal statement of a query, that will be passed to a structured IR system, to process it.\rThe kind of structured CAS query considered by NEXI can take two possible forms: a) //C[D], which returns C units about D, and b) //A[B]//C[D], which returns C de- scendants of A where A is about B and C is about D. A and C are paths (sequences of XML elements or structural units), specifying structural restrictions, whereas B and D are filters, which specify content restrictions, and // is the descendant operator. Each content restriction will include one or several about clauses, connected by either and or or operators; each about clause contains a text (a sequence of words or terms) together with a relative path, from the el- ement which is the container of the clause to the element contained in it where this text should be located. C is the target path (the last element in C is the one that we want to retrieve) and path A is the context.\rAn example of a NEXI-structured query is the following:\r//A[about(., text2) and about(.//F, text3) and about(.//J, text4)] //D[about(., text1) and about(.//N, text5)]\rWhat we want to retrieve with this query are D elements which are contained within A elements. The target D ele- ments should speak about text1 and contain an N element speaking about text5; the context A elements should be about text2 and also contain F and J elements dealing with text3 and text4, respectively\rNEXI is also able to represent CO queries with the wild- card ’*’: // ∗ [about(., text1]. This expression means to re- trieve any element relevant to text1.\rAlthough NEXI is a relatively easy-to-use language, the formulation of queries with it usually requires a kind of ex- pertise by the user, reason by which several techniques have been designed to avoid the direct use of NEXI, although the final query provided to the retrieval system is a NEXI query.\rA first example is NPLX [8], which accepts natural lan- guage queries in a simple text field and generates NEXI queries. By an exhaustive analysis of the query by means of Natural Language Techniques (NPL), NPLX tries to find references to the document structure in the sentence and build an appropriate NEXI query.\rA second example is Bricks [10], a query-by-template in- terface, that allows the user to input structured queries by means of text fields, for content needs, and list boxes, for structural needs. Later, when the query is formulated through the GUI, it is translated into NEXI and provided to the search engine. Bricks allows the user to formulate queries in several steps: first, the desired retrieval element, and later, additional information needs and restrictions.\rA comparison of both alternatives in term of usability and effectiveness is presented in [9].\rA totally different approach is the prototype XmlBrowser [5], where the users formulate queries by drawing a XML tree, where the nodes are the XML elements in the collection\r2.\rBACKGROUND AND RELATED WORKS\rAlthough INEX has been a great success since its begin- ning in 2002 as evaluation forum and lots of new structured IR systems and models have been developed, always with a document-centric view of XML, and tested with the differ- ent collections adopted as official, few of them, even outside of this workshop, have focussed on the design of a user in- terface to facilitate the formulation of CAS queries.\rAmong them, and worried about the usability of the IR systems in terms of XML-related tasks, we could cite several works. Fuhr et al [4] build a user interface for XIRQL, a query language derived from XPath. They try to avoid to the user the learning of the syntax of XIRQL, and at the same time, to hide the structure of the collections. The main idea of this interface is having a specific area to formulate single query conditions, and a second one where the user\r2Andalusia is the southern region of Spain.\rPage 35 of 122\r\nand the arcs are the structural relationships between the nodes. Constraints in nodes and arcs can be established, which basically are textual contents.\rTarget\rText: XML retrieval Context\rText: user interface Text: Campos\rElement:\radd restriction\rabstract\r3.\rDESCRIPTION OF THE USER INTER-\rFACE FOR CAS QUERIES\rIn this section, the user interface for supporting the for- mulation of CAS queries is presented. The final output of the query formulation process, and totally transparent to the user, will be a CAS query formulated in the NEXI lan- guage, which will be the input to any structured IR system able to process NEXI queries.\rIn order to reduce the complexity both in the query formu- lation by the user and in the NEXI generation process, the CAS queries that can be formulated in this user interface, present the following structure:\r//A[about(.//B1, text1) and . . . and about(.//Bn, textn)] //C[about(., text0) and about(.//Cm, textm) and . . . and about(.//Cz, textz)],\ri.e. only one type of retrievable element, C, with its associ- ated textual query (and 0 or more abouts clauses – Cn,. . . ,Cz), plus 0 or more context clauses (abouts) (B1,. . . , Bn).\rTherefore, and considering the components of this NEXI query, we have to design an appropriate visual method to express a target element (the document element in which the user is interested for retrieval purposes) and its associated text query, as well as the context element(s) (the document element(s) that establishes a restriction over the target ele- ments) and its (their) corresponding text query(ies).\rFor this purpose, the form is composed of two groups of graphic components: those used to input the information re- lated to the target, and those for the context. More specif- ically, in each group, the user will find a list box, where she/he could select a unit from, plus its associated text field, where the query terms are introduced in it. In both cases, for the target and context, the list boxes will contain compre- hensible labels of the XML tags, instead of their names in the documents themselves, so they are totally transparent to the users. Specifically, for the target list box, only those retriev- able tags are shown, while for the context, only those tags where restrictions can be established, are included. Leaving the text field blank and no element selected from the list box from the context group, the NEXI query will be only composed of the target part (//C[about(., text0)).\rIn Figure 1, we may see a design of the interface, accord- ing to the requirements given in the previous paragraphs. The two differentiated parts, the target and context groups, are represented. In the former, the text field and the list box are used to select the type of retrievable element and the textual query. In the case of the example of the figure, the user is pointing out that she/he is interested in abstracts dealing with “XML retrieval”, in the context of a collection of scientific articles. In the latter, following the same phi- losophy, the user is able to input the context of the search, i.e. restrictions imposed by the selection of other types of elements and the formulation of the associated query. In the example, the restrictions for those abstracts are that prefer- ably contained in articles with titles about “user interface” and the author “Campos’. The lower part of the figure (not included in the interface) shows the NEXI query generated with the visual query. The user could include the number of restrictions that she/he consider (using the buttom with\rtitle Element: author\rElement:\rabstract article\rauthor\rbibliography section\r//article[about(.//title, user interface) and about(.//author, campos)] //abstract[about(.,XML retrieval)]\rFigure 1: The user interface for CAS queries.\rFigure 2: User interface for CAS queries in Seda.\rthe text “add restrictions“) and, once formulated, she/he could remove any of them (clicking in the corresponding black cross on the right hand side). If the user selects the special label in the list box of the target group named ’any’, she/he is asking the structured IR system to return any type of relevant element.\rAs mentioned before, this design has been implemented in Seda, an operational structured IR system to retrieve of- ficial publications of the Andalusian Parliament [2], marked up in XML. The search engine underlying this system is Garnata, implementing a retrieval model for structured doc- uments based on Bayesian networks and Influence Diagrams [3]. Figure 2 shows a screen shot of the visual query formu- lation interface (in Spanish)3 with other additional features.\rOnce the basic components of the interface have been pre- sented, it is the moment to stablish the differences with re- spect to Bricks [10], the most similar approach found in the specialized literature. The main one is that the user in this interface must select an element, from a list box, which will be the first in the path in the NEXI query, i.e. the root element after // (’In’ in their terminology). From that ele- ment, she/he must to select the retrievable element in which she/he is interested and its associated text query (’find’ and\r3The user is requesting a complete speech dealing with“pro- fessional training“, where the speaker is the President of the Andalusian government, and integrated in a debate of a po- litical initiative related to the “education law“.\rPage 36 of 122\r\n’about’ in their notation), and some restrictions, again se- lecting one or more pairs of element and associated query text (’with’ and ’about’ following their terminology). We think that if a user interface of this class has to assume an almost total lack of knowledge of the structure of the docu- ments in the collection, to leave the decision of selecting the root element of the query to the user is not a good option. In our case, this decision is totally transparent to the user, only providing the target and restrictions, which is very intuitive.\rA second difference, consequence of this design, is that the construction of the NEXI query in Bricks is direct, as they have the first element of the NEXI query (the ’A’ element of the example query of Section 2), in contrast to our approach, because with the information provided by the user, that first element has to be determined. In the following section, the method designed to generate the NEXI query is presented.\rIt is composed of two main graphic component groups, one for specifying the target of the CAS query and other for in- dicating the context or restrictions. In both cases, the user select from a list of descriptive labels the XML elements in which she/he is interested and input the associated text queries. With these data, a NEXI query is constructed by mean of a simple procedure, and passed to the search engine in charge of the retrieval of the relevant elements. We think the presented interface is very intuitive and easy to use, fa- cilitating the always complex process of giving expression to the user’s information need.\rWith respect to the further research, as this interface is working on an operational system, and we know, from the users’ feedback, that is easy and powerful, in spite of this, we are designing a usability study in order to know more formally the users’ thoughts about it, as well as objective measures. The objective then is to improve it to overcome the possible problems. In addition, as we are designing XML relevance feedback techniques for the underlying search en- gine, Garnata, we are planning to re-design the user interface in order to incorporate this new feature.\rAcknowledgements\rWork jointly supported by the Spanish Ministerio de Cien- cia e Innovacio ́n (TIN2008-06566.C04.01) & research pro- gramme Consolider Ingenio 2010: MIPRCV (CSD2007-00018).\r6. REFERENCES\r4.\rFROM VISUAL QUERY TO NEXI QUERY\rIn order to convert a visual query to a NEXI query, the fol- lowing data, extracted from the user interface, are required as input of the process: target element (the desired type of elements to be retrieved), and the text query target text for that target element, plus a set of pairs (context element1, context text1),...,(context elementN,context textN), contex- tualizing the target element, and finally the collection Doc- ument Type Definition (DTD). The output is a NEXI query with the following pattern:\r//pivot element[context about list] //target element[target about list].\rThen, the translation process will have to find the different components of this NEXI query from the input data.\rOnce the XPath of all the different elements involved in the query are determined from the DTD, the first step is to find the pivot element. This is performed extracting, from the set of paths composed of target element and the N con- text element’s that contain the path of target element, the common path of all of them. The last element in this path is considered the pivot element.\rWith respect to context about list, it will be composed of N about clauses joined by the ’and’ operator. Inside each about, the element restriction is ’.’ if the paths from the target element and the context elementi are the same, or the last element in the context elementi path, otherwise. The text of the about clause will be context texti.\rFinally, target about list is composed of several about clauses connected with the ’and’ operator. The first about is related to the target element, containing a ’.’ in the element part and target text in the text part. The rest of abouts come from those context element’s whose paths contain the path of target element. Specifically, the element part of the about clause is the last element of the path of context elementi. The text part is its associated context texti.\rWhen “any” is selected from the available labels in the list\rbox, target element equals ’*’. Finally, if no context is pro- vided, then the NEXI query is //target element[about(., target text)].\rIn general, this is an efficient method that mainly works with string operations. The generation of the NEXI query is very fast, negligible by the user.\r5. CONCLUSIONS AND FURTHER WORK\rThis paper has presented a graphic user interface used to facilitate the formulation of CAS queries by the user, with- out the need of knowing any XML query language and being an expertise in the internal structure of the XML collection.\r[1] [2]\r[3]\r[4]\r[5]\r[6]\r[7] [8]\r[9]\r[10]\rChiaramella, Y. (2001). Information retrieval and structured documents. LNCS, 1980, 291–314.\rde Campos, L.M., Ferna ́ndez-Luna, J.M., Huete, J.F., Mart ́ın-Dancausa, C., Tagua-Jim ́enez, A., Tur-Vigil, M.C. (2009). An integrated system for managing the Andalusian Parliament’s digital library. Program: Elect. Lib. and Inf. Sys., 43(2), 156-174.\rde Campos, L.M., Ferna ́ndez-Luna, J.M., Huete, J.F., Romero, A.E. (2007). Influence diagrams and structured retrieval: Garnata implementing the SID and CID models at INEX’06. LNCS., 4518, 165–177. Fuhr, N., Großjohann, K., Kriewel, S. (2003). A Query Language and User Interface for XML Information Retrieval. LNCS., 2818, 59–75.\rHarrathi, R., Calabretto, S. (2007). A query graph for visual querying structured documents. In Proc. Int. Conf. Digit. Inf. Manag., 116–120.\rTrotman, A. & Lalmas, M. (2006). Why structural hints in queries do not help XML-retrieval. SIGIR, 711–712.\rTrotman, A. & Sigurbjo ̈rnsson, B. (2005). Narrowed Extended XPath I (NEXI). LNCS., 3493, 16–40. Woodley, A., Geva, S. (2004). NPLX - An XML-IR system with a natural language interface. In Proc. 9th Australasian Doc. Comp. Symp., 71–74.\rWoodley, A., Geva, S., Edwards, S.L. (2006). Comparing XML-IR Query Formation Interfaces. Australian J. of Int. Inf. Proc. Sys., 9(2), 64–71.\rvan Zwol, R., Baas, J., van Oostendorp, H., Wiering, F. (2006). Bricks: The Building Blocks to Tackle Query Formulation in Structured Document Retrieval. Lect. Notes Comput. Sc., 3936, 314–325.\rPage 37 of 122\r\nThe HCI Browser Tool for Studying Web Search Behavior\rABSTRACT\rIn this paper, we introduce the HCI Browser, a Mozilla Firefox extension designed to support studies of Web search behaviors. The HCI Browser presents tasks to the user, collects browser event data as the user searches for information, records answers that are found, and administers pre- and post-task questionnaires. The HCI Browser is a configurable tool that HCI and IR researchers can use to conduct studies and gather data about users’ Web information seeking behaviors. It is especially well suited for “batch mode” laboratory studies in which multiple participants complete a study at the same time, but work independently. The HCI Browser is open-source software and is available for download at: http://ils.unc.edu/hcibrowser\r1. INTRODUCTION\rLaboratory studies of Web information seeking (including searching, browsing, managing, and refinding information) often involve presenting tasks to users and observing their behaviors as they use a web browser to look for information requested by the task. Researchers are interested in factors such as the web pages visited, links clicked, the amount of time spent on each page, and the use of the back button. In addition, before and after each task, researchers may wish to administer short questionnaires to gather additional data about the participants’ experiences.\rResearchers have built tools to help support studies of web search behaviors and have noted the challenges involved with capturing naturalistic user behaviors for web search [4]. Below, we summarize main approaches to capturing web browsing data and describe representative data collection tools that have been developed. We then outline the needs and motivations that led us to build our new tool, the HCI Browser.\r1.1 Data Collection Approaches\rThere are many approaches to observing and collecting data about information seeking behaviors. Here, we focus on technologies to support automatically collecting data about browsing events such as page loads, link clicks, use of browser menus and buttons, etc. Four main approaches are: 1) HTTP proxies, 2) using an external program to monitor web browser events, 3) writing a custom web browser with built-in instrumentation, and 4) adding instrumentation code to an existing web browser. Due to space limitations, we only briefly summarize these approaches below. Keller, et al. [4] is a good resource for more detail.\rHTTP proxies – Proxies intercept HTTP requests and can log data about URL page requests. Proxies can also add tracking code to pages before they are sent on to the client browser. However, proxies are somewhat limited in the types of data they can collect because they do not have full access to the user\rinterface events (i.e. menus, buttons, keypresses, etc.) in the web browser itself.\rExternal monitoring programs – Many operating systems and web browsers have “hooks” that allow external programs to monitor user interface and application specific events. WebTracker [7] and WebLogger [6] are two tools that use this approach to monitor an active web browser, observing and recording events such as link clicks and the use of menus and buttons. Time-stamped events are then written to a log file. URLTracer [3] uses a similar approach to write a simple log of all the URLs visited during a browsing session. Researchers have also used “spyware” and other event monitoring tools to capture web browsing events. A strength of this approach is that typically the monitoring tool can be installed without interfering with the user’s normal browser configuration.\rCustom Web Browsers – Writing a custom web browser gives a researcher a large amount of control over what events are monitored and how they are logged. However, building a custom browser can require a large amount of programming expertise and time. Often, a custom browser is not built from scratch, but instead uses a component such as the Microsoft Web Browser Control (WBC). The Curious Browser [2] is an example of a custom-built web browser than has special instrumentation code to log user interaction events. In a previous project, we also built a custom web browser using the WBC [1].\rOne of the major downsides to developing a custom browser is that that the user interface is likely to be somewhat different than the full-featured, widely adopted browsers that are familiar to most users (i.e. different than Internet Explorer, Firefox, or Safari). Researchers may not have time to re-implement all the features of available in mainstream browsers. These differences may alter user behaviors in ways that are not well understood.\rExtensions to an existing browser – Adding instrumentation code to an existing browser is a powerful approach that combines advantages of the other methods. The idea is simple – instead of using an external monitoring program or writing a custom browser – add code to an existing browser. Most modern browsers have support for third-party plug-ins and extensions. For example, Keller et al. [4] implemented a “browser helper object (BHO)” that can be loaded every time Internet Explorer is run and can log browsing activity. The Lemur IR toolkit project recently introduced the Lemur Query Log Toolbar using this approach [5]. It is an open source browser plug-in tool that captures events such as page loads, tab switches, and searches issued to major search engines. Versions are available for both Firefox and Internet Explorer.\r1.2 Motivations\rThe tools described above are all valuable research tools, but none filled all the needs we have for a study that is investigating how\rRobert Capra\rSchool of Information and Library Science University of North Carolina at Chapel Hill\rrcapra3@unc.edu\rCopyright is held by the author/owner(s).\rHCIR 2009, October 23, 2009, Washington, DC, USA.\rPage 38 of 122\r\nusers find and refind information on the Web. Specifically, we need a tool that will: 1) integrate with an existing Web browser to provide a familiar browsing experience, 2) record a wide variety of user interactions with the web pages and the browser itself, 3) provide support for administrative aspects of conducting a laboratory study such as administering pre- and post- task questionnaires, recording the “answers” that participants found for the tasks given, and managing other details such as closing any opened browser windows before the start of the next tasks. To support these needs, we developed a Mozilla Firefox extension called the HCI Browser. The HCI Browser is open-source code and we have utilized some concepts from the open-source Lemur Query Log Toolbar project [5]. This work also builds off our previous experience building an instrumented web browser using Visual Basic and the Microsoft Web Browser Control [1].\r2. HCI BROWSER\rThe HCI Browser is designed to: present experimental tasks to users, collect and log browser event data, manage the opening and closing of windows as needed, and to administer optional pre- and post-task questionnaires.\r2.1 HCI Browser Overview\rWhen the HCI Browser is started a dialog box is shown that prompts the experimenter to enter a session number, participant number and starting task (Figure 4). The session number and participant id are used to label the data that is recorded for this session and can be any string that the experimenter wishes to use. The starting task number specifies what task to start with based on the order given in the task configuration file (configuration files are described in section 2.2). It is useful in case there is a need to re-start the program at a particular task number.\rAfter clicking \"OK\", an introduction screen is displayed (Figure 5). The intro.txt configuration file is used to specify what text to display on this screen. This screen is useful to provide instructions and general information to the participant before beginning the tasks. Also, the experimenter can enter the information on the start-up screen before the participant arrives and then leave this introduction on the screen as the first thing the participant will see when they sit down at the computer.\rWhen the participant clicks \"OK\" on the introduction screen, an optional set of pre-task questions can be presented (Figure 6). These questions can be changed using a configuration file and can be of three types: multiple choice, Likert-type/semantic differential, and open answer. If the configuration file is not present, then the pre-task question screen is skipped.\rNext, the participant is taken to the main browser window which displays the first task in the toolbar area (Figure 7). This is a standard Firefox browser and the participant can search, browse, and navigate as usual. We decided to display the task and controls in the toolbar area at the top of the window so that web pages designed to fit standard screen resolutions would not require horizontal scrolling. The tradeoff is that vertical space is taken by the toolbar display and thus pages may require more vertical scrolling. In future versions, we may implement options for displaying the tasks in the toolbar, in a sidebar, or with no task presentation area, to allow the experimenter to decide which configuration best suits their needs.\rIn Figure 7, the user has navigated to a particular web site that has information requested by the task. There are two buttons in the toolbar: \"Found an answer on this page\", and \"Done with answers for this task\". The participant can use these buttons to submit answers they find and indicate when they are done with the task. The number of answers submitted are displayed in the lower left of the toolbar, along with an indication of the maximum number of answers they may submit (these are configurable).\rWhen the participant finds an answer and clicks the \"Found an answer\" button, the toolbar changes as shown in Figure 8. The URL of the page is automatically entered into the \"URL of answer\" box, and the user can type in text of the answer in the \"Answer text\" box.\rWhen the participant wishes to submit an answer as one of their \"official\" answers for this task, they can click the \"Submit this answer\" button. If they are to submit additional answers for this task, the controls revert back to show buttons for \"Found an answer\" and \"Done with answers\". When they are done with finding answers or have found the maximum number of answers for this task (configurable), then the system will automatically close all opened tabs and windows and display the (optional) post-task questions (not shown here, but the interface is similar to the pre-task questions).\rAs with the pre-task questions, the post-task questions are configurable by the experimenter and may be left out. When the participant has completed the post-task questions (or if they are left out), the program will then advance to the next task. When the last task is reached, a message is displayed letting the participant know that they have completed all the tasks.\r2.2 HCI Browser Configuration Files\rEvery time the HCI Browser is loaded, four configuration files are read: a introduction file with text to show to users on an introduction screen, a task file (Figure 1) with the text of the tasks to present to the user, a pre-task questions file (Figure 2) with a set of questions to be asked prior to each task, and a post-task questions file (not shown) with a list of questions to be asked after each task. The pre- and post-task questions can be of three different types: 1) multiple choice, 2) Likert-type / semantic differential, and 3) free-text/open response. Note that in Figures 1 and 2, line numbers are shown for illustration, but they are not part of the actual files.\rThe task file (Figure 1) contains the text of the tasks to present to the users. Each task has two lines. The first line specifies the text of the task. The second line indicates the maximum number of answers that can be submitted for that task. In a future version of the HCI Browser, we plan to implement a minimum number of answers that will be specified on this line also.\rThe pretask.txt file is used to configure the pre-task questions. Three question types are supported:\r• MultipleChoice – displays the question text followed by a vertical list of the choices\r• LikertType – displays the question text followed by a horizontal list of the choices (note: this can be used for Likert-type and semantic differential scales)\r• OpenAnswer – displays the question followed by a free- response text box\rPage 39 of 122\r\nthe question (line 18).\r1248198386334  21-7-2009\r1248198386398  21-7-2009\r1248198388384  21-7-2009\r1248198389498  21-7-2009\r1248198389580  21-7-2009\r1248198390167  21-7-2009\r1248198401531  21-7-2009\r1248198411929  21-7-2009\r1248198413706  21-7-2009\r1248198413712  21-7-2009\r1248198413718  21-7-2009\r1248198413724  21-7-2009\r1248198415971  21-7-2009\r1248198418755  21-7-2009\r1248198419965  21-7-2009\r1248198420088  21-7-2009\r1248198422564  21-7-2009\r1248198433480  21-7-2009\r1248198433488  21-7-2009\r13:46:26 S4 P27\r13:46:26 S4 P27\r13:46:28 S4 P27\r13:46:29 S4 P27\r13:46:29 S4 P27\r13:46:30 S4 P27\r13:46:41 S4 P27\r13:46:51 S4 P27\r13:46:53 S4 P27\r13:46:53 S4 P27\r13:46:53 S4 P27\r13:46:53 S4 P27\r13:46:55 S4 P27\r13:46:58 S4 P27\r13:46:59 S4 P27\r13:47:00 S4 P27\r13:47:02 S4 P27\r13:47:13 S4 P27\r13:47:13 S4 P27\rhttp://www.google.com/firefox?client=firefox-a&rls=or http://www.google.com/firefox?client=firefox-a&rls=or http://news.google.com/nwshp?client=firefox-a&rls=org http://news.google.com/nwshp?client=firefox-a&rls=org http://news.google.com/nwshp?client=firefox-a&rls=org http://news.google.com/nwshp?client=firefox-a&rls=org http://news.google.com/nwshp?client=firefox-a&rls=org http://news.google.com/news/url?sa=t&ct2=us%2F1_0_s_0 about:blank http://news.google.com/nwshp?client=firef Currently open windows and tabs are logged below, but Window=0, tab=0, url=http://news.google.com/nwshp?cli Window=0, tab=1, url=about:blank http://news.google.c http://news.google.com/nwshp?client=firefox-a&rls=org http://www.latimes.com/news/nationworld/nation/la-fg- http://www.latimes.com/news/nationworld/nation/la-fg- http://www.latimes.com/news/nationworld/nation/la-fg- http://www.latimes.com/news/nationworld/nation/la-fg-\r2.3 Data Logging\rIn addition to collecting data from the questionnaires, while the performing the tasks (i.e. searching for the information), the HCI Browser monitors and logs of a wide array of browser events. The current version logs: pages loaded, links clicked, window and tab focus changes, open/close of windows and tabs, back/forward button clicks, URLs typed in the address bar, scrolling, history/bookmark menu activity. A new log file is automatically created for each task, and log entries include a timestamp, session number, participant number, and task number. An example section of a log file is shown in Figure 3.\rThe HCI Browser is available as open-source code. For information, downloads, and updates visit: http://ils.unc.edu/hcibrowser\r01 How tall is the U.S. capital building in Washington, DC?\r02 1\r03 What is being done to help reduce childhood obesity in the U.S.?\r04 1\r05 What are some reported causes of global warming?\rFigure 1. Example Task Configuration File\r01 MultipleChoice\r02 3\r03 What is your favorite flavor of ice cream? 04 Vanilla\r05 Chocolate\r06 Strawberry\r07 ---\r08 LikertType\r09 5\r10 Ice cream is one of my favorite foods.\r11 Strongly agree\r12 Agree\r13 Neutral\r14 Disagree\r15 Strongly disagree\r16 ---\r17 OpenAnswer\r18 What toppings do you like on your ice cream?\r19 ---\rFigure 2. Example Pre-Task Questions Configuration File\r3.\r[1] [2] [3]\r[4] To understand the format of the configuration file, we will step\rthrough the example in Figure 2. The first line of a question\rspecifies the question type (lines 01, 08, and 17). For the MultipleChoice and LikertType questions, the next line specifies\rthe number of answer choices. For example, lines 01 and 02\rspecify that this will be a multiple choice question with 3 answer\rchoices. The next line (e.g. line 03) is the text of the question that\rwill be displayed to the participant. Lines 04-06 specify the 3\ranswer choices for this question, one choice per line. The number\rof answer choices must correspond to the number specified on\rline 02. Finally, line 07 has exactly three dashes, and acts as a separator between questions. The LikertType question follows a\rformat that is identical to the MultipleChoice described above.\rThe OpenAnswer question only has two lines: one to specify the\rquestion type (e.g. line 17) and the next line to specify the text of\rREFERENCES\rCapra, R. (2008). Studying Elapsed Time and Task Factors in Re-Finding Electronic Information. Workshop on Personal Information Management at CHI 2008.\rClaypool, M., Le, P., Wased, M., and Brown, D. 2001. Implicit Interest Indicators. In Proceedings of the 6th International Conference on Intelligent User Interfaces.\rShaun Kaasten and Saul Greenberg. URL Tracer: URL Tracing Software for Internet Explorer. Retreived on August 24, 2009 from: http://grouplab.cpsc.ucalgary.ca/cookbook/ index.php/Utilities/URLTracer\rKeller, M., Hawkey, K., Inkpen, K., and Watters, C. (2008). Challenges of Capturing Natural Web-Based User Behaviors. International Journal of Human-Computer Interaction, 24(4), 385-409.\rLemur Query Log Toolbar. http://www.lemurproject.org/querylogtoolbar\rReeder, R. W., Pirolli, P., and Card, S. K. (2001). WebEyeMapper and WebLogger: Tools for Analyzing Eye Tracking Data Collected in Web-Use Studies. In CHI '01 Extended Abstracts.\rTurnbull, D. (1998). Webtacker: A Tool for Understanding Web Use. Retreived on May 18, 2009 from: http://www.ischool.utexas.edu/~donturn/research/ webtracker/index.html\r[5] [6]\r[7]\rLoadCap\rFocus\rLClick\rScroll\rFocus\rLoadCap\rScroll\rRClick\rAddTab\rInfo\rInfo\rInfo\rScroll\rLoadCap\rSelTab\rFocus\rScroll\rsubmittedAnswerURL http://www.latimes.com/news/nationworld/n submittedAnswerText The rare total eclipse will be visible th\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rT1 intask\rFigure 3. Example Log File\rPage 40 of 122\r\nFigure 4. Start-up Screen\rFigure 5. Introductory Text Screen Figure 6. Pre-Task Questions\rFigure 7. Task Presentation in Toolbar\rFigure 8. Answer Submission\rPage 41 of 122\r\nImproving Search-Driven Development with Collaborative Information Retrieval Techniques\rJuan M. Fernández-Luna Departamento de Ciencias de la Computación e Inteligencia Artificial. Universidad de Granada,18071. Granada, Spain jmfluna@decsai.ugr.es\rRamiro Pérez-Vázquez Departamento de Ciencias de la Computación. Universidad Central de Las Villas, 50300. Las Villas, Cuba rperez@uclv.edu.cu\rABSTRACT\rSoftware developers frequently spend time searching for in- formation, generally source-code. In the last few years this habit has increased the community’s interest to improve it and some are staring to refer to as Search-Driven Develop- ment (SDD). In this work we examine the SDD as a collab- orative and commonplace task. However, current integrated development environments (IDEs) do not include informa- tion retrieval systems with support for explicit collaboration among developers with shared technical information need. We then introduce PosseSrc, a prototype outside the IDEs that enables teams of remote developers to collaborate in real time during the search sessions. PosseSrc improve the SDD by supporting several modern state-of-the-art collab- orative information retrieval (CIR) techniques such as ses- sion persistence, division of labor, sharing of knowledge and group awareness.\rCategories and Subject Descriptors\rH.5.3 [Information Interfaces and presentation (e.g., HCI)]: Group and Organization Interfaces; H.3.3 [Information Storage and Retrieval]: Search Process.\rGeneral Terms\rDesign, Human Factors\rKeywords\rSearch-Driven Development, Collaborative Information Re- trieval, Collaborative Search, Source-Code Search, Multi- User Search Interface.\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR ’09 Washington DC, USA Copyright 2009 ACM ...$10.00.\rJuan F. Huete Departamento de Ciencias de la Computación e Inteligencia Artificial. Universidad de Granada,18071. Granada, Spain jhg@decsai.ugr.es\rJulio C. Rodríguez-Cano Departamento de Informática. Universidad de Holguín, 80100. Holguín, Cuba jcrodriguez@facinf.uho.edu.cu\r1. INTRODUCTION\r“Good programmers know what to write. Great ones know what to rewrite (and reuse)”. – E. S. Reymond[10]\rYou can rewrite or reuse good source-code, but first you must find it. That is the fundamental key by which some in- formation retrieval (IR) systems have become a critical tools for software developers. Currently there are some special- ized IR systems for source-code search. Examples include Google Code Search, Krugle, CodeFetch, Koders, and Co- dase. Some of then such as Koders can be integrated with IDEs such as Eclipse and Visual Studio.NET.\rMore recently there has been some significant efforts both from academia and the industry to fix SDD as a new research area motivated by the observation that software developers spend most of their times in searching pertinent information that they need to solve their task at hand [1]. K. Krugler and J. D. Mitchell remark in [6] that “about 25% of a de- veloper’s time is spent searching for information. It’s well spent, though – finding reusable code can get a project done on time and with high quality results”.\rIn addition, software development can be considerated as a collaborative activity in which business analysts, cus- tomers, system engineers, architects, and developers inter- act among them. The concurrent edition of models and processes requires synchronous collaboration between archi- tects and developers who cannot be physically present at a common location [5].\rHowever, current IR systems do not have support for ex- plicit collaboration among developers with shared technical information needs, which frequently look for additional doc- umentation on the API, read newsgroups for people having the same problem, search the company’s site for help with the API, and search for source code examples where other people successfully used the API [6]. Fortunately, in the last few years, some researchers have realized that collaboration is an important feature, which should be analyzed in de- tail in order to be integrated with professional IR systems, upgrading them to collaborative information retrieval(CIR) systems.\rPage 42 of 122\r\nFigure 1: PosseSrc prototype. Search: search control panel; options: main options buttons; recommend: recommender; search result: individual search result and recommendations; instant messaging: chat tool embedded; information: general item information; previewer: item selected viewer\rCIR is an emerging research field that belongs to a special- ized area within the IR discipline. Therefore CIR includes the research areas that traditionally have been part of IR, but with an especial emphasis on the explicit collaboration among people with shared information needs. This fact re- quires that IR mixture with other disciplines such as Human- Computer Interaction (HCI) and Computer-Supported Co- operative Work (CSCW).\rIn this paper we introduce PosseSrc, a prototype outside the IDEs that enables remote team developers to collaborate in real time during the source-code search and any other related technical information. The design of PosseSrc was motivate by a brief survey that we applied to 50 students and professors related to software development projects in higher education in the domain of Information Technologies. Our survey results indicate that collaborative source-code search is a commonplace task. When we asked: Have you ever collaborated with other programmers to search source-code?, 78.0% responded yes. In addition, we asked: Which are the activities that have motivated you to collaborate during the source-code search?, the most common answers were: a) meetings of the team members to clarify programming doubts while someone searches for source-code examples, b) dividing the search by each team member and sharing the final results, c) saving and documenting the search results of each one for sharing them, and d) consulting or answering doubts via chat or email.\rBased on survey respondents’ descriptions, we identified four modern state-of-the-art CIR techniques for supporting\rcollaborative source-code search. Session Persistence: stor- ing a search session in a persistent format is a key require- ment to facilitate collaboration during the session, revising the search at a later time, or sharing the results of a search with others [7]. Division of labor: Morris’s survey in [7] de- scribes ad-hoc methods to avoid duplication of effort during a searching task, such as dividing up the space of potential keywords, searching engines, or sub-tasks among different group members. Supporting mechanisms for dividing up and sharing work among participants is important for the success of a UI for multi-user search. Sharing of knowledge: in any collaborative setting there will be a large and diverse knowledge base shared among groups of members. Each one will bring their own experiences, expertise and topic knowl- edge to a particular searching task. What is needed is a way to enable the sharing of knowledge within the group [4]. Finally Group awareness: awareness is an essential el- ement in distributed collaborative environments. Over the last decade, a number of researchers have explored the role of group awareness for supporting collaboration between dis- tributed groups. Specifically in CIR, awareness is another key requirement [7].\rWe have organized this paper as follows. In the next sec- tion (2), we shall describe the main ideas of the design con- siderations of PosseSrc, its implementation and an overview of some related work with both CIR and SDD research areas. Finally, we conclude this paper in Section 3 by summarizing the exposed topics and our research future direction.\rPage 43 of 122\r\nFigure 2: Collaborative portal\r2. POSSESRC\rPosseSrc is designed to enable either synchronous or asyn- chronous, but explicit remote collaboration among team de- velopers with shared technical information. Figure 1 shows the interface of the prototype client interface. The search- box wraps the search control panel (SCP), it permits to spec- ify the developers queries, programming language or project on which the search will be accomplished. Moreover it can specify a searching field: comments, source-code, class or methods declaration, and whole source files. Rather inter- estingly, source-code search all by itself doesn’t solve the whole problem. We need all of the technical information around and about the source-code to be able to really fly. For instance, the best examples of how to use some piece of source-code is often embedded as a small source-code snip- pet inside a magazine article or blog entry, or occasionally, even in the official technical documentation [6]. For these reasons in the SCP you can select documents or the Web as collections too.\rThe SCP also offers the possibility to specify the divi- sion of labor principle. It determines which principle to use to divide the search results among team developers: a)Meta-search engines and split: the results of each search engine available are merged in one results list, which is au- tomatic divided among developers; b) Multi-search engines and switch: the results of each search engine available are switched among developers, where one developer can re- view only the results of an specific search engine; and c) Single-search engine and split: the results of the selected search engine are automatic divided among team developers. The options-box wraps the principal options of the PosseSrc that permits dynamic management of the GUI. For exam- ple, a developer can show the chat tool embedded (instant messaging-box), a collaborative portal where the developers can negotiate the creation of a collaborative search session (CSS) (Figure 2), a recommendations panel (recommend- box) to carry out explicit recommendations among develop- ers, add and show comments of the current and historical search results, and the previewer panel (previewer-box) to review the results. In the search result-box the individual results and the recommendations made by others end-users\rFigure 3: Creating a collaborative search session.\rare shown. The green-box permits getting specific informa- tion from the document selected in the results panel.\rFrom the collaborative portal one developer can create a CSS, which consists of a team of developers working to- gether to satisfy their shared technical information needs. For each CSS is necessary to establish (Figure 3). First, the main topic refereed to the shared technical information need of the developers. Second, the maximum number of devel- opers allowed in the CSS. Third, the integrity criteria. The validity condition for minimum and maximum number of de- velopers in a CSS. It can be: a) hard – the CSS is released if the integrity criteria are not satisfied; and b) soft – the CSS is suspended if the integrity criteria are not satisfied. And fourth, Membership Policy. It establishes how a potential member joins and leaves a session. All potential members can negotiate an invitation to join a session throughout the collaborative portal in different ways: a) static – a poten- tial member must join to a CSS by previous negotiation and before the work has been started; b) dynamic and closed – each potential member must by explicitly invited to join the CSS; and c) dynamic and open – potential members can join a CSS on invitation or by own initiative at any time.\r2.1 Implementation\rFor the implementation of PosseSrc we use CIRLab (Col- laborative Information Retrieval Laboratory), a groupware framework for CIR research and experimentation [3], Java as programming language and AMENITIES (A MEthodology for aNalysis and desIgn of cooperaTIve systEmS) as soft- ware engineering methodology. CIRLab has been designed applying design patterns and an object-oriented middleware platform to maximize its reusability and adaptability in new contexts with a minimum of programming efforts. The dis- tribution and communication facilities of CIRLab are ICE1 (Internet Communications Engine) conforming. ICE appli- cations are suitable for using them in heterogeneous envi- ronments: client and server can be written in different pro- gramming languages, run on different operating systems and hardware architectures, and communicate using a variety of\r1 http://www.zeroc.com\rPage 44 of 122\r\nnetworking technologies. CIRLab also wraps some open- source three party APIs (e.g. search engines and a database engine). To do searches in different parts of the source- code (e.g. comments, class and function definitions) we ex- tend CIRlab with parsers that allow indexing fields (parts of the source-code) when combined with search engines (e.g. Apache Lucene).\r2.2 Related Work\rPosseSrc include several areas of research, highlights of which CIR and SDD. On the one hand, some researchers have identified different search scenarios where is necessary to extend the IR systems with collaborative capabilities. For example, in the Web context, SearchTogether [8] is a sys- tem which enables remote users to synchronously or asyn- chronously collaborate when searching the Web. It supports collaboration with several mechanisms of group awareness, division of labor, and persistence. On the other hand, the SDD community present different prototypes and systems. For example, Sourcerer [2] is an infrastructure for large-scale indexing and analysis of open source code. Sourcerer crawls Internet looking for Java source-code from a variety of loca- tions, such as open source repositories, public web sites, and version control systems.\rIn contrast to these approaches, PosseSrc makes a contri- bution in current SDD providing explicit support for teams of developers, enabling developers to collaborate on both the process and results of a search. It provides collabora- tive search functions for exploring and managing source-code repositories and documents about technical information in software development context. In order to support such CIR techniques PosseSrc provides some collaborative ser- vices. The embedded chat tool enables direct communica- tion among different developers. Also relevant search results can be shared with the explicit recommender mechanisms. Another important feature enabling improvement is the au- tomatic division of labor. Through awareness mechanisms all developers are always informed about the team activities to avoid the unnecessary duplication of effort. Awareness is a valuable learning mechanism that help the less expe- rienced developers to view the syntax used by their team- mates, and then be inspired to reformulate their queries. All search results can be annotated, either for personal use, like a summary, or in the team context, for discussion threads and ratings.\rjudged relevant by the user that were marked relevant in the ground truth), and selected recall/viewed recall (Rs/Rv) as their dependent measures. Moreover, and taking into con- sideration our survey results, when 92.9% of our respondents use their workstation as an important dynamic collection of relevant information, we will add to PosseSrc on the base of CIRLab the capability of indexing local collections.\r4. ACKNOWLEDGEMENTS\rThis work has been jointly supported by the Spanish Min- isterio de Ciencia e Innovacio ́n (TIN2008-06566.C04.01) and Spanish research programme Consolider Ingenio 2010: MIPRCV (CSD2007-00018).\r5. REFERENCES\r[1] S. Bajracharya, A. Kuhn, and Y. Ye. Suite 2009: First international workshop on search-driven development - users, infrastructure, tools and evaluation. International Conference on Software Engineering Companion, 0:445–446, 2009.\r[2] S. Bajracharya, J. Ossher, and C. Lopes. Sourcerer: An internet-scale software repository. In SUITE ’09: Proceedings of the 2009 ICSE Workshop on Search-Driven Development-Users, Infrastructure, Tools and Evaluation, pages 1–4, Washington, DC, USA, 2009. IEEE Computer Society.\r[3] S. Cleger-Tamayo, J. M. Fernandez-Luna, J. F. Huete, R. Perez-Vazquez, and J. C. Rodriguez-Cano. A proposal for an experimental platform on collaborative information retrieval. International Symposium on Collaborative Technologies and Systems, 0:485–493, 2009.\r[4] C. Foley, A. F. Smeaton, and H. Lee. Synchronous collaborative information retrieval with relevance feedback. In CollaborateCom 2006 - 2nd International Conference on Collaborative Computing: Networking, Applications and Worksharing, pages 1–4, 2006.\r[5] M. Jim ́enez, M. Piattini, and A. Vizca ́ıno. Challenges and improvements in distributed software development: A systematic review. 2009.\r[6] K. Krugler and J. D. Mitchell. Search-driven development: Five reasons why search is your most powerful tool, 2007.\r[7] M. R. Morris. A survey of collaborative web search practices. In CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, pages 1657–1660, New York, NY, USA, 2008. ACM.\r[8] M. R. Morris and E. Horvitz. Searchtogether: an interface for collaborative web search. In UIST ’07: Proceedings of the 20th annual ACM symposium on User interface software and technology, pages 3–12, New York, NY, USA, 2007. ACM.\r[9] J. Pickens, G. Golovchinsky, C. Shah, P. Qvarfordt, and M. Back. Algorithmic mediation for collaborative exploratory search. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 315–322, New York, NY, USA, 2008. ACM.\r[10] E. S. Raymond. The Cathedral and the Bazaar. O’Reilly & Associates, Inc., Sebastopol, CA, USA, 1999.\r3.\rCONCLUSION AND FUTURE WORK\rNovel CIR techniques such as session persistence, division of labor, knowledge sharing and awareness can be applied in several domains. For example, in the Web context, in- teractive multimedia, education and medical environment. We identified SDD as another applicable field, given both the collaborative nature and the interest in having special- ized source-code search tools in the area of software devel- opment. In this sense we present PosseSrc, a prototype de- signed to enable either synchronous or asynchronous, but explicit remote collaboration among team developers with shared technical information need.\rTo conduct the PosseSrc’s evaluation in a close future we identify the metric proposed by Pickens et al. in [9] as a good intention, where they proposed viewed precision (Pv, the fraction of documents seen by the user that were rele- vant) and selected precision (Ps, the fraction of documents\rPage 45 of 122\r\nA visualization interface for interactive search refinement\rABSTRACT\rFernando Figueira Filho∗ Institute of Computing State University of Campinas (Unicamp), Brazil\rfernando@las.ic.unicamp.br\rJoão Porto de Albuquerque School of Arts, Sciences and Humanities University of Sao Paulo, Brazil joao.porto@usp.br\rAndré Resende Institute of Computing State University of Campinas (Unicamp), Brazil\rresende@las.ic.unicamp.br\rPaulo Lício de Geus Institute of Computing State University of Campinas (Unicamp), Brazil paulo@las.ic.unicamp.br\rGary M. Olson\rBren School of Information and Computer Sciences University of California, Irvine, USA gary.olson@uci.edu\rThe greatest advantage of tagging systems is that they pro- vide a means to gather the community vocabulary for further classification. Another important characteristic is that they carry different levels of specificity, ranging from very general, widely-used terms to domain-specific terms. This is espe- cially useful in the case of on-line communities within which people are trying to interact and find shared content. In these environments, people may have different backgrounds and distinct areas of expertise, which leads to different per- spectives on classification [1].\rRegarding the user interface, there are two ways to take advantage of user-generated annotations when looking for information. First, the keyword-based search, which con- sists of a text box and a search button. The problem with this approach is that it assumes that the user knows how to formulate the query. This is especially hard for people who are trying to find information in different knowledge domains. A recently published article [5] points out that it is a common practice for researchers to find, assess, and exploit a range of information by scanning portions of many articles, instead of looking for a single article to read, in what the authors call “strategic reading”. We also have seen this behavior within the open-source community. In order to solve a technical problem, sometimes one need portions of information that may be scattered throughout a series of different postings within a web forum. In these cases, find- ing the correct keywords which will lead to relevant results can be a time-consuming task. Term suggestion techniques attempt to address this issue, but still depend on an initial query in order to provide further suggestions. This issue is particularly relevant when exploring information in knowl- edge domains within which the user does not have a strong background, e.g. novice users searching for problem solu- tions in a web forum.\rSecond, many websites provide tag clouds or weighted lists, which consist of a visual depiction of user-generated annotations. In this approach, the criteria to show a given term is its use frequency, i.e. how many times users applied that term to annotate content. However, there are some problems with this type of visualization. On one hand, usu- ally only popular terms are depicted, which might not be useful to a user who is searching within one or more specific\rIt is common practice nowadays to find, assess and explore the Web by groping scattered information presented through many search results. Browsing interfaces and query sug- gestion techniques attempt to guide the user by providing term recommendations and query phrases. In this paper, we introduce the browsing interface of Kolline, a commu- nity search engine under development. Two case studies are described and two distinct web browsing interfaces are analyzed. Based on this analysis, we present a new brows- ing interface, describing our design decisions and providing directions for future work.\rCategories and Subject Descriptors\rH.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—web-based interaction, collab- orative computing, organizational design\rGeneral Terms\rDesign, Human factors\rKeywords\rWeb 2.0, user-generated annotations, browsing interfaces\r1. INTRODUCTION\rIn recent years, “Web 2.0” [3] applications have been em- ploying tagging as a way for annotating published content.\r∗Work done while at the University of California, Irvine.\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR ’09 Washington, DC USA\rCopyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.\rPage 46 of 122\r\ntopics [8]. On the other hand, a user can choose only one term at a time and very often one need a conjunction of terms to suitably express the search task. Consequently, it is impossible to refine a search by only using the tag cloud.\rWith the aim of addressing those issues, this work presents the navigation interface of Kolline, a community search en- gine currently under development. It features a term recom- mendation tool, which suggests terms based on the user’s previous interactions. The tool does not require that the user provides an initial query and the interactions can be done solely by clicking on the recommendation tool. In ad- dition, users can refine their search context by choosing new terms which are semantically-related using an underlying ontology. The tool recommends terms which hold subsump- tion relationships, so a user can refine the search by clicking on general terms at first, and then narrow down the search context by choosing more specific terms. A text box is also provided, so the user can add, remove or modify terms dur- ing the interaction.\rThe paper is organized as follows: Section 2 describes the two cases we studied before designing our application. Sec- tion 3 explores some concepts and examples of browsing in- terfaces for searching, finally introducing our solution. The paper finishes with Section 4 providing an overview of our evaluation plan for the future and drawing conclusions.\rbroad and non-specific, such as “paste the title of an arti- cle you find amusing or interesting” and (b) when the tag cloud contained a term relevant to the question. The first case obviously does not apply to answering technical ques- tions. Most people who visit a Linux distribution forum are looking for help from other community members on a spe- cific topic. However, it could be scenario (b), i.e. the user finds a term in the tag cloud which is relevant to answering a technical problem. But, since the cloud shows only the most used tags, it is not clear if this is sufficient to fulfill a specific search task. In other words, the users will proba- bly have to refine their search by adding other terms. This refinement phase is important for reducing the overall num- ber of hits and excluding irrelevant information from search results. However, most tag clouds or weighted lists do not provide this functionality.\rOur second case is a community of professors and re- searchers of the University of S ̃ao Paulo. The School of Arts, Sciences and Humanities is an interdisciplinary insti- tute where professors hold positions in a great variety of research areas. To stimulate scientific collaboration, an in- stitutional website is under development, which will contain information about each researcher, organized by area of ex- pertise. In a first phase, professors were asked to provide terms which would describe their research interests and cur- rent activities. The union of all colected terms is shown as a list, but because of the great diversity of topics, the result does not fit in one page.\rThe problems with this visualization approach are twofold. First, each professor uses a particular level of specificity to describe his/her research area. General terms such as\r“molecular biology” are separated from specific terms like “proteins”, although both research areas may have a certain level of intersection. A weighted list approach which shows the most frequent terms in order to reduce the list size is not a suitable solution, because it would not show specific terms that are relevant to the researchers. Second, profes- sors working in the same areas describe them differently, which is the synonymy problem commonly found in tagging systems [2]. For this reason, semantically-related terms end up in different positions on the list, so it is difficult to rec-\rognize inter-related subjects and research areas.\rAlthough these cases are related to different communities, the practice of browsing and scanning many pieces of infor- mation to find relevant content is a very common issue. In both cases there are difficulties related to the query formu- lation, i.e. one only recognizes a relevant result when they go through it. In the case of the web forum, relevant results are posts, while in the case of the institute website, relevant results are professors or researchers with a shared goal or interest. We need a tool to visualize user-generated annota- tions which is able (a) to differentiate general and specific terms into different levels and (b) to provide a refinement mechanism which allows a user to browse horizontally, i.e. between different topics and, at the same time, vertically, i.e.\rdoing an in-depth analysis and looking for specific terms.\r2.\rCASE STUDIES\rTo characterize the problem, we explored two cases of vi- sualization interfaces for user-generated annotations. First, the case of Ubuntu Forums1, which represents the major source of information in the Web about this Linux distri- bution and unites an open-source community of develop- ers and users interested in sharing information about trou- bleshooting, new features, and other related content. Users exchange information by adding new posts that are shown in the form of threaded discussions. Some users annotate these threads with tags, which helps to categorize content by us- ing a community-oriented, non-controlled vocabulary. Fig. 1 shows a typical tag cloud containing the most frequent terms associated with threaded discussions in the forum.\rFigure 1: Ubuntu Forums’ tag cloud\rTo better understand whether this type of visualization agregates any significant value to finding information, we re- fer to a user study which attempted to assess the usefulness of tag clouds in comparison to the traditional keyword-based search. [8] conducted an experiment, giving participants the option of using both approaches to answer various questions. They found that while some participants preferred to use the text search box exclusively, a significant proportion of par- ticipants used the tag cloud to find information. There were two scenarios in which the tag cloud’s use outweighed the search box’s use: (a) when the information-seeking task was\r1 http://ubuntuforums.org\r3.\rBROWSING INTERFACES\rRepresenting different levels of abstraction without pol- luting the interface is a challenging design task. A common way is to represent each level using indentation, e.g. the\rPage 47 of 122\r\nClusty2 search interface (Fig. 2a). The problem with this approach is that the user usually needs to scroll the page as he/she explores the structure, which requires an extra effort to keep the focus on a given abstraction level, i.e. a term and its proximate relationships. An efficient visualization technique which attempts to address this issue can be found in Google’s Wonder Wheel3 (Fig. 2b). It is a good example of a focus & context interface, which encompasses visualiza- tion techniques that allow a user to center his view on a part of the screen that is displayed in full detail (focus), while at the same time perceiving the wider screen surroundings in a less detailed manner (context). The major advantage of using these techniques is the improved space-time efficiency for the user, i.e. the information displayed per screen area unit is more useful and, consequently, the time required to find an item of interest is reduced as it is more likely to be already displayed [4].\r3.1 Kolline’s interface\rOur solution consists of an interactive tool for visualiz- ing hierarchies of user-generated annotations. Terms are related using an ontology which is, in turn, derived semi- automatically by applying a probabilistic model similar to the one presented in [6]. In the case of Ubuntu Forums, we extracted both text corpora and user-generated tags from threaded discussions. As for the institute website, we expect to gather patterns of term co-occurrence from researchers’ papers. The resulting ontology is a hierarchy, in such a way that the closer a term is to the root, the more general it is. The purpose of our interface is to allow the user to browse this hierarchy, at first selecting general terms and then refin- ing the search context progressively by adding more specific terms. Fig. 3 depicts Kolline’s interface and highlights the functionality of our query formulation tool.\rThe design of the query formulation tool is based on a colored pie and each slice represents a term in the ontology. The scheme was inspired by an electronic memory game pop- ularized in the eighties called Simon. The main goal of this game was memorizing the sequence of colors displayed by the interface, adding to the sequence one color at a time. In our design, the colors have the purpose of enhancing the user’s working memory. [9] shows that recognition mem- ory is 5%–10% better on colored images in comparison to black & white images. Thus, one important design deci- sion is based on the idea that colors may have an important role on helping the user to memorize previous steps when interacting with the interface.\rAnother important design decision is to avoid scrolling. [7] points out that this approach provides a better experi- ence, especially for novice users. In both cases shown in Fig. 2, the structure grows vertically as the user browses the interface. As a result, scrolling eventually becomes a required effort during the interaction. To address this issue, our query formulation tool stays static and within a single, limited area of the screen, showing just the two previously selected levels as inner circles, i.e. context, and new term recommendations in the outer circle, i.e. focus. The path below the quadrant shows all previously selected terms and allows the user to go directly to a certain level. This has an important role in keeping the user’s attention on the focus, without loosing the visual contact of the context.\rThe tool works as follows (Fig. 3). On selection of one of the general terms displayed by the interface, a transition changes the tool’s shape. It becomes a quadrant through a smooth transition to transmit the idea of changing the focus. Each previous level of the hierarchy, i.e. inner circle, keeps the color of the previously selected term. At each new se- lection, new semantically-related terms are recommended in the outer side of the quadrant. The user can move the mouse over the inner circles to view the context, which causes the previously selected terms to be highlighted. Each new in- teraction with the tool changes the remaining parts of the interface. The search box is automatically updated with the effective expression resulting from the user’s selection. Newer selections refine the search results which in turn gives an instant feedback, so the user can make a decision to con- tinue refining the search context or to go back and browse horizontally over the ontology. To go back, the user can click: (a) on the back arrow displayed near the center of the quadrant; (b) on an inner circle or (c) on a previously selected term in the path below the quadrant.\r(a) (b)\rFigure 2: (a) Clusty navigation menu and (b) Google Wonder Wheel.\rFig. 2b shows an example of interaction. Let us suppose that the user is interested in downloading a peer-to-peer client, so he/she starts entering the query “p2p”. A new set of query suggestions appears and gains focus. Then, after selecting the “p2p software” query suggestion, the user is pre- sented with new suggestions, among which is the query “file sharing”. Incidentally, the user may shift the search context and receive suggestions such as “file hosting” and “file up- load”. If the purpose is to browse horizontally, the tool is very appropriate, leading the user to distinct domains with some level of specificity. However, the tool excludes the ini- tial input term “p2p” from the query and eventually moves the users away from their search goal. Because the tool sug- gests related queries, it does not work as a query formulation tool. In other words, it does not necessarily keep all previ- ously selected terms, suggesting queries that may not have a semantic intersection with the previous interactions.\r2 http://clusty.com\r3At the time of writing this paper, one could reach the tool by selecting “Show options” in the Google’s main page.\rPage 48 of 122\r\nFigure 3: Kolline interface on top and a graphical representation of successive interactions on bottom.\r4. FUTURE WORK AND CONCLUSION\rAs for the evaluation, we will conduct a user study in two phases. First, we want to identify the strategy used by users when performing search tasks and observe their browsing practices. We are particularly interested in better understanding the users’ main difficulties when formulating queries and identifying relevant results. Subjects will be re- cruited to participate in individual, moderated sessions. A screen capturing software will record user activity and mod- erator will take notes. The aim of the second phase is to assess Kolline’s effectiveness in comparison with the tools regularly used by users for searching. For this purpose, a comparison test will be conducted and a group of partici- pants will be asked to perform a set of predefined tasks, in a between-subjects design.\rThis paper presented a query formulation tool which em- ploys visualization techniques for browsing. We analyzed two cases which involve user-generated annotations to clas- sify content and described two examples of browsing in- terfaces that attempt to provide assistance to the user in information-seeking tasks. Our design decisions are aimed at addressing the problems found in the case studies and at dealing with the issues identified in usual web browsing in- terfaces. Therefore, our interface differentiates general and specific terms into different levels and provides a refinement mechanism which allows a user to browse horizontally and vertically over large ontologies.\r5. REFERENCES\r[1] G. Bowker and S. Star. Sorting Things Out: Classification and Its Consequences. MIT Press, Cambridge, MA, 1999.\r[2] S. Golder and B. Huberman. Usage patterns of collaborative tagging systems. Journal of Information Science, 32(2):198–208, 2006.\r[3] T. O’Reilly. What is web 2.0 | o’reilly media. Available online: http://www.oreillynet.com/pub/a/oreilly/ tim/news/2005/09/30/what-is-web-20.html. Last access: 8/21/2009, September 2005.\r[4] J. Porto de Albuquerque, H. Isenberg, H. Krumm, and P. L. de Geus. Improving the configuration management of large network security systems. DSOM’05: Proc. of the 16th IFIP/IEEE International Workshop on Distributed Systems: Operations and Management, pages 36–47, October 2005.\r[5] A. H. Renear and C. L. Palmer. Strategic reading, ontologies, and the future of scientific publishing. Science, 325:828–832, August 2009.\r[6] M. Sanderson and B. Croft. Deriving concept hierarchies from text. SIGIR ’99: Proc. of the 22nd international ACM SIGIR conf. on research and development in information retrieval, pages 206–213, 1999.\r[7] E. Schwarz, I. Beldie, and S. Pastoor. Comparison of paging and scrolling for changing screen contents by inexperienced users. Human factors, 25(3):279–282, 1983.\r[8] J. Sinclair and M. Cardew-Hall. The folksonomy tag cloud: when is it useful? Journal of Information Science, 34(1):15, 2008.\r[9] F. Wichmann, L. Sharpe, and K. Gegenfurtner. The contributions of color to recognition memory for natural scenes. Learning, Memory, 28(3):509–520, 2002.\rPage 49 of 122\r\nCognitive Dimensions Analysis of Interfaces for Information Seeking\rGene Golovchinsky FX Palo Alto Laboratory, Inc. 3400 Hillview Ave, Bldg. 4 Palo Alto, CA 94304 USA\rgene@fxpal.com\rABSTRACT\rCognitive Dimensions is a framework for analyzing human- computer interaction. It is used for meta-analysis, that is, for talking about characteristics of systems without getting bogged down in details of a particular implementation. In this paper, I discuss some of the dimensions of this theory and how they can be applied to analyze information seeking interfaces. The goal of this analysis is to introduce a useful vocabulary that practitioners and researchers can use to describe systems, and to guide interface design toward more usable and useful systems.\rCategories and Subject Descriptors H.5.2 [User Interfaces]: Evaluation/methodology\rGeneral Terms\rHuman Factors\rKeywords\rCognitive Dimensions, Information seeking, user interfaces, evaluation\r1. INTRODUCTION\rCognitive Dimensions is a technique for analyzing complex information artifacts, including programming languages, device interfaces, and interactive software user interfaces.[3][5] It is designed to be a meta-analysis, a broad-brush approach that looks at structural aspects of the system and identifies characteristics that may impede or enable certain kinds of interactions with the system. It can be used in a summative or formative manner to evaluate existing systems and to drive design of new systems.\rIn this paper, I apply this tool to the domain of user interfaces for information seeking and exploration. This domain is characterized by complex, cognitively-rich activities. To be effective, information seeking tools need to be designed in a manner consistent with people’s cognitive abilities: interfaces that work with people’s strengths can be effective even when driven by relatively simple indexing and retrieval schemes; conversely, powerful retrieval engines can be made less usable by coupling them to awkward or ill-designed interfaces.\r2. SEARCHINTERFACES\rInformation seeking is an inherently difficult activity due to a\rIn Proceedings of the Third Workshop on Human-Computer Interaction and Information Retrieval (HCIR ’09), October 23, 2009.\r© 2009 Gene Golovchinsky\rnumber of factors: peoples’ information needs are often ill- defined, [1] they may lack the vocabulary required to express the information need, [10] and the need may evolve over time as new information is identified. [7]\rThese characteristics impose requirements on interfaces through which people look for information. To be useful, interfaces have to be simple to avoid burdening the searcher with distracting or unnecessary complexity, but not too simple to support the cognitive tasks characteristic of information seeking.\r3. COGNITIVEDIMENSIONS\rCognitive Dimensions is an analytic tool that focuses on the process of interaction, rather than on static analysis of artifacts. [4] It differs from other analytic approaches such as GOMS/KLM [6] in that Cognitive Dimensions does not require a specialized, detailed, and time-consuming analysis that is predicated on very specific interface characteristics. Instead, it allows an interface to be discussed and compared with alternatives using broad terms, represented as different dimensions. The full theory identifies a large number of these dimensions, but only some of them are useful for most analyses of interactive information seeking systems. These dimensions will be discussed below; see [3] for a detailed discussion of all dimensions.\rIt is important to note that although the dimensions reflect different aspects of interaction, they are not completely orthogonal. In practice, this means that an interface flaw may be reflected simultaneously in more than one dimension.\r3.1 Premature commitment\rThis dimension reflects the sequence of steps that a user must perform to achieve a specific outcome. If the user must make a decision early on in some interaction without necessarily having all information to understand the choice, we classify that as premature commitment. For example, being required to provide personally-identifying information prior to being able to interact with a system even in a light-weight manner is an example of premature commitment. So is forcing a user to click on a link in a search result to see some critical piece of information such as the price or an abstract.\rRequiring people to ask the system for information that might have just as easily been shown right away was shown to reduce the use of that information. [11] Applied to information retrieval, this suggests that search results should include enough metadata that might help people assess the utility of the document, and accounts for the popularity of snippets as a way of explaining search results. There are limits, of course, to how much information can be presented for each result without making it\rPage 50 of 122\r\ndifficult for the user to understand the information, but designers should consider the tasks that cause people to search, and what information about specific results would make it easier to assess relevance or utility.\r3.2 Viscosity\rViscosity assesses a design’s resistance to change. If, for example, an interface requires the searcher to go through a series of menus or dialog boxes to switch between author search and content search, we say that the design has high viscosity. It is particularly unfortunate if high viscosity is coupled with premature commitment: the user is required to make choices without fully understanding the consequences and it is the difficult to undo these actions once additional information is learned.\rAutomatic query expansion based on recent browsing history (e.g., [1]) can generate viscosity as the system learns associations between terms and people that may outlast the utility of the association for a particular individual.\rYelp!’s faceted search interface (www.yelp.com) offers another example of unnecessary viscosity: A query can be formulated by selecting relevant facets, but once an item is selected, the facet information goes away, forcing the user to backtrack to revise the query. If the original design does not support good viscosity, it may be hard to introduce it later, although there may be measurable benefits to doing it. [8] Section 4 offers a more detailed analysis of the Yelp! interface.\r3.3 Hiddendependencies\rThis dimension assesses the presence of hidden links among components of the system whose existence may be hard for users to learn. If these links impact people understanding of important system functions, the design should be rated unfavorably on this dimension. For example, “personalized search” learns from users’ [9] or groups’ [1] interactions with search results about their preferences, and uses that personalization information to affect search rankings. While this approach may improve precision, it may become progressively more difficult to understand why a particular document was or was not retrieved in response to a given query.\rThis dimension is also related to viscosity. In this case, however, the interface may not reflect to the user that a prior action is now affecting system responses, and it may not be obvious how (or even possible) to undo the effects of hidden dependencies.\r3.4 Visibility\rVisibility reflects how easy it is to view the various aspects of the system. It is related to the notion of affordance. A deep menu system may exhibit poor visibility. For example, Google’s Trends search may generate output automatically in response to certain queries (cf. consistency, below) but if the searcher knows that they want to perform a search on structured data, they need to either have to remember the name and search for it, or they need to navigate a deep menu hierarchy (more/even more/labs/Google Trends) to discover the right place to search.\rPoor visibility is particularly problematic with faceted search if the user cannot easily add, remove, or refine facet specifications.\r3.5 Consistency\rThis is an obvious measure of the degree of similarity of means of accomplishing similar goals in different parts of the interface. It applies to layout (e.g., where people look for the search interface on a web site, where facets selection lists are located, etc.) and to the availability of features. The previous Yelp! example shows a degree of inconsistency because the query refinements are not available on a details page of a search result. Medynskiy et al [8] describe other challenges to making that interface more consistent.\r3.6 Hardmentaloperations\rOperations that rely on a user’s concentrated attention may pose usability problems, particularly when a user may not have the right background knowledge to perform the operation, or may be operating with divided attention. Wolfram|Alpha’s minimal interface that requires users to enter syntactically-complex queries is a good example of hard mental operations; Boolean query interfaces (notorious for being error prone for a variety of reasons) are a good example of hard mental operations.\rOccurrences of hard mental operations may be exacerbated by high viscosity or premature commitment situations where the user may find it difficult to know what to do or what to undo when an error or unexpected result is observed.\r3.7 Role-expressiveness\rThis dimension reflects how well the various visual components of an interface reflect their purpose and the operations available on them. Can the user find the search box? Is it obvious how to compare documents?\rWhile common controls have become reasonably standardized, less common tools such as query expansion or certain kinds of faceted search may require more attention from the interface designer to make the purpose of the controls and the manner in which they should be used obvious, particularly when they are intended to support activity that may involve hard mental operations. The interface should strive for transparency rather than being cluttered with many controls that are used infrequently or whose purpose is not immediately clear.\r3.8 Progressiveevaluation\rHow easy is it for people to assess what they’ve discovered, how much progress they’ve made toward their goal? This dimension becomes particularly important for exploratory search. Interfaces that make it difficult to see which documents have been ‘saved’ or bookmarked, or ones that hide users’ query history, requiring additional interaction to see what has been done may hamper people’s exploratory search activities, because being able to get an overview of what has been found or query tactics that have been used may be a useful tool for assessing progress toward satisfying the information need that motivated the search in the first place.\r4. ANEXAMPLE\rYou are planning to attend what promises to be an excellent workshop in the DC Area, and would like to plan ahead for some nice meals. The Yelp site offers a large listing of restaurants in the area, so it is an obvious choice to start looking. A search for “restaurants near catholic university of america\" produces a map\rPage 51 of 122\r\nand a list of four restaurants: one has a review, two are fast food chains, and the fourth is called “Capital City Rehab Center.” Clearly the query needs to be refined, for which purpose the site offers two possibilities: the ambiguously labeled “show filters” link, and a link labeled “Mo’ Map” (Figure 1 and Figure 2).\rthe filter changes from a two-mile to a five-mile radius, increasing the number of matches and changing the size and scale of the map.\rYelp offers a link to browse nearby restaurants, but completely forgets the filters that had been set up just before. Instead, it shows a full list of available cuisines in the Zip code of the selected restaurant, but this view does not allow multiple categories to be selected, again demonstrating high viscosity and poor consistency. The “show filters” link is available below this list of categories, but it is not opened, requiring additional interaction from the user to refine the query. Furthermore, it no longer offers a category aspect for multiple selection, and offers “San Francisco” as a possible city to search. When the filter is engaged to re-create the original query, the list of cuisines persists, showing all cuisines available in the DC area, rather than the version available initially (Figure 3). The lack of consistency here is staggering.\rThis example illustrates several usability problems encountered during a short search session. It is by no means a definitive usability analysis of the Yelp! site, and is meant only to show the flavor of cognitive dimensions analysis. Although none of the problems identified above is critical, they do, in combination, affect the quality of the search interaction and may cause people to miss useful results or to repeat themselves. In more mission- critical or time-sensitive situations, these interface problems can contribute to more costly mistakes than not finding a great place to have dinner.\r5. CONCLUSIONS\rT.R.G. Green’s Cognitive Dimensions Theory offers an interesting and powerful toolbox that can be used to characterize and reason about search interfaces without descending into the minutia of particular designs. The vocabulary of cognitive dimensions can form an effective shorthand for expressing complex characteristics of interfaces and systems, and therefore can improve communication between designers, system builders, and other stakeholders. While it was designed for broad applicability to information artifacts of all kinds, it is particularly useful for characterizing the kinds of complex systems that people are using to fulfill their information needs.\r6. REFERENCES\r[1] Balfe, E. and Smyth, B. 2004. Query Mining for Community Based Web Search. In Proceedings of the 2004 IEEE/WIC/ACM international Conference on Web intelligence (September 20 - 24, 2004). Web Intelligence. IEEE Computer Society, Washington, DC, 594-598. DOI= http://dx.doi.org/10.1109/WI.2004.120\r[2] Belkin, N. J.; Oddy, R. & Brooks, H. 1982. ASK for Information Retrieval. Journal of Documentation, 38, 61-71 (part 1) & 145-164 (part 2)\r[3] Green, T. R. G. 1989. Cognitive dimensions of notations. In A. Sutcliffe and L. Macaulay (Eds.) People and Computers V. Cambridge: Cambridge University Press, pp. 443-460\r[4] Green, T.R.G. and Blackwell, A. 1998. Cognitive Dimensions of Information Artefacts: a tutorial. Available online at http://www.ndirect.co.uk/~thomas.green/workStuff/Papers/\rFigure 1. \"Show Filters\" link\rFigure 2. \"Mo' Map\"\rZooming out on the map increases the number of available restaurants to 121, a nice example of low viscosity. To refine the search, however, you need to know that “show filters” will allow you to select restaurants by cuisine, price, etc. “Filters’ is an overly-technical term that has poor role expressiveness and visibility. Giving a few examples (e.g., “by price”, “by cuisine,” etc.) would make it easier to transition to the next query refinement stage.\rThe filter interface lets you specify distance, features, price, and category, in addition to expanding to other cities and sorting the results. As you scroll down the list (which, with the distance expanded to two miles, now has 1695 entries), the map moves with you, again illustrating low viscosity. When price and features are specified, the category list is updated automatically, although only showing four items by default requires an extra step to see more categories (Figure 3).\rWith the selected cuisines, the list is reduced to 75 restaurants. The sort order is not apparent: there is no indication in the interface, and casual inspection of the top five results rules out distance, number of reviews, and ratings. That leaves “Best Match” as the only seemingly possible order criterion, but it is not clear what that means. Thus we would classify this as a n example of a hard mental operation that affects the transparency of results.\rThe information displayed for each entry in the list also shows high viscosity and poor visibility because even though the price was restricted, the actual values for the retrieved restaurants were not shown. Instead, each link has to be interrogated individually to get that information. Nor does the system allow results to be grouped by price or by category, requiring the filter to be modified instead. This is another example of high viscosity.\rIt appears impossible to save promising restaurants in an ad hoc manner to generate a short list to pick from at the end of the search. This shows poor progressive evaluation, requiring some external record of promising locations.\rSelecting a restaurant page creates a new set of usability challenges: although the map is still shown, it no longer displays other matching restaurants, although now it allows restaurants to be “bookmarked,” showing high viscosity and poor consistency. It should be possible to bookmark a restaurant in any view. Viscosity is even worse when the back button is pressed, because\rFigure 3. Selecting categories\rPage 52 of 122\r\n[5] Green, T. R. G. 2000. Instructions and descriptions: some cognitive aspects of programming and similar activities. Invited paper, in Di Gesù, V., Levialdi, S. and Tarantino, L., (Eds.) Proceedings of Working Conference on Advanced Visual Interfaces (AVI 2000). New York: ACM Press, pp 21-28.\r[6] John, B. and Kieras, D. E. 1996. The GOMS Family of User Interface Analysis Techniques: Comparison and Contrast, ACM Transactions on Computer-Human Interaction 3,4, 320-351.\r[7] Marchionini, G. 1995. Information Seeking in Electronic Environments. Cambridge University Press.\r[8] Medynskiy, Y., Dontcheva, M., and Drucker, S. M. 2009. Exploring websites through contextual facets. In Proceedings of the 27th international Conference on Human Factors in Computing Systems (Boston, MA, USA, April 04 - 09, 2009). CHI '09. ACM, New York, NY, 2013-2022. DOI= http://doi.acm.org/10.1145/1518701.1519007\r[9] Teevan, J., Dumais, S. T., and Horvitz, E. 2005. Personalizing search via automated analysis of interests and\ractivities. In Proceedings of the 28th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Salvador, Brazil, August 15 - 19, 2005). SIGIR '05. ACM, New York, NY, 449-456. DOI= http://doi.acm.org/10.1145/1076034.1076111\r[10] Torrey, C., Churchill, E. F., and McDonald, D. W. 2009. Learning how: the search for craft knowledge on the internet. In Proceedings of the 27th international Conference on Human Factors in Computing Systems (Boston, MA, USA, April 04 - 09, 2009). CHI '09. ACM, New York, NY, 1371- 1380. DOI= http://doi.acm.org/10.1145/1518701.1518908\r[11] Wright, P. 1991. Cognitive overheads and prostheses: some issues in evaluating hypertexts. In Proceedings of the Third Annual ACM Conference on Hypertext (San Antonio, Texas, United States, December 15 - 18, 1991). HYPERTEXT '91. ACM, New York, NY, 1-12. DOI= http://doi.acm.org/10.1145/122974.122975\rPage 53 of 122\r\nCognitive Load and Web Search Tasks\rJacek Gwizdka\rDept. of Library and Information Studies, School of Communication and Information, Rutgers University New Brunswick, NJ, 08901 USA\rHCIR2009@gwizdka.com\rABSTRACT\rAssessing cognitive load on web search is useful for characterizing search system features, search tasks and task stages with respect to their demands on the searcher’s mental effort. It is also helpful in examining how individual differences among searchers (e.g. cognitive abilities) affect the search process and its outcomes. We discuss assessment of cognitive load from the perspective of primary and secondary task performance. Our discussion is illustrated by results from a controlled web search study (N=48). No relationship was found between objective task difficulty and performance on the secondary task. There was, however, a significant relationship between search task stages and performance on the secondary task.\rCategories and Subject Descriptors\rH.1.2 [Models and Principles]: User/Machine Systems – human information processing H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval – search process\rGeneral Terms\rMeasurement, Performance, Experimentation, Human Factors.\rKeywords\rCognitive load, search task, user behavior.\r1. INTRODUCTION\rWeb search behavior is affected by the task, system, and individual searcher characteristics. These factors, either alone or in combination, influence the level of difficulty experienced by a searcher. One kind of difficulty is related to mental, or cognitive, requirements that are imposed by the search system or the task itself. Understanding what contributes to a user’s cognitive load on search tasks is crucial to understanding the search process and to identifying search tasks types and search system features that impose increased levels of load on users. As new user interfaces and interactive features are introduced into the information search systems we need to understand how the new functionality affects user performance and the system usability, usefulness, and acceptance. For example, user relevance feedback is a feature that was reported to be avoided by users due to the heightened cognitive load [1].\rIn the next section we briefly discuss cognitive load and provide a short overview recent research that used cognitive load in the context of information search. We then highlight our results demonstrating that mental effort varies across search task stages.\r2. BACKGROUND\rThe concept of cognitive load has been used in various fields that deal with the human mind interacting with external stimuli (e.g., ergonomics, psychology, learning). In this paper, we define cognitive load can as the mental effort required for a particular person to complete their task using a given system. Hence, at any\rCopyright is held by the author/owner(s).\rHCIR Workshop 2009, Washington, D.C., October 23 2009.\rpoint in task performance, cognitive load is relative to the user, the task being completed, and the system employed to accomplish the task.\rIt should be clear that cognitive load is of interest to interactive information retrieval researchers for two reasons. First, it can be used to characterize search interfaces with respect to cognitive cost. Second, it can be used to characterize user tasks and their elements with respect the required mental effort. Both perspectives have a long history in human factors and human- computer interaction literature. Most recently, the first approach was elaborated by Wilson and schraefel at the last year’s HCIR workshop [16]. Wilson and schraefel proposed Cognitive Load Theory [4] as a tool useful in estimating cognitive costs of information search interfaces and proposed an inspection-based evaluation framework [18]. In other related recent work that exemplifies the first approach, Harper and colleagues established web page ranking according to their perceived visual complexity and linked it with cognitive load [9].\rIn CLT terminology, the first approach deals mainly with extrinsic load, that is with the complexity imposed by search interface and system. The second approach, deals mainly with intrinsic load, that is with search task demands on user’s cognitive resources. The primary goal of the first approach is to lower the extrinsic load so that user can commit more cognitive resource to the good germane load that facilitates task performance. The primary goal of the second approach is to understand better mental requirements of search tasks. A factor that often mediates the effects the task and the system are the user’s cognitive abilities (e.g., [6]).\rThis paper promotes the second approach, and also considers selected cognitive abilities in addition to task performance factors.\r2.1 Measurement of Cognitive Load\rMethods used to date to assess cognitive load included searcher observation, self-reports (e.g., using questionnaires, think-aloud protocols, and post-search interviews), dual-task techniques [5], [11], and various approaches that employ external devices to collect additional data on users (e.g., eye-tracking, pressure- sensitive mouse and other physiological sensors [10]). The two latter groups of techniques have the advantage of enabling real- time, on-task data collection. However, use of external devices can be expensive and impractical. Hence, the promise of dual-task (DT) method that allows for an indirect objective assessment of effort on the primary task. Only few studies employed this method to assess cognitive load in online search tasks [12][5].\rThe dual-task technique measures directly instantaneous cognitive load at discrete points in time. The discrete values are typically used to calculate averages over time intervals of interest (i.e., during performance of a task or a task stage). The average values reflect the intensity of the load [13], [19]. The intensity is related to the overall load perceived by a person, but is not necessarily the same, as it is often assumed.\rPage 54 of 122\r\nWe present a study that employed the dual-task method as the technique for assessing cognitive load on web search tasks.\r3. METHODOLOGY\rThe details of the experimental methodology were reported in [7], [8]. However, the results presented in this paper have not been reported earlier. This section provides only this information that is needed for understanding the main points.\rForty-eight subjects participated in a controlled web-based information search. Two cognitive abilities were assessed, working memory and spatial ability. The study search tasks were designed to differ in terms of their difficulty and structure. Two types of search tasks were used: Fact Finding (FF) - find one or more specific pieces of information, and Information Gathering (IG) - collect several pieces of information about a given topic. The tasks were also divided into three categories based on the structure of the underlying information need, 1) Simple (S), satisfied by a single piece of information; 2) Hierarchical (H), satisfied by finding multiple characteristics of a single concept (a depth search); 3) Parallel (P), satisfied by finding multiple concepts that exist at the same level in a conceptual hierarchy (a breadth search) [15]. Based on these characteristics, the tasks were categorized into three levels of “objective” difficulty. FF-S was assigned low difficulty level, FF-P and FF-H middle- difficulty level, and IG-H and IG-P high difficulty level. During the course of each study session, participant performed a set of six tasks of differing type and structure. The search tasks were performed on the English Wikipedia by using two search engines with the associated search interfaces: U1 Google, and U2 ALVIS [3]. The order of tasks was partially balanced with respect to the objective task difficulty to obtain all possible combinations of low-medium-high and high-medium-low difficulty within the groups of three tasks. This yielded four task rotations that were repeated for two orders of user interfaces. Thus there were eight task/UI rotations.\rA secondary task (DT) was introduced to obtain indirect objective measures of user’s cognitive load on the primary search task. A small pop-up window was displayed at a fixed location on a computer screen at random time. The pop-up contained a word with a name of a color. The color of the word’s font either matched or did not match the name of the color. Participants’ were asked to click on the pop-up as soon as they noticed it. The secondary task involved motor action, as well as visuo-spatial and verbal/semantic processing. The modalities of the primary task and the secondary task overlapped. One could have reasonably assumed that higher demands on cognitive resources by the primary search task would be reflected in lower performance on the secondary task.\r3.1 Data Collection and the Measures\rUser interaction was recorder by Morae screen cam software from TechSmith and by the secondary task software. The interaction logs that were used in the analysis presented in this paper included time-stamped sequences of visited web pages, keyboard clicks, and mouse clicks. The latter were recorded for the primary and the secondary task.\rUser search process was divided into four main task stages (Figure 1). We used a semi-automatic process to segment user interaction data into task stages. The process involved classifying URLs, and detecting patterns in the keyboard and mouse data. The\rdata collected for 48 users contained 288 tasks and 1447 task stages.\rThe two main controlled factors were the objective task difficulty (OBJ_DIFF) and the search system (UI). The additional two independent factors were the levels of working memory (WM) and spatial ability (SA). We assessed intensity of cognitive load within each task stage by calculating the average reaction time (RT) to the secondary task events.\r4. RESULTS\rThe analysis presented in this paper focuses on the relationship between the independent variables and the performance on the secondary task (RT). The analysis was performed at the task and the task-stage levels.\rFigure 1. State diagram of task stages.\rAn analysis of covariance (ANCOVA) performed with the objective task difficulty, task stage and user interface as fixed factors and with the cognitive abilities as covariates revealed that mean reaction time differed significantly between task stages (F(3,862)=6.2, p<.001) and was significantly related to both cognitive abilities (F(1,862)=5.5, p<.05 for WM and F(1,862)=24.7, p<.001 for SA). There was no significant effect of the objective task difficulty or of the user interface. Post-hoc analysis (Bonferroni test) showed that the mean reaction times during the query and the bookmarking stage were significantly longer than during the search results list and the content stage (p<.05). Other differences were not significant. The differences in reaction time between the task stages are shown in Figure 2.\rPage 55 of 122\r\nFigure 2. Mean reaction time to the secondary task events.\rReaction time could be considered as containing a component related to person (dependent on motor, perceptual, and cognitive ability of the person), a component related to task stage (dependent on the cognitive demands of the stage), and some other components (Equation 1).\rRTtotal = C +RTperson + RTtask _ stage (1)\rTo further examine the two significant sources of variability in reaction time, individual user variability and task stage, we considered them separately. An analysis of variance was\r€performed with participant identifiers as the main factor and reaction time as the dependent. The predicted mean values of reaction time and the residuals were then analyzed separately as dependents with other previously described independent factors. This statistical procedure is essentially equivalent to subtracting mean reaction time that is typical for a user in the given circumstances (the predicted value that represents RTperson)\rWe described user study that employed dual-task method as a technique for assessing cognitive load on web search tasks. The results showed that mental effort varied across search task stages. To our knowledge, this is the first study that demonstrated such an effect.\rOur study also makes a methodological contribution. The results indicate that measures of cognitive load intensity may be sensitive to dynamic changes in task demands (such as the changes between task stages) and not sensitive to the differences between tasks. This finding explains why Schmutz and colleagues [14] and why our earlier analysis [7] of dual-task performance did not find significant relationships between reaction times and tasks.\rUnderstanding mental effort imposed by task stages informs the design of search systems. It indicates, indirectly, during which stages the searchers may be more likely to have “spare” mental capacity and be willing to provide additional information to the system (e.g., relevance feedback). It could also be used in the design of notification delivery from other computing tasks [1].\rBeyond the implications of specific results, the described method of assessing cognitive load can be applied in other web search contexts. It could be used in experiments designed to measure extrinsic load (related to the specific user interface), and, possibly, to corroborate results of evaluation frameworks such as the recently proposed approach [18].\r6. ACKNOWLEDGMENTS\rThis research was sponsored, in part, by a grant from Rutgers University Research Council #RCG202130.\r7. REFERENCES\r[1] Adamczyk PD, Bailey BP. If not now, when?: The effects of interruption at different moments within task execution. Proceedings of CHI’2004.\r[2] Back J, Oppenheim C. A model of cognitive load for IR: Implications for user relevance feedback interaction. Information Research 2001; 6(2). Reference available from: http://informationr.net/ir/6-2/ws2.html\r[3] Buntine W, Valtonen K, Taylor M. The ALVIS Document Model for a Semantic Search Engine. Proceedings of the 2nd Annual European Semantic Web Conference; May 29, 2005. Heraklion, Crete.\r[4] Chandler P, and Sweller J. Cognitive Load Theory and the Format of Instruction. Cognition and Instruction 1991; 8(4): 293-332.\r[5] Dennis S, Bruza P, McArthur R. Web searching: A process- oriented experimental study of three interactive search paradigms. J Am Soc Inf Sci Tech 2002; 53(2): 120-133.\r[6] Gwizdka J, Chignell M. Individual differences and task- based user interface evaluation: A case study of pending tasks in email. Interact Comput 2004; 16(4): 769-797.\r[7] Gwizdka J. Assessing Cognitive Load on Web Search Tasks. (in press). To appear in The Ergonomics Open Journal. Bentham Open Access.\r[8] Gwizdka J, Lopatovska I. The Role of Subjective Factors in the Information Search Process. To appear in Journal of American Society for Information Science and Technology (JASIST). Early access online 2009. DOI: 10.1002/asi.21183\rfrom the overall reaction time [13]. The resulting reaction time (the residual that represents C +RT ) does not contain\rtask _ stage\rvariability that could be ascribed to individual participants. This\rprocedure removes the differences in the users’ motor and\rcognitive skills. The expectation was confirmed by finding that the “typical” users’ reaction time was significantly related to the\r€\rBefore we could draw final conclusions one more check needed to be performed. The secondary task involved using a mouse. The two task stages, during which the average reaction time was found to be the slowest (query entry and bookmarking/saving a relevant document), involved typically a fair amount of keyboard activity (query entry or tag entry). An additional check was thus performed to ensure that the longer reaction times were not related to the increased keyboard activity. Indeed, we found that the number of keystrokes and the time on keyboard were not related to the reaction time. We could conclude that the differences in reaction time among task stages were likely not due to motor activities.\r5. SUMMARY&CONCLUSIONS\rThe aim of this paper was to present assessment of cognitive load on search tasks as a way of gaining better understanding of the web search process by characterizing it with the levels of cognitive load.\rusers’ cognitive abilities, while the residual reaction time was significantly related to the task stage. Clearly, the differences between task stages were reflected in different reaction times to the secondary task events.\r€\rPage 56 of 122\r\n[9] Harper S, Michailidou E, Stevens R. Toward a definition of visual complexity as an implicit measure of cognitive load. ACM Transactions on Applied Perception (TAP) 2009; 6(2): 1-18.\r[10] Ikehara CS, Crosby ME. Assessing cognitive load with physiological sensors. Proceedings of the 38th Hawaii International Conference on System Sciences (HICSS); 3-6 January 2005. Big Island, HI, USA. IEEE Computer Society 2005. pp. 295a\r[11] Kaki M. Findex: search result categories help users when document ranking fails. Proceedings of the ACM Conference on Human Factors in Computing Systems CHI; April 2-7, 2005. Portland, Oregon, USA. ACM Press; pp. 131–140.\r[12] Kim YM, Rieh SY. Dual-Task Performance as a Measure for Mental Effort in Library Searching and Web Searching. Proceedings of the 68th Annual Meeting of the American Society for Information Science & Technology. Oct. 28 – Nov. 2, 2005. Charlotte, NC.\r[13] Madrid IR, Van Oostendorp H, Puerta Melguizo MC. The effects of the number of links and navigation support on cognitive load and learning with hypertext: The mediating role of reading order. Comput Human Behav 2009; 25(1): 66-75.\r[14] Schmutz, P., Heinz, S., Métrailler, Y. & Opwis, K. (2009). Cognitive Load in eCommerce Applications - Measurement and Effects on User Satisfaction. Adv in Human Comp Interact.\r[15] Toms E, O’Brien H, Mackenzie T, et al. Task Effects on Interactive Search: The Query Factor. In: Fuhr N, Kamps J, Lalmas M, Trotman A. Eds. Focused Access to XML Documents. Lect Notes Comput Sci 4862. Springer Verlag 2008; pp. 359-372.\r[16] Wilson ML, schraefel mc. Improving Exploratory Search Interfaces: Adding Value or Information Overload? In: Second W orkshop on Human-Computer Interaction and Information Retrieval, 23rd October 2008, Redmond, WA, USA.\r[17] Wilson ML, schraefel mc. Reading between the lines: identifying user behaviour between logged interactions. In: SIGIR09 Workshop: Understanding the User - Logging and interpreting user interactions in information search and retrieval, 23rd July 2009, Boston, MA, USA.\r[18] Wilson ML, schraefel mc, White RW. Evaluating Advanced Search Interfaces using Established Information-Seeking Models. J Am Soc Inf Sci Tech 2009; 60 (7): 1407-1422.\r[19] Xie, B., & Salvendy, G. (2000). Prediction of Metal Workload in Single and Multiple Task Environments. Int J Cog Erg, 4(3), 213-242.\rPage 57 of 122\r\nVisualising Digital Video Libraries for TV Broadcasting Industry: A User-Centred Approach\rVideo Archives\r1. INTRODUCTION\rIn TV Broadcasting industry, professionals frequently have to search a vast video archive. Finding the desired video is often a difficult task using search engines for commercial or professional use. Moreover, retrieving a suitable fragment of a few seconds mostly requires users to skim many hours of stored video data using traditional video player software. In order to optimise this task, more advanced video browsers and visualisations are currently being employed in several research projects [2, 3, 6, 12, 14].\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR ’09 Washington DC, USA Copyright held by the authors.\rTo provide suitable visualisations for the target group, we followed a user-centred software engineering approach. By involving end users from the beginning of the development process, it is more likely that the visualisation of the final user interface corresponds to their needs and goals [13]. The development process that is applied, is based on a framework for user-centred software engineering [4]. Figure 1 shows all stages of the process, including extracts of the artefacts that were used during each stage.\rThe end users involved in this development process are TV researchers. In the first stage of the process, where the new system is examined, a Contextual Inquiry (CI) is con- ducted in cooperation with the TV broadcasting company. A CI involves observing and interviewing end-users while they are performing their daily activities. This user study learned us that TV researchers need to browse large amounts of data. Their main job is to search video fragments of news broadcasts presenting particular people or situations that will be used in a TV programme or news broadcast. Based on keywords and other search criteria (e.g. date, pro- gramme title) the archive is searched to find suitable video fragments. Currently the videos in the archive are annotated\rMieke Haesen Jan Meskens Karin Coninx Hasselt University - tUL - IBBT,\rExpertise Centre for Digital Media, Wetenschapspark 2, B-3590 Diepenbeek, Belgium firstname.lastname@uhasselt.be\rABSTRACT\rFinding a suitable video fragment in a vast video archive is mostly a complex task. Even professional users have to skim many hours of stored video data before they find the desired content. In this paper, we present a user-centred software engineering approach that is employed to create a novel news video explorer for TV broadcasting industry. This approach helps to ensure the balance between the tech- nological progress in the field of information retrieval on the one hand and the needs and goals of the end users on the other hand.\rCategories and Subject Descriptors\rH.1.2 [Models and Principles]: User/Machine Systems— Human Factors; H.5.2 [Information Interfaces and Pre- sentation]: User Interfaces—User-Centered Design ; H.3.3 [Information Storage and Retrival]: Information Search and Retrieval—Information filtering, Selection Process\rGeneral Terms\rDesign, Human Factors\rKeywords\rUser-Centred Software Engineering, Searching and Browsing 2.\rPage 58 of 122\rIn this paper, a User-Centred Software Engineering (UCSE) approach is being employed to construct novel video infor- mation retrieval visualisations for the TV broadcasting do- main. As a first step in this approach, TV researchers are interviewed and observed while they are working in their natural environment. This results in a better understanding of their practices and problems and helps taking the needs and goals of end users into account from the beginning of the process. By observing the end-users before any design takes place, the resulting visualisations can offer increased ease of use, efficiency and satisfaction [13].\rThe work in this paper is carried out within the context of the AMASS++ project [7]. This project aims to investigate the alignment and summarization of multimedia archives. The visualisations presented throughout this paper are built on top of the AMASS++ annotated news video corpus.\rIn summary, the major contributions in this paper are:\r• •\ra UCSE process employed in cooperation with a TV broadcasting company;\ran interactive prototype for exploring news videos, re- sulting from the aforementioned UCSE approach. The visualisations employed in this prototype are suited for TV researchers.\rUSER-CENTRED PROCESS\r\nFigure 1: The user-centred process that was adopted for the development of the user interface.\rmanually, which allows the selection of appropriate archive videos. However, once a video is selected from the archive, the TV researcher has to browse the entire video manually in order to find and select a suitable fragment. Moreover, to carry out the different tasks, the user needs to combine several separate applications, which decreases efficiency.\rThe CI resulted into a scenario of use and an accompany- ing storyboard, that exemplify how one integrated future application can be used for searching archives, browsing an archive video and adding video fragments to a favorites folder. The storyboard was used to discuss the application with the stakeholders and provided the first data for the structured interaction analysis, in which a dialogue model and a conceptual model were created. Each artefact created in these early stages, was used for prototyping the UI.\rFigure 2: The search user interface.\rThe first low-fidelity prototypes of the user interface were created using pencil and paper and Powerpoint. Following, the high-fidelity prototypes were created in .NET. During several iterations, the low- and high-fidelity prototypes were verified in stakeholder meetings, features were added grad- ually and a graphic designer was involved for the detailed UI design. The UI designs and visualisations, included in the high-fidelity prototype, are discussed in the following section.\r3. NEWS VIDEO EXPLORER PROTOTYPE\rDuring the aforementioned UCSE process, a news video explorer prototype was iteratively constructed. The user interface of this system contains two major parts: a search user interface and a video browser. While the former helps with finding the suitable video, the latter supports end users in skimming this video to locate and save content of interest.\r3.1 Search User Interface\rIn order to find the right video, users start with enter- ing a search query containing a keyword and/or date range in the search user interface (see Figure 2, left). Based on this information, the system retrieves a set of relevant news videos (see Figure 2, right). The location and size of each video thumbnail indicate the relevance with respect to the search query: the most relevant videos are bigger and lo- cated in the center of the screen. By replacing and resizing the video thumbnails, users can also sort the search results themselves.\rEach video is represented by an animated slideshow of key-frames, which are computed by the shotcut detection algorithm described by Osian et al. [9]. When a video seems to be interesting, users can double click on it to open the video browser. We employed a keyframe-based abstraction technique since these have been shown to be effective in helping people quickly obtain a general understanding of what is contained in a video [2].\r3.2 Video Browser\rThe video browser combines an advanced time slider, based on the time sliders in commercial video players such as Ap- ple Quicktime Player and Microsoft Windows Media Player, with a timeline video visualisation [6]. A time slider is em- ployed to manipulate the current time of the played video\rPage 59 of 122\r\nFigure 3: The video browser.\rfragment (Figure 3, part A) and to specify an area of interest around this time (Figure 3, part B). The timeline (Figure 3, part C) gives a detailed view on the content in this area of interest.\rThe combination of the advanced video time slider with the timeline is in line with the basic idea of focus+context in information visualisation [1]. Context, on the one hand, is visualised using the video time slider, where red dots in- dicate the parts of the video relevant to the search query. Focus, on the other hand, is visualised in the timeline. Sim- ilar to other timeline based approaches [11], we use semantic zooming to specify the level of detail in the timeline. By re- sizing the focus area in the time slider (Figure 3, part C), users can zoom in or out on the video timeline. As shown schematically in Figure 4: a small focus area increases the level of detail in the timeline, a wider focus area decreases this level of detail.\rThe timeline shows a layered view on the video as com- puted by information retrieval algorithms for video and man- ual content annotations [7, 9, 10]. At the first layer, the title of every news item in this video is shown. This layer is further subdivided in several sublayers each containing story and scene information. The two remaining layers of the third timeline contain thumbnails of each shot in the movie and the names of the persons that appear in these shots. For example, Figure 3 shows that the topic about the Israeli- Palestinian conflict starts with the news anchor, followed by a presentation, the anchor again and a reportage. The faces layer reveals the names of the anchor (Alistair Yates) and presenter (Ban Ki-moon) together with a thumbnail.\rThe video browser contains several mechanisms to keep an overview on the large amount of information that is visu- alised in the timeline. Each layer can be maximised/minimised by clicking on its toolglass icon. In order to hide a layer,\rFigure 4: The semantic zoom function.\rusers can gray out the eye buttons (see Figure 3, part D), comparable to layers in traditional graphics software pack- ages. Content filters (see Figure 3, part E) are provided to filter content from the timeline. For example, a user can check the anchor filter to remove the anchor blocks from the timeline. Users can also bookmark interesting blocks of content for later use.\r4. DISCUSSION AND FUTURE WORK\rThis paper presented a news video explorer for the TV broadcasting industry, realised by means of a UCSE process. First, the user tasks were observed and analysed, followed by\rPage 60 of 122\r\nthe creation of structured interaction diagrams. Through- out the low- and high-fidelity prototyping stages, the struc- tured interaction diagrams were used for verification of the prototypes. At several stages in the process, artefacts were discussed and verified in stakeholder meetings.\rDuring the user-centred process, intermediate prototypes were frequently verified in stakeholder meetings. This re- sulted in interesting recommendations for the news video explorer presented in this paper. Although experts of the domain were involved in these meetings, thorough valida- tion is needed to estimate the value of our news video ex- plorer for TV researchers. Comparative and repeated user experiments could be helpful to improve our prototype and to discover the way in which the news video explorer can change daily practice for the TV researchers over time.\rOur current prototype allows TV researchers to search and browse the video archive on their desktop pc. However, for meetings and assembling videos, they often have to move to other locations where it is not possible to consult the videos or different applications need to be used. In their current system, video files have to be saved on a central server in order to make them accessible on multiple PCs and loca- tions. The use of modern devices such as multitouch tables or mobile devices might improve this approach. Therefore, we are currently investigating how the UI designs presented in this paper can be extended to other platforms such as a multitouch table and a ultra mobile pc. While a multitouch application is helpful for presenting and discussing archive videos during editorial meetings, a mobile application can assist journalists for carrying on particular videos or quick searches on location.\rAs indicated by the arrow on the left of Figure 1, tar- geting novel computing platforms is done by starting a new iteration of the user-centred process. Tool support [8, 5] for storyboarding and multi-device UI development will be investigated and deployed to provide smooth transitions be- tween several stages in the user-centred process.\rBesides video libraries, the AMASS++ project aims to provide technologies for cross-media and cross-language search and summarization in several application domains. There- fore, we will explore text based visualisations that allow users to quickly browse a text and its related multimedia. Additional visualisations of the search results, including map and timeline views, will also be considered here.\r5. ACKNOWLEDGMENTS\rThe research described in this paper is based on the re- sults of IWT project AMASS++ (SBO-060051). The au- thors would like to thank the consortium for their contribu- tions made to this work and the companies of the user com- mittee for their involvement in the user studies and their valuable feedback. Furthermore, we would like to thank Jimmy Cleuren for his contribution to the high-fidelity pro- totypes and Karel Robert for the graphic design of the user interface.\r6. REFERENCES\r[1] S. K. Card, J. D. Mackinlay, and B. Shneiderman.\rReadings in information visualization: using vision to\rthink. Morgan Kaufmann, 1999.\r[2] K.-Y. Cheng, S.-J. Luo, B.-Y. Chen, and H.-H. Chu.\rSmartplayer: user-centric video fast-forwarding. In\rCHI ’09: Proceedings of the 27th international conference on Human factors in computing systems, pages 789–798, New York, NY, USA, 2009. ACM.\r[3] M. G. Christel. Supporting video library exploratory search: when storyboards are not enough. In CIVR ’08: Proceedings of the 2008 international conference on Content-based image and video retrieval, pages 447–456, New York, NY, USA, 2008. ACM.\r[4] M. Haesen, K. Coninx, J. V. den Bergh, and\rK. Luyten. Muicser: A process framework for multi-disciplinary user-centred software engineering processes. In HCSE ’08: Second Conference on Human-Centered Software Engineering,, pages 150–165, 2008.\r[5] M. Haesen, K. Luyten, and K. Coninx. Get your requirements straight: Storyboarding revisited. In Interact 2009 (to appear).\r[6] A. Haubold and J. R. Kender. Vast mm: multimedia browser for presentation video. In CIVR ’07: Proceedings of the 6th ACM international conference on Image and video retrieval, pages 41–48, New York, NY, USA, 2007. ACM.\r[7] S. Martens, J. H. W. Becker, T. Tuytelaars, and M.-F. Moens. Multimodal data collection in the AMASS++project. In Multimodal Corpora: from Models of Natural Interaction to Systems and Applications, pages 60–63. European language resources association, 2008.\r[8] J. Meskens, J. Vermeulen, K. Luyten, and K. Coninx. Gummy for multi-platform user interface designs: shape me, multiply me, fix me, use me. In AVI ’08: Proceedings of the working conference on Advanced visual interfaces, pages 233–240, New York, NY, USA, 2008. ACM.\r[9] M. Osian and L. Van Gool. Video shot characterization. Mach. Vision Appl., 15(3):172–177, 2004.\r[10] P. T. Pham, M.-F. Moens, and T. Tuytelaars. Linking Names and Faces: Seeing the Problem in Different Ways. In Workshop on Faces in ’Real-Life’ Images: Detection, Alignment, and Recognition, Marseille France, 2008. Erik Learned-Miller and Andras Ferencz and Fr ́ed ́eric Jurie.\r[11] C. Plaisant, B. Milash, A. Rose, S. Widoff, and\rB. Shneiderman. Lifelines: visualizing personal histories. In CHI ’96: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 221–ff., New York, NY, USA, 1996. ACM.\r[12] B. T. Truong and S. Venkatesh. Video abstraction: A systematic review and classification. ACM Trans. Multimedia Comput. Commun. Appl., 3(1):3, 2007.\r[13] I. Wassink, O. Kulyk, E. van Dijk, G. van der Veer, and P. van der Vet. Applying a user-centred approach to interactive visualization design. In Trends in Interactive Visualization. Springer Verlag, London, 2008. ISBN=978-1-84800-268-5.\r[14] M. Worring, C. Snoek, O. de Rooij, G. P. Nguyen, R. van Balen, and D. Koelma. Mediamill: Advanced browsing in news video archives. In CIVR, pages 533–536, 2006.\rPage 61 of 122\r\nLog Based Analysis of How Faceted and Text Based Search Interact in a Library Catalog Interface\rXi Niu\rUniversity of North Carolina at Chapel Hill CB#3360, 100 Manning Hall Chapel Hill, NC 27599-3360 xiniu@email.unc.edu\rABSTRACT\rFaceted based search is an increasingly common part of search interfaces. This study examines the use of a library catalog search interface which supports but text searching and faceted based searching. Log analysis is performed of library catalog search records to analyze how and when faceted based searching is used in conjunction with text based searching. The logs are from the Triangle Research Libraries Network, which all use an Endeca based catalog search system. Results show that faceted based search is used much less frequently than text searching, and the usage clusters into certain categories of search behaviors.\rCategories and Subject Descriptors\rH.1.2 [Information Systems]: USER/ MACHINE SYSTEMS—\rHuman information processing\rGeneral Terms\rHuman Factors, Measurement\rKeywords\rFaceted based searching, text searching, library catalog, searching behavior, transaction log analysis, cluster analysis, Markov model, action pattern\r1. INTRODUCTION\rThe purpose of this paper is to demonstrate a method for harvesting, storing, and analyzing data from the transaction logs of modern, faceted, search and browse Online Public Access Catalogs (OPACs). Faceted navigation OPACs, such as the ones in current use at the Triangle Research Libraries network (TRLN, comprised of the University of North Carolina, North Carolina State University, Duke, and North Carolina Central University), provide users with the ability to explore library collections by text searching and facet selection. Faceted search engines are emerging as the latest trend for search and navigation on library Online Public Access Catalogs (OPACs), as well as public libraries, WorldCat and general purpose search engines. Different from traditional OPACs, faceted search exposes the metadata (facets) that summarize records generated by the initial query as part of an interactive interface, allowing users to drill down along a particular dimension to desired results.\rSince the faceted navigation catalogs are new, there is little data or literature available to suggest that these catalogs actually improve the user experience over traditional OPACs that offer only text searching. Through transaction log data analysis, this research aims at revealing the ways users interact with Endeca\rCory Lown\rNorth Carolina State University NC State Library Raleigh, NC 27695-7111 corylown@gmail.com\rBradley M Hemminger University of North Carolina at Chapel Hill CB#3360, 100 Manning Hall Chapel Hill, NC 27599-3360 bmh@ils.unc.edu\rPage 62 of 122\rsystems in UNC Library and find how people incorporate facets into their search process. In addition, it makes a methodological contribution of how we extract and process the transaction log data and how we apply some analytical methods (Markov stochastic modeling, cluster analysis) to the data to find search patterns for library catalog patrons.\r2. RESEARCH QUESTIONS\rThe broad research question for this research is to investigate how people use facets in combination with text search in faceted library catalog. Under this broad question, we have three sub- questions: 1) How do people use facets to help them in their search process? Is it only as a refinement? Is text searching the assumed starting point for most faceted searches? How frequently do they use faceted search? 2) Do search sessions naturally segregate into certain types of search patterns that are discernable by log analysis? 3) When facets are utilized during the search process, are there any typical action sequences (patterns) commonly seen?\rThe first sub-question can mostly be answered by the descriptive statistics from the log data. The second and the third sub- questions require additional analytical methods to resolve.\r3. RESEARCHMETHODS\r3.1 DataExtractingandProcessing\rWhile records from all the TRLN libraries are being analyzed as part of this project, the data reported in this paper are from the UNC library catalog over the period of Dec 16, 2008 to April 2, 2009. Over 1,200,000 query records logged by UNC Library Apache Server were cleaned and processed using Perl scripts and a MySQL database. Perl scripts were used to filter, parse, group and code the raw log. The MySQL database was used primarily as a way of sorting the data according to a particular variable and also joining two datasets based on a particular field.\rA typical transaction record looks like this (on a single line):\r71.70.185.34 - - [09/Mar/2009:00:13:53 -0400] \"GET /search?Ntk=Keyword&Ne=2+200043+206475+206590+11&N=206432&Ntt =boston+globe HTTP/1.1\" 200 40035 \"http://search.lib.unc.edu/search?Nty=1&Ntk=Keyword&Ntt=boston+globe\" \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.4; en-US; rv:1.9.0.7) Gecko/2009021906 Firefox/3.0.7\"\rIf we parse the single line of the server log into components, we will get the information like this:\r\nTable 1. Single line of server log parsed into components\rThe URL parameters encode the user's search request. The parameters of interest are listed in table 2.\rTable 2. URL parameters\rThere are totally 24 possible finer codes and 11 coarser codes. Since the amount of the log data is huge, all the coding work was completed automatically by Perl script by comparing the state change.\r3.3 ClusterAnalysis\rTo answer the second research question, cluster analysis is utilized. Cluster analysis is a statistical technique to create categories that fit observations. In this study, search sessions are to be clustered according to their similarity. In other words, the within-group sessions are alike while the between-group sessions are different. Relative proportions of action codes in a particular session are treated as the attributes for clustering. To reduce the computational cost and avoid subjectively predefining the number of clusters, a hybrid approach of hierarchical and non hierarchical methods is employed. In the first stage, 133951 sessions are divided into 100 clusters using a non-hierarchical method with the SAS procedure FASTCLUS. The output of this procedure serves as the input to the second stage, a hierarchical method using SAS procedure CLUSTER with Ward’s algorithm. We adopted the coarser codes as the characteristic variables for the cluster analysis because the finer ones were too detailed to separate the clusters.\r3.4 SequentialEventAnalysis\rTo solve the third research question, sequential event analysis is applied to examine the action patterns of sessions. Specifically, two methods are employed. The first is Markov models checking transitions from one state to another. The other is maximal repeating patterns (MRP) identifying the actions sequences as long as possible for the searchers when they are searching the catalog.\r3.4.1 Markov Models\rA Markov model is a stochastic process with the Markov property which means that the description of the present state fully captures all the information that could influence the future evolution of the process. Future states will be reached through a probabilistic process instead of a deterministic one. The order of Markov models means how many previous states (including the current state) influence the choice of the next state probabilistically. The simplest form of a Markov model is called a zero-order model. It is simply the frequency with which each state occurred. The first-order Markov model, also called a state transition matrix, reports the probability of the transition from all the possible current states to all the possible future states. First- order Markov models are the types of models most frequently found in the ILS literature (Wildemuth, 2009). Higher-order models can also be created and evaluated. A second-order Markov model takes into account the previous two states in trying to predict the next state, and so forth.\rIn this study, higher order Markov models are applied to describe the sequence of moves performed by searchers in a session. Furthermore, an order test (based on the chi-square goodness-of- fit, Anderson and Goodman (1963)) is conducted to indicate the statistical significance of whether the transitions are zero-, first-, second or higher order processes. For example, consider a session which could be represented as TB-MT-FM-FA-VR (the process of beginning a simple text search, then entering multiple terms in\rClient IP Address\r71.70.185.34\rDate and Time Stamp\r[09/Mar/2009:00:13:53 -0400]\rRequest URL\r/search?Ntk=Keyword&Ne=2+200043+206475 +206590+11&N=206432&Ntt=boston+globe\rReferring URL\rhttp://search.lib.unc.edu/search?Nty=1&Ntk= Keyword&Ntt=boston+globe\rSearch Field “Ntk”\rKeyword\rText String (Query) “Ntt”\rboston+globe\rFacet Used “N”\r206432 (unique ID of facet value, stands for “Format: online”)\rFacet Opened to Browse “Ne”\r2+200043+206475+206590+11 (“Availability”, ”Location”, ”Format”, ”Su bject”, and “Publication Year”)\r'Did you mean?' search feature on or off “Nty”\r1\rThe feature is on\rOne of the difficulties for processing the log data is to identify individual users (sessions). For the log data, a single line of record which represents a request is treated as a transaction. Knowing only the IP addresses and query times, knowing how to chunk several transactions into a session is not trivial. Based on previous literature, we considered transactions occurring from the same IP address, and without a delay of 30 minutes or longer to be part of the same session. We identified 133951 search sessions for the dataset.\r3.2 CodingSchema\rAn \"action\" refers to a user's interaction with the system. In most cases, a transaction represents a single action. Related to \"actions\" are the codes used to indicate generic categories of requests. There are a finite number of things that the user can manipulate when interacting with the system and therefore a finite number of codes representing actions. In this study, granularity is the main concern of adopting a coding schema. According to Wildemuth and Moore (1995), more detailed coding schemes are too fine- grained to make statistical analysis effective, while a coarser scheme would not provide enough detail. We decide to use both fine-grained coding and course-grained coding schemas to complement each other. One can determine the action the user took through transition from one state to another by comparing what has changed in the request URL and referring URL. In the example above, the only difference between the two URLs is the appearance of “Ne=2+200043+206475+206590+11&N=206432” in the request URL. Therefore, it is inferred that the user opened several facets and clicked one at this step.\rPage 63 of 122\r\nthe search box, and then showing more values under one facet, next refining the search by adding a value of the facet, and finally viewing a record). If the process is tested to have the order of 2, significant action sequences of this session are all three-move thread, like TB-MT-FM. That is, the likelihood of being the state FM depends on having been in states TB and MT previously.\r3.4.2 Maximal Repeating Pattern\rPrevious literature indicates that people’s information behavior varies greatly from one person to another. It is helpful to find patterns that are frequently adopted by searchers. Siochi and Ehrich’s (1991) algorithm for identifying maximal repeating patterns (MRPs) among sequences of behavior is applied to serve this purpose. They defined an MRP as “a repeating pattern that is as long as possible, or is an independently occurring substring of a longer pattern” (Siochi & Ehrich, 1991, p.316). Thus, the algorithm systematically identifies those sequences of events that occur repeatedly within the data set. By examining what the MRPs look like and the frequencies at which they occur, we can pick out those we want to investigate further.\r3.5 VisualizationofSequentialEvents\rOne of the primary goals of this study is to develop an automatic way to visualize the search action steps users take when searching the catalog. During the pilot study, a visualization method was developed for manually producing a graphical representation of action sequences within a session. Shapes and colors represent the different actions taken by the user. Twenty-five sample search sessions were chosen randomly and then graphical representations of their searchers were generated. Recreating a number of sessions manually helps the investigator better understand the coding issues, and the different search process, but is not feasible for analyzing millions of search records. Automated visualization tools or processes are required for this. Currently, work is underway to automatically generate XML that describes the actions, so that simple style sheets can be used to graphically render the coded actions for viewing using standard web browsers.\r4. RESULTS\r4.1 Arepeoplereallyusingfacetstohelpthem\rrefine their search?\rAfter processing through Perl scripts and MySQL database, the transactions were grouped into 133951 sessions. The average number of moves (actions) in a session is 9. The most frequently occurring moves are MultipleTermText and ViewRecord, which are the traditional operations in the classic library catalog. Facet Operations (OpenFacet, CloseFacet, ShowMoreFacet, AddFacet, RemoveFacet, RefineYears) only account up to 6% of the total moves. This number is much less than our previous study on the usage of NCSU’s Endeca library catalog (40%; Cory, 2008) and still less than some other Endeca reports. It might be due to the fact that UNC has only been using this facet catalog for less than one year, and quite a few users are not familiar with it. Additionally, had we excluded some actions which are not of searching behavior kind, such as RSS Search and ViewRecord, the percent of facet operations would have increased.\r4.2 GroupsofUsers\rAs mentioned above, a hybrid approach of non-hierarchical and hierarchical clustering techniques is employed to this study. As result, 8 clusters were identified based on the dendrogram and semi-partial R squared statistic. According to the characteristic variables (percentages of actions) of each cluster, we label these 8 groups as SimpleTextSearch group (few moves and most work is entering text string in the search box), DetailedTextSearch group (most work is evenly distributed in entering text string and viewing records), InDepthTextSearch group (clicking next pages frequently), AdvancedSearch group (using advanced search mode much more frequently), FacetTextSearch group (facet operations combined with text search), FollowupSearch group (clicking into the links provided by a particular record), RSS group (using RSS feed feature), and Outliers (conducted by Roberts).\r4.3 Patterns of actions commonly adopted by\rsearchers in FacetTextSearch group\rSince FacetTextSearch (CL 8) is the population of interest in this research, Chi-square likelihood ratio test is employed to test the order of Markov model for this cluster. As result, the significant order is 2. That means the probability of being the current state depends on previous two states. Therefore, three-move sequence is the significant segment to describe the usage pattern in this group. The top 10 most frequent three-move sequences are summarized and the top pattern is all text search moves (TextSearch--TextSearch--TextSearch). Next most common is the pattern of all facet moves (ModifyFacets--ModifyFacets— ModifyFacets). The top two patterns together only account for 12% of all the possible patterns. We may infer that there is a wide distribution of usage patterns and that the common patterns among users are fewer than expected. From the 3rd to the 10th position, patterns are combination of text search, modifying facets and showing/hiding facets.\rFrom the transition matrix (first order Markov model) generated for this search group, we know that the most likely preceding action for modifying facets is text search (38.61%) and for showing/hiding facets is also text search (36.59%). Therefore, we could infer that text search is assumed as the most likely starting point for faceted searches.\rApplying the maximal repeating pattern algorithm (MRP) developed by Siochi to the facet search group, we identified 54 frequent (frequency higher than 500) patterns with 3 to 6 actions in each pattern. These 54 sequences are further grouped into three families: 1) facet search and then viewing record; 2) text search, facet search and then viewing record; 3) repeating (2) twice.\r4.4 VisualizingSequentialEvents\rA set of rules was developed for the graphical representation of action sequence, with different shapes and flows representing the search process. For shapes: the green rectangle stands for TextSearch; dark green rectangle means AdvSearch; white rectangle means off catalog website; red rectangle denotes ViewRecord; yellow rectangle stands for NextPage; blue diamond means ModifyFacets; Bright blue diamond means\rPage 64 of 122\r\nShowHideFacets; white cloud means SortResults; and grey shapes means clicking the back button of the browser. In the chart, vertical flow stands for entering new search words which will generate a new result set, while the horizontal flow means refining the current search under the same text search words. Below is an example of the application of this rule to one search\rsession. In this graph, the search words, the search field, the facet being incorporated, and the time spent on a particular manipulation are all displayed. The longer third and fourth lines indicate that the searcher kept refining his/her search through adding and removing facets or displaying facets.\rof the American Society for Information Science and Technology, 52(11), 888-904.\r[8] Chen, H. M., & Cooper, M. D. (2002). Stochastic modeling of usage patterns in a web-based information system. Journal of the American Society for Information Science and Technology, 53(7), 536-548.\r[9] Goodrum, A. A., Bejune, M. M., & Siochi, A. C. (2003). A state transition analysis of image search patterns on the web. Lecture Notes in Computer Science, , 281-290.\r[10] Jansen, B. J. (2005). Seeking and implementing automated assistance during the search process. Information Processing and Management, 41(4), 909-928.\r[11] Jansen, B. J. (2006). Search Log Analysis: What Is It; What’s Been Done; How to Do It. Library and Information Science Research, 28(3), 407-432.\r[12] Jansen, B. J., Spink, A., Blakely, C., & Koshman, S. (2007). Defining a Session On Web Search Engines. Journal of the American Society for Information Science and Technology, 58(6), 862-871.\r[13] Koch, T., Golub, K., & Ardo, A. (2006). Users browsing behaviour in a DDC-based web service: A log analysis. Cataloging & Classification Quarterly, 42(3-4), 163-186.\r[14] Lown, C. (2008). A transaction log analysis of NCSU's faceted navigation OPAC.\r[15] Novotny, E. (2004). I Don’t Think, I Click: A Protocol Analysis Study of Use of a Library Online Catalog in the Internet Age. College & Research Libraries, 65(6),525–37.\r[16] Olson, T.A. (2007). Utility of a faceted catalog for scholarly research. Library Hi Tech, 25(4 ), 550-561.\r[17] Qiu, L. (1993). Markov models of search state patterns in a hypertext information retrieval system. Journal of the American Society for Information Science, 44(7)\r[18] Siochi, A.C., & Ehrich, R.W. (1991). Computer analysis of user interfaces based on repetition in transcripts of user sessions. ACM Transaction on Information Systems, 9(4), 309–335.\r[19] Spink, A. (1996). Multiple search sessions model of end-user behavior: An exploratory study. Journal of the American Society for Information Science, 47(8)\r[20] Wildemuth, B. M. (2004). The effects of domain knowledge on search tactic formulation. Journal of the American Society for Information Science and Technology, 55(3), 246-258.\r[21] Wildemuth, B.M., & Morre, M.E. (1995). End-user search behaviors and their relationship to search effectiveness. Bulletin of the Medical Library Association, 83(3): 294–304.\r[22] Yu, H. & Young, M. (2004) The Impact of Web Search Engines on Subject Searching in OPAC. Information Technology and Libraries. 23(4), 168-180.\rFigure 1. Example visualization of one search session\rREFERENCES\r[1] Anderson, T., & Goodman, L. (1963). Statistical inference about Markov chains. Readings in Mathematical Psychology. (Vol. 2, pp. 241–262). New York: Wiley.\r[2] Antelman, K., et al. (2006) Toward a Twenty-First Century Library Catalog.Information Technology and Libraries. 25(3) 128-139.\r[3] Bates, M. J. (1979). Information search tactics. Journal of the American Society for Information Science, 30(4), 205-214.\r[4] Borgman, C. (1996). Why Are Online Catalogs Still Hard to Use? Journal of the American Society for Information Science, 47(7), 493- 503.\r[5] Callender, J. (2001) Perl for Website Management. O'Reilly Media, Inc.\r[6] Chapman, J. (1981). A state transition analysis of online information- seeking behavior. Journal of the American Society for Information Science, 32(5), 325-333.\r[7] Chen, H. M., & Cooper, M. D. (2001). Using clustering techniques to detect usage patterns in a web-based information system. Journal\rPage 65 of 122\r\nFreebase Cubed: Text-based Collection Queries for Large, Richly Interconnected Data Sets\rABSTRACT\rAny large data set such as Freebase that contains a large number of types and properties accumulated over actual use rather than fixed at design time poses challenges to designing easy-to-use faceted browsers. This is because the faceted browser cannot be tuned with domain knowledge at design time, but must operate in a generic manner, and thus become unwieldy.\rIn this work, we propose that support for a particular kind of text- based queries can let users perform faceted browsing and set- based browsing operations on such data sets with the ease and familiarity of conventional keyword search. For example, the text query “german car companies founders” can replace the actions of filtering all Freebase data by type to “company”, by industry to “car”, and by country to “Germany”, and then pivoting to those companies’ founders. From there, the user can perform faceted browsing actions to refine the already narrow collection further. We describe an algorithm for parsing these collection queries and demonstrate an implementation that works on Freebase.\rCategories and Subject Descriptors\rH.5.2 [User Interface]: Interaction Styles, Natural Language.\rGeneral Terms\rAlgorithms, Design, Human Factors, Languages.\rKeywords\rSearch, pidgin, faceted browsing, set-based browsing, graph data.\r1. INTRODUCTION\rThe faceted browsing paradigm has been very effective in letting casual users browse through large data sets by performing simple actions of picking suggested filters to apply. This paradigm remains effective as long as the schemas in the system are known a priori so that the interface can be configured based on the schemas. For example, an online retailer can be expected to know all types of product that it offers, as well as important aspects of each type of product (e.g., resolution for televisions, maximum\rzoom for cameras). Such domain knowledge helps configure the faceted browser, making it optimal for the domain in question.\rDomain knowledge is difficult to obtain and use in such a data set as Freebase in which new schemas are added over actual use rather than fixed at design time. Freebase currently contains some 3,000 types (e.g., company, book author) and over 30,000 properties (e.g., country where a company is founded, books written by an author). Users can add new topics to existing types or they can add entirely new types and properties. Providing a faceted browser over even just the stable types and properties is still a challenge for two reasons.\r• Consumer-facing faceted browsers typically have one over- arching type under which all data can be organized. For example, online retailers deal primarily with products; libraries deal with books. In Freebase, there is no over- arching type: users are as likely to search for companies as they are for book authors, or any one of the 3,000 types.\r• Types in existing faceted browsers are isolated. When users search for televisions in an online retailer, there’s little chance that they would be concerned with cameras at the same time. In contrast, on Freebase where types are highly interconnected, users might want collections defined by multiple types, such as “pharmaceutical companies funding republican politicians’ campaigns.” The more interconnected the types are, the more the potential ways to define collections, and the more facets the browser has to offer.\rThese challenges are inherent in any data set that resembles Freebase. That includes other comprehensive semantic web data sets such as Dbpedia, or even smaller, personal semantic web data sets accumulated by gathering tidbits from several data sources using something like Tabulator [3] or Piggy Bank [4].\rWe note that these challenges are acute at the beginning of any faceted browsing session when the collection to deal with is still large and/or heterogeneous. After applying a few filters, the collection gets small and homogeneous enough for faceted browsing to be effective again.\rWe propose that just when the user wants to search a large, interconnected data set, a particular class of text-based queries can be supported for filtering the data down to a manageable\rDavid F. Huynh\rMetaweb Technologies, Inc.\r631 Howard Street, Suite 400 San Francisco, CA 94105\rdavid@metaweb.com\rFigure 1. As the user types a query into the Freebase Cubed suggest widget, the query is interpreted and the interpretations are shown in a drop-down menu. When an interpretation is hovered using the mouse or selected using the keyboard, a fly-out appears to show the interpretation’s details.\rPage 66 of 122\r\ncollection. These collection queries have a syntax that maps to faceted browsing operations (filtering) and set-based browsing operations (pivoting), and are sufficiently natural for casual users. An example is “german car companies founders”, which maps to filtering by type to company, by industry to car, by country to German, and then pivoting from those companies to their founders. Other examples include:\r• frank wright buildings\r• female african american politicians • kate winslet drama films directors\rWe observe that some existing web searches already follow this syntax. Issuing such a query today performs a keyword text search, whereas we propose that it be translated to and performed as a structured query.\rWe describe an algorithm for parsing collection queries, demonstrate a prototype called “Freebase Cubed,” and show how, as a simple search textbox, it fits into more places than faceted browsing UIs can (Figure 1). We plan to use this widget on Freebase.com and in Freebase-powered web applications to introduce users early to collections and surface the richness of the data within Freebase even in places where full-blown faceted browsing UIs cannot be afforded.\r2. GENERALDEMAND\rWhile Freebase as an enormous, richly-interconnected data source needs something more than the conventional faceted browsing paradigm to be accessible to the general public, we wanted to further determine if that solution will also benefit the Web itself, and the Data Web to come. To know if web users already issue web searches resembling collection queries, we conducted a preliminary investigation over web searches relevant to the types in Freebase using Google Insights [1] and Google Suggest API [2]. For each of the few hundred most populated types in Freebase, we pluralized its name (e.g., /government/polician to “policians”) and submitted that text to Google Insights, from which we could download a table of web searches that Google deemed related to that text (e.g., “indian politicians”, “female politicians”). For each of those related web searches, we ran it through the Google Suggest API to retrieve its search volume. While there is no official documentation for the Google Suggest API, developers on the Web have assumed that the “num_queries” values that it returns are search volumes. For example, Google Suggest API returns 102 million “num_queries” for “black politicians”. If this assumption holds, our investigation indicates sizeable search volumes for web searches related to type names in Freebase, and many of these web searches do take the form of collection queries.\rFrom this investigation, we contend that these non-negligible search volumes on what we consider to be collection queries are evidence that some web users already want to search for information on collections (e.g., “black politicians”) rather than single, specific topics (e.g., “barack obama”). Not only does this early result signify demand, but it also suggests that if we were able to support collection queries, then users would be ready to use them without much learning.\r3. INTEGRATION USE CASES\rIn addition to the need to make Freebase browse-able and the general demand for supporting collection queries, we also have\ranother family of use cases in mind. Freebase is a rich source of data that can be used to augment web sites. For example, a prolific political blogger might wish that her readers can easily browse through or search her blog posts not just by the names of politicians mentioned, but also by the politicians’ attributes, such as their parties, their religions, their states, etc., which might or might not be mentioned explicitly in each post. All such data is or can be maintained in Freebase, and if each blog post carries Freebase identifiers of politicians, or any other topic that it also mentions, then compelling search and browse interfaces can be built on top of the two data sources: the blog’s index of topic identifiers for each post, and Freebase.\rDepending on each particular integration scenario, there might or might not be enough screen real estate for a full-blown faceted browser. There might be just enough room for a search text field, but a desire to support semantically rich searches nevertheless. Even if there is room for a faceted browser, configuring the browser any way would not leverage all the data in Freebase. So while the blogger might anticipate the need for facets such as “political party,” “state,” and “religion,” the reader looking for “movie actor politicians” won’t be satisfied by those facets. A more free-form mechanism is needed, and supporting text-based collection queries is one possible answer.\rSupporting collection queries in a particular embedding scenario has different requirements than on the Freebase site. Specifically, we want to support shortcuts such that, say, on a book review blog, the user can just type “african american authors” rather than “african american authors’ books” to search for books by African American authors.\r4. ALGORITHM\rA text query might be targeting a single topic or a collection of topics. Thus, given a text query, we would need to make either single topic suggestions or collection suggestions, or both. In order to make collection suggestions, we need to interpret the query as a collection query; this is discussed in 4.1. In order to suggest some combination of single topic suggestions and collection suggestions, we need an overarching algorithm for generating such combination; this is discussed in 4.2.\rThe entire discussion is posed in the context of Freebase, but it should be applicable to any similar heterogeneous graph data set. The natural language in which text queries are posed is assumed to be English. The generic prototype called Freebase Cubed is accessible at http://cubed.freebaseapps.com/, and a book embedding demo is available at http://cubed.freebaseapps.com/ embed-books.\r4.1 CollectionQueryInterpretation\rA text query is a collection query when, by our definition, it can be translated to a sequence of filtering and/or pivoting operations. Assuming that a text query is a collection query, we interpret it in three phases.\r4.1.1 Chunking\rFirst, we decide how the text query should be broken down (chunked). For example, “robert redford drama films” can be chunked in many ways, e.g.,\r• robert + redford drama films\rPage 67 of 122\r\n• robert redford + drama films\r• robert redford drama + films\r• robert + redford + drama films • robert redford + drama + films\rIn each chunking solution, each chunk is supposed to correspond to a topic (such as /en/robert_redford), or a type (e.g., /film/film), or a property (e.g., /film/film/directed_by). To determine what each chunk matches, we query for the chunk’s text against Freebase’s text search service. Several search matches are kept per chunk, but the best match dictates the chunk’s form (type, property, or topic). Any chunk whose text is a plural noun (e.g., “films” rather than “film”) is biased to type or property form more than topic form. Demonyms (e.g., Canadian) are resolved to their corresponding countries (e.g., /en/canada).\rChunking solutions can be ordered by how well the chunks in each solution match against data and schema in Freebase. They are fed in that order into the next phase.\r4.1.2 Seeding\rGiven a chunking solution which consists of an ordered list of chunks, this phase picks one of those chunks to be the starting point from which filtering and pivoting operations are applied. There are two types of seed:\r• seed collection defined by a type chunk, such as “authors” in “french authors’ books”\r• seed topic defined by a topic chunk, such as “jfk” in “jfk’s children”.\rAs there are substantially fewer types than topics, a type match in the chunking phase is much less ambiguous than a topic match. Thus, as a rule of thumb in picking seeds, we favor type-form chunks over topic-form chunks.\rA chunking solution plus the choice of a seed form a seeding solution. A structured query called the seed query is formulated to represent the seed collection or the seed topic.\r4.1.3 Growing\rGiven a seeding solution, this next phase interprets the rest of the chunks, one by one, as filtering and pivoting operations. For example, having chunked “french authors’ books” into french + authors + books and picked the “authors” chunk as the seed collection, this phase interprets the “french” chunk as a filtering operation (filtering by nationality), and the “books” chunk as a pivoting operation (pivoting from authors to their works).\rThe chunk immediately to the left of the already interpreted chunks is considered first, and then the chunk to the right, in order to favor the adjective-noun ordering of English. For example, starting from the seed “films” in the collection query “drama films actors,” we consider filtering the films by genre first before considering to pivot to the films’ actors. If the user phases the query as “films drama actors,” we can still interpret it.\rType-form chunks and property-form chunks are interpreted as pivoting operations, and topic chunks are interpreted as filtering operations. Applying such an operation means extending the current structured query either by adding a constraint for filtering or by wrapping the current query as a nested query for pivoting.\rThe current structured query denotes a single topic of some types or a collection of topics sharing some common types. The chunk to be considered for growing that query is also associated with one or more types. For example, in the query “drama films actors,” the chunk “drama” is typed /film/genre, and the chunk “actors” is typed /film/actor. The seed collection “films” is typed /film/film. To grow the seed collection with the chunk “drama”, we retrieve properties that point from type /film/film to /film/genre. There is only one such property: /film/film/genre. Next, to grow the collection of drama films with the chunk “actors”, we retrieve properties that point from type /film/film to /film/actor, and we get /film/film/starring.\rThere are cases where we get more than one connecting property. Consider the query “robert redford films” in which “robert redford” is typed /film/actor, /film/director, and /film/producer. In growing the seed collection “films” with the chunk “robert redford”, we get three different connecting properties: /film/film/starring, /film/film/directed_by, and /film/film/produced_by. These lead to three final interpretations of the query which are shown as three suggestions (Figure 1); and the user is asked to select one.\rNote that for each chunk, the chunking phase keeps several search matches. These are useful when the chunk’s best match depends on other chunks in the text query. For example, in the query “apple products”, the universal best match of “apple” is the fruit, but in the context of “products”, the best match is Apple Inc. the company.\rWhile we typically grow the structured query one chunk at a time, when we encounter a property-form chunk, we can use it to qualify the next immediate chunk, and in doing so, grow the query by two chunks in one shot. For example, in the query “robert redford directed films,” the chunk “directed” matches the property /film/film/directed_by, which we use to qualify the connection between “films” and “robert redford”. This leads to a single interpretation (rather than three previously).\rGiven a seeding solution, when all chunks have been used up in growing the seed, we have one possible complete interpretation of the original text query, which can be expressed as a structured query. A single seeding solution can yield several interpretations.\r4.1.4 Decision Tree and Greedy Implementation\rAll three phases—chunking, seeding, and growing—involve many decisions to make, each of which has many possible solutions. The whole process can be viewed as a decision tree in which the leaves are the complete interpretations of the original text query at the root of the tree. In our prototypical implementation, this decision tree is traversed depth-first, and at each node, branches are ordered by local scores, yielding fast but greedy performance.\r4.1.5 Pseudo-types\rImplicit in our discussion so far is the assumption that a user’s notion of a general collection of topics (e.g., “films”) is modeled as a type in Freebase (/film/film). This is not always true. For example, we might expect “volcano” to correspond to a type in Freebase, but it does not. Rather, it corresponds to a kind of mountain, and “mountain” corresponds to the type /geography/mountain. Thus, “volcanoes” is translated to a structured query for topics of type /geography/mountain and having /geography/mountain/mountain_type /en/volcano. This is\rPage 68 of 122\r\nour concept of pseudo-types, which bridge the gap between the user’s notion of types and the actual types in Freebase.\r4.2 Unified Suggest Widget\rThe previous sub-section discusses the core algorithm for interpreting a collection query. In real use, the user might enter a single topic query, or, as discussed in section 3, might enter an abbreviated collection query. In order to accommodate as all three kinds of query, we need an overarching algorithm, which consists of many more phases as discussed below.\r4.2.1 Unifying\rGiven a text query, we submit it as-is to the Freebase search service and retain some top results based on some threshold criteria. Next, we try to match the query against past collection queries that other users have issued. For example, when the user just types “french,” we can get the partial matches “french authors,” “french wines,” etc. These partial matches both save the user from typing as well as hint the user of our novel collection query support. If all of these matches are partial, then we interpret the query using the core algorithm discussed in 4.1. The output of this phase is a list of zero or more single topic search matches, zero or more partial matches of past collection queries, and zero or more interpretations of the text query as a collection query.\r4.2.2 Extending\rTo support collection query in an embedding scenario within a specific context, as discussed in section 3, we also need to understand abbreviated queries. The suggest widget can be configured to give hints about which types to expect from a query, and if it is abbreviated, how to generate a full query from that. For example, in a book embedding scenario, full queries should be of type /book/written_work, and abbreviated queries can be of type /book/author or /book/literary_genre. Knowing these types helps us quickly find appropriate partial matches from past queries in the unifying phase.\r4.2.3 Condensing and Approving\rThis phase eliminates duplicate interpretations from the previous phase as well as interpretations that resolve to empty collections (due to lack of data in Freebase or in the real world).\r4.2.4 Explaining\rThis last phase generates natural language text explaining each interpretation back to the user. For example, the text query “robert redford films” can be interpreted in three different ways, and explained back to the user as “films starring Robert Redford,” “films directed by Robert Redford,” and “films produced by Robert Redford.” The algorithm for generating a textual explanation from the structured query of an interpretation is complicated and not yet sufficiently fleshed out to be explained here.\r5. RELATEDWORK\rKaufmann’s doctoral thesis [5], which investigates natural language interfaces to semantic web data, is a highly related body\rof work. One of the systems she built, NLP-Reduce, translates text-based natural language queries into structured queries also using a pattern-matching approach like ours. NLP-Reduce is even more forgiving in that it removes even more stop words and allows for full questions or fuller question fragments. But whereas NLP-Reduce is aimed to address generic questions (e.g., “how big are the lakes in Illinois?”), our work aims to only retrieve collections of topics. Our focus allows us to make assumptions about shortcuts that users would tend to make, particularly that they would phrase queries as lists of keywords that map to filtering and pivoting operations. We believe that this is closer to how web users use existing search engines. Furthermore, we are unable to verify how well NLP-Reduce would work on a large and heterogeneous data set as Freebase. Kaufmann’s systems have only been evaluated on 3 data sets, each having no more than 10 types, 20 properties, and 10,000 topics (instances). On the other hand, Freebase has 3,000 types, 30,000 properties, and almost 9 million topics. This difference in magnitude should have implications on both data processing performance as well as the effectiveness of heuristics: large, heterogeneous data sets tend to have more name collisions, and the more flexible the query is allowed to be, the more explosive the number of interpretations there are.\r6. FUTUREWORK\rWhile we contend in section 2 that some web users already formulate collection queries, it is not clear how they would react to precise collections as search results as opposed to million fuzzy keyword matches that existing search engines return. It is also not clear if suggestions from partial matches against past collection queries are enough to make web users aware of this new capability and confident to formulate new, similar collection queries themselves. If they are enough, then we can prime the past collection query index with pre-canned queries generated from some typical patterns such as “<nationality> <profession>”. Finally, the Freebase Cubed suggest widget requires some UI iterations and usability testing to make sure that users understand the difference between single topic suggestions and collection suggestions. Those are some of the research tasks to be done next.\r7. REFERENCES\r[1] Google Insights. http://www.google.com/insights/search/.\r[2] Google Suggest API.\r[3] Berners-Lee, T., Y. Chen, L. Chilton, D. Connolly, R. Dhanaraj, J. Hollenbach, A. Lerer, and D. Sheets. Tabulator: Exploring and Analyzing Linked Data on the Semantic Web. ISWC, 2006.\r[4] Huynh, D., S. Mazzocchi, and D. Karger. Piggy Bank: Experiencing the Semantic Web inside Your Web Browser. ISWC 2005.\r[5] Kaufmann, E. Talking to the Semantic Web? Natural Language Query Interfaces for Casual End-users. Doctoral thesis, 2008.\rPage 69 of 122\r\nSystem Controlled Assistance for Improving Search Performance\rABSTRACT\rThis position paper outlines the concept of system assistance as a method to improve searching performance. I present an investigation concerning the effects of user-controlled versus system-controlled assistance on searching performance using a within subjects, counterbalanced empirical evaluation. Forty-three subjects interacted with two fully functional, information retrieval systems offering searching assistance based on implicit feedback. The systems were identical in all respects except that one offered searching assistance via a help link, and the other offered system- controlled support at specified points during the search progress based on patterns of searcher interactions. The evaluation used the W2G Text REtrieval Conference document collection with six topics. Research results indicate that offering system-controlled assistance based on patterns of implicit feedback can improve searching performance based on user selected relevant documents, with an approximately 30% performance increase overall. I discuss the implications for the design of future searching systems with assistance that is based on user implicit feedback patterns.\rCategories and Subject Descriptors\rH.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—query formulation, search process; H.3.4 [Information Storage and Retrieval]: Systems and software— performance evaluation (efficiency and effectiveness)\rGeneral Terms\rDesign, Experimentation, Human Factors, Verification.\rKeywords\rSearching assistance, explanations, implicit feedback, personalization, searching systems, user evaluation\r1. INTRODUCTION\rThe improvement of searching systems is an active research area, with the aim of addressing some of the issues users have when interacting with these systems [6, 8]. Searcher issues include finding appropriate query terms, retrieving too many results, not retrieving enough results, and retrieving zero results [8], among many others. These issues are especially pronounced in searching\rcontexts where users lack the domain knowledge or contextual awareness to use effectively the searching system. These types of searches are typified by user uncertainty about the information need, the content space, or the search engine’s capabilities.\rTo address these situations, contextual help and similar information retrieval (IR) systems attempt to aid the searcher during the search process by either executing search tactics for or offering assistance to the user in order to help in locating relevant information. These systems usually rely on implicit feedback. Ingwersen and Belkin [2] highlight that IR systems will increasingly depend on implicit interactions in order to improve IR performance.\rHowever, there has been little empirical evaluation of whether or not this searching assistance is beneficial to users during the search process. Is this assistance helpful? If so, when is helpful? When do searchers desire assistance? The research results presented in this article address these questions. We specifically examine whether system-controlled searching assistance is beneficial to users during the search process.\rThe paper begins with a short review of literature concerning IR systems offering searching assistance. I then provide a description of the two applications we developed and utilized in this research. Next, I discuss the empirical study we conducted to evaluate the effectiveness of system directed assistance on searching performance. The paper then presents the results of an analysis, draws implications for searching system design, and then discusses directions for future research.\r2. RESEARCH QUESTIONS\rThis research is a user study utilizing a searching assistance application developed to investigate some of these issues. The research question is Does system-controlled searching assistance based on patterns of implicit feedback improve performance during the search process?\rThe research hypothesis: There is a significant increase in searching performance when using system-controlled searching assistance compared to a system offering user-controlled assistance during the search process, as measured by the number of relevant documents that the user selects during a session.\rI measure the number of documents that the user selects as relevant from all documents retrieved in a session. A session is defined as: an episode of a searcher using a searching system during which a series of interactions occur between the searcher and system. I considered a document relevant based on implicit feedback from the user and explicit qualitative data indicating relevance. For example, during the experiment, a searcher may bookmark a document and state that the document was relevant to the information need.\rBernard J. Jansen\rCollege of Information Sciences and Technology The Pennsylvania State University University Park, Pennsylvania 16802 jjansen@acm.org\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rPage 70 of 122\r\n3. SYSTEMDEVELOPMENT\rFor this research, I used a client-side software application developed, possessing a suite of searching assistance features. Our application development goal was that the component would rely on implicit feedback, gathering information solely from normal user – system interactions occurring during the search process.\rThe paper presents a brief overview of the system (shown in Figure 1), with two earlier versions of the application presented in [3, 4]. These earlier applications were solely user-controlled. User evaluations of these applications pointed to the need for more system control of assistance during the searching process, resulting in the research presented here.\r3.1 SystemDesign\rWhen a session begins, the application monitors the searcher via a wrapper to the browser. When the application detects a valid action, it records that action and the specific object receiving the action. For example, if a searcher was viewing a Web page and bookmarked it, the application would record this as (bookmark Web page). The application then associates appropriate search assistance for the user based on the particular action and the system’s analysis of the object. In this example, the system would offer the searcher relevance feedback terms from Web page. The more the system records and integrates these (a, o) pairs, the more complex could be the model of the information need. The application currently monitors the searcher’s interactions with the system, tracking implicit feedback actions. The application logs actions of bookmark, copy, email, print, scroll, save, execute (i.e., submit and click) and view (without scrolling).\r3.2 AutomatedAssistance\rThe application monitors the session for one of the implicit feedback actions and associated objects using a browser wrapper. When the system detects a valid action (i.e., (a, o) pair), it records the action and the specified object receiving the action. The system then offers appropriate search assistance to the user based on the particular action, the pattern of previous interaction during the session (for the system-controlled version of the application), and the system’s analysis of the object. The current searching assistance features of the application are:\r1. Managing Results: The automated assistance application provides suggestions to improve the query in order to either increase or decrease the number of results using query operators.\r2. Query Refinement: The system uses the Microsoft Office thesaurus to suggest other query terms, but the application can utilize any online thesaurus via an application program interface.\r3. Query Reformulation: The system displays similar queries from prior users based on number of previous submissions and terms in the current query.\r4. Relevance Feedback: The system implements a version of relevance feedback using terms from a user selected document or passage object. The system provides suggested terms from the document that the user may want to implement in a follow-on query.\r5.Spelling: The system offers spelling suggestion using the system’s online dictionary, Microsoft Office Dictionary, although it can access any online dictionary via the appropriate API.\rFor the user study, there were two versions of the system. In the user-controlled version of the system, the Assistance module displayed searching assistance whenever it was available via a “View Help” button. In version two of the system, the Assistance module displayed assistance only when the Pattern Recognition module detected pre-set patterns.\r3.3 PatternRecognition\rThe Pattern Recognition module (see Figure 1) was based on prior work [3], where we conducted exploratory sequential data analysis of users interacting with searching. The analysis was conducted to determine when in the search process users desire system intervention. Using transaction logs, videotapes, and lab notes from this study [3], I coded the user – system interactions for each subject. Once I had coded all interactions, I sequentially ordered these interactions (i.e., states) for each searcher.\rFrom these findings, a module was developed to monitor patterns and interject assistance only at certain points in the search process in order to reduce task interruption. This Pattern Recognition module accepts implicit feedback data from the Tracking module, storing (a, o) pairs and implicit feedback actions.\rWhen the Pattern Recognition module identifies pre-coded implicit feedback patterns, it passes the (a, o) pairs to the appropriate module and alerts the Assistance module to display the assistance. The current preset patterns and are:\r  Execute Query – View Results Page (with scrolling or without scrolling).\r  Implicit Relevance action (i.e., bookmark, copy, print, save) – Navigation (of the browser).\r  Implement automated assistance – View Results Page.\rIf the Pattern module does not detect these conditions, then the system does not display any searching assistance. The image is of the assistance that is automatically displayed when the application detects a set pattern.\r4. USERSTUDY\rWe used two systems in this evaluation that were identical in all respects, except that one offers system-controlled searching assistance, and the other user-controlled assistance. The backend searching systems used for the empirical study were Microsoft’s Internet Information Service (IIS). The IIS systems were running on an IBM-compatible platform using the Windows XP operating system and Microsoft Internet Explorer as the systems’ interfaces.\rFor the system-controlled searching assistance, we integrated the assistance application via a wrapper to the Internet Explorer browser. For the user-controlled system, we used a duplicate automated assistance application with the Pattern Recognition module disabled so that the system would display the searching assistance in the browser whenever available.\rPage 71 of 122\r\nBrowserInterface\rAssistance M odules\rA s s is t a n c e\rSimilarQueries\rQueryTerms\rRelevance Feedback\rReformulation\r(a, o) pairs\rRefinement\rTracking\rPattern Recognition\rFigure 1. Searching Assistance Modules and Information Flow with the Browser Interface.\r4.1 Pre-StudyMeasures\rThe subjects for the evaluation were 43 college students attending a major U.S. university. All were familiar with the use of Web search engines. Most of the students were studying information science and technology or other aspects of engineering. So, our sample has an understanding of computers and information technology. The subjects were given no additional training on the searching systems, a pre-evaluation demographic survey was administered, used previously [3].\r4.2 DocumentCollectionandTopics\rThe study used the W2G Text REtrieval Conference (TREC) document collection with six topics. We parsed the aggregate files into their individual component documents. The TREC collection is a standard test collection for online search systems (see http://trec.nist.gov/ for more information). The test collection after parsing contained approximately 200,000 documents. Each TREC collection comes with a set of topics for which there are relevant documents in the collection. The six topics we used for this study were: Behavioral genetics, Tropical Storms, Quilts being used to generate income, Robotic technology, Estonia economic issues, and Super critical fluids. In the results reported here, we were interested in the documents that the users selected as relevant, so we did not utilized the TREC relevant judgments.\r4.3 ExperimentalSet-up\rAt the start of the study, I provided each of the subjects a short statement instructing them to search on a given topic in order to prepare a report, which is in line with the definition of relevance judgments for the TREC documents. The subjects had fifteen minutes on each system to find as many relevant documents as possible. We determined the length of the search session based on reported measures of the typical Web search session [5].\rThe subjects were notified that the systems contained an automatic feature to assist them while they were searching. When the subjects utilized the user-controlled system, they were shown\ra screen capture of the assistance button and an example of the assistance shown if they clicked the button. They were instructed that they could access the assistance by clicking the button, or they could ignore the offer of assistance with no detrimental effect on the system. When the subject used the system-controlled assistance, we showed them a screen capture of an example of the displayed assistance. Again, the subjects were instructed that they could view the assistance or ignore the assistance with no detrimental effect on the system.\rFor the searching sessions, each of the subjects was given one of the six search topics, read the one paragraph explanation provided with the TREC collection, and then afforded the written explanation to them. They were asked to search as when they normally conduct online research, taking whatever actions they usually take when locating documents of interest online. In this respect, I adhered to recommendations to place the searching need within a scenario [1, 7].\rAll subjects used both systems, searching on a different topic on each system. The systems were counterbalanced to ensure that an equal number of subjects searched first on each system. The topics were also rotated after every sixth subject to ensure topic order did not introduce learning effects that would bias the searching performance.\r5. RESULTS\rThe following sections present the results of the empirical evaluation. The hypotheses were evaluated by performing a paired t-test using the number of relevant documents identified by the study participants during their sessions on each system. There was a significant difference in performance between the two systems (t = 2.553, p < 0.01, df=42). Therefore, we fail to reject the hypothesis; there is a statistically significant performance improvement with a system-controlled assistance searching system. Table 1 displays the number of relevance documents identified by participants using the user-controlled assistance and the system-controlled assistance.\rPage 72 of 122\rAssistance\r\nThere were thirty subjects (70%) who located more relevant documents using system-controlled assistance compared to 9 (21%) who preformed better on the system with user-controlled assistance. Four subjects (9%) located the same number of relevant documents on both systems.\rTable 1. Identification of Relevant Documents.\rThe limitations of the study are the use of the TREC topics and scenarios, which may not reflect the difficulty level of many searching tasks. However, these tasks are certainly reflective of many challenging information needs such as exploratory searching and competitive intelligence. Another limitation is the requirement of the participants to locate as many relevant documents as possible, which is not reflective of the retrieval goal of other searching tasks such as home page or fact finding. However, this is a common searching goal for reports, market research, health issue, or other tasks where the searcher wants to become informed on a topic.\r7. CONCLUSION\rThe results of the research conducted so far are very promising. In this paper, we present the design of a general purpose system- controlled searching assistance application that uses implicit feedback to provide recommendations on searching tactics. We evaluated the use of user-controlled versus system-controlled assistance with real users in order to measure the performance benefit of system-controlled searching assistance. This assistance was based on patterns of implicit feedback.\r8. REFERENCES\r[1] Borlund, P. and Ingwersen, P. The development of a method for the evaluation of interactive information retrieval systems. Journal of Documentation, 53 (5). 225-250.\r[2] Ingwersen, P. and Belkin, N. Information Retrieval in Context - IRiX. SIGIR Forum, 38 (2). 50-52.\r[3] Jansen, B.J. and McNeese, M.D. Evaluating the Effectiveness of and Patterns of Interactions with Automated Searching Assistance. Journal of the American Society for Information Science and Technology.\r[4] Jansen, B.J. and Pooch, U. Assisting the Searcher: Utilizing Software Agents for Web Search Systems. Internet Research - Electronic Networking Applications and Policy, 14 (1). 19- 33.\r[5] Jansen, B.J. and Spink, A., An Analysis of Web Information Seeking and Use: Documents Retrieved Versus Documents Viewed. in 4th International Conference on Internet Computing, (Las Vegas, Nevada, 2003), 65-69.\r[6] Jansen, B.J., Spink, A. and Saracevic, T., Failure Analysis in Query Construction: Data and Analysis from a Large Sample of Web Queries. in 3rd ACM Conference on Digital Libraries, (Pittsburgh, PA, 1998), 289-290.\r[7] Rosson, M.B. and Carroll, J.M. Usability Engineering: Scenario-Based Development of Human-Computer Interaction. Morgan Kaufmann, New York, 2002.\r[8] Yee, M. System Design and Cataloging Meet the User: User Interfaces to Online Public Access Catalogs. Journal of the American Society for Information Science, 42 (2). 78-98.\rUser- Controlled\rSystem- Controlled\rRelevant Documents From All Users\r175\r227\rMean Number of Relevant Documents\r4.07\r5.28\rStandard Deviation\r2.87\r3.72\rSubjects Locating More Relevant Documents on System with Automated Assistance\r30\r70%\rSubjects Locating More Relevant Documents on System without Automated Assistance\r9\r21%\rSubjects Locating Same Number of Relevant Documents on Both Systems\r4\r9%\rTotal\r43\r100%\r6. DISCUSSION\rIn 70% of the cases (30 subjects), searchers on the system with system-controlled searching assistance performed better than on the system user-controlled searching assistance. This is especially noteworthy since the assistance was based totally on implicit feedback, which is not as exact as explicit feedback. However, the Web is a natural environment for the use of implicit methods, and our results indicate that it is a worthwhile area to pursue.\rHowever, there were also 30% of the users that were not helped by or performed worse on the system-controlled searching assistance system. There were 9 searchers who performed worse, and 4 searchers who performed the same on both systems. This would indicate that one might not be able to apply system- controlled assistance techniques wholesale and still achieve maximum outcome. Rather, a more individualized and targeted approach within the context of the searching process may be worthwhile.\rThis also indicates that individual differences are also likely active when it comes to utilizing and accepting searching assistance to improve performance, as is the case for other human- computer interface applications.\rPage 73 of 122\r\nDesigning for Enterprise Search in a Global Organization\rMaria Johansson Findwise AB\rDrottninggatan 5\r411 14 Gothenburg, Sweden maria.johansson@findwise.se\rABSTRACT\rEnterprise Search is used by organizations to capitalize on their internal knowledge by providing quick access to all internal information, helping users re-finding and discovering new information, as well as creating the necessary conditions for collaboration across organizational and geographical boundaries. In this large organization a search application was created to meet these goals. This paper focuses on the main design concepts of the second release of the search application, and how these were affected by experiences gained throughout the project. This design focused on simplicity and discoverability. Preliminary results show that the design is usable and that users find it easier to find the information they are looking for. A general increase in user satisfaction is also established.\rCategories and Subject Descriptors\rH5.2 [Information interfaces and presentation]: User Interfaces\rGeneral Terms\rMeasurement, Design, Experimentation,\rKeywords\rEnterprise Search, Faceted Search, usage testing\r1. INTRODUCTION\rEvery business day, employees need to access information stored in various enterprise applications and databases. Employees want one entrance to all corporate information. They often perceive the company intranet as one fuzzy cloud of information, while in reality it is a set of highly isolated information silos. Enterprise search is meant to address this need by providing access to relevant information and by consolidating ranking and presenting it properly. But how does one achieve this? The larger the organization the more divergent the information access needs.\rUsers within this large global organization have very different needs when it comes to finding information. Marketing employees complain that too much technical nonsense is embedded in the search results, while technical users say they are lacking technical depth in the available material. They all want access to all existing information in an environment where security is highly prioritized and information access strongly restricted. Quotes like these are common:\r“We want to search in all information.”\r“Google can search the whole internet so why can’t we search our own intranet?”\r“Why can’t our intranet be like Google?”\rLina Westerling Findwise AB\rSveavägen 31\r111 34 Stockholm, Sweden lina.westerling@findwise.se\rBut one user also said this:\r“It is a lot harder to find that one exact thing that you are looking for than finding loads of general information on a subject on Google.”\rAnd she was right. If a user cannot express what she is looking for in a good way it is much easier to find a lot of general information about the subject than that one document she read once (but forget what it was and who wrote it). Users need help defining what they are looking for. So how does one go about creating a useful enterprise search application?\r2. COMMUNICATION\rThe first step the project team took was to assemble a group of people that were interested in enterprise search. These people included:\r• A steering committee, and a group of stakeholders that provided the project team with valuable input.\r• Pilot testers who took part in workshops and interviews before the launch of the first pilot and then evaluated the new pilot releases of the search application. These people also often acted as search ambassadors; spreading the word about the project to their colleagues.\r• Beta testers, who took part in a beta test prior to the first launch of the application. The word about the new search application spread through the other three groups without much effort from the project team. Thanks to some communication material and the involvement of these groups approximately 1500 employees took part in the beta test prior to the launch.\rA large survey was also conducted within the company with the purpose of collecting information about the state of Enterprise Search. After the first release of the search application a follow up survey was conducted in order to measure the results of the newly released application. This information was combined with data from the search logs to analyze the search behaviors and information needs of the workers within the organization and resulted in a list of prioritized areas that needed further improvement. Usability was one of those areas. So how would you go about designing a usable enterprise search application for a global organization?\r3. DESIGNINGFORENTERPRISE\rSEARCH\rDesigning for enterprise search is challenging work that involves packaging complex functionality in an easy-to-use interface. Enterprise search is a means for companies to capitalize on their organizational knowledge. This can be done by helping users:\rPage 74 of 122\r1\r\n• Speed up their everyday tasks\r• Discover new information\r• Finding accurate information e.g. information they\rknow they can rely on.\r• Re-finding information they know exist, but cannot find\r• Improving the opportunity for collaboration and sharing\rknowledge within the organization\r• Find information that is relevant for them in their\rcontext or work.\rHow does this correlate to users desires for a Googlified intranet? It is not a coincidence that the verb “to google” has been added to several renowned dictionaries, such as those from Oxford and Merriam-Webster. Search has been the de facto gateway to the Web for some years now. And the users say: “Give us something like Google or better.” This is what we chose to call the Google effect on user expectations.\rDue to the highly complex information needs in the organization the project team initially had a vision to create the “one application to rule them all”, the Enterprise Search Application. The application should be easily converted into a desktop application all users could have on their desktop. The search application would be the starting point for everything on the intranet and all users should go to the search application for information. But of course users already had their accustomed behavioral patterns and ways of finding information.\rThe first release of the new search application was a great improvement to what had been available earlier. Employees claimed that they were able to find the information they were looking for! But something still did not work. Many users found the application overwhelming and complicated to use. All the embedded functionality made it slow to load. Questions about being more like Google still came up.\r3.1 UsageTests\rThe design team decided to do a larger series of usage tests on the newly released application. And as suspected, users did not use the search application as the one starting point for all information. Instead users were showing behavior that followed that of the berry-picking model [1]. Search was only one way of finding information. And users did not separate different search applications from each other. The central search application was seen as the same as the old database search in the document management system; they were all search.\rThe importance of having a simple and graphically appealing\rinterface is well established within HCI literature [2]. Instead of\rthe one central powerful application, the design team decided to\rchange the design into a simple application that at first glance 3.4 resembles Google. Combine this application with services that\rmake it possible for other systems to use the search engine for searching within their own data, users can get quick access to “search” from where ever they are.\rSo how does one create an easy Google like application that still will meet all the complicated information needs of various user groups? The design team’s answer was to make the advanced functionality simple by hiding the complexity in plain sight. Thus creating an application that looked really simple, but with fast and easy access to more complex functionality.\r3.2 Speeding up Every Day Tasks\rQuery logs from the search application show that names of applications and products are over-represented in searches\rUsers need to find information that is relevant in their context of work. An example of this is the needs to search for products and the relationships between versions of products as well as the related documentation and support material. There were many different examples like these where a particular scenario applied for a specific user group. So the design needed to incorporate both the general and the specific.\rThe design incorporated a general view of information where everything was searchable. This would be ideal for looking up general administrative information such as information about parental leave or holidays, or finding the official public information about the products sold by the company. The general search view is also a good way of getting an overview of all the information available on a topic. It resembles a standard Google\rPage 75 of 122\rsuggesting that these are important everyday tasks for the users of the system. These queries are considered as navigational searches rather than informational [3]. Using the Pareto principle [6] a query suggestion list was compiled from items represented in the search logs. When a user first enters the application and starts typing a query she gets a list of suggestions for matching items. Clicking on one of the items directly takes the user to the application or product she was looking for. Since no search against the index is done this functionality not only speeds up everyday navigational searches but also saves performance. A user can search for travel expenses not knowing the name of the travel expense application and she will get a suggestion for the system and navigate to it by the click of a button.\rThe information compiled from the query logs was also used for the purpose of creating quick links for further aiding people in finding commonly requested information. The quick links are also used to make sure that users found accurate information. So if a user searches for a name of a product the official approved product page will show up on top above blog posts or research documents for that product.\r3.3 Refinding Information\rResearch shows that people are very likely to revisit information they have viewed in the past and to repeat queries that they have used before [4]. Refinding information is important because many users know that the information exists on the intranet; they just do not know where it is located. Users even have problems finding their own documents. Functionality helping users to refind their information include:\r• • •\r•\rSearching my items, a quick way for users to find documents they are working on.\rColleagues’ items, a quick way of searching for documents written by a user’s colleagues.\rPersonalized search views where a user can choose to search within her part of the intranet where she knows the information is applicable to her situation. This will also help users feel that the information they find is accurate and appropriate.\rBookmarkable URLs so users can save searches as bookmarks in the browser. A single search result can also be bookmarked from within the search application. Users can search within their bookmarked search results and the results are also marked with an icon in search results.\rPersonalization with Search Views\r2\r\nstyle results list but incorporating a few extra features such as types of search results and sources as well as a few standard facets metadata tailored to the type of search result, icons for different for quick filtering of search results.\rFigure 1. Overview of the search application. Note that the image is an example and does not portray the real application\rThe design also incorporates several different search views targeted for a specific user group, represented in the GUI as tabs. Users on a local branch in South America could search only within their part of the intranet, customer support could search within all support documentation as well as lessons learned from other projects from all over the world. A set of predefined views on the internal information was created for this purpose. Early adopters or department managers could also set up specific views on the information and in just a few seconds share them with their coworkers.\rThis provided search in all information for all but also fitted the personal needs of a large number of user groups. Users could find the information that they needed in their context of work. They could also easily share these views with their colleagues.\r3.5 PresentingSearchResults\rThe search results list is the essence of the search application. Presenting proper information about the results is essential for meeting the goals set for the application. The new results list in many ways looked like Google. But the design team wanted to find a way for users to discover the possibilities available with the new powerful search tool. The search results list was therefore\rfitted with an expanding function, where users with the click of a button could see more information about a search result. The expanded result included:\r• More metadata about results where some of the metadata were links. The user could then directly access the system, site or information about the author or a product. This increases discovery of new related information and also speeds up everyday tasks.\r• Icon displaying bookmarked results for easily refinding information.\r• Advanced ways to filter the search on the specific type of result, or search within a specific site or subset of information directly from the search result.\r• Links to related information and functionality within the different source systems such as approved versions of a document.\r• Preview of the contents of documents so users can have a look at the document and read content directly, without having to enter another application or download and open a specific program such as Microsoft Word or Adobe reader for pdfs.\rPage 76 of 122\r3\r\n3.6\r•\rInformation about related products, collaboration areas, abbreviations and definitions were also displayed above the result list to further aid the users quest for information. The related information helps users discover new information they did not know existed.\rImproving Opportunity for Collaboration\r• Include even more information sources to assure that the search application includes all necessary information sources.\r• Focus on contextualizing and facilitating local search. The search application needs to take into consideration the users geographic as well as organizational location, and also their role/business process in the organization in order to filter and rank results according to the users context.\r• Continue to focus on the usability and performance of the search application.\r• Further work on the communication about the new application is also needed to inform even more employees about the value of the new search application.\r5. ACKNOWLEDGMENTS\rOur thanks to all the people who took interest in our project and participated in interviews and usage testing to help improve the enterprise search application.\r6. REFERENCES\r[1] Bates, Marcia J. 1989 The Design of Browsing and Berry- picking Techniques for the Online Search Interface Online Review 13 (October 1989): 407-424.\r[2] Hearst Marti, A. 2009 Search User Interfaces. Cambridge University Press ISBN 9780521113793\r[3] Jansen, Bernard J. 2008 Determining the Informational, navigational and transactional intent of web queries. Information Processing and Management 44 (2008) 1251– 1266\r[4] W. Jones, S. Dumais, and H. Bruce. Once Found, What Then? A Study Of Keeping Behaviors In The Personal Use Of Web Information. Proceedings Of The American Society For Information Science And Technology, 39(1):391–402, 2002.\r[5] Spool Jared M. 1999 Web Site Usability: A designer Guide [6] http://en.wikipedia.org/wiki/Pareto_principle\rRelated information also included collaboration areas that match a users query thus helping users discover new communities or opportunities for collaboration that they might not have known existed. Collaboration was also aided simply by making the collaboration areas searchable in the search application. Search provided easy access to the collaboration areas, even for those who have not started using them yet.\rUsers can easily share searches through bookmarkable URL:s. They can also share their search views and customizations with their colleagues.\rThe result was an application that seemed very simple at first glance, but still included all the different functionality needed in order to fulfill the information needs of the organization’s different user groups. The new design was evaluated through usage test and though it included the same functionality as the old search application the results were completely different. Users found it not only easier to use but also easy to discover new information. They found it easier to determine whether a search result was interesting or matched what they were looking for, which minimizes the behavior of pogo-sticking [5]. The facets options that have not been understood or used previously were highly appreciated. All in all this confirms the importance of a simple and graphically appealing design.\r4. CONCLUSIONS\rThe search application described in this paper had an overall positive impact on findability for this company. 9 out of 10 users use it every week. But even though the search tool has improved a great deal, there is still room for further enhancement in order for the company to fully capitalize on the investment in Enterprise Search. The improvement areas include:\r• More functionality tailored to a specific scenario or user group in detail.\r• Embedding the search functionality in other IT systems.\rPage 77 of 122\r4\r\nCultural Differences in Information Behavior\rAnita Komlodi Department of Information Systems, UMBC\r1000 Hilltop Circle Baltimore, MD 21250\rUSA\r1 410 455 3212\rkomlodi@umbc.edu\rABSTRACT\rWith the availability of online translation services and the large amount of English-language content on the Web, more and more global users come in contact with content that was not created in their own language or culture. While some sites make efforts to localize their user interfaces and content, many simply translate content and use the same user interface. This is in direct contrast with findings that different cultures approach knowledge, information, and interaction with information in different ways. This paper will describe work in progress to study some national cultural differences in information behavior and the problems users face while interacting with information that was created in a language and culture different from their own.\rKeywords\rCross-cultural comparison, information behavior, user study\r1. INTRODUCTION\rThe World Wide Web has integrated into the everyday information behaviors of many users. Information seekers often turn to the Web for solutions related to problems ranging from everyday life to health issues and professional information problems. The availability of large amounts of content from developed countries on the Internet and faster and faster network speeds create a global user group for many websites. While access to native-language content is constantly improving, it is often the case that Web users interact with information that was not originally created in their language or cultures.\rMany websites and search services often simply provide translated content without localizing the user interfaces or the format of the information. As an example, Figures 1 & 2 represent the Google interface in the US and in Spain. While the language is different, the layout, organization, and the content of the screens remain very similar.\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rConference’04, Month 1–2, 2004, City, State, Country. Copyright 2004 ACM 1-58113-000-0/00/0004...$5.00.\rKaroly Hercegfi Department of Ergonomics and Psychology, Budapest University of Technology and Economics Egry Jozsef utca 1. H-1111 Budapest, Hungary\r36 1 463 2654\rhercegfi@erg.bme.hu\rFigure 1. The Google interface in the US.\rFigure 2. The Google interface in Spain.\rEdward Hall, a leading cultural anthropologist of the 20th century wrote that “One of the functions of culture is to provide a highly selective screen between man and the outside world. In its many forms, culture therefore designates what we pay attention to and what we ignore.” (Hall, 1976, p. 85) Culture can influence how we process and organize information and various cultural characteristics can influence many aspects of the information- seeking process (Komlodi & Carlin, 2004). This influence exists at the group and organizational levels as well; however, the focus of this paper is national culture.\rOne specific example is the case of search histories and different time concepts. The importance of search histories for search systems and user interfaces is often acknowledged (Hearst, 2009).\rPage 78 of 122\r\nIn Western cultures most search history displays reflect event in a linear, temporally ordered list (Twidale, 1998; Komlodi, 2007). These displays reflect the Western culture’s time concept where time is a linear, exhaustible resource (Hall, 1990). Hall (1990) defined monochromic and polychromic cultures. Monochromic cultures usually handle single tasks at a time organized in a linear fashion. Northern Europe, Northern America, Canada, and Australia fall into this group. In many other areas of the world, such as Latin America, Southern Europe, and Asia, time is a circular, renewable resource and people usually juggle multiple tasks at the same time (Hall, 1990). Do linear representations of task histories serve the needs of the users from polychronic cultures? Since most of this research, including user studies, design, and evaluation, originates in Western cultures, we do not have a good answer to this question.\rAnother example of cultural differences that impact humans’ interaction with information is that of categorization. Categorization reflects how we understand and make sense of the world and it is strongly impacted by the cultural and educational system we grow up in. Cultural differences in classification systems and categorization have long been described (Clemmensen et al., 2009). The impact of various categorization systems on website design is very strong, as the foundation of the design of any website is the underlying categorization of the information, the information architecture. Users who are not familiar with the traditions of information organization in the culture of the website can face difficulties while navigating the site.\rThus, the design of sites that provide information access to global user groups should consider these cultural differences. However, to understand the need for localization, we first need to understand the strongest culturally influenced differences in Web information seeking behavior.\r2. RELATEDRESEARCH\rComparisons of information-seeking behavior across cultures are limited. Studies in information science look at international user groups in Western cultures, such as international students (Park, 1997, Yi, 2007) or immigrants (Fisher et al., 2004). These studies take cultural differences into account, however, they examine groups that are in a sense bicultural or in the process of becoming bicultural by integrating into a new culture. They also often examine the specific needs of the group but do not necessarily compare them to another cultural group. These studies are often conducted as needs assessment efforts for library or other public information services and examine behavior in the context of these institutions.\rVery few studies looked at cross-cultural comparisons in information seeking outside of these needs studies in information science. However, the studies that took place found interesting differences. Iivonnen and White (2001) showed varying levels of cultural difference in information seeking on the Web among Finnish and American students. They focused on the choice for initial search strategies, not the full search process. Duncker (2002) examined the differences in searching via the online public access catalog of a library between indigenous and European New Zealanders and found significant differences in their interpretations and perceptions of the library. Duncker’s study provides an interesting insight into New Zealand’s Maori\rpopulation’s attitudes toward and relationship to information, more specifically information encoded and preserved in a library. Their notion of information sharing and capture are dramatically different from the library’s view of collection, organization, and preservation, which creates conflicts between the library and its Maori patrons. This study is an important example of the complexities of the impact of culture on a user’s attitudes toward and interactions with information.\rBoth of these studies attributed differences to various cultural differences: searching style, cognitive style, language use, perceptions of search systems (IIivonnen and White, 2001); and traditions of story telling and information sharing (Duncker, 2002). We will consider these intervening factors in our own study. Evers (2001) found differences in various culture groups’ search, navigation, and user interface understanding within one website while studying differences in the use of a virtual learning environment. She attributed these differences to a set of variations in cultural characteristics. Yan, Finn, and Lu’s (2007) studied international students at Virginia Tech but went a step further than other needs studies and specifically compared the American and international students’ information behavior at Virginia Tech. They found differences in several areas such as preferences for initial information channels, library use frequency, familiarity with library services, and use of instructional resources.\rInformation systems researchers also often examine cultural differences surrounding information technology in business settings. Studies look at the impact of both national and organizational culture. We will focus on those that study national culture. A 2003 review (Ford, et al., 2003) found that most studies examined information systems (IS) management, while IS development, operations, and usage were often ignored as topics of research. This review focused on studies that incorporated Hofstede’s (2001) ncultural dimensions. The category of IT usage studies is especially important for our study, as these describe user behavior related to various IT use areas, including information behavior. Leidner and Kayworth in 2006 found a different state of affairs when they extended their review to include all studies looking at cultural differences that presented new findings, and not just those using Hofstede’s (2001) dimensions. They identified 30 studies that examined either IT use or outcome, the category where cross-cultural studies of information behavior may occur. Eighteen of these studies involved national cultural variables. An overwhelming result of these studies is that national culture makes an important difference in IT use and outcomes. As Web-based information seeking is carried out in the context of a specific technology, these results forecast important differences for information- seeking tasks as well.\rSome of the studies more specifically involved tasks related to information seeking or other human-information interaction tasks, such as the larger domain of knowledge management. Chau et al. (2002) found that US users use the Internet mostly for information seeking, while their Hong Kong counterparts used it mostly for social communication and tied these finding to values related to the community versus individual achievement. Calhoun et al. (2002) found differences in how users in high and low context cultures process information. Hall (1990) defined high context cultures as those where much of the information content of a message in embedded in the context and not in the message itself.\rPage 79 of 122\r\nLow context cultures express more of the information content in the message itself and do not rely on the context for the interpretation of the message. In Calhoun et al.’s 2002 study, users from a high context culture (Korea) were more easily overwhelmed by the information provided by their IS than users from the low context culture of the US. These differences further imply that a systematic cross cultural study of Web information seeking behavior will identify important differences.\r3. PROPOSEDRESEARCH\rIn this study we will address two main research questions related to the problem described in the introduction: 1) Do users in two different cultures exhibit different information behaviors on the Web and do they report different information-seeking habits? If yes, which phases and types of information behaviors are the most strongly impacted by culture?, 2) How does information seeking in the users’ own language and for content created in their own culture differ from looking for information in a different language and created in a different culture? These broad research questions will drive an exploratory study of behavior to narrow down the impact of various user factors on search behavior.\rParticipants will be recruited in the US and Hungary. Participants with a significant knowledge of a second language will be recruited in order to allow for data collection answering the second research question. At the beginning of the session participants will be asked to fill out several questionnaires to collect data on the following independent variables: 1) a demographic questionnaire, including age, gender, cultural background, computer and Web experience, and Web information-seeking and use experience, 2) a cognitive style questionnaire, for example the Myers Briggs Type Indicator, 3) a cultural dimensions questionnaire, for example Hofstede’s (2001) questionnaire to establish the participant’s placement on various cultural dimensions.\rThe demographic questionnaire will help us confirm the homogeneity of our samples in terms of various demographic variables. The cognitive style questionnaire will inform our analysis by linking performance to cognitive style which has been shown to influence information seeking. In one of our previous studies (Hercegfi & Kiss, 2009), a specific aim of the series of experiments was to compare the behavior of users during solving information-seeking tasks. We were able to identify some significant differences between the behavior of users with different demographic backgrounds and cognitive styles. The new series of experiments will focus on the effects of a new dimension: the cross-cultural aspect. The application of the cultural dimension questionnaire will ensure that participants exhibit various characteristics typical of the groups they are members of.\rNext, the participants will be asked to carry out several information seeking tasks. Some of the tasks will be prescribed for them, while others will be defined by the participants. The prescribed tasks will be representative of various information- seeking task types, including: known item seeking, subject driven and exhaustive searches. There will be no starting points defined for the tasks, as both Iivonen and White (2001) and Liao et al., (2007) found that there were strong cultural differences in terms of search starting points. At the end of the session the participants\rwill be interviewed about their information-seeking experience and habits.\rObjective parameters of the users’ behavior during the session will be recorded, including their computer activity log, physiological data such as hear rate and skin conductance, and eye gaze movements. From the activity log we will record the following data: starting points, time spent on pages and steps, types of steps, number of steps, sequence of steps, and number of search results. We will use the physiological data to identify high mental effort steps (Hercegfi et al., 2009) and emotional reactions (Hercegfi et al., 2009). Both of these will supplement the analysis of the video capture of the participant’s facial expressions and body postures. The eye gaze data will help us identify lower-level steps in the users’ activities and hotspots on the pages that were particularly popular.\r4. CONCLUSIONS\rPrevious studies have shown national cultural variations in information seeking and use behavior in various contexts. Most studies either looked at the needs of a specific user group, studied only a part of the information seeking and use process, or examined information behavior as a high level activity compared with other activities, but not the specific steps of the process. The proposed study will systematically study the information seeking process and identify those area most impacted by culture. While the results will be limited for those two cultures, other groups with similar cultural characteristics can also benefit from the findings. The results can also provide the basis for future studies involving more cultures. It is hoped that the results of the user study can provide guidance for the designers of information websites that serve a global audience.\rREFERENCES\r[1] Calhoun, K. J., Teng, J. T. C., Cheon, M. J. 2002. Impact of national culture on information technology usage behavior: An exploratory study of decision making in Korea and the USA. Behaviour and Information Technology. Vol. 21, No. 4. pp. 293-302.\r[2] Chau, P. Y. K., Cole, M., Massey, A. P., Montoya-Weiss, M., O’Keefe, R. M. 2002 Cultural differences in the online behavior of consumers. Communications of the ACM. Vol. 45, No. 10. pp. 138-143.\r[3] Clemmensen, T.; Hertzum, M.; Hornbaek, K.; Qingxin, S.; Yammiyavar, P. 2009. Cultural cognition in usability evaluation. Interacting with Computers. Vol. 21. pp. 212- 220.\r[4] Evers, V. 2001 Cultural aspects of user interface understanding. Doctoral Dissertation. Open University, London, England, p. 377.\r[5] Duncker, E. 2002 Cross-Cultural Usability of the Library Metaphor, JCDL ’02, Portland, Oregon 13-17 July 2002.\r[6] Fisher, K.E., Durrance, J.C. & Hinton, M.B. (2004). Information grounds and the use of need-based services by immigrants in Queens , NY : a context-based, outcome evaluation approach. Journal of the American Society for Information Science & Technology, 55(8), 754-766.\r[7] Ford, D. P.; Connelly, C. E.; Meister, D. B. 2003 Information Systems Research and Hofstede’s Culture’s\rPage 80 of 122\r\nConsequences: An uneasy and incomplete partnership. IEEE Transactions on Engineering Management, Vol. 50, No. 1. pp. 8-25.\r[8] Hall, E. T. 1976 Beyond culture, New York: Doubleday.\r[9] Hall, E. T. 1990 The hidden dimension, New York: Doubleday.\r[10] Hearst, M. 2009. Search User Interfaces. Cambridge University Press. Accessed online: http://searchuserinterfaces.com/\r[11] Hercegfi, K., Csillik, O., Bodnár, É., Sass, J., Izsó, L. 2009 Designers of Different Cognitive Styles Editing E-Learning Materials Studied by Monitoring Physiological and Other Data Simultaneously. 8th International Conference on Engineering Psychology and Cognitive Ergonomics, HCII2009, San Diego, California, USA, 14-24 July 2009, Proceedings LNAI 5639, Springer, ISBN 978-3-642-02727- 7, pp.179–186.\r[12] Hercegfi, K., Kiss, O.E. (2009): Assessment of e-Learning Material with the INTERFACE System. In: Szűcs, A., Tait, A., Widal, M., Bernath, U. (eds): Distance and E-Learning in Transition. John Wiley & Sons and ISTE, Hoboken, NJ, USA. ISBN 978-1-84821-132-2, Chapter 45, pp.645-657.\r[13] Hofstede, Geert H. 2001 Culture's consequences : comparing values, behaviors, institutions, and organizations across nations, Thousand Oaks, Calif. : Sage Publications.\r[14] Iivonen, Mirja; White, Marilyn Domas. 2001 The choice of initial web search strategies: A comparison between Finnish\rand American searchers, Journal of Documentation, v57 n4, pp. 465-491.\r[15] Komlodi, Anita. Carlin, Michael. (2004) Identifying cultural variables in information-seeking behavior. Proceedings of the Tenth Americas Conference on Information Systems (AMCIS). New York, NY, Association for Information Systems. Pp. 477-481.\r[16] Komlodi, Anita; Marchionini, Gary; Soergel, Dagobert (2007) Search history support for finding information: User interface design recommendations from a user study. Information Processing and Management. Oxford, New York, NY, Pergamon Press. Vol. 43. Pp. 10-29.\r[17] Leidner, D. E.; Kayworth, T. 2006 A review of culture in information systems research: Toward a theory of information technology culture conflict. MIS Quarterly, Vol. 30. No. 2. pp. 357-399.\r[18] Park, I. 1997 A comparative study of major OPACs in selected academic libraries for developing countries – User study and subjective user evaluation. International Information and Library Review. Vol 29, pp. 67-83.\r[19] Twidale, M.; Nichols, D. 1998 Designing Interfaces to Support Collaboration in Information Retrieval. Interacting with Computers, 10(2):177–193.\r[20] Yi, Z. 2007 International student perceptions of information needs and use. The Journal of Academic Librarianship. Vol. 33, No. 6, pp. 666-673.\rPage 81 of 122\r\nAdapting an Information Visualization Tool for Mobile Information Retrieval\rABSTRACT\rSherry Koshman School of Information Sciences University of Pittsburgh Pittsburgh, PA U.S.A.15260 +1 412-624-9441\rskoshman@sis.pitt.edu\rJae-wook Ahn School of Information Sciences University of Pittsburgh Pittsburgh, PA U.S.A.15260 +1 412-624-9437\rjahn@mail.sis.pitt.edu\r1. INTRODUCTION\rMobile computing environments are engaging users for various information retrieval (IR) tasks due to the availability of wireless access, mobile browsers, improved mobile devices, and their ubiquity in the information landscape [2]. Factual queries dominate mobile IR for items such as weather, calendar information, and traffic reports. However, persistent access to the web is expanding and mobile search is becoming a prominent mode of interaction that users adapt from familiar desktop environments [10].\rText display on the small screen size of mobile devices is limited and information visualization tools offer potential in presenting concise representations of retrieved results to users. Mobile search has been the object of academic exploration; however the presentation of visualized mobile search results has not. Mobile IR is in its very early stage and information visualization can be integrated into the user’s mental model of search result presentations since display options for mobile information retrieval are not yet firmly established.\rThe contribution of this paper is significant to mobile information retrieval visualization since it presents a technical case study of adapting the desktop VIBE interface to a mobile device. MVIBE (Mobile VIBE) is one of the first graphical information visualization tools for IR to be developed from its desktop version. In addition, this is one of the first studies to examine the new information visualization on three different mobile platforms using personal digital assistants (PDAs) devoid of telephony.\r2. RELATEDWORK\rThe predominant themes of previous work relating to this project include mobile information retrieval, mobile visualization interfaces, and mobile device testing for evaluating visualized representations.\rMobile Information Retrieval. Research encapsulates work on rendering web pages for mobile displays and the usability of web page layouts for various types of tasks [24,30,25,26}. Users’ information seeking activities for web-based data are examined through mobile transaction log analyses and the approaches to mobile search offer avenues to apply visualization techniques to retrieved results [11,14,16,15,28].\rMobile users’ browsing and searching patterns show that browsing is predominant, however query searching reflected lengthier interaction times and higher user interest [9,23]. Visual\rThe application of information visualization (infoviz) tools to mobile devices for information retrieval (IR) is uncommon. This has been attributed to the complex challenges related to mobile devices including the technical restrictions upon generating a small screen graphical visual representation for abstract information. This paper reports on a work in progress on the basic interface design and creation of MVIBE (Mobile VIBE), a new mobile version of VIBE (Visual Information Browsing Environment), which is an information visualization tool developed for information retrieval. MVIBE was developed and tested on the Apple, Linux, and Windows mobile platforms. User feedback was obtained and some of the reported challenges are common to mobile technology and others to general information visualization. At this early stage, the overall question is: can mobile devices be effective for generating a viable visualization of search results? The paper concludes with observations gained during the adaptation process, recommendations for the next phase of Mobile VIBE development, and future design considerations for developing information visualization interfaces on mobile devices.\rCategories and Subject Descriptors\rH.5.2 [Information Interfaces and Presentation]: User Interfaces – prototyping, screen design.\rGeneral Terms\rDesign, Verification\rKeywords\rMobile Information Visualization, Mobile Information Retrieval\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR09, October 23, 2009, Washington, D.C., U.S.A.. Copyright 2009 ACM 1-58113-000-0/00/0004...$5.00.\rPage 82 of 122\r\ntag clouds are used for query representation in a system where users can view other user queries rather than search results for a particular geographic location in which they are situated [13]. Queries which were used most frequently for that location appear in a larger text size.\rMobile Visualization Interfaces. Initial work on visualized mobile web applications was directed toward resolving the end user’s request for web-based location information. As a result, many mobile visualization studies focused on web or GPS (Global Positioning System) based mobile mapping data to aid users’ navigation tasks [3,5]. Digital mobile maps represent geographic information and networked routes that are a direct analog to the physical environment.\rResearch in mobile information visualization for information retrieval is limited. The lack of robust research may be attributed to its focus upon visualizing non-physical abstract information and the restrictions of mobile technology [7]. However, visualizing abstract document spaces on mobile screens is discussed by [17,29]. To address the issue of high information density on small screen, [27] developed a graphical interface based on “liquid browsing”, an animated scatter plot that resembles suspended bubbles in a liquid which is most suited to pressure sensitive screens that are not common among handheld devices. Work on visualized screen layouts, node occlusion, and underutilized screen space issues have mixed results [29,12,4,6].\rMobile Devices. With exception of the iPod touch, Nokia and Hewlett Packard (HP) devices are primarily used in previous mobile visualized research as prototypes [29,4,6]. Work conducted specifically on personal digital assistants was shown for search result clustering and the scatterplot prototype [4,6].\r3. DESKTOPVIBE\rVIBE (Visual Information Browsing Environment) is a desktop prototype system developed by researchers at Molde College, Norway and at the School of Information Sciences, University of Pittsburgh. A more current desktop version was built as a Java Swing (or applet). Its original implementation had a strong impact on the direction of information visualization in IR for over a decade [22,18,8,21,19,20,1]. The selection of VIBE for this work was based on its wide implementation that reduced the uncertainty associated with information visualization system use.\rBasic elements of the VIBE interface include a visualized query that has round circular icons which represent Points of Interest (POIs) or user selected terms from a drop down menu of options. The resulting document set is depicted as polygons that are plotted in proximity to the POIs according to a term frequency distribution algorithm (Figure 1). The larger the document icon, the more frequent occurrence of terms related to the document. Desktop VIBE has a robust set of features to manipulate the visualization. For example, color is prominently used to mark POIs and to indicate overlapping retrieved documents. POIs may be dragged, added or removed from the display. Relationships between the POIs and the documents can be depicted using the “lines” or “net” features The salient premise in VIBE’s design is that the query and resulting document set can be visualized in one screen for user browsing and item selection.\rFigure 1: Desktop VIBE Display.\rThe first comprehensive usability study of the desktop VIBE system showed that its interface and availability of robust features for IR made it a prime candidate for information visualization adaptation in the mobile environment [19,20].\r4. MOBILEVIBE(MVIBE)\rThe design of Mobile VIBE began with a review of the initial user study with desktop VIBE [19,20]. It was observed that while desktop VIBE had well developed interface features, they could not all be deployed simultaneously in a mobile visualization.\rTask 1: Defeaturing. The first objective in creating Mobile VIBE was to defeature the desktop VIBE interface by selecting interface options to yield a simpler and clearer mobile interface for end users [21]. Original desktop system options such as “star”, “lasso”, “astro” or “Boolean” views were not included in the mobile design. Colors were limited to green for the POI display, red for their moveable state indicating that POIs are selected for the user to relocate them, blue for the “lines” feature and document icons, and a white background for the display.\rSalient features were selected and applied to the mobile version on the basis that they were amenable to a mobile screen display and function. The list includes three system-generated default display items that enable data coding and seven user controlled features to use in the display (Table 1).\rTable 1: Mobile VIBE Interface Features\rMobile VIBE Default Features\rPOIs. Points of Interest, which are document terms represented as circular icons on the display.\rPOI Labels. Text labels that show the query terms above the POIs (Points of Interest).\rDocument Icons. Squares on the display that represent retrieved documents.\rMobile VIBE User Display Options\rShow Document Titles. Allows the user to make visible the text derived from the search results positioned near the document icons.\rBegin Dragging. Notifies the user of the initial POI movement.\rEnd Dragging. Notifies the user of the POI movement’s end.\rShow Lines. Allows the user to display axes between two terms\rPage 83 of 122\r\n(POIs). All document icons situated on the line are related to the terms on either end. It is useful for determining the influence of POIs on document icons.\rMove POIs. A move is activated by clicking on a POI and then clicking on another part of the screen to place it.\rColor. Associated with the Move POIs feature. The default POI color is green and the color changes to red while the user relocates the POI on the screen.\rReset POIs. Places POIs in the default circular arrangement.\rTask 2: Mobile Development. Mobile VIBE was authored in JavaScript in order to make the code multi-platform compliant and to take advantage of a wider range of support from various mobile devices. Unlike the desktop environment, Java applets are not fully supported by mobile devices at this point, whereas JavaScript is more ubiquitous. Mobile VIBE is a JavaScript port of the Java Swing version of VIBE. Due to the similarity of Java and JavaScript syntax, the adaptation was relatively straightforward and was mostly done by direct translation. The porting became possible due to recent support of the CANVAS tag from state-of-the-art Web browsers, such as Safari, Firefox, and Opera. The CANVAS tag allows the manipulation of every pixel in the area it occupies, so it was possible to freely draw the VIBE visualization on it.\rData were encoded on Mobile VIBE in a consistent manner with its predecessor. Document icons are geometric squares, however the icon size to represent term frequency of the POIs was not implemented in the mobile version. Figure 2 shows an example of the graphical Mobile VIBE interface on the HP iPAQ running Opera 9.5 Beta on Windows Mobile 6. The circular green icons represent the terms or POIs, term labels appear above the icons and the lines feature is activated to connect the terms on the display. The square icons are the retrieved items and the interface options are presented at the bottom of the display. In this example, the visualization is highlighted with square borders. The portrait view is shown although the device may be used in a portrait or landscape position to view the visualization on the HP iPAQ and iPod touch.\rSince all three of the mobile Web browsers used in the current investigation support JavaScript and the CANVAS tag, the drawing of the visualization in the mobile environment was done without any modification of the desktop version of the VIBE JavaScript code. However, due to the different input methods they support, the interaction mechanism needed to be updated. The code was loaded on to three representative personal digital assistants, which are described next.\rTask 3: Mobile Interaction. The three PDAs, their screen resolutions and browsers include: the iPod touch running iPhone OS 2.1.1 (480x320) and Safari 3.1.1; the Nokia N810 running Maemo Linux 4.1 OS2008 (800x480) and the Mozilla-based MicroB 1.0.4 browser; and the HP iPAQ 211 Enterprise Edition (640x480) running Windows Mobile 6 and the Opera 9.5 Beta browser.\rFigure 2: MVIBE Interface on the HP iPAQ.\rApple’s iPod touch supports the most unique input method with its multi-touch interface, which allows users to use their two fingers to click, drag, squeeze, and rotate objects on the screen. Apple provides a simple set of JavaScript event handling functions for the multi-touch events and the mouse dragging action of the desktop version could be easily adapted to the new touch-based POI dragging. The Nokia and HP machines use stylus pens as input devices and they could drag the POIs without modification of the original VIBE JavaScript code. However, it was difficult to drag the POIs with the stylus pen due to the smaller size of the mobile screen and the viewport panning nature of the devices (the screen itself frequently panned around while dragging the POIs).\rTherefore, a simpler method for moving POIs was added: a click- move-and click again function. With this functionality, when a user clicks on a POI, its color changes to red, showing that it is in a movable state. Then the user moves his/her stylus pen to any arbitrary position on the screen (not dragging) and then clicks on it. The POI, which was just selected, jumps to the new position.\rFour query terms were represented as four POIs on the screen as green circles and the documents were displayed as small squares equipped with their titles (Figures 2 and 3). The most noticeable problem was the clutter of visualization elements such as the document titles and lines among the POIs (Figure 3a). To resolve the visual clutter, options to turn off the lines and titles were added (Figure 3b). Moreover, users can freely move the POIs and relieve the clutter of the document titles (Figure 3c).\rA small sample data set was constructed and visualized. The data are a single web search result, generated by the Google web search engine. The query, “information visualization document retrieval”, was entered on Google and the top 10 documents returned by Google were downloaded. The 10 web pages were indexed using the Indri search engine (www.lemurproject.org/indri) and the probability values between each document and each query term, “information”,\rPage 84 of 122\r\n“visualization”, “document”, and “visualization” were calculated. The VIBE engine can then read the probability values and make use of the probability ratios for the visualization.\rfor information retrieval, to a mobile device. The recommended steps to accomplish this goal include: 1) using a defeaturing technique to initially select few interface options for reducing visual complexity; 2) selecting a cross-platform development tool to address multiple mobile environments (e.g. JavaScript); 3) keeping visualization design principles aligned with mobile interaction techniques; and 4) obtaining user feedback at an early stage to facilitate incremental development\rThe initial step of defeaturing an infoviz interface is important for establishing a baseline for development. Testing designs on multiple platforms reveals how issues may appear in one platform, but not another. This is significant for users’ selecting multiple device types. It was observed that users experienced different levels of difficulty when interacting with the visualization due to browser function interference (e.g. zooming). For this reason, browser-based mobile visualization interaction design needs to be reformulated to correspond with current mobile browser techniques. The visualization functionality may require adaptation to remain compatible such as changing the “move POI” MVIBE option from drag and drop to point and click (See Section 4: Task 3). Balancing mobile functions with the visualization interface design requires further work.\rMVIBE is a work in progress and future work is planned. Users had positive responses in understanding MVIBE’s interface features. Most graphical features were evident in the display however, the textual document titles’ appearance and lines occlusion posed difficulty. Techniques, such as rollovers, are being considered to address the titles issue. What is surprising is the difference of interaction difficulties among the three devices. It is known that the variability of platforms among mobile devices is a larger functionality issue in comparison to the desktop environment, however how it specifically affects a mobile visualization is a new avenue for exploration. Future research will concentrate on fixing the movement, selection, zooming, resizing issues since incongruities were found among the devices and the visualization system. Additional IR features will be incrementally added and tested with a larger data set and more participants to evaluate the visualization in the mobile IR process. A personalization option will be investigated.\rThe early testing of Mobile VIBE shows promise for the field of mobile infoviz for information retrieval. The MVIBE case study demonstrates several factors to address when creating IR visualizations for mobile devices. It presents an affirmative answer to the initial guiding question in that mobile devices can be effective for generating a graphical IR visualization. Manufacturers’ mobile products are improving and mobile challenges are becoming less onerous to overcome in order to generate future IR visualizations in the rapidly expanding mobile environment.\r7. REFERENCES\r[1] Ahn, J., Brusilovsky, P. and Sosnovsky, S., QuizVIBE: accessing educational objects with adaptive relevance- based visualization. In World Conference on E-Learning, E-Learn Proceedings, (Honolulu, Hawaii, 2006), Association for the Advancement of Computing in Education (AACE), 2707-2714.\r[2] Ballard, B. Designing the Mobile User Experience. John Wiley & Sons, Chichester, England, 2007.\r(a)\r(b) (c)\rFigure 3: MVIBE on the iPod touch.\r5. EARLYUSERFEEDBACK\rUser feedback on the Mobile VIBE interface was elicited at this very early stage. Six doctoral students (four females and two males) from the School of Information Sciences volunteered to critique the Mobile VIBE interface. All but one had over one year of mobile device experience and had been exposed to information visualization. Their feedback is shown in Table 2.\rTable 2: User Feedback Summary\rMVIBE Display Legibility. Received positive user responses.\rMobile Device Type. Affected POI label identification and users ranked this task’s effectiveness on the mobile devices in descending order: the iPod, Nokia, then the HP.\rReadable Document Titles. Received low user ratings according to the same ranking of mobile devices as shown above. Occlusion is problematic.\rInterface Icon Visibility. Positive user responses, except on the HP.\rLines Feature. Uniform favorable response by users, except on the HP.\rMove POIs. Mixed success reported by users across devices.\rResize Display. Reportedly worked on the Nokia and HP, not the iPod.\rUnderstanding MVIBE features. Positive responses received from most users.\rThe users’ observations of MVIBE on the iPod touch included the difficulty with screen control, how large finger size may be prohibitive to its use, how it was difficult to move the POIs, and how the multi-touch zoom does not work with Mobile VIBE. The users’ favorite activities with MVIBE on the iPod touch included interacting with the display orientation, moving POIs, and using the “Reset POI” option. On the HP iPAQ, the preferred activity was the zoom functionality.\r6. CONCLUSIONSANDFUTUREWORK\rThis technical case study presents an initial venture into the adaptation of an established desktop visualization tool designated\rPage 85 of 122\r\n[3] Burigat, S., Chittaro, L. and Gabrielli, S., Visualization and multimodality: visualizing locations of off-screen objects on mobile devices: a comparative evaluation of three approaches. In Proceedings of the 8th Conference on Human-Computer Interaction with Mobile Devices and Services, (Helsinki, Finland, 2006), ACM, 239-246.\r[4] Buring, T. and Reiterer, H., Input and visualization: ZuiScat: querying and visualizing information spaces on personal digital assistants. In Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices & Services, (Salzburg, Austria, 2005), ACM, 129-136.\r[5] Carmo, M.B., Afonso, A.P . and Matos, P .P ., Query results and processing: visualization of geographic query results for small screen devices. In Proceedings of the 4th ACM workshop on Geographical information retrieval, (Lisbon, Portugal, 2007), ACM, 63-64.\r[6] Carpineto, C., Mizzaro, S., Romano, G. and Snidero, M. Mobile information retrieval with search results clustering: prototypes and evaluations. Journal of the American Society for Information Science and Technology, 60 (5) 2009, 877-895.\r[7] Chittaro, L. Visualizing information on mobile devices. Computer, 39 (3) 2006, 40-45.\r[8] Christel, M. and Huang, C., SVG for navigating digital news video. In Ninth ACM International Conference on Multimedia, (Ottawa, Canada, 2001), ACM, 483 - 485.\r[9] Church, K., Smyth, B., Cotter, P. and Bradley, K. Mobile information access: A study of emerging search behavior on the mobile Internet. ACM Transactions on the Web, 1 (1) 2007, 1-38.\r[10] Costa, C.J., Silva, J. and Aparcio, M., Evaluating web usability using small display devices. In Proceedings of the 25th Annual ACM International Conference on Design of Communication, (El Paso, Texas, USA, 2007), ACM, 263- 268.\r[11] Cui, Y. and Roto, V., How people use the web on mobile devices. In International World Wide Web Conference, (Beijing, China, 2008), ACM, 905-914.\r[12] Ghinea, G., Heigum, J. and Fongen, A., Information visualization for mobile devices: a novel approach based on the MagicEyeView. In Wireless Pervasive Computing, 2008. ISWPC 2008. 3rd International Symposium, (Santorini, 2008), 566-570.\r[13] Jones, M., Buchanan, G., Harper, R. and Xech, P.-L., Questions not answers: a novel mobile search technique. In Proceedings of the SIGCHI Conference on Human factors in Computing Systems, (San Jose, California, USA, 2007), ACM, 155-158.\r[14] Kamvar, M. and Baluja, S., A large scale study of wireless search behavior: Google mobile search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, (Montreal, Quebec 2006), ACM, 701-709.\r[15] Kamvar, M., Kellar, M., Patel, R. and Xu, Y., Computers and iPhones and Mobile Phones, oh My! In International World Wide Web Conference, (Madrid, Spain, 2009), ACM, 801-810.\r[16] Karlson, A., Robertson, G., Robbins, D., Czerwinski, M. and Smith, G., FaThumb: a facet-based interface for mobile search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, (Montreal, Quebec, Canada, 2006), ACM, 711-720.\r[17] Karstens, B., Kreuseler, M. and Shumann, H., Visualization of complex structures on mobile handhelds. In Proceedings of the International Workshop on Mobile Computing, (2003).\r[18] Korfhage, R.R. Information Storage and Retrieval. John Wiley & Sons, New York, 1997.\r[19] Koshman, S. Testing user interaction with a prototype visualization-based information retrieval system. Journal of the American Society for Information Science and Technology 56 (8) 2005, 824-833.\r[20] Koshman, S. A usability study comparing a prototype visualization-based system with a text-based system for information retrieval. Journal of Documentation 60 (5) 2004, 565-580.\r[21] Morse, E., Lewis, M. and Olsen, K. Testing visual information retrieval methodologies case study: comparative analysis of textual, icon, graphical and \"spring\" displays. Journal of the American Society for Information Science and Technology 53 (1) 2002, 28-40.\r[22] Olsen, K.A., Korfhage, R.R., Sochats, K.M., Spring, M.B. and Williams, J.G. Visualization of a document collection: the VIBE system. Information Processing & Management 29 (1) 1993, 69-81.\r[23] Roto, V. Search on mobile phones. Journal of the American Society for Information Science and Technology, 57 (6) 2006, 834-837.\r[24] Roto, V ., Popescu, A., Koivisto, A. and V artiainen, E., Minimap: a web page visualization method for mobile phones. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, (Montreal, Quebec, Canada, 2006), ACM, 35-44.\r[25] Simon, R. and Frolich, P., A mobile application framework for the geospatial web. In International World Wide Web Conference (IW3C2), (Banff, Alberta, 2007), 381-390.\r[26] Spence, R. Information Visualization: Design for Interaction, Second Edition. ACM Press, New York, 2007.\r[27] Waldeck, C. and Balfanz, D., Mobile liquid 2D scatter space (ML2DSS). In Eighth International Conference on Information Visualization (London, UK, 2004), 494-498.\r[28] Yi, J., Maghoul, F. and Pedersen, J., Deciphering mobile search patterns: a study of Yahoo! mobile search queries. In International World Wide Web Conference, (Beijing, China, 2008), ACM, 257-266.\r[29] Yoo, H.Y. and Cheon, S.H., Visualization by information type on mobile device. In Proceedings of the Asia Pacific Symposium on Information Visualisation - Volume 60, (Tokyo, Japan, 2006), Australian Computer Society, Inc., 143-146.\r[30] Zhang, D. Web content adaptation for mobile handheld devices. Communications of the ACM, 50 (2) 2007, 75-79.\rPage 86 of 122\r\nA Theoretical Framework for Subjective Relevance\rKatrina Muller\rSILS\rUniversity of North Carolina Chapel Hill, NC 27599-3360 USA +1 919 259 6832\rkmuller@email.unc.edu\rABSTRACT\rThis paper explicitly models subjective relevance by deconstructing its elements. We outline the various dimensions of subjective relevance, considering internal and external factors as well as interactions. We employ a utility framework for modeling, both conceptually and mathematically, subjective relevance and its multiple dimensions, aspects and interactions.\rCategories and Subject Descriptors\rH.1.2 [Information Storage and Retrieval]: Models and Principles – User/Machine Systems – Human factors\rGeneral Terms\rHuman Factors, Theory\rKeywords\raffective, cognitive, judgment, relevance, user-centered, utility.\r1. INTRODUCTION\rAlthough it is well documented that relevance is multidimensional and dynamic, most studies still assess it with a single measure at a single point in time [3]. Moreover, relevance is assessed in the same way regardless of the user‘s information seeking task, interest or personal characteristics. This leads to inquiries about what type of relevance is being modeled in information seeking studies, what is being measured and what criteria users employ when deciding which information objects are relevant to them. While simplifications and abstractions are often necessary in order to study phenomenon, this does not mean that efforts should not be made to better understand and measure relevance. As Saracevic notes, ―relevance is a, if not the, key notion in information science in general and information retrieval in particular‖ (p. 1915) [15]. Given its centrality and importance (indeed, some notion of relevance is used in almost all human computer information retrieval studies), increased effort should be made to model and measure relevance.\rIn previous work, we discussed some of the limitations of current conceptualization and operationalizations of relevance in information retrieval [10]. In this paper, we present a conceptual model for subjective relevance and demonstrate a strategy for operationalization. This paper proposes utility as both a measure of, and as a theoretical framework for, subjective relevance. This approach has been presented by other researchers [4,12]. We believe the concept of utility is useful because it considers relevance from the user‘s point of view. In this paper we consider three types of user-centered relevance: cognitive relevance, affective relevance, and situational relevance [14,15]. We then present a conceptual and mathematical model of these three types of relevance and discuss interactions among them.\r2.\rDiane Kelly\rSILS\rUniversity of North Carolina Chapel Hill, NC 27599-3360 USA +1 919 962 8065\rdianek@email.unc.edu\rRELEVANCE AND UTILITY\rPage 87 of 122\rResearchers have used many terms to describe subjective relevance or different aspects of it [1,4,8,14,16,18]. The number of these possible aspects is innumerable, as are the potential interactive effects of those aspects. Although this can present serious problems for measurement it also gives researchers leeway to construct measures appropriate for testing their hypotheses about relevance judgments and information seeking behavior.\rConsidering its historical development, utility seems like a good general concept on which to ground any discussion of subjective relevance. In Jeremy Bentham‘s An Introduction to the Principles of Morals and Legislation [2] he postulated that people should and do make decisions in order to maximize their pleasure while minimizing their pain. Bentham discusses the idea of happiness which is a pleasurable state of mind that comes from success or attainment of what is good [7]. By this definition, Bentham‘s greatest happiness principle is a necessary and sufficient condition for a criterion of relevance judgments. Bentham and his contemporaries used utility as a measure of this happiness or satisfaction.\rSome of the terms used in reference to relevance are pertinence, psychological relevance, situation relevance, usefulness and value. All of these, and any others that come to mind, can be included in the definition of happiness above. Therefore maximizing utility (a measure of happiness) is synonymous with maximizing any manifestation of subjective relevance.\rCooper was one of the first researchers in IR to consider utility as a measure of retrieval effectiveness. His seminal work lays the foundation for our current discussion [4,5]. By challenging traditional, system centered views of relevance Cooper opens the door for researchers to provide rich theories of relevance that not only reflect algorithmic and topical forms of relevance but those that consider the user‘s needs and situation as well [1,8,14,16,18]. Our work synthesizes these theoretical constructs of subjective relevance into a cohesive framework using a utility model. Our goal here is only to present the theoretical framework, a validation of appropriate instruments and experiments testing the theory are planned for future work.\rHaving placed subjective relevance in a utility framework we next examine the choice variables that determine users' preferences. It is the direct and indirect effect of these aspects, their interactions with each other and external factors that describe the strategies and motivations which affect users' preferences and the subsequent relevance outcomes.\r3. DIMENSIONS OF RELEVANCE\rSaracevic conceptualized relevance along five dimensions: (1) system or algorithm; (2) topical; (3) pertinence or cognitive; (4)\r\nsituational; and (5) motivational or affective [12, 13]. System or algorithm relevance describes the relationship between a query and the collection of information objects. This type of relevance is operationalized by a particular algorithm, and does not involve user judgment. Topical relevance is associated with the aboutness of a particular document. For instance, if the user‘s query is  ̳elephants,‘ then a document containing a discussion of elephants is topically relevant. Pertinence, or cognitive relevance, describes the relationship between a user‘s perception of his information need, what he currently knows about the information need and a document. This is very much related to psychological relevance [6], which considers the degree of cognitive transformation or learning that is caused by reading a document. Situational relevance, originally coined by Wilson [18], is concerned with the idea that relevance judgments change according to task and situation. Finally, motivational or affective relevance considers the intentions, goals, motivations and emotions of the user.\rCognitive relevance and affective relevance are internal processes. Situational aspects usually start out as external conditions which eventually have an impact on internal processes. Classifying these relevance dimensions into internal and external categories is an important step toward the development of a viable theory of subjective relevance. Equally as important is considering the interaction between and among these internal and external factors. The effects that situational factors have on cognitive and affective relevance as well as the effects that these inward processes have on each other impacts how people make relevance judgments. To be clear we distinguish between internal processes and external conditions below.\r3.1 Internal Processes\rThe internal aspects of subjective relevance—cognitive and affective—are constructs which do not exist outside the user. Cognitive relevance is the perception that the user develops of the information object given his or her cognitive state [14,15]. It is critical judgment and rational decision making. It is the recognition of an information need and a strategy to meet that need. Affective relevance, on the other hand, is the user's inner emotional state. It includes hopes, dreams and desires as well as goals and motivations.\r3.2 External Conditions\rSituational aspects of relevance originate outside of the user but can have serious impact on his or her internal state. To illustrate this, we will use one well-studied aspect of situation that has been found to impact relevance behavior, information task. We will use this example throughout out the remainder of this paper. Many researchers have studied how task affects relevance outcomes [3,11,19]. Some tasks can be quite cognitively demanding, while some are affectively charged. Different users have greater motivations for working on some tasks than others. Internal emotional and cognitive states are influenced by various situational factors including the environment and a user‘s time constraints.\rAs previously mentioned the relationships between and among all dimensions of subjective relevance are extremely important to the understanding of the formation of relevance judgments. We posit that affective and cognitive relevance are strongly interdependent, that task effects are manifest through either or both of these aspects and that preferences over information objects as well as their various attributes is informed by both cognitive and affective\rforces. These interactions will be presented more formally in the following model.\r4. THEMODEL\rDrawing from the user-side of Saracevic's relevance aspects, we present a model of subjective relevance using utility as the unifying concept. For the purpose of this paper we regard utility as being defined as including all expressions of user satisfaction and user-centered relevance derived from an interaction between the user and an information object, regardless of source or context. This concept includes pertinence, psychological relevance, situational relevance, usefulness, value and any other term used to qualify a user‘s cognitive or affective reactions to an information object, assuming of course that the interaction has some measurable effect on the user‘s sense of well being, happiness or satisfaction. This definition may seem broad, but since all the above concepts are equally unobservable and are based solely on user‘s preferences (by definition) no distinction need be made between definitions and criteria as to what specifically we think we are measuring. Basically, what the user sees as relevant is relevant [16]. Our work here is not concerned with whether or not any particular information object is relevant but with the various criteria users employ to make relevance judgments [15].\rThe internal and external aspects of relevance introduced in Section 3 are presented graphically in Figure 1. Cognitive and affective relevance both have a direct bearing on utility. Their interdependence is modeled by the two way arrow between them. To date there are no adequate measures for cognitive or affective relevance. For future studies we suggest the use of indexes made up of self-reported questions measured by Likert-type scales much like the measures used for utility. Understanding the various dimensions of cognitive and affective relevance and the various indicators of these dimensions (e.g., scale items) are important future questions.\rAlthough situational relevance has no direct effect on utility, it does affect both cognitive and affective relevance. Situational effects must be processed internally in order to have an impact on subjective relevance. Since task has an effect on relevance we model situational relevance as having indirect effects on subjective relevance based on its direct effects on cognitive and affective relevance.\rSo the effects of cognitive and affective relevance on utility originate from three dimensions: (1) the user's personal characteristics such as personal preferences, emotional nature and cognitive ability, (2) the effects that situation and other external factors have on his or her motivations, and internal states and (3) the interaction between these two states. These are all illustrated in Figure 1 below.\rPage 88 of 122\r\nSituational Relevance\rCognitive Relevance\rAffective Relevance\rUtility\rFig. 1. Interactions of subjective relevance aspects.\r4.1 MathematicalModel\rIn this section we present a mathematical representation of the constructs and interactions described in the theory section and illustrated in Figure 1. We use a utility function to represent the relationship between subjective relevance and its various aspects. We start with a general equation with no functional form.\rUtility(cog, aff, sit) = f(cog, aff, sit) (1)\rA user‘s utility of an information object is a function of cognitive relevance, affective relevance and situational relevance, as we spoke of above. Equation 1 serves a framework from which to proceed. It states the concept we wish to model, in this case utility. It also lists what is hypothesized as the determinants of utility, in this case cognitive, affective and situation relevance.\rSince situational relevance has no direct effect on utility but does have direct effects on both affective and cognitive relevance we extract it from Equation 1 and express cognitive and affective relevance, and through them utility, as being conditional on situational relevance.\requation, task effects are still implicitly affecting cognitive and affective relevance.\rAs pointed out in our model and earlier, measurement is a huge issue in relevance research. Our theory depends on the development of adequate measure for testing of hypotheses and for the discovery of new aspects and dimensions of interest.\r4.2 ComparativeStatics\rIn economics, comparative statics is used to develop testable hypotheses from theoretical models [6]. It is used to analyze the effect that a change in one variable has on another. In utility theory it is used to measure the effects of changes in prices and income [13,17]. Prices and income are external constraints which are generally called exogenous variables. They are determined outside the model. In an experimental setting the researcher usually has some control over the exogenous variables, holding some constant and manipulating others. Task is the exogenous variable in our model. It is the external condition.\rEndogenous variables are those that are determined inside the model. In the typical utility model quantities of goods are the endogenous variable. How much agents buy or consume is determined in part by the exogenous variables, price and income. Since utility is dependent on these quantities it is also an endogenous variable.\rUtility is the variable of interest in both the classic consumer model and our model of subjective relevance. Here we use comparative statics to examine the change in utility (subjective relevance) as the result of a change in task. The effects that task have on cognitive and affective relevances are an intermediate step.\rNow assume the situation changes from T0 to T1 where T0 represents a task with high affective content and T1 represents a task which has more cognitive demands associated with it. We further assume that the effect that T1 has on affective relevance remains constant. So we have:\r(4)\rNow the user balances the marginal benefits from using cognitive relevance criteria and using affective relevance criteria to the point where those marginal benefits are equal to each other and equal to zero. In other words the user cannot make any changes which will yield a better relevance decision according to his or her subjective criteria. This is called the marginal rate of substitution [13, 17]. Mathematically it is expressed as:\r(5)\rSo if cognitive influences on the user‘s utility goes up than the affective influences must go down to maintain the equilibrium in Equation 5.\r5. CONCLUSION\rIn this paper we present a theory of subjective relevance. We use utility to conceptually model subjective relevance. We incorporate two dimensions (internal and external) and three aspects (cognitive, affective, situational) into our model. We then present a mathematical model and use comparative statics to illustrate the model.\rUtility(cog, aff | T ) = f(cog, aff | T ) ii\ri=0,1\r(2)\rAs situational relevance changes, cognitive and affective relevances change as a result. These, in turn, have an effect on utility, in part because of the variation in task. In effect, task moderates the effects of affective and cognitive relevance.\rThe categorization of tasks depends on the type of study being performed. Tasks can be categorized by the user in naturalistic studies, or they can be imposed by the research in controlled experiments. Classification schemes vary but all represent the states of situational relevance that the researcher is interested in analyzing. The effects of task on cognitive and affective aspects of relevance are a ripe area for future research.\rEquation 2 is analogous to having a separate utility function for each task class. Task effects on utility can be measured by the differences in mean utility across each task groups. More importantly, with adequate measures of affective and cognitive relevance, the means of these constructs can be compared across task groups as well.\rAt this point our model does not describe the relationships or interactions between and among these constructs; to do so we need to impose a functional form.\rUtility(cog, aff | Ti) = (β0+ βccog + βa aff + βca cog*aff)| Ti (3)\rThere are a few points of interest with Equation 3. First, its functional form is quadratic, due to the interaction between cog and aff. Secondly, although it is not explicitly stated in the\rPage 89 of 122\rcog  aff  0. T T\rU  U 0. cog aff\r\nOf course this model has short comings. Although it is intuitive that all subjective relevance comes from a combination of cognitive and affective factors, it is difficult to fully define these terms much less measure their effects. Still, thinking about subjective relevance along the lines proposed in this paper has advantages. The first is that no competing theory has yet established itself [15]. Secondly, its framework provides direction in relevance measurement research and creates a structural framework for testing hypotheses about internal, external, and interactive elements of subjective relevance.\rOne other shortcoming of our model is the exclusion of system- centered relevance from our analysis. The system side and user side also interact with each other. This interaction is hard to analyze. Having a relatively good understanding of the system side, research on the user side will likely bring a huge benefit and move toward an integrated understanding of the two.\rAlthough utility is an excellent surrogate for subjective relevance, the other latent variables are more difficult to measure. If one topic of research is to follow from the study, developing appropriate measures for the cognitive and affective aspects of relevance would make the largest contribution. Cognitive and affective relevances exist only as internal processes. Situational relevance, however, consist of preferences over external objects. Assuming that the user strives to maximize his or her satisfaction or well- being over observable choices and self reported preferences, utility makes a good measure.\rAs much work has been done on task effects on relevance this still remains an important component of relevance research. Task is a variable in the system which can be easily manipulated thereby allowing for experiments and subsequent testing. It is also the only variable we consider as representing situational relevance. There are, of course, other such variables, but the growing body of research on task makes it a good candidate with which to start.\rHaving established a theoretical framework for subjective relevance, our future research will be to develop instruments for measuring cognitive, affective and situational aspects, and conduct an experiment to estimate the effects of this these constructs and the relationships among them.\r6. REFERENCES\r[1] Barry, C. 1994. User-defined Relevance Criteria: An Exploratory Study. Journal of the American Society for Information Science, 45, 149-159.\r[2] Bentham, J. 1781. An Introduction to the Principles of Morals and Legislation. Retrieved from http://www.utilitarianism.com/jeremy-bentham/index.html.\r[3] Borlund, P. 2003. The concept of relevance in IR. Journal of the American Society for Information Science and Technology, 54(10), 913-925.\r[4] Cooper, W. S. 1973a. On Selecting a Measure of Retrieval Effectiveness, Part 1: The ―Subjective‖ Philosophy of Evaluation. Journal of the American Society for Information Science, 24, 87-100.\r[5] Cooper, W. S. 1973b. On Selecting a Measure of Retrieval Effectiveness, Part 2: Implementation of the philosophy. Journal of the American Society for Information Science, 24, 413-431.\r[6] Currier, K.M. 2000. Comparative Statics analysis in Economics. World Scientific: River Edge, N.J.\r[7] Happiness. In Oxford English Dictionary online. Retrieved April 7, 2009 from http://dictionary.oed.com\r[8] Harter, S. 1992. Psychological relevance and information science. Journal of the American Society for Information Science. 49(3), 602-615.\r[9] Ingwersen, P. & Järvelin, K. 2005. The Turn: Integration of Information Seeking and Retrieval in Context. Springer, Dordrecht.\r[10] Kelly, D. 2007. Web page relevance: What are we measuring?\rWorkshop on Web Information Seeking and Interaction at the 30th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR '07), Amsterdam, The Netherlands.\r[11] Kelly, D. & Belkin, N.J. 2004. Display time as implicit feedback: Understanding task effects. In Proceedings of the 27th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR '04), Sheffield, UK, 377-384.\r[12] Lopatovska, I., & Mokros, H. B. 2008. Willingness to pay and experienced utility as measures of affective value of information objects: Users‘ accounts. Information Processing & Management, 44(1), 92-104.\r[13] Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University Press, New York.\r[14] Saracevic, T. 1996. Relevance Reconsidered. In P. Ingwersen & N. O. Pors (Eds.), Integration in Perspective. Proceedings of the Second International Conference on Conception of Library & Information Science (CoLIS 2), Denmark, 201-- 218.\r[15] Saracevic, T. 2007. Relevance: A Review of the Literature and a Framework for Thinking on the Notion in Information Science. Part II: Nature and Manifestations of Relevance. Journal of the American Society for Information Science and Technology, 58(13), 1915-1933.\r[16] Swanson D. 1986. Subjective Versus Objective Relevance in Bibliographic Retrieval Systems. The Library Quarterly, 56(4), 389-398.\r[17] Varian, H.R. 1978. Microeconomic Analysis. W.W. Norton & Co., New York.\r[18] Wilson, P. 1973. Situational Relevance. Information Storage & Retrieval, 9, 457-469\r[19] White, R.W. & Kelly, D. 2006. A study on the effects of personalization and task information on implicit feedback performance. Proceedings of the 15th ACM international conference on Information and knowledge management, (CIKM '06), 297-306.\rPage 90 of 122\r\nQuery Reuse in Exploratory Search Tasks\rChirag Shah and Gary Marchionini School of Information & Library Science\rUniversity of North Carolina\rChapel Hill, NC 27599-3360 chirag@unc.edu, march@ils.unc.edu\rABSTRACT\rIn this paper, we present a number of observations and analyses from a user study. The study involved 84 subjects working on two different exploratory tasks for two sessions, which were one to two weeks apart. We found that a large portion of queries consisted of repetition of previously used query by the same user. There was also a high amount of overlap among the queries of different users for a given task, thus confirming the assumption that people tend to express their information request in the same/similar way for the same information need.\rCategories and Subject Descriptors\rH.3.3 [Information Storage and Retrieval]: Information Search and Retrieval – Query formulation, Search process.\rGeneral Terms\rDesign, Experimentation, Human Factors.\rKeywords\rExploratory search tasks, query re-usage.\r1. INTRODUCTION\rExploratory searches typically exhibit a wide range of queries that in many cases take place over multiple sessions (White & Roth, 2009). As people search over time, they often reuse the same query either consciously or not. This phenomenon is pervasive, as illustrated by Teevan (2007) who found that one-third of all queries received by a search engine have been posed by the same user at least once before. Likewise, different people often use the same queries for the same or similar needs, a basis for several recommendation system techniques (e.g., Smyth, 2003). Understanding how and why people pose the same queries and how queries overlap across people are important problems theoretically and can also be used in practical ways to improve results for the individuals who executed the query or others who execute similar queries.\rIn this paper, we are present results from a study of how people use and reuse queries in multisession exploratory search tasks and to what extent these queries overlap across people and sessions.\rPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\rHCIR 2009, October 23, 2009, Washington DC, USA. Copyright 2009 ACM x-xxxxx-xxx-x/xx/xxxx...$5.00.\rOur analysis show that (1) there is a number of queries that people tend to reuse, and that (2) for the same information need, people tend to use same/similar queries.\r2. METHOD\rWe were interested in looking at how people work in collaboration while performing an exploratory search task. We brought 42 pairs of people (total 84 subjects) to the lab for two sessions, which were separated by one to two weeks. The subjects were university students and staff from age 17 to 50. Pairs were recruited under the condition that they had previously done some collaborative project(s) together. Participants worked in different rooms so they cannot see or talk to each other directly. They were given a chat client for communication while working on the assigned tasks. For their first session participants completed some demographic questions and had a short practice session to familiarize themselves with the chat setup, which worked as a browser plug-in. They were asked to work through a task and after 20 minutes they were then interrupted, asked to complete a short questionnaire and asked to switch to the second task. They worked for about 20 minutes on the second task, completed another questionnaire and ended the session. Sessions lasted 75- 90 minutes, which included a number of personal questionnaires and a group interview.\rFor the second session, the subjects were asked to resume their tasks from the previous session to collect more relevant information and summarize their findings. They worked on each of the tasks for about 20 minutes, including creating their summaries.\rThe two tasks given to the subjects are listed below.\rTask-1: Economic recession\r“A leading newspaper has hired your team to create a comprehensive report on the causes and consequences of the current economic recession in the US. As a part of your contract, you are required to collect all the relevant information from any available online sources that you can find.\rTo prepare this report, search and visit any website that you want and look for specific aspects as given in the guideline below. As you find useful information, highlight and save relevant snippets. Later, you can use these snippets to compile your report. You may also want to save the relevant websites as bookmarks, but remember - your main objective here is to collect as many relevant snippets as possible.\rYour report on this topic should address the following issues: reasons behind this recession, effects on some major areas, such as health-care, home ownership, and financial sector (stock market), unemployment statistics over a period of time, proposal,\rPage 91 of 122\r\nexecution, and effects of the economy stimulation plan, and people's opinions and reactions on economy's downfall.”\rTask-2: Social networking\r“The College Network News Channel wants to do a documentary on the effects of social networking services and software. Your team is responsible for collecting various relevant information (including statistics) from the Web. As a part of your assignment, you are required to collect all the relevant information from any available online sources that you can find.\rTo prepare this report, search and visit any website that you want and look for specific aspects as given in the guideline below. As you find useful information, highlight and save relevant snippets. Later, you can use these snippets to compile your report. You may also want to save the relevant websites as bookmarks, but remember - your main objective here is to collect as many relevant snippets as possible.\rYour report on this topic should address the following issues: emergence and spread of social networking sites, such as MySpace, Facebook, Twitter, and del.icio.us, statistics about popularity of such sites (How many users? How much time they spend? How much content?), impacts on students and professionals, commerce around these sites (How do they make money? How do users use them to make money?), and examples of usage of such services in various domains, such as health-care and politics.”\rWhile the experiment was designed to study how people seek information in collaboration, for the purpose of this paper, we will consider an individual subject as a unit.\r3. OBSERVATIONSANDANALYSES\rIn this section we present several observations and analyses of how individuals use and re-use queries, and how individual queries overlap with those of the other people with the same information need.\r3.1 Query usage\rWe first consider overall query usage and re-usage. Our subjects used total 4207 queries (aggregated over both the tasks and sessions), of which 1605 were source-wise unique queries, and 1522 were overall unique queries. Thus, nearly 40% of the queries issued were repeated at least once (by the same or some other subjects). Figure 1 shows the sources that were queried by the participants in aggregate with source-wise number of queries (total and unique). A significant portion of all the queries was sent to Google, with CNN (mostly for Task-1) and Bing next most used. It is interesting to note that with every source, a large portion of queries were repeats.\rFigure 2 lists the top sites that the users visited for each task. Google was the most visited site for both the tasks, followed by news sites, such as the New York Times and CNN. For Task-1, the subjects also visited Bureau of Labor Statistics (BLS) and Recession.org website, where many up-to-date statistics on the current economic recession can be obtained. We found that these websites were discovered mostly due to queries such as\r“unemployment statistics US” and “economic recession” rather than by following links with sites or directly typing in URLs.1\rFigure 1: Source-wise query usage and overlap in aggregation (number of queries in log scale)\rTask-1 Task-2\rFigure 2: Top sites visited for both the tasks\r1 A few times subjects even typed queries such as “bls” in Google.\rPage 92 of 122\r\n3.2 Queryre-usageandoverlap\rWe were interested in studying four particular questions about query re-usage (within individuals) and overlap (between individuals). These questions and the corresponding observations and analyses follow.\rQ1. How often do people re-use their own queries?\rFigure 3 plots the total number of queries used for each task and session, along with how many of them were unique. As we can see, a large portion of queries for a given session was already used in that session (about 20 minutes in length). This is also reflected in Table 1, where the query re-use statistics are reported. Both the tasks had on average about 40% of query re-use.\rFigure 3: Task and session-wise individual query re-usage\rTable 1: Task and session-wise individual query re-usage statistics\rWe also looked at the differences in query re-usage behavior for each user across tasks and sessions. In order to do this, we compared a user’s query re-usage for a given task and session to other task and/or session. We then did a pair-wise comparison for all 84 subjects for their tasks and sessions. The significance test results for these comparisons are provided in Table 2. As we can see, for Task-2, there was a statistically significant difference between how people re-used their queries between two sessions. We also found statistically significant difference in query re-usage between the second sessions for both the tasks. Other comparisons showed non-significant differences in individual query re-usage behavior.\rQ2. How often people re-use their queries from the previous session?\rOne of our interests in this study was to look at browsing and query re-use across multiple sessions. We found (Figure 4) that only about 5-10% of the queries used in the second session were\rrepeats from the first session. However, when we expanded our matching criteria to include subqueries (e.g., “economics recession” is a subquery of “economics recession US”), we found a much larger re-usage portion. In fact, for Task-2, we found nearly half of the queries being repeats (as the same exact query or a subquery) from the first session. Our analysis showed that for Task-2, more than half of the queries had “social networking” in them. This may be due to the fact that almost all the facets in this task also had “social networking” as a sub-facet; the subjects found it difficult to investigate those facets without the context of social networking. This also became apparent in the interviews we did after the tasks. For Task-1, on the other hand, the subjects could run fairly independent queries for covering different facets, such as “unemployment stats” and “recession causes”.\rTable 2. Two-tailed paired t-test for measuring the significance of difference in query reusing for different tasks and sessions. Statistical significance at p<0.05 is show in bold.\rFigure 4: Second session query and subquery re-usage rates from the first session\rQ3. What proportion of queries overlap across different people for the same task?\rFigure 5 shows the portion of queries that one used was also used by some other subject for the same task. Similar to above, we looked at not only exact query match, but also subquery matches. We can see that for Task-2, the subjects had a much better agreement on what the queries were reused. This confirms our justification given at the end of the previous question analysis.\rQ4. What proportion of queries is similar across different people of different teams for the same task?\rInstead of simply looking at exact matching queries, we also looked at how close two given queries are. To find this closeness, we used Edit Distance measure. The results are plotted in Figure 6\rComparison\rValue of p\rTask-1: Session-1 to Task-1: Session-2\r0.877\rTask-1: Session-1 to Task-2: Session-1\r0.506\rTask-2: Session-1 to Task-2: Session-2\r0.000\rTask-1: Session-2 to Task-2: Session-2\r0.002\rSession-1\rSession-2\rAverage\rTask-1\r44.02%\r37.79%\r40.91%\rTask-2\r39.92%\r36.84%\r38.38%\rAverage\r41.97%\r37.32%\rPage 93 of 122\r\nand 7. In these figures, the X-axis shows the edit distance between two queries, and Y-axis shows the number of queries. Thus, for Task-1, there were 528 queries that had a closest query with distance zero (exact match), 162 queries that had a closest query with distance one, and so on. For simplicity, edit distance only up to 20 is shown in both the graphs.\rFigure 5: Average query and subquery re-usage proportion for a subject with respect to other subjects\rOnce again, we find that many queries that our subjects used for a given task were the same or very similar to the queries other(s) have used.\rWe also found that in case of Task-2, there was a greater agreement among the participants in formulating the queries, as compared to Task-1 (e.g., 725 queries with zero edit distance for Task-2 vs. 528 for Task-1).\rFigure 7: Edit Distance among the queries for Task-2. X-axis shows Edit Distance between a pair of queries, and Y-axis shows number of queries.\r4. DISCUSSION\rFrom an analysis of query usage of 84 subjects working on exploratory tasks over two sessions, we found support for query re-usage for individuals, and high overlap among the queries of multiple subjects for a given task. Such observations and analyses about query usage and re-usage confirm is that people with same information need tend to express their information need in the same/similar way. This is a driving motivation for collaborative filtering work and query assistance/suggestion. The substantial reuse and overlap demonstrate that such techniques may be even more useful for exploratory searching.\r5. REFERENCES\r[1] Smyth, B., Balfe, E., Briggs, P., Coyle, M., Freyne, J. (2003). Collaborative Web Search. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), p. 1417-1419. August 9-15, 2003. Acapulco, Mexico.\r[2] Teevan, J. 2007. The re:search engine: simultaneous support for finding and re-finding. In Proceedings of the 20th Annual ACM Symposium on User interface Software and Technology (Newport, Rhode Island, USA, October 07 - 10, 2007). UIST '07. ACM, New York, NY, 23-32.\r[3] White, R. & Roth, R. (2009). Exploratory search; Beyond the query-response paradigm. San Franscisco: Morgan- Claypool.\rFigure 6: Edit Distance among the queries for Task-1. X-axis shows Edit Distance between a pair of queries, and Y-axis shows number of queries.\rPage 94 of 122\r\nTowards Timed Predictions of Human Performance for Interactive Information Retrieval Evaluation\rABSTRACT\rToday’s popular retrieval metrics are largely divorced from any notion of a user interface or a user model. These met- rics such as mean average precision produce measures of ranked results quality rather than predictions of human per- formance. Using GOMS, we modify the Cranfield-style of evaluation to create a new evaluation method that makes testable predictions of human performance. While not yet validated by user studies, we demonstrate using our evalua- tion method that such an evaluation technique gives infor- mation retrieval researchers the ability to understand how changes in the interface or in the underlying retrieval al- gorithm impact user performance. Future work should be directed to the creation and validation of evaluation meth- ods that predict user performance and incorporate explicit user interfaces and user models.\r1. INTRODUCTION\rWhile the information retrieval (IR) community has known since the work of Dunlop [8] that IR evaluation could be im- proved with automated usability methods from the field of human computer interaction (HCI) [9], retrieval metrics de- void of explicit user interfaces and user models continue to dominate IR evaluation.\rAs a step towards answering our call to move Cranfield- style evaluation towards a more realistic evaluation [14], we use GOMS, an automated usability method, to create an evaluation method that makes testable predictions of hu- man performance. Cranfield-style evaluation measures the ranking quality of a retrieval algorithm given a test collec- tion of documents, search topics, and relevance judgments. An example of a commonly used metric in Cranfield-style evaluation is precision at rank 10 (P10). The precision at rank 10 is equal to number of relevant documents found within the first 10 documents returned by a retrieval algo- rithm divided by 10. While these metrics can be somewhat correlated with user performance [1, 2, 15] they do not make testable predictions of user performance.\rGOMS estimates the time for expert users to complete a task given a certain interface [4, 10]. The acronym GOMS stands for Goals, Operators, Methods, and Selections. In simple terms, GOMS is about finding the sequence of oper- ations on a user interface that allows the user to achieve the user’s goal in the shortest amount of time. GOMS allows an\rCopyright is held by the author/owner(s).\rThird Workshop on Human-Computer Interaction and Information Re- trieval (HCIR’09), October 23, 2009, Washington DC, USA.\rinterface designer to obtain predicted task times for different interfaces before more expensive user testing.\rIn our case, the IR user has a goal of finding as many relevant documents as possible. The operations are the ac- tions possible with a hypothetical user interface. We em- body “methods and selections” in what we refer to as a user model. The user model we create in this paper is a simple, first step towards better models. For example, our model lacks the ability to perform query reformulation. Consider- able research effort will be required to create user models based on observed user behavior. Even with a better user model, the overall evaluation methodology will need to be validated with user studies to determine the accuracy of the performance predictions [5, 6].\rBy combining GOMS with the Cranfield-style of evalu- ation, we obtain a simulation of user behavior for which all user actions have associated times. For example, from GOMS we know that moving the mouse to a button will take on average 1.1 seconds [11]. From this simulation many testable measures of human performance are computable. In this paper, we compute the number of relevant documents read by the simulated user within 10 minutes.\rEvaluation methods, such as ours, that explicitly incorpo- rate a user interface and a user model allow IR researchers to investigate the impact of interface changes on user per- formance before turning to more expensive user studies for confirmation. In other words, IR researchers can simulate user behavior over a hypothetical user interface to generate testable hypotheses. For example, based on our experimen- tal results, we hypothesize that the user interface determines the relationship between ranked retrieval quality and user performance.\rNext we describe our method in more detail and then follow with our experiments and preliminary results.\r2. METHODS AND MATERIALS\rOur evaluation methodology consists of a hypothetical user interface and a user model defined over that interface. Our hypothetical interface is a simplified version of today’s common web search interface. The interface provides a text box that allows the user to enter and submit a keyword-like query. On submission of the query, the user is presented with 10 query-biased summaries of the top ranked results produced by an underlying retrieval algorithm in response to the query. Each result summary provides a hyperlink or button that when clicked on will take the user to the full document. The interface provides the means for the user to hit a “back button” and return to the search results. The\rMark D. Smucker Department of Management Sciences University of Waterloo msmucker@uwaterloo.ca\rPage 95 of 122\r\nLet t be the total search time.\rEnter query and hit return. (t ← t+K(length(query)+1)) Wait for results & move hands to mouse. (t ← t + W ) for i ← 1 to Number of Results do\rRead and evaluate summary. (t ← t + SE) D ← document at result i\rjudgment ← qrels judgment of D\rif judgment is non-relevant then\rWith probability P0 decide to read D. else if judgment is relevant then\rWith probability P1 decide to read D. else // judgment is highly relevant\rWith probability P2 decide to read D. end if\rif Decided to read D then\rPoint mouse to link/button. (t ← t + P )\rClick mouse button. (t ← t + BB)\rWait for result page to load. (t ← t + W)\rRead and evaluate D. (t ← t + DE)\rif judgment is relevant or judgment is highly relevant then\rnumRelevantRead ← numRelevantRead + 1 end if\rPoint mouse to back button. (t ← t + P )\rClick mouse button. (t ← t + BB) end if\rif ((i+1) mod 10) = 1 then // Only 10 results per page Point mouse to next page link/button. (t ← t + P ) Click mouse button. (t ← t + BB)\rWait for next page of results. (t ← t + W)\rend if end for\rFigure 1: User model. The time each action takes is shown in parentheses. Table 1 lists the model parameters and their values.\rsearch results interface also provides a link or button to take the user to a new page with the next 10 ranked results.\rFigure 1 shows our user model. First, the simulated user enters the query by typing and then waits for the first 10 search results. The user then proceeds to read and evalu- ate the result summaries one after the other. With some probability conditional on the relevance of the underlying document, the user will decide to click on a summary and read the document. After reading the document, the user hits a “back button” and continues reading and evaluating the search result summaries. When the user reaches the end of the summaries on a page, the user clicks on a link or button to request the next 10 results. All actions have associated times.\rOur user model is simple, for demonstration purposes, and not an attempt to capture the complex process of search. For example, while query reformulation could be made possible with our hypothetical interface, our user model is incapable of reformulating queries. Eye tracking research has clearly shown that users quickly reformulate queries that don’t pro- duce top ranked relevant documents [12].\rTable 1 lists the parameter settings of our user model. These settings come primarily from two places. For GOMS, we utilize the keystroke level model (KLM) [3]. In this model, the operators are defined at the level of keystrokes\rTable 1: User model parameters. All times are in seconds. Figure 1 shows the user model.\rand mouse movements. Timings for these operators are av- erages obtained from various user studies [11]. In our use of GOMS, we inadvertently omitted use of the “mental” opera- tor. Even so, most mental actions in our model are involved in the evaluation of the search result summaries and docu- ments and are captured by the SE and DE parameters.\rOur other source for parameter settings comes from the work of Turpin, Scholer, Ja ̈rvelin, Wi and Culpepper [16] who created a methodology to include search result sum- maries into standard list quality metrics such as precision at 10 (P10) and mean average precision (MAP). As part of their work, they asked users to determine whether or not to click on a summary and view the corresponding docu- ment. If the user felt the summary would lead to a relevant document, the user would decide to click on the summary. Users then judged the relevance of documents on a 4 point graded scale. On average, users took 19 seconds to evaluate a summary and 88 seconds to evaluate a document. While we know that eye tracking results show that users usually spend much less than 19 seconds reading a summary [7], we utilize Turpin et al.’s timings to be consistent with their measures of summary evaluation accuracy.\rIn a simulation analysis of TREC 9 and 10 submitted runs, Turpin et al. mapped their two highest relevance categories to TREC’s “highly relevant” and their least relevant cate- gory to “relevant” and finally mapped non-relevant to non- relevant. With this mapping, the probability that a user would click on a summary was 0.77 for highly relevant doc- uments, 0.53 for relevant, and 0.25 for non-relevant. These summary evaluation accuracies are in line with the 75% ac- curacy found by Sanderson [13]. We use these probabilities in our experiments with the same TREC 9 runs.\r3. EXPERIMENTS\rFor our experiments we use the 40 automatic, title only ad-hoc web retrieval runs from TREC 9. For each run we compute the precision at 10 (P10) as well as the number\rKeystroke (average non-secretarial typist 40 wpm) [11]\rK = 0.28 s\rType a sequence of n keys [11]\rn×K s\rPoint the mouse to a target on the display [11]\rP = 1.1 s\rPress or release the mouse button [11]\rB = 0.1 s\rClick mouse button (press and release) [11]\rBB = 0.2 s\rMove hands to keyboard or mouse [11]\rH = 0.4 s\rMental act of routine thinking or percep- tion [11]\rM = 1.2 s\rWait for search results or web page to load\rW=1s\rTime to evaluate a search result summary [16]\rSE = 19 s\rTime to evaluate a document for rele- vance [16]\rDE = 88 s\rProbability of clicking on non-relevant sum- mary [16]\rP0 = 0.25\rProbability of clicking on relevant sum- mary [16]\rP1 = 0.53\rProbability of clicking on highly relevant summary [16]\rP2 = 0.77\rPage 96 of 122\r\nPerfect Summaries\rRead Documents Twice as Fast Better Summaries\rRead Summaries Twice as Fast Normal\rAverage Improvement over Normal Model\rCondition\rPerfect Summaries\rRead Documents Twice as Fast Better Summaries\rRead Summaries Twice as Fast\rPercent Imp. 80% 38% 23% 17%\rTable 2: Results for the 4 interface improvements described in Section 3. For each of the 40 TREC-9 runs, user performance is measured as the number of relevant documents read within 10 minutes.\rof relevant documents read by our simulated user within 10 minutes. Because there is inherent randomness in our user model caused by the different probabilities of clicking on a result summary, we simulate usage 1000 times for each topic of each run and average the predicted performance.\rIn addition, we examine 4 possible interface improvements:\r1. We modify the result summaries so that users can eval- uate them twice as fast (9.5 s rather than 19 s).\r2. We improve the evaluation accuracy of summaries. For relevant and highly relevant documents, the summary evaluation accuracy increases by 25% (0.53 to 0.663 and 0.77 to 0.963) and for non-relevant documents the error rate decreases by 25% (0.25 to 0.188).\r3. We provide some means for the users to evaluate doc- uments twice as fast (44 s rather than 88 s).\r4. We make summaries perfect. All relevant and highly relevant documents are viewed, and users waste no time reading non-relevant documents. While likely an impossible interface improvement if evaluation time remains unchanged, this change allows us to see the maximum possible gain for improvements in summary evaluation accuracy.\rWe naively assume all interface improvements do not af- fect other aspects of the search process. For example, for improvement 1 above, users can evaluate summaries faster with no decrease in evaluation accuracy.\r4. RESULTS AND DISCUSSION\rTable 2 shows that our evaluation method predicts that each of the interface improvements would increase the num- ber of relevant documents evaluated by the user within 10 minutes.\rWhile our interface improvements are all “what-if” exper- iments, we can see in Figure 2 that under the assumptions of our evaluation method, the user interface determines the relationship between ranked retrieval quality and user per- formance. What good is a 20% improvement in P10? The answer depends on the quality of the user interface. Better interfaces better translate retrieval gains into user perfor- mance gains.\rBased on Figure 2, should we conclude that P10 is a metric that mirrors user performance when performance is defined to be the number of relevant documents examined within 10 minutes? No. We’ve replaced one evaluation method of re- trieval quality with another but neither have been validated against actual human performance.\r0.00 0.05\r0.10 0.15 0.20 Precision at 10 (P10)\r0.25 0.30\rPage 97 of 122\rFigure 2: This figures shows the precision at 10 (P10) vs. the predicted number of relevant docu- ments read within 10 minutes for each of the 40 TREC 9 runs and the 5 interface conditions de- scribed in Sections 2 and 3.\rWhat we have with our new evaluation method is a method that aims to be directly predictive of the variable of concern: human performance. Precision at 10 or MAP does not at- tempt to predict human performance. P10 and MAP and metrics like them output a measure of list quality that is loosely coupled with user performance.\rWhat a method like our simple example does is that it marries together retrieval quality, a user model, and the hy- pothetical user interface and makes a prediction concerning user performance. All of these 4 important parts of an evalu- ation of retrieval performance are explicit in our evaluation.\rThe significant shift in thinking that our evaluation method brings about is that when an evaluation method contains all of these components, we gain the ability to start asking ques- tions about what will most improve human performance. In other words, we can look to see where the user is spending time. Is the most time spent manipulating the interface? Or is it spent wading through non-relevant documents? Or is it spent reading documents? Our evaluation method allows the IR researcher to gain insight to these questions.\r5. CONCLUSION\rWe combined an automated usability method, GOMS, with the Cranfield-style of evaluation to produce a new eval- uation method that produces testable predictions of human performance. This evaluation method allows IR researchers to investigate the impact of various interface improvements and also to see the degree to which changes in retrieval qual- ity affect user performance. Future work remains to create accurate, predictive evaluation methods that explicitly in- corporate both the user interface and a model of the user’s search behavior.\rPredicted Number of Relevant Documents Read in 10 Minutes 0.0 0.5 1.0 1.5 2.0 2.5\r\n6. ACKNOWLEDGMENTS\rThis work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), and in part by the University of Waterloo. Any opinions, find- ings and conclusions or recommendations expressed in this material are those of the author and do not necessarily re- flect those of the sponsors.\r7. REFERENCES\r[1] A. Al-Maskari, M. Sanderson, P. Clough, and E. Airio. The good and the bad system: does the test collection predict users’ effectiveness? In SIGIR’08, pages 59–66. ACM, 2008.\r[2] J. Allan, B. Carterette, and J. Lewis. When will information retrieval be “good enough”? In SIGIR’05, pages 433–440. ACM, 2005.\r[3] S. K. Card, T. P. Moran, and A. Newell. The keystroke-level model for user performance time with interactive systems. CACM, 23(7):396–410, 1980.\r[4] S. K. Card, A. Newell, and T. P. Moran. The Psychology of Human-Computer Interaction. Lawrence Erlbaum Associates, Inc., Mahwah, NJ, USA, 1983.\r[5] W. S. Cooper. On selecting a measure of retrieval effectiveness. JASIS, 24(2):87–100, Mar/Apr 1973.\r[6] W. S. Cooper. On selecting a measure of retrieval effectiveness: Part ii. implementation of the philosophy. JASIS, 24(6):413–424, Nov/Dec 1973.\r[7] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In CHI’07, pages 407–416. ACM, 2007.\r[8] M. D. Dunlop. Time, relevance and interaction modelling for information retrieval. In SIGIR’97, pages 206–213. ACM, 1997.\r[9] M. Y. Ivory and M. A. Hearst. The state of the art in automating usability evaluation of user interfaces. ACM Computing Surveys, 33(4):470–516, 2001.\r[10] B. E. John and D. E. Kieras. Using GOMS for user interface design and evaluation: which technique? ACM Transactions on Computer-Human Interaction, 3(4):287–319, 1996.\r[11] D. Kieras. Using the keystroke-level model to estimate execution times. ftp://ftp.eecs.umich.edu/people/ kieras/GOMS/KLM.pdf, copy obtained via Google, http://74.125.95.132/search?q=cache: wvKGAmd5KIIJ:ftp://ftp.eecs.umich.edu/people/ kieras/GOMS/KLM.pdf, 2001.\r[12] L. Lorigo, M. Haridasan, H. Brynjarsd ́ottir, L. Xia,\rT. Joachims, G. Gay, L. Granka, F. Pellacini, and\rB. Pan. Eye tracking and online search: Lessons learned and challenges ahead. JASIS, 59(7):1041–1052, 2008.\r[13] M. Sanderson. Accurate user directed summarization from existing tools. In CIKM’98, pages 45–51. ACM, 1998.\r[14] M. D. Smucker. A plan for making information retrieval evaluation synonymous with human performance prediction. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, pages 11–12, July 2009.\r[15] A. Turpin and F. Scholer. User performance versus precision measures for simple search tasks. In SIGIR’06, pages 11–18. ACM, 2006.\r[16] A. Turpin, F. Scholer, K. Jarvelin, M. Wu, and J. S. Culpepper. Including summaries in system evaluation. In SIGIR’09, pages 508–515. ACM, 2009.\rPage 98 of 122\r\nThe Information Availability Problem\rDaniel Tunkelang Endeca\rdt@endeca.com\rABSTRACT\rIn recent years, library and information scientists, particularly those concerned with interactive information retrieval, have complained that the information retrieval community--both researchers and practitioners--overemphasizes precision as a performance measure. More precisely, the IR community favors measures that emphasize precision in the top-ranked results, either explicitly (e.g., p@10) or implicitly (e.g., average precision, DCG). This essay advocates the study of the information availability problem, a general information seeking problem ill- served by today's models, evaluation measures, and tools. It defines the problem, proposes evaluation criteria for it, and explores how current and future tools could address it. Finally, it considers a testing approach based on the “games with a purpose” framework.\rCategories and Subject Descriptors\rH.3.3 [Information Storage and Retrieval]: Information Search and Retrieval – information filtering; H.1.2 [Models and Principles]: User/Machine Systems – human factors, human information processing\rGeneral Terms\rAlgorithms, Performance, Experimentation, Human Factors.\rKeywords\rinteractive information retrieval, recall, models, evaluation\r1. INTRODUCTION\rTefko Saracevic has described the chasm between system- centered information retrieval researchers and user-centered library and information scientists as a \"battle royal\" [1], perhaps best summed up in the dialogue with which Rijsbergen opens his book on The Geometry of Information Retrieval [2]. The participants are \"B\" (Bruce Croft, representing the system- centered view), \"N\" (Nick Belkin, representing the user-centered view), and \"K\" (the author). While neither side has surrendered, both acknowledge that the system-centered approach controls far more territory, in terms of both research publications and influence on commercial implementations.\rMoreover, it is not just any system-centered approach that has dominated, but specifically one that focuses on ranked retrieval and precision in the top-ranked results. Google's \"I Feel Lucky\" button reflects an endorsement of p@1 as a performance measure. In a less extreme form, the conventional wisdom is that users will quickly abandon a web site or application in frustration if they cannot resolve their information need using the first page of results returned by a search engine, i.e., the ten blue links.\r2. RELATEDWORK\rDespite precision in the top-ranked results as a dominant performance measure, there is no lack of alternatives in the research literature. The following list makes no claim to be exhaustive (I leave that ambitious project to Stefano Mizzaro [3]!), but rather offers highlights representing different conceptions of retrieval performance.\rRecall. Originally used as a set retrieval measure, recall largely shows up today as a component of the f1 measure (the harmonic mean of precision and recall) or in specialized domains like e- discovery. Interestingly, a recent essay by Zobel et al. questions whether recall makes sense as a measure even for those domains [4]. Moreover, research by Dostert and Kelly suggests that people are poor estimators of recall while pursuing recall-oriented search tasks [5].\rk-call. Karger and Chen proposed k-call at n, a binary measure returns 1 if at least k of the top n results are relevant, 0 otherwise [6]. The k-call measure is similar to the %no measure proposed earlier by Ellen Voorhees [7].\rUncertainty. Kuhlthau [8] and others following her (e.g., Wilson et al. [9]) have looked at uncertainty as a holistic effectiveness measure for the information seeking process.\rPRP for IIR. Fuhr proposed this framework to generalize the probability ranking principle, a classical basis for batch retrieval, to an interactive framework that more realistically models actual information seeking behavior [10].\rFindability. Ma et al. propose a findability measure as how reliably players locate a page in a human computation game [12].\r3. MODEL\rThe information availability problem represents an extreme case for which the importance of recall dominates that of precision. Its premise is that an information seeker faces uncertainty as to whether or not some specified information of interest is available through an information seeking support system. Instances of this problem include many high-value information tasks, such as those facing national security and legal/patent professionals, who spend hours or days trying to determine whether the desired information exists. In the problem’s most basic form, the information, if available, resides in a single document.\rThe information availability problem is a realistic use case for testing the effectiveness of information seeking support systems— particularly those that aim to support interaction and exploration. On one hand, the problem is sufficiently concrete to allow for quantitative assessment of user performance. On the other hand, the problem is inherently about task performance rather than\rPage 99 of 122\r\nquery performance, and makes it possible to compare the effectiveness of different interface approaches, or of variations within the same interface.\r4. EVALUATION\rWe propose the following three evaluation criteria:\rTask success. If the information of interest is available, the user achieves positive success by discovering it. If not, then the user achieves negative success by correctly deducing that it is not available. Task failure occurs when the user gives up prematurely, even though the information is available.\rEfficiency. The efficiency of the user is measured simply by the amount of time required to complete the task (successfully or not). Since the task is directed, the user is not expected to spend time in undirected exploration.\rConfidence. When the user achieves positive success, there is no question as to the user’s confidence in the result. The negative case, however, is another story. The user’s confidence in a negative outcome could range from complete (but possibly misplaced) certainty to complete doubt, e.g., giving up out of frustration. The user’s subjective (and self-reported) confidence is an important measure for the negative case.\r5. CURRENTTOOLS\rBefore considering new tools to address the information availability problem, let us consider how existing tools address it.\rFirst, there is the tool most commonly applied to information seeking today in general: ranked retrieval. For some instances of the information availability problem, ranked retrieval is quite effective—namely, the easy cases where the information is available and high precision in the top ranked results quickly leads to positive success. Unfortunately, ranked retrieval stumbles when the information need is difficult for a user to express unambiguously, particularly given the limitations of the user's knowledge of how the information in the system is represented and how the system processes queries [11]. More importantly, ranked retrieval breaks down completely in the negative case: users eventually get frustrated with reformulating their queries and then give up. It is not clear how users can calibrate their confidence in a negative outcome, other than by learning from their own experiences, i.e., extrapolating from instances where they later discover that they failed to find available information.\rThen, there are tools that are explicitly designed to support exploration and interactive information retrieval. These, which include faceted search and query suggestion systems, seem more promising. In particular, approaches built on set retrieval rather than ranked retrieval are a better fit for a problem that emphasize recall rather than precision. These approaches, however, require sophisticated indexing of the content that may not always be practical. For example, faceted search requires that documents be associated with facet values—which in turn requires both a faceted classification scheme and a means of applying it to the corpus.\r6. POTENTIALNEWTOOLS\rTwo extensions to today’s query suggestion systems could help address the information availability problem.\rThe first is the use of query suggestion to increase recall by broadening query results, rather than to increase precision by narrowing or re-ranking them.\rThe second is the incorporation of query previews into a query suggestion system in order to reduce the actual and perceived cost of exploration.\rThe figure above shows a prototype of a tool that incorporates these two elements. In the example interaction shown, a user has imitated a search against a news collection with the query north korea. The system produces a list of ranked query term suggestions: pyonyang, south korea, nuclear weapons, etc. The user can expand the query to include these terms either by selecting their corresponding checkboxes or by moving a slider down to include all of the terms up to that point in the list.\rAs the user manipulates the query expansion, the system offers instantaneous feedback showing how the change affects the query. The feedback includes three elements:\rTerm relatedness. For a given term that the user is considering adding to the query (here, seoul), the system shows statistics relating the term to original query (in this case, north korea).\rExample documents. As the user expands the query, the system immediate shows example documents from the expanded result set. These example documents are newly introduced documents that are representative of the expanded set.\rTopic summary. The tag cloud shows the topics most represented in the result set associated with the current query expansion. The instantaneous feedback allows the user to visualize topic drift and back off if the expansion takes the query on an unproductive tangent.\rWith respect to the information availability problem, such an approach complements techniques aimed at increasing query precision. We can imagine a two-phase approach. In the first phase, the user employs techniques like faceted search to progressively narrow a query—a query elaboration process aimed at precisely expressing the user’s intent. In the second phase, the user employs a tool like that shown in the prototype to broaden from this precise query and thus expand recall. Ultimately, the goal is for the user to achieve high recall for his or her information need, and thus to either efficiently achieve positive or negative success.\rPage 100 of 122\r\n7. GAMESWITHAPURPOSE\rA major challenge with information availability as a research problem is the need for a cost-effective procedure to evaluate candidate solutions. As Voorhees points out, even minor changes to the Cranfield abstraction in order to evaluate interactive information retrieval result take a severe toll on cost-effectiveness of evaluation [13]. User studies are expensive!\rAn alternative approach follows the \"games with a purpose\" agenda proposed by Von Ahn [14]. This approach uses games to motivate people to perform information-related tasks, and has been applied successfully to such tasks as image tagging and optical character recognition.\rThere is even a game that evokes the information availability problem. In Phetch [15], users assume one of two roles, seekers and describers. The seekers compete to find an image based on a text description provided by the describer. The describer’s goal is to help the seekers succeed, while the seekers compete with one another to find the target image within a fixed time limit, using search engine that has indexed the images based on tags generated from yet another game. In order to discourage random guessing, the game penalizes seekers for wrong guesses. The figure above shows an example of a seeker’s view of the game.\rThe Phetch game does not include the possibility of negative success—the target image is always available. But it would be straightforward to adapt the Phetch game so that the target image was removed from all result sets returned to a seeker. The game would need an additional feature—the option for a seeker to assert that the image is unavailable. Correctly making this assertion would lead to success; incorrectly making it would be penalized.\rAn appealing aspect of games with a purpose is that they wrap realistic tasks inside a highly adaptable framework that yields quantitative results. Such a framework may be particularly suitable for the information availability problem.\r8. CONCLUSION\rWhile most of the work on information retrieval has focused on ranked retrieval and precision in the top ranked results, the information availability problem offers a realistic but general scenario that emphasizes recall. We have proposed evaluation criteria and ideas for tools that seem promising for addressing it. Finally, we believe that the games with a purpose framework offers the possibility of cost-effective evaluation.\r9. ACKNOWLEDGMENTS\rThe author thanks Endeca for supporting this work. Specifically, colleagues Joyce Wang and Vladimir Zelevinsky developed the prototype shown in Section 6.\r10. REFERENCES\r[1] Saracevic, T. 2007. Relevance: A review of the literature and\ra framework for thinking on the notion in information science. JASIST 58(3): 1915-1933.\r[2] Van Rijsbergen, C. J. 2004. The geometry of information retrieval. New York: Cambridge University Press.\r[3] Mizzaro, S. 1997. Relevance: The Whole History. JASIST 48(9): 810-832.\r[4] Zobel, J., Moffat, A., and Park, L. 2009. Against Recall: Is It Persistence, Cardinality, Density, Coverage, or Totality?. SIGIR Forum 43(1):3-15.\r[5] Dostert, M. and Kelly, D. 2009. Users' stopping behaviors and estimates of recall. In Proc. of SIGIR 2009: 820-821.\r[6] Chen, H. and Karger, D. 2006. Less is more: probabilistic models for retrieving fewer relevant documents. In Proc. of SIGIR 2006: 429-436.\r[7] Voorhees, E. 2004. Measuring ineffectiveness. In Proc. of SIGIR 2004: 562-563.\r[8] Kuhlthau, C. 1993. A principle of uncertainty for information seeking. Jour. of Documentation 49(4): 39-55.\r[9] Wilson, T. D., Ford, N., Ellis, D., Foster, A., and Spink, A. 2002. Information seeking and mediated searching. Part 2: uncertainty and its correlates. JASIST 53(9): 704-715.\r[10] Fuhr, N. 2008. A probability ranking principle for interactive information retrieval. Information Retrieval 11(3): 251-265.\r[11] Furnas, G., Landauer, T., Gomez, L., and Dumais, S. 1987. The Vocabulary Problem in Human-System Communication. CACM 30(11): 964-971.\r[12] Ma, H., Chandrasekar, R., Quirk, C., and Gupta, A. 2009. Page hunt: improving search engines using human computation games. Proc. of SIGIR 2009: 746-747.\r[13] Voorhees, E. 2006. Building Test Collections for Adaptive Information Retrieval: What to Abstract for What cost? In Proc. of First International Workshop on Adaptive Information Retrieval (AIR).\r[14] Von Ahn, L. 2006. Games with a Purpose. IEEE Computer 39(6): 92-94.\r[15] Von Ahn, L., Ginosar, S., Kedia, M., Liu, R., and Blum, M. 2006. Improving accessibility of the web with a computer game. In Proc. of SIGCHI 2006: 79-82.\rPage 101 of 122\r\nExploratory Search Over Temporal Event Sequences:\rNovel Requirements, Operations, and a Process Model\rTaowei David Wang, Krist Wongsuphasawat, Catherine Plaisant, and Ben Shneiderman\rABSTRACT\rDeveloping a detailed requirement analysis facilitates the building of interactive visualization systems that support exploratory analysis of multiple temporal event sequences. We discuss our experiences with collaborators in several domains on how they have used our systems and present a process model for exploratory search as the generalization of our experiences. This process model is intended as an outline of high-level analysis activities, and we hope can be a useful model for future and on- going exploratory search tools.\rINTRODUCTION\rDeveloping hypotheses about relationships among temporal events and assessing their plausibility are important exploratory tasks in a variety of domains. These tasks can be broken down roughly in two parts: (1) discovering notable event sequences, and (2) evaluating the prevalence of such sequences to strengthen analysts’ confidence in their hypotheses.\rTo this end, several interactive visualization approaches have been proposed to support exploratory analysis in temporal event sequences: business intelligence and financial fraud detection [6], clinical care and medical research [1][3][4][10], and web session logs [2]. These approaches seek to solve the problems analysts face when using a command-line query interface or a pure data- mining approach. However, these approaches have significant differences in their support for interactive exploratory analysis. In particular, they have different support for aggregation, comparison, and advanced exploratory search features over temporal categorical data.\rThis paper focuses on analysis tasks, requirements, and designs for event sequences (e.g. database of electronic health records that contain diagnoses, treatments, interventions, and admission/discharge information, etc.) We introduce two prototype visualization systems: Lifelines2 [9][10] (Figure 1) and Similan [11] (Figure 2). Because the two systems are at different stages of development, and apply different strategies, they support different requirements. We discuss the requirements for\rexploratory analysis over this type of data, and how these systems address these requirements. We then discuss how our case study users utilize these strategies. Finally, we draw from our users’ experiences to present a preliminary process model of information seeking in the context of event histories.\rSENTINEL EVENTS, ALIGN, RANK, AND FILTER\rIn many situations, domain analysts have a question regarding a particular event. We call this central event “sentinel event”. Analysts may seek (1) what are the most commonly occurring events immediately prior to or after the sentinel event, (2) what is the distribution of another event with respect to the sentinel event, (3) or study the length of time between a sentinel event and another event. For example, clinical researchers may be interested in the distribution of mammogram procedures in all patients, prior to their diagnosis of breast cancer, and also seek the average length of time between first diagnosis of cancer and the time of death is.\rHowever, visualizations typically do not provide analysts a way to rearrange the data around sentinel events for a more effective presentation. Instead, the data is often fixed on a linear time line, making sentinel events, which can occur anywhere, hard to spot.\rTo address this problem, we designed the alignment operator. Alignment allows analysts to dynamically re- center the data around a sentinel event across all event histories. This allows patterns specific to the sentinel event stand out. In Figure 1, all histories are centered on the sentinel event 1st Radiology Contrast (yellow triangles), obviates all events around the sentinel event. When histories are aligned, the calendar is set to be relative to the alignment instead of on absolute dates.\rIn Lifelines2 and Similan, analysts can specify a sentinel event by choosing the nth first or last event of a certain type. Additionally, they can also specify all events of a certain type to be all be sentinel events. This multiple alignment allows analysts to study distribution of events near to all occurrences of a specific type.\rDepartment of Computer Science University of Maryland, College Park, MD 20742 {tw7, kristw, plaisant, ben}@cs.umd.edu\rPage 102 of 122\r\nIn Lifelines2, the alignment operator is complemented with more traditional information visualization operators: rank and filter. Analysts can rank all event histories by, for example, the number of occurrences of high-blood pressure diagnoses, reordering the most severe patients to be on the top of the list.\rFigure 1. Screen shot of the Lifelines2 interface. The right portion is the control panel for a variety of operators. Top left is the main visualization panel, where each event history is shown as a horizontal strip on a time line. Each individual event is shown as a color-coded triangle (one event type is one color). The view shows that all histories are aligned by “Radiology Contrast” (the yellow triangles). The bottom half shows the temporal summary view of the red, blue, and green events over the visible time frame.\rFigure 2. Screen shot of Similan. The right portion is the control panel. The left portion contains three major panels. The center panel is the visualization of all event histories. The top panel shows the target history the user has selected. All histories in the center panel are ranked by their similarity to the target. The similarity scores are represented by color-coded bars. The bottom panel shows the comparison between the target against a currently selected history (shown in yellow background in the center panel). The user has selected a timeframe (red rectangular region) over which the match algorithm operates.\rThere are two modes for filter. Analysts can filter in the similar manner as rank by specifying a number of occurrences of a specified event type. All histories that do not have at least that number of that event type will be filtered out. Analysts can optionally designate the occurrences of these events to be only before or after a\rPage 103 of 122\r\nsentinel event. Secondly, analysts can specify a pattern of events to filter out histories that do not contain such pattern in an efficient manner [8]. An event pattern is a temporally ordered sequence of events or absence of events that analysts are interested. For example, analysts can use filter to find all patients who “were diagnosed with high-blood pressure, followed by no diagnosis of heart attack before a stroke.”\rFINDING SIMILAR TEMPORAL EVENT SEQUENCES\rThe align, rank, and filter are the basic operators that allow analysts to study events of high interest and to find related events. However, sometimes analysts are interested in finding temporal event sequences that are similar to a specific history. For example, when a physician encounters a patient with symptoms that are rare and treatment options unknown, the physician may want to find past patients who share similar symptoms or medical history, and investigate the outcomes of different treatments.\rThis specific type of search has two main components. Analysts must specify what portion of a history is important, and what similarity means. In Similan, analysts would first picks a target history, and then choose a range on the time line to select a portion of that history that is relevant. The similarity matching is broken down to two parts. Similan first uses the Hungarian Algorithm [11] see how each history best matches the target. After the matches are found, Similan then assigns a similarity measure based on the number of mismatches and the “cost” of the match (based on temporal distance). Analysts can adjust the importance of mismatches. Analysts can also adjust the importance of out-of-order matches or matches with a large temporal differential.\rEvery history is then assigned a similarity measure, and displayed in descending order so that the most similar ones are on the top of the list. This is similar in spirit to the Rank-by-Feature framework, and allows analysts to review all histories before fine-tuning their search criteria. Analysts can review a similarity search and adjust the parameters of the similarity measure as described above to better suit their purposes.\rThe similarity search is further augmented to support “custom records”. This means that analysts can manually specify a pattern to search instead of having to find one from an existing history.\rGROUPING, SUMMARY, AND COMPARISON\rA natural extension to the variety of search mechanisms is to form subsets of histories for comparison. For example, hospital administrators may compare the differences of red blood cell counts for emergency room patients who experienced trauma and those who had not.\rIn Lifelines2, result of any filter operation can be explicitly made into a group. Analysts can choose to view\rany existing groups. They can also aggregate events for each group by using temporal summaries. Temporal summaries are stacked bar charts, where each stack represents one event type, aggregated over all histories. Analysts can examine the distribution of multiple event types at a glance [9]. The summaries are naturally integrated with alignment, so analysts can examine aggregations with respect to sentinel events.\rUsing temporal summaries, analysts can perform comparison among groups. A typical usage is to create two mutually exclusive groups and then put them side-by- side to study the temporal trend differences. A second use case is to successively narrow down a group of event histories and create successively smaller groups. Examining these groups’ summaries gives analysts insight on whether this exploratory search path is on the right track.\rTHE EXPLORATORY PROCESS MODEL\rFrom working with our collaborators in medicine, student academic records, and law enforcement on drug trafficking phone records, we offer a preliminary process model of how our collaborators use our information visualization systems. Although the preliminary process model has numbered steps, our collaborators typically traverse in steps 2-4 in a pattern that is often not sequential.\r1. Examine data for confidence (overview) 2. Exploratory Search\ra. Iteratively apply visual operators b. Evaluate results of manipulation c. Deal with unexpected discoveries\r3. Analysis, Explanation\ra. Examine paths of search as a whole\rb. Determine to what extend are the\rquestions answered\ri. At the limitation of the system\rii. At the limitation of the data c. Refine existing questions\r4. Report results to colleagues a. Document findings\rb. Disseminate subsets of data 5. Move onto new questions\rOne of the most common results of users looking at their own data through a visualization technique for the first time is the surprise that there are artifacts in the data (systematic errors, lack of consistency, etc.). This is because they have never seen it in an effective format before. As such, our collaborators would cursorily browse the data to make sure the data reflects what they know.\rAfter gaining confidence of the visualization and of the data, they would start seeking answers to their pre- conceived questions. However, new questions often\rPage 104 of 122\r\nspawn when they notice interesting or unexpected data. At this point they would utilize their domain knowledge to try to explain what they see, or they would write down the new question for later exploration. We noticed that analysts may apply alignment on different sentinel events in the same exploratory session to look at the data in different views. They would actively manipulate the display by ranking, filtering iteratively, or change how similarity is weighted in Similan’s search. However, alignment remains the strongest indicator on what focus they have on the data.\rWe found that aggregation techniques such as temporal summaries allow the analysts to look at the data quickly. Many of them learned to visually focus only on the summaries. They would also inspect the previously created groups by comparing their summaries to see qualitatively what kind of progress they have been making, and decide whether the path they are taking has potential. When they see a view of the data that answers their questions or contain interesting discoveries, they would save the state of their progress – saving the current group, and taking screen shots.\rAlthough the process model we present here is still very preliminary, it already suggests elements that are indispensible to exploratory search in temporal categorical data. The first is a way to “anchor” the visualization for a particular path of search (like alignment), and allow analysts to quickly and dynamically change the anchor. The second is an overview of the entire dataset so that a mental model can be built quickly as the data is being manipulated. Next, a way to explicitly track users’ steps of exploration is important. Finally, features that support viewing and comparison of different steps of exploration are critical to backtracking and taking excursions in the search process. We recommend these features for future applications.\rDISCUSSION\rPerforming exploratory analyses using a command-line query tool suffer from the problem that users have no mental model of the data. As a result, users have a hard time making judgments on how to refine their exploratory steps. Similarly, in a pure data-mining approach, lack of a mental model of the data makes interpretation of the results tricky. Information visualization allows opportunities for users to orient themselves at each step of the exploratory search, and enables maintenance of a consistent mental model throughout the process.\rThis paper presents several visualization and interaction techniques to let users control their exploratory paths and sustain a working mental model in searching temporal events. We argue that these approaches are more amenable to exploratory search. Information retrieval applications on temporal data can leverage work presented here to provide users a more fulfilling search experience. We discuss a preliminary process model for\revent sequences, and we hope to see interactive visualization techniques to be used in conjunction with information retrieval or data mining techniques to connect to their users as in [5][6].\rACKNOWLEGEMENT\rThis project is supported in part by the Washington Hospital Center and Harvard - Partners HealthCare.\rREFERENCES\r1.Fails, J. Karlson, A., Shahamat, L., and Shneiderman, B., A visual interface for multivariate temporal data: finding patterns of events over time”, Proc.IEEE VAST, 2006\r2.Lam, H., Russell, D. M., Tang, D.,Munzner, T., Session viewer: supporting visual exploratory analysis of web session logs. Proc. IEEE VAST, 2007\r3.Plaisant, C., Lam, S., Shneiderman, B., Smith, M., Roseman, D., Marchand, G., Gillam, M., Feied, C., Handler, J., and Rappaport, H., Searching electronic health records for temporal patterns in patient histories: a case study with microsoft amalga,” Proc. AMIA Annual Fall Symposium, 2008.\r4.Ong, J., DataMontage Software, http://www.stottlerhenke.com/datamontage, 2006.\r5.Post, A. R., Harrison, J. H.. Protempa: A method for specifying and identifying temporal sequences in retrospective data for patient selection. JAMIA, 2007.\r6.Shahar, Y., Cheng, C., Intelligent visualization and exploration of time-oriented clinical data. Proc. HICSS 1999.\r7. Suntinger, M., Schiefer J., Obweger H., and Groller, M.E. The event tunnel: interactive visualization of complex event streams for business process pattern analysis. Pros. of IEEE Pacific Visualization Symposium ‘08, 111-118, 2008.\r8.Wang, T.D., Deshpande, A., Shneiderman, B., A temporal pattern search algorithm for personal histories, Tech Report # HCIL-2009-14, 2009, http://hcil.cs.umd.edu/trs/2009-14/2009-14.pdf.\r9.Wang, T.D., Plaisant, C, Shneiderman, B., Spring, N., Roseman, D., Marchand G., Mukherjee, V., and Smith, M., Temporal summaries: supporting temporal categorical searching, aggregation and comparison. To appear in Proc. IEEE Infovis, 2009.\r10.Wang, T.D., Plaisant C, Quinn, A., Stanchak, R., Shneiderman B., and Murphy, S., Aligning temporal data by sentinel events: discovering patterns in electronic health records. Proc. CHI, 2008.\r11.Wongsuphasawat K. and Shneiderman B., finding comparable temporal categorical records: a similarity measure with an interactive visualization. To appear in Proc. IEEE VAST, 2009.\rPage 105 of 122\r\nKeyword Search: Quite Exploratory Actually\rMax L. Wilson\rFuture Interaction Technologies Lab Swansea University, UK m.l.wilson@swansea.ac.uk\rABSTRACT\rThis short position paper describes some evidence found that counters the argument that there are better ways to support exploratory search than keyword search. Instead, this paper suggests that keyword search actually provides people with the freedom to search in relation to their own current state of understanding, rather than in the terms controlled by a search system. The challenge for future exploratory search systems, therefore, may be to maintain and enhance such freedoms.\rINTRODUCTION\rSome of the main arguments for research into exploratory search are that there are times when keyword search is not sufficient to support users. Such occasions include times when users who are unsure about a certain domain of information, uncertain about the terminology used by a search system, or unsure, even, about their own information needs [7]. Alternatively, therefore, many have been trying to support users in more exploratory conditions with alternative visualizations and user interfaces. Faceted browsing, clusters, and tag clouds, for example, are techniques that are designed to expose the structure of, or relationships within information to users, so that they can better understand a domain of information.\rSo why is it that keyword search persists? In some occasions, as described below, users have even preferred keyword search during exploratory tasks. While this may be because of people’s familiarity with keyword search, the argument being made here is that exploration involves activities for which keyword search can be quite appropriate. The core of these learning activities, for example, is in making sense of how unfamiliar information fits in with a user’s current understanding. It is potentially important therefore, that exploration allows users to freely express their current understanding. Further, however, hypothesis testing is also an important aspect of sensemaking, where searchers, as they learn, may want to see how results change according to their own ideas and developing conclusions.\rEVIDENCE FOR KEYWORD SEARCH\rAbove, it is suggested that there is some evidence for when users have preferred keyword search for more exploratory activities. In our own research, for example, we have seen that users found the facets in the mSpace browser useful\rmore often for expressing multiple compound constraints in queries, than during exploration [10]. In another study, Capra et al. compared the RB++ browser and an un- configured Endeca1 interface to the Bureau of Labor Statistics website2. First, the website, which of course has been designed for the dataset, performed well for all tasks. Further, however users specifically noted, during exploratory tasks, the lack of keyword search in the RB++ browser [3] (now included in the latest version).\rMore recently, our own research has created an analytical evaluation method [11] that can inspect search interface designs for how they support users in each of 16 searcher conditions. This method was used to evaluate, for example, the interfaces in the above examples [8]. Further, Google’s keyword search was analysed, as shown in Figure 1, where the 16 profiles are described in Figure 2. These 16 search conditions range from users who know exactly what they want, and how to describe it (profile 16) to those who are learning and do not know what they will find (profile 1) [1].\rIn Figure 1, it might be noticed that the least supported searcher profile by keyword search is not profile 1, but profile 5, where users are scanning for an unknown document to take away, by recognizing it when they see it. This represents more browsing behaviour, where the user is trying to use keywords to describe a particular target that they are hoping exists. The support for exploration, however (towards profile 16) actually increases. Conversely, the most supported profiles are those where the user is trying to find a known target, by recognizing, and using keywords in their head. This process is actually better supported, with the help of query suggestions and spelling corrections, than users who know exactly what they want and can specify it, where users have to pick the terms that will most likely put their desired target at the top of the results list.\rSENSEMAKING\rMaking sense of information revolves around a user bridging a gap between their own knowledge and new information they have found [4]. In analysing how people hand-off information from one person to another, during\r1http://www.endeca.com 2 http://www.bls.gov\rPage 106 of 122\r\nshift changes for example, pitching information at the right level of knowledge and understanding for the receiver, is important [6]. During any sensemaking process, therefore, it should be important to see how their own state of knowledge, however superficial, affects results.\rFigure 1: An analysis of keyword search across different searcher profiles, where 16 is the most knowledgeable about their target, and profile 1 represents those learning and exploring [8].\rFigure 2: The 16 searcher profiles from Belkin et al. [1].\rPeople’s own terms can also have a significant affect on memory and information processing. In a study of recalling blogs that participants had previously tagged, Budiu noted that participants performed best when they had tagged it using their own terms rather than the terms within the blog itself [2]. One possible hypothesis from these results is that users may perhaps struggle to interact with unfamiliar terminology laid out in faceted classifications, when they might rather try to communicate their own state of understanding. Even within facets of metadata, users are given the task of trying to find metadata they recognise, which, for all they know, may not be a valid option within the facet. At this point, it may be less effort for the user to say ‘this is what I know’, which is undoubtedly the way conversations would go when seeking the support of\rexperts, or librarians as it used to be. There may be, in fact, no simpler way to express one’s knowledge than to enter terms they understand into an empty box.\rSO WHAT DOES THAT MEAN FOR HCIR?\rThe root of the argument being built here, is that free-text search, is so called because it gives people freedom. The challenge for exploratory search and HCIR, therefore, is to try and maintain or incorporate freedom into interface designs or new visualizations. With many HCIR interface features, like faceted browsing, involving classification schemes built from the data or constructed from the domain of information, this may be challenging. Clustering engines, for another example, cluster around the data or metadata, and cluster labels could mean nothing to the user at all. While it is not uncommon for facets to be filtered by keyword searches [5], or, as in mSpace, for highlights to appear in facets, which relate to a result found in a keyword search [9], it might be of more exploratory value to provide stemming and support for synonyms to highlight related terminology in facets.\rAnother challenge for HCIR design, based on what we know of sensemaking and handoffs, maybe to monitor users and then try to pitch information at their level. It might be that dynamic faceted systems, which select the appropriate facets to show at any one time rather than simply all possible facets, may meet this requirement to some extent already. It might also be possible, however, to modify the terminology in facets, or vary the language in result lists, to terms that the user would understand. Understanding users though, of course, is a hard challenge.\rI by no means have the answers here, but the core of the challenge to the HCIR community will be to properly, beyond the hypothesis of a position paper, investigate the question: why is it that keyword search persists, and is often helpful for exploratory search? It will be this discovery that will allow us to try and replicate the benefits in future designs. Until then, however, the challenge is, while leveraging the benefits of metadata, to try making freedom the core of our human computer interaction designs for information retrieval.\rCONCLUSIONS\rThe aim of this position paper has not been to suggest that the study of exploratory search is not important, or that research into alternative visualizations is not important. There are times, for example, especially where multiple or explicit constraints might be applied, such as in e- commerce, where faceted metadata is particularly useful. Instead, the aim of this position paper has been to highlight that there are elements of the keyword-response paradigm that are actually quite appropriate for exploratory search. While the challenge is to properly find out why keyword search has performed well in exploratory search, until then, the position here is that we should try to replicate keyword search’s freedom in our future exploratory search designs.\rPage 107 of 122\r\nREFERENCES\r1. Belkin, N.J., Marchetti, P.G. and Cool, C., Braque: design of an interface to support user interaction in information retrieval. Information Processing and Management, 29, 3 (1993). 325-344.\r2. Budiu, R., Pirolli, P. and Hong, L., Remembrance of things tagged: how tagging effort affects tag production and human memory. In CHI'09, ACM New York, NY, USA (2009), 615-624.\r3. Capra, R., Marchionini, G., Oh, J.S., Stutzman, F. and Zhang, Y., Effects of structure and interaction style on distinct search tasks. In Proc. JCDL 2007, ACM Press (2007), 442-451.\r4. Dervin, B., Foreman-Wernet, L. and Lauterbach, E. Sense-Making Methology Reader: Selected Writings of Brenda Dervin. Hampton Press, 2003.\r5. Hearst, M., Design Recommendations for Hierarchical Faceted Search Interfaces. In Design Recommendations for Hierarchical Faceted Search Interfaces (2006).\r6. Sharma, N., Sensemaking Handoff: When and How? Proceedings of the American Society for Information Science and Technology, 45, 1 (2009). 1-12.\r7. White, R.W., Kules, B., Drucker, S.M. and schraefel, m.c., Introduction. Communications of the ACM, 49, 4 (2006). 36-39.\r8. Wilson, M.L. An Analytical Inspection Framework for Evaluating the Search Tactics and User Profiles Supported by Information Seeking Interfaces, University of Southampton, 2009, 249.\r9. Wilson, M.L., André, P. and schraefel, m.c., Backward Highlighting: Enhancing Faceted Search. In UIST'08, ACM Press (2008), 235-238.\r10. Wilson, M.L. and schraefel, m.c., A longitudinal study of exploratory and keyword search. In A longitudinal study of exploratory and keyword search, ACM Press (2008), 52-56.\r11. Wilson, M.L., schraefel, m.c. and White, R.W., Evaluating Advanced Search Interfaces using Established Information-Seeking Models. Journal of the American Society for Information Science and Technology, 60, 7 (2009). 1407-1422.\rPage 108 of 122\r\nUsing Twitter to Assess Information Needs: Early Results\rABSTRACT\rInformation needs tell us why search terms are used, helping to disambiguate, for example, what exactly people are looking for with queries such as ‘Orange’ or ‘Java’. It is hard to understand goals and motivations, however, from the keywords entered into search engines alone. This paper discusses the pilot analysis of 180,000 tweets, containing search-related terms, to try and understand how people describe their own needs and goals. The early analysis shows that some terms academically associated with searching behaviours were infrequently used by twitter users, and that the use of terminology varied depending on the subject of search. The results also show that specific topics of searching tasks can be identified directly within tweets. Future analysis of the still on-going 5-month study will constitute more formal text analytical methods and try to build a corpus of real search tasks.\rINTRODUCTION\rSearch is a very loaded term. We seek, search, look, find, and explore information. Traditionally information retrieval has focused on matching keywords to documents, which we now see in most web search engines. Information needs, however, tell us whether searchers have entered ‘orange’ in order to find information about citruses, colours, or corporations. Further, information needs are typically part of larger work tasks [2], where the goal of searching for ‘orange’ may be to write a report, plan a food shop, manage a diet, or buy a phone, etc. Understanding information needs and work tasks, therefore, tells us whether interfaces need to be supporting activities such as: exploration, synthesis of information, comparison, or evaluation [11]. Further, understanding information needs tell us how we should design interfaces that support effective human computer interaction during information retrieval.\rIn this paper, the early stages of an analysis into how people describe and converse about their own information needs are presented. After discussing related work on information needs and analysing twitter, the method and results of this pilot stage analysis are presented. The paper concludes with some potential findings, before discussing the future plans for the full analysis of a 5-month archive of tweets.\rRELATED WORK\rInformation Needs\rGaining insight into real information needs is not trivial. Advances however, have been made by, for example,\rstudying search engine logs [4] and comparing keywords with relevance judgements [13]. Broder [1] noted that web searches typically fall under three categories of: transactional, navigational, and informational. Transactional queries are for web-based activities, such as buying, downloading, printing, etc. Navigational queries are simply to find a known website. Finally, Informational queries are those performed while trying to learn. Rose and Levinson [13] extended these into a hierarchy of goal types, such as types of learning, and types of transactions. Other research (e.g. [10]) has been trying to automatically infer goals based on click behaviour of a searcher over time.\rThe value of understanding information needs and goals is further emphasized by the inclusion of context when setting search related tasks in studies. TREC tasks [3], which are used to benchmark the performance of search systems, are created in association with topics so that it is clear what constitutes accurate results. Capra and Kules [9] further identified the types of contextual information that are important to provide to study participants when creating exploratory search tasks for user studies.\rJarvelin and Ingwersen, in discussing many aspects of information seeking, also noted that separate research areas have focused on both information needs and perceived information needs, where search is more closely related to how users currently understand their information needs [5]. Part of exploratory search and learning often involves first understanding a problem space, and then resolving it.\rFigure 1: Tweets that included the exact text: 'searching the net...', shown in a word tree.\rUsing Twitter as a Resource\rTwitter is becoming a popular medium for communication, and recent work has begun analysing: networks, how\rMax L. Wilson\rFuture Interaction Technologies Lab Swansea University, UK m.l.wilson@swansea.ac.uk\rPage 109 of 122\r\npeople communicate, and what they talk about [6]. Pear Analytics, for example, classified tweets as being either: News, Spam, Self-Promotion, Pointless Babble, Conversational, or Pass-Along. Their results showed that around 40% was babble, 37% was conversational, and, in third place, Pass-Along constituted 9% of the tweets [7]. Similarly, the Web Ecology Project released a sentiment analysis of tweets regarding Michael Jackson’s death [8]. In comparison to a typical archive of tweets, the Michael Jackson archive included a significantly larger portion of negative tweets.\rGATHERING INFORMATION NEEDS FROM TWITTER\rWith the aim of better understanding real information needs, Twitter was analysed as a worldwide resource of people’s public discussions, to find conversation about searching behaviour. Although Pear Analytics said that twitter is mostly used for babble and conversation, these are the elements of their taxonomy, as opposed to news, spam, and self-promotion, that will provide value for this study. Figure 1 shows a basic example, where people used the exact words: ‘searching the net...’. The analysis described here is of the first 2 weeks of a larger 5 month investigation into the ways people describe their own searches on twitter.\rMethod\rTo gather tweets that describe searching behaviour, a Twitter search was automatically queried every hour for the most recent 100 tweets for each of the 10 search related terms1 listed in Table 1. The terms, mainly selected from academic publications from search communities, were also passed through a thesaurus to identify and consider additional English language terms. Alternatives, as in those not used, were checked with a single search on twitter to assess current frequency of use on twitter. The chosen terms were those above a significant drop-off point. This process was performed for two weeks during this pilot analysis. To catch as wide a net as possible, all tweets including these terms were archived without any analysis of whether they were describing searches. That is, although Figure 1 shows a basic example of where people explicitly talk about searching the ‘[inter]net’, this research has aimed to discover real-world information needs and work tasks, which may involve search behaviour in real or physical environments, as in Figure 3. Further, each of these terms were queried in their past, present, and future variations, such as the query ‘find OR finding OR found’.\rTo analyse the tweets, several methods are being considered. The initial analysis here is designed to be more qualitative to a) reveal early interesting qualitative insights, such as in Figure 2, and b) help inform the way that the final dataset should be more formally analysed. Initially, for visualization, tag clouds were considered, however these\r1 Unfortunately, the term ‘browse’ failed during this pilot study, but has been fixed for the 5 month study.\rrevealed very little about what people searched for. After a more structured semantic analysis of tweets, however, tag clouds of identified search topics may provide interesting insights. At this stage, aside from some high-level statistics, Word Trees, using IBM’s ManyEyes project [14], were used to manually and qualitatively explore the content.\rFigure 2: This exact phrase appeared in 2 separate tweets.\rResults\rIn total, 189,452 unique tweets were captured from 163,564 authors. Additionally, 14,959 re-tweets were archived, where users echo the tweets of others to their own network.\rTable 1: Showing a breakdown of the tweets collected during the first 2 week archiving process.\rTerm\rUnique Tweets\rReTweets\rAuthors\rExploring\r21,287\r1,414\r19,119\rFinding\r26,333\r1,107\r25,656\rForaging\r910\r1,627\r790\rHunting\r26,534\r1,123\r22,666\rInvestigating\r19,255\r2,016\r14,488\rLooking\r22,783\r1,267\r21,142\rRetrieving\r3,506\r1,500\r3,269\rSearching\r25,493\r1,788\r20,095\rSeeking\r15,767\r1,380\r12,987\rStudying\r27,584\r1,737\r23,352\rTotals\r189,452\r14,959\r163,564\rFrequency of term use\rOne contribution of this analysis is to see the popularity of different terms as people describe their searching actions. ‘Studying’ was the most popular term used, but, despite being a popular metaphor for how people may search [12], the ‘Foraging’ term, and its temporal variations, were hardly used. Similarly, and perhaps surprisingly, the term ‘Retrieving’, and its variations, were used significantly fewer times than many of many of the other terms. The terms ‘Searching’, ‘Hunting’, and ‘Finding’ were also popular terms, but ‘Hunting’ in particular was often used in relation to sport, as discussed below. While ‘Looking’, as might be expected, was quite popular, two terms relating to exploration (‘Exploring’ and ‘Investigating’) were also quite popular. The term ‘Seeking’, while perhaps quite an academic term for search, was used almost half as often as most other terms, but significantly more than the term ‘Retrieving’.\rPage 110 of 122\r\nFigure 3: Tweets described searching behaviour in both physical and digital environments.\rLanguage associated with terms\rAnother contribution of performing this qualitative analysis, is in being able to see how different terms are used to describe different kinds of searching. Figure 4, for example, shows that the ‘Finding’ terms were often associated with finding an ideal or optimal results. When followed by the word ‘my’, however, the task was often re- finding, and usually for locating where technology was in the home. A third regular use of the ‘Finding’ terms was followed by the word ‘out’, which typically represented more exploratory tasks.\rThe variations of the term ‘Exploring’ were typically used in regards to new places, such as cities and neighbourhoods. Many people self-reported as exploring twitter for the first time. Exploration, however, was often associated with abstract objects, such as ideas, options, and possibilities, but also with genre’s of music and film.\rFigure 4: Use of the term 'Finding' when followed by 'the'. This combination was often associated with ideal individual results, the ‘right’, ‘perfect’, or ‘best’.\rPerhaps interestingly, the variations of ‘Foraging’ were nearly always used in conjunction with food terms. Although people rarely used the term, people self-reported as foraging in cupboards, fridges, kitchens, and freezers, with the aim of locating food at mealtimes. When not\rassociated with food, the term foraging described behaviour in outdoor areas, such as yards or woods, but also within documents.\r‘Hunting’, when not being used to discuss sport, was for: new jobs, people (including witches), and technology. Like the term ‘finding’, ‘Hunting’ was often used in association with adjectives representing optimal results, such as a best, cheapest, or perfect.\rThe ‘Investigating’ terms were typically used in relation to crimes. When used, however, they were often investigating the informational boundaries around such events, as investigating: claims, correctness, cause, and circumstance.\rLike ‘Hunting’ and ‘Finding’, the term ‘Looking’ terms were often related to people, jobs, and technology, and their optimal variations, including ‘best’, ‘right’, and ‘perfect’. The term was also used, however, in association with looking for a new place to live, excuses, and the original copies of objects. People also often described looking for entertainment items, such as music, books, and movies.\rWhen used, ‘Retrieving’ terms were related to gathering lost or distant items, often one’s daughter. The majority of subjects in these tweets, however, were digital, such as retrieving lost or archived passwords, records, files, pictures, and tweets.\r‘Searching’ terms were used for a large range of subjects. While sometimes used in relation to optimal (best, next, perfect) technologies, ‘Searching’ was also used for food, missing people, soul mates, truth, music, friends, and pictures. The ‘Searching’ terms, however, produced the highest number of exact quoted search terms, discussed below. The ‘Searching’ terms also returned the highest number of tweets that described venues for search, such as Google, Facebook, eBay, Twitter, etc.\rWhen not used for adult advertisements, the term ‘Seeking’ was primarily used for finding people for jobs, or a place to stay. It was also heavily used with exploratory and abstract terms such as ‘the truth’ and ‘to be understood’. ‘Seeking’ terms were also used in breaching peoples boarders, such as ‘new lands’ and ‘faces’.\rFinally, the studying term was primarily used when discussing forth-coming exams. Sometimes, however, studying was associated with self-driven learning on topics such as the bible, psychology, and photography. Consequently, the ‘Studying’ terms provide some interesting topics for learning tasks in studies, including the history of tobacco and the effects of erosion.\rSpecific subjects of search\rFinally, a third contribution of the analysis is in identifying specific searching tasks. Figure 5, for example, shows three complicated self-reported information needs. The first represents a complex search need, where the user has two pieces of related information. The second and third represent more exploratory learning tasks. Figure 6,\rPage 111 of 122\r\nhowever, shows that many twitter users directly provided search terms they had used, using speech marks. Figure 6 indicates those that explicitly used the past-tense variation of ‘Searching’ followed by the word ‘for’ and then speech marks.\rFigure 5: One complex search task and two exploratory tasks described by twitter users.\rFigure 6: Twitters often labelled, using speech marks, exact specific terms they had queried different services for.\rCONCLUSIONS\rThis work has reported the early pilot analysis of a work-in- progress investigation into tweets that included searching- related terminology, archived in the first 2 weeks of a larger 5 month study. The analysis revealed early insights into how often, and in regard to which forms of search, different search terms were used by twitter users when discussing their own searching behaviours. Where previous research has typically tried to deduce information needs from search engine logs, this research is trying to identify information needs from publically available conversations on the web.\rIn completion of the full 5 month long study, more formal text-analysis techniques will be applied, perhaps including a sentiment analysis [8], to find out if, for example, the search behaviours that people feel are worth tweeting mainly surround difficult or novel searches. Further, such an analysis may be able to identify the frequency, subject, and success of different types of searching goals [13]. Part of the aim, therefore, will be in building a resource of realistic search tasks for different types of searching contexts, which can be used in future user studies, and informed by people’s own self-driven descriptions of searching behaviour. The research described here, however, provides early insights into how people describe and communicate their own searching activities to others.\rUnderstanding how people perceive their searching activities and needs can help inform the design of interfaces for human computer interaction during information retrieval.\rREFERENCES\r[1] Broder, A. A taxonomy of web search. ACM SIGIR Forum, 36, 2 (2002). 3-10.\r[2] Byström, K. and Hansen, P. Work tasks as units for analysis in information seeking and retrieval studies. in Bruce, H., Fidel, R., Ingwersen, P. and Vakkari, P. eds. Emerging Frameworks and Methods, Libraries Unlimited, Greenwood Village, CO, 2002, 239-251.\r[3] Harman, D.K. The TREC conferences. (1997). 247-256.\r[4] Jansen, B.J. and Spink, A. How are we searching the World Wide Web?: A comparison of nine search engine transaction logs. 42, 1 (2006). 248- 263.\r[5] Järvelin, K. and Ingwersen, P. Information seeking research needs extension towards tasks and technology. 10, 1 (2004). 10-11.\r[6] Java, A., Song, X., Finin, T. and Tseng, B., Why we twitter: understanding microblogging usage and communities. in KDD'07, (2007), ACM New York, NY, USA, 56-65.\r[7] Kelly, R. Twitter Study - August 2009, Pear Analytics, 2009, 1-13.\r[8] Kim, E., Gilbert, S., Edwards, M.J. and Graeff, E. Detecting Sadness in 140 Characters: Sentiment Analysis of Mourning Michael Jackson on Twitter, Web Ecology Project, Boston, MA, USA, 2009, 1- 15.\r[9] Kules, B. and Capra, R., Creating exploratory tasks for a faceted search interface. in HCIR'08, (2008).\r[10] Lee, U., Liu, Z. and Cho, J., Automatic identification of user goals in web search. in WWW'05, (2005), ACM New York, NY, USA, 391-400.\r[11] Marchionini, G. Exploratory search: from finding to understanding. Commun. ACM, 49, 4 (2006). 41-46.\r[12] Pirolli, P. and Card, S., Information foraging in information access environments. in CHI'95, (1995), ACM Press, 51-58.\r[13] Rose, D. and Levinson, D., Understanding user goals in web search. in WWW'04, (2004), ACM New York, NY, USA, 13-19.\r[14] Viegas, F., et al. Manyeyes: a site for visualization at internet scale. IEEE Trans. Visualization and Computer Graphics, 13, 6 (2007). 1121-1128.\rPage 112 of 122\r\nIntegrating user-generated content description to search interface design\rABSTRACT\rIn this paper, the ideas discussed will focus on the integration of user tags into information search and interface design. There are two propositions: 1) user- created tagging is a valuable source of user‟s personal views and annotations that can augment the content description of information resources; 2) information search can be viewed as seeking meaning in information use and need. It is suggested to draw user meaning from the tag data by employing the topic and comment as two dimensions of linguistic meaning and to represent the meaning as a simple semantic relation that can be used for clustering the search results to supplement the traditional topic-based information matching. The sample analysis was done with the user tagging data positioned in the Delicious site to identify the semantic relations of linguistic meaning.\r1. Introduction\rTagging is one of the fastest growing applications on the web and has been gaining popularity lately. Tagging allows users to label and assign terms to information objects for later access. The aggregated tags, a product of collaborative tagging, can not only be used as information organization and management tools for the user who created them, but also they can be shared by other users to search, browse and access information resources. The aggregated tags can add user meaning to the content to represent document content from the users‟ perspectives in addition to the original content. With the folksonmic advantage of free forming content description from the bottom-up, user tags are a vital source to help retrieval queries for the public and can augment the authoritative document organization and classification systems as well [4, 6]. This paper will discuss the incorporation of user tagging data into information search interface design for displaying search results to help supplement the traditional topic-based keyword searching.\rCurrent search engines and information retrieval systems are based on the keyword matching of the terms presented between the document representation (i.e., surrogate) and the user query. Matching is done for each term independent of other terms in the user query or in the document content often causing the user meaning to be lost in the search process. With the constantly increasing\rgrowth of information resources on the Internet, information search on a topic usually results in a large number of information contents; this makes it necessary for the user to go through a long list of search results to find relevant ones or to refine the search. It has become a challenge to provide a meaningful user interface for users to effectively browse and filter out the search results.\rOne way to meaningfully present the content of the search result is with the use of semantic relations from user tagging data. Integrating user tags to the content description can help users browse and make relevance judgments. The interface will provide a search result that represents the users‟ descriptions of specific attributes and attached meaning from the user tags as well as providing additional topical identification. A simple semantic relation of user tagging is suggested by employing the concept of topic and comment to devise a meaningful user interface. Topic and comment is adapted from social linguistics as the two distinct components of linguistic meaning.\rThe following three sections present: 1) tagging as a user- created content description is useful to information representation, 2) information seeking and search can be seen as seeking meaning, and 3) constructing semantic relations of content representation from user tagging data is suggested with sample analysis of tagging data. A set of sample analyses will then follow.\r2. User-generated content description for\rinformation search\rTagging is not merely a tool or mechanism for creating information structures but also a tool for reconceptualizing information architecture [3]. One of its values is in the process of freely describing and assigning labels to information resources, creating folksonomies. Through this process of categorizing and assigning terms, meaning can emerge from the users on the information content, which can add the personal specific user meaning attached to the content description beyond the generic (i.e., free of context) topic identification. The idea of a bottom-up process of a user‟s direct personal specific description can serve as an additional access mechanism for other users to share annotative reviews and narratives.\rKyunghye Yoon\rState University of New York at Oswego Oswego, NY 13126 kyoon@cs.oswego.edu\rPage 113 of 122\r\nThe aggregated tagging across the users opened up a new way for the public users to contribute to generations of content description; this information resource can augment the traditional information organization such as library classification systems produced by professionals and authorities [11]. The aggregated user content description is viewed as a valuable source to augment the traditional document representation of the rigid and unitary language model for digital libraries and second- generation web development [12]. According to the social constructionist view of information science, the traditional information description assumes that “documents have a substance [i.e., objectively identifiable meanings or messages that can be represented in a clear structure of terms (nouns)]” [12].\rThere have been a few attempts to make use of folksonomic characteristics of user tagging incorporated in the traditional and controlled vocabulary-based classification and representation schemes. Face tags is one example that shows a semantic approach to collaborative tagging by incorporating faceted class ificat ion schemes to facilitate mult id imensional browsing where it is assumed that users provide the structure with a folksonomy [9]. Bubble up tags is another example in which aggregated terms of the most popular tags are assumed to represent the content [10]. Terms together in a group may indicate a semantic relationship and association among terms and can be a useful content description. Even though the co-occurrence of terms does not identify any explicit relationship, the value seems to be in the highly movable usage of the terms and their linguistic relations to a user group at specific points in time and space [2]. Overall, these studies suggested an implicit structure in the usage of terms in folksonomy and a rich source for metadata filters based on shared or divergent approaches to the categorization of knowledge [2].\rFacets of tags and bubble up tags are an attempt to incorporate the multiple dimensions of words and their relations together, but they are limited in conveying linguistic meaning. They are carried within the topic- based information organization paradigm that assumes independent terms for information search rather than a meaning created by a set of terms with semantic relations.\r3. Information seeking and meaning\rTopic and comment, the two distinct lingu istic components of a meaning, provide an approach to identifying a semantic relation of information seeking [13, 14, 15]. According to functional linguistics, meanings are complete by both topic and comment. Topic is what it is about and comment is what the speaker attaches and relates to the topic. The concept of topic and comment has been applied to information science and the traditional classification theory, which adopted topic as a dominant element to represent document content [1].\rIn the traditional information system with topic-oriented information organization, aboutness of a text is the core dimension for document representation and classification. This is seen in the current information organization and\rretrieval systems: the query of a user is a list of keywords, and document representation (i.e., surrogate) is composed of a list of terms that appear in the text (e.g., inverted index), both of which do not reflect the semantic relationships among the terms but the independent occurrences. The keyword matching has been criticized for the uni-dimensional and generic characterization of topic in the field of information science [7]. It is because topic alone is not a sufficient criterion and needs to be supplemented by other criteria such as situational factors.\rThe meaningful connection between a user need and the information content was sought in a study done of information seeking interaction that empirically examined topic and comment as necessary components of information need description [14, 15]. The study confirmed that both topic and comment are essential in user‟s information need articulation as the two orthogonal dimensions of information to meaningfully describe the user‟s information as well as to represent information content meaningful to a specific user need and use context. The sequence analysis of the user-source interaction showed that topic was employed first to set a common ground for the interaction and then comment provided aspects that stressed the specific use context such as the goal of the user‟s information seeking or the intended use of information [13, 14]. The term comment implies not only the discrete individual attributes of the use context but also the relations among them to the user meaning.\rTh is suggests a strong basis for an argument that information search can be improved if the meaning of the information and the need are related by the two components of topic and comment together. Often users may not address both the topic and comment components when they search for information even though the connection of topic and comment is the full meaning represented in the content. It is because their cognitive state lacks the full meaning when they are in need (i.e., they do not know about something, thus they need to find out about it). But providing the topic and comment relations of the content of search result will be useful for the user‟s relevance check. Given that topic is first employed for the information need specification and then comment is the subsequent necessary component, it is suggested that the comment dimension should be considered as an addition to the content description of the search result, under the current topic-based matching paradigm.\r4. Tag data in search interface design\rThe idea of utilizing the folksonomic description of user tagging data can be attained by representing the two linguistic components of topic and comment of the content as a simple semantic relation. It is a simple semantic relation with two nodes of topic and comment, and a connector, the relation between the two nodes. It is distinguished from the general semantic relation of organized terms to their corresponding hierarchical concepts, in an ontological sense which was applied in information research with explicit and logical associations of concepts and relations [8]. Most ontological relations\rPage 114 of 122\r\nwere concerned with generic (i.e., free of context) and unitary topic-based relations, which were found of little utility in facilitating information retrieval [5]. Utilizing folksonomic descriptions seems to facilitate a stronger and dynamic engagement of user searching based on the use of contexts. It is suggested, in this paper, to take a simple relation of topic and comment from the user-generated content descriptions.\r4.1 Analysis of tag data\rThe analysis was exploratory, initially investigating the possibility of inferring topic and comment relations from user tags created from content description. Even though words appearing together in a document may indicate a useful association, there is yet no explicit relationship identified “even if two tags were used in concert all the time by a wide variety of users across multiple resources, we couldn‟t make any claims about them other than that they are highly related” [10]. Therefore, the analysis was mainly exploratory investigation of capturing semantic meaning from the user by using semantic definitions and relations in natural language, which is expected to provide a basis for later automated inquiries. The analysis focused on the descriptive tags with notes of personal annotation and review among different kinds of tags such as resource type, source and ownership, descriptive and personal [10].\rDelicious (del.icio.us) is chosen for a context of sample analysis because this site is one of the earlier social bookmarking sites, that has minimal limitations in the types of web sites users access and in the way they assign tags and note. A set of sample tagged websites (i.e., bookmarks) were intentionally chosen from articles in journals and blogs in order to retain the independent document unit value of information for content analysis. Table 1 in the appendix shows the four sample sites with top tags listed on the collected date. Overall, there were little explicit semantic relations among the top tags indicated from the list alone. Therefore the analysis was done at the individual user level with user tags and notes. User tags contain a list of single terms the user assigned to the bookmarked website. User notes are a full text of user meaning in natural language form that Delicious allows users to freely attach.\r4.2 Example analysis\rThe analysis was focused on the co-occurrence of the tag terms within individual user‟s tag lists. User notes were particularly helpful to induce the user meaning associated with specific use of tag terms. Users used notes to remind themselves why they bookmarked the material, and why it was important. 1 Often, user notes included quotes from the original content or from the link where the article was located. It also included the user‟s own annotative descriptions and comments. In either case, it was in a natural language text, not a single word, to possibly present a semantic basis close to the user meaning. Thus it helped to understand a user‟s view and analysis of the user defined concept.\r1 This came from user interviews for other related studies not published yet\rFor each sample, a simple semantic relation was constructed from the content analysis of user notes and sets of tags. The intention was to create two different clusters of concepts for the topic and the comment. Even though comment dimension includes verb phrases, the cluster for comment was treated as a cluster of noun phrases similar to that of topic because most user tags were nouns. The relation inferred by verbs from user notes provided a connection between the two nodes. Figures 1 through 4 show the examples of the simple semantic relation of the tag terms. Each of the terms was selected from the top tags and placed in the circle while the cluster of related concepts was formed with connecting circles. Usually, the cluster of a concept included terms to represent the concept at multiple levels as the semantic progression in the text develops. The term is related to the broader concepts as it goes down. There are two main clusters: one of topic in the left and one of comment in the right.\rSome of the terms were driven from user‟s specific attachment rather than presented in the original text. For example, in Figure 2, the concept of internet, hypertext, web and web2.0 was added by users even though the terms were not mentioned in the actual content: the article was forecasting such technologies. Arrows represent the relation between the clusters with the inferred relationship in quotation marks (usually a verb). It was also in line with the bubble up tags that some words tend to occur together. For example, “medicine” and “science” in example 3; and “internet” and “technology” in example 4; the two terms listed mostly together in each set of user tags.\rThe simple semantic relations resemble Hutchins‟ micro structure and macro structure of text semantic progression of the two components of theme and rheme [7]. Macro structure is a semantic relation representing the underlying propositions in the global semantic progression; whereas micro structure is a semantic association of specific and individual segments within the semantic coherence of the global progression [7]. Each cluster of the simple semantic relation from the analysis included terms in hierarchical progression, which is not necessarily the same as in ontological relation even though it does include a broader context of use. Some individual user tags were pertained to the micro structure (i.e., a part of the content rather than the whole text) but the count did not seem to be significant enough to reach the top tags across the user group. The broad level concepts are usually from user- created meanings related to the content such as application area and use dimension that were not included in the original content.\r4.3 Suggestions\rThe simple semantic relation of topic and comment inferred from user tags can be applied to the user interface to provide document description with clustering to help users to better grasp the content in a search situation. Information search starts with one or a partial dimension of information (i.e., topic or comment) as an incomplete meaning with a few keywords because users do not know how to fully represent the need. The search is done by\rPage 115 of 122\r\nmatching the keyword(s) between the user query and the document surrogate. Then the search results are displayed and this can be done by consolidating both topic and comment dimension of the information content. It will be useful to display the document of the search result how the document content is related to the keywords used in the query in relation to its full meaning of the content.\rThe use of the semantic relation in grouping the search results is most effective. One specific way is to group the search results by this new dimension attached to the one used in search, which is found from the semantic relation. Taking Figure 1 as an example, the text can be matched to a query “simplicity” or “simplicity in UI design.” In displaying the text as one of the search result, the description not only contains the “simplicity” how it matches the user query which is mostly done in the current search systems but also the other dimension, “overrated” or “doesn‟t sell” of the content. This new dimension will discriminate the text from others in the search results all of which will match the topic of “simplicity” but with a variety of diverse meanings such as “to improve design process” or “as critical design principle” to make up a few.\rAnother way is to use the terms from the user tags in matching and incorporate the semantic relations in displaying the results. For example with Figure 2, the text can be matched to a query, “Internet history,” or “evolution of Internet” even though the text does not have the term Internet. Then this text will be grouped with those that foretell the Internet. Under the topic used in the search, “Internet history,” the new dimension can be used to create clusters of search results other than those explicitly used in the user query.\r5. Conclusion\rThe value of collaborated tagging was viewed in the creation of the aggregated user assigned information content description that can meaningfully connect other users to the information in the collection. The discussion addressed the nature of users‟ information behavior inherent in the meaning attached to information contents. An attempt was made to capture user-created meaning attached to the content from user tagging data with the implicit relations of their meaning that they were trying to connect with information objects. Topic and comment was used as the basis for simple semantic relations of the user tags. Sample analyses showed interesting evidence for further in-depth investigation of the user-created tagging.\r6. Reference\r[1] Begthol, C. (1986). Bibliographic classification theory and text linguistics: aboutness analysis, inter textuality and the cognitive act of classifying documents. Journal of Documentation 42(2), 84-113.\r[2] Bruns, A. (2008). Blogs, Wikipedia, Second Life and beyond: from production to produsage. New York: Peter Lang.\r[3] Campbell, G. D. & Fast, K. V., (2006) From pace- layering to resilience theory: The complex implications of tagging for information architecture, Proceedings of IA\rSummit 2006 (Vancouver, March 23-27, 2006), ASIS&T. DOI=http://www.iasummit.org/2006/files/164_Presentatio n_Desc.pdf\r[4] Golder, S. A. and Huberman, B. A. (2006). The structure of collaborative tagging systems. Journal of Information Science 32 (2), 198-208.\r[5] Green, R. (1995). Topical relevance relationships. I. Why topic matching fails. Journal of the American Society for Information Science 46 (9), 646-653.\r[6] Heymann, P., Koutrika, G., and Garcia-Molina, H. (2007). Can social bookmarking improve web search? Technical Report InfoLab 2007-33, Department of Computer Science, Stanford University, Stanford, CA, USA, DOI= http://dbpubs.stanford.edu:8090/pub/2007-33\r[7] Hutchins, W. J., 1977 On the problem of „aboutness‟ in document analysis, Journal of Informatics, vol.1, no.1, 17-35.\r[8] Khoo, C. S. G. and Na, J. C. (2006). Semantic relations in information science. Annual Review of Information Science\rand Technology 40 (2006), 157-228.\r[9] Quintarelli, E. R., Resmini, A. and Rosati, L. (2007). Face tag: Integrating bottom-up and top-down classification in a social tagging system. Bulletin of the American Society for Information Science and Technology 33(5), 10-15.\r[10] Smith, Gene. (2008). Tagging – people powered metadata for the social web. New Riders, CA.\r[11] Trant, J. and Wayman, B. (2006). Investigating social tagging and folksonomy in art museums with steve.museum,\rDOI=http://www.arch imuse.com/research/www2006- tagging-steve.pdf\r[12] Tuominen, K., Talja, S. and Savolainen R. (2003). Multiperspective digital libraries: The implications of constructionism for the development of digital libraries. Journal of the American Society for Information Science and Technology 54 (6), 561-569.\r[13] Yoon, K. (2007). A study of interpersonal information seeking: the role of topic and comment in the articulation of certainty and uncertainty of information need. Information Research, 12(2) paper 304. DOI= http://InformationR.net/ir/12-2/paper304.html\r[14] Yoon, K. (2002). Ph. D. Dissertation, School of Information Studies, Syracuse University\r[15] Yoon, K. and Nilan, M. S. (1999). Toward a reconceptualization of information seeking research: focus on the exchange of meaning. Information Processing and Management 35, 871-890.\rPage 116 of 122\r\nAppendix\rexample Title\rNumber of Bookmarks\rNumber of Notes\rTop Tags with counts\r1\rSimplicity overrated\r450 114\ris\rhighly\r4\rIs Google making us stupid?\r4500\r1146\rGoogle 2056 Internet 1450 Technology 1248 Culture 1080 Reading 1001 Brain 861 Articles 578 Society 426 Web2.0 404 Education 380 Psychology 349 Web 338 Research 291 Thinking 238 Media 236 Writing 230 Learning 216 Trends 204\rBlog 170 Information 170 Toread 169 Science 162 Articles 162 Future 133 Books 133 Attention 121 Intelligence 120 Cognition 103 Mind 92 Atlantic 81\rJuly 8, 2009\r2\r3\rAs we may think\rAnnals of Medicine: The checklist\r1187\r409\r351\r98\rHistory 433\rMedicine 172\rTechnology 384\rHealth 126\rInternet 315\rScience 88\rHypertext 286\rChecklist 80\rVennervarbush 280\rProductivity 80\rMemex 260\rArticle 91\rArticle 195\rInteresting 48\rScience 139\rToread 46\rInformation 103\rNewyorker 38\rWeb 95\rProcess 32\rResearch 76\rHealthcare 24\rReference 72\rResearch 16\rBush 63\rMedical 16\rComputer 58\rInnovation 16\rKnowledge 51\rOrganization 15\rCulture 51\rLearning 14\rVannevar 50\rManagement 14\rWeb2.0 48\rGtd 13\rFuture 47\rEngineering 13\rPhilosophy 43\rBlog 13\rVannevar_bush 40\rInformation 12\rVannevar-bush 40\rBusiness 12\rArticles 36\rChecklists 10\rLibrary 33\rArticles 9\rComputers 32\rDesign 9\rMedia 30\rQuality 8\rMemory 30\rProjectmanagement 8\rIdeas 30\rComplexity 8\rEssay 27\rEssay 6\rVannevar+bush 22\r2007 5\rMay 12, 2009\rJune 23, 2009\rDesign 275 Simplicity 225 Usability 215 Marketing 105 Articles 57 Development 45 Interesting 29 Technology 28 Ui 28\rBlog 26\rCulture 25 Psychology 21\rUx 21\rNorman 17 Complexity 17 Software 16\rHci 15\rInterface 12 Interaction 12 Experience 11 Features 9\rProduct 8 Consumer 7 Donnorman 7 Interaction_design 6 People 6 Don_norman 6 Computers 6\rEssay 6\rDate collected\rApr. 21 2009\rTable 1. Sample top tags used for example analysis\rPage 117 of 122\r\nsimplicity\rcomplexity [more] features\r“say they want” “actually buy”\rconsumer\rpsychology\rpeople\rMarketing culture\rproduct Design\ruserbility\rtechnology UI\rhci\rFigure 1. simple semantic relation of example 1\rFuture\rHistory (of)\rMemex\rTechnology\rInternet, web, Hypertext, W eb2.0, Computer, Media\rInformation\rKnowledge\rPhilosophy\rCulture\r“access to”\rLibrary Memory\rScience, Research\rFigure 2. Simple semantic relation of example 2\rPage 118 of 122\r“stress on” “overrated”\r“foretell” “predict”\r\nChecklist (use)\r9(for) Medicine, Health, Healthcare\r“increase”\r“improve” Science\rProductivity Quality\rResearch\rProcess “manage”\rProject manage ment\r(for) Business\rDesign\r(for)\rComplexity\r“is”\rSoftware development\rOrganization\r(for) Engineering\rInnovation\rFigure 3. Simple semantic relation of example 3\rFuture (of)\rGoogle\rInternet, web 2.0, web\rTechnology Computer\r“from deep reading to skimming” “from deep thinking to short attention”\r“change” “affect”\rMedia\r(the way of) Reading\rBrain, Intelligence, Cognition, Thinking, Attention, Mind,\rPsychology, Science, Research\rW riting\rLearning Education\rCulture, Trends, Society\rFigure 4. Simple semantic relation of example 4\rPage 119 of 122\r\nAmbiguity and Context-Aware Query Reformulation\rABSTRACT\rHui Zhang\rSchool of Library and Information Science Indiana University\rhz3@indiana.edu\r1. Establish word meanings by harvesting and clustering query\rIn this position paper, we suggest that query ambiguity is a major challenge for IR and there is space of improvement for existing approaches. Thus, we propose a novel disambiguation approach that constructs word meanings based on context mining from user sessions in search engine query log. Our preliminary result makes us believe that it is a promising direction. We also discuss how a search interface benefits from this approach in supporting faceted and exploratory search by context-based query reformulation.\r1. INTRODUCTION\rAn effective information retrieval (IR) interface should behave similarly to a consultant in a way that it will assist users fulfilling their information needs despite the insufficient state of knowledge. Some progresses have been made towards this goal such as query expansion and relevance feedback in the past several decades. However, the problem of query ambiguity is poorly addressed in previous studies, which is abnormal given its significant impact to human-computer interaction and IR.\rThere are two major forms of ambiguity in natural language: semantic and syntactic. The source of semantic ambiguity comes from the usage of polysemy1 whereas the source of syntactic ambiguity comes from the construction of sentences and phrases. Because most of the queries submitted to the IR systems are short [9], polysemy and phrasal structure are prevalent sources of ambiguity for IR. In addition to natural language, another major source of ambiguity comes from user intention. For example, a query such as DNA could indicate user’s information needs on any of the following topics: health care, law enforcement, or biology.\rMost of the researches on query disambiguation rely on existing knowledge resources such as dictionary and WordNet for word sense definitions (e.g., [4, 6, 10]). The limitation of these approaches is that the information contained in the knowledge resources is either inappropriate or outdated for the underlying tasks. Schutze [7] proposed a unsupervised approach by inducing word sense from term clusters. However, the computing cost of this approach is high, which makes it unfeasible for analyzing large collections even with modern hardware.\rProvide support to end users on query editing and reformulation is one of the core functions of an IR interface. Many initial user queries are unspecific and incomplete due to anomalous state of knowledge, or ASK [2]. A recent study estimated that 16% of the queries submitted to a web search engine are ambiguous [8]. However, because IR system is ineffective on handling query ambiguity, many misleading query suggestions are made. To overcome the limitation of previous methods, we propose solutions for the following three tasks:\r1 One word has multiple possible meanings\rcontext from user sessions in a query log.\r2. Resolve query ambiguity with structured knowledge in Wikipedia and statistical learning.\r3. Assist diversity and exploratory search with context-aware query reformulation.\rTask 1 is a preparatory task, which can be done off-line. However, tasks 2 and 3 are at heart of a search interface and they have significant impacts on the retrieval performance and user satisfaction. We will discuss issues, methods, and preliminary results for each of the tasks in the sections below.\r2. Inducing Word Sense with User Session\rClustering\rAmbiguity of word meaning rises because of the lack of common consent. Therefore, without a universal ontology that provides such common background, one can only establish meanings based on context.\rThere is growing interest in leveraging user query log for search optimization. However, previous studies focus on association at word level, which ignore the problem of query disambiguation. Consequently, improvement on query disambiguation, if any, is insignificant as a by-product of bag-of-words approach. In contrast, we propose a method that will establish word meanings by harvesting contexts from user sessions in a large query log. In the preliminary study, we defined a user session as all click- through pairs (query and clicked URL) submitted by the same user within thirty minutes. The hypothesis of our approach is that:\r• Queries in the same query session normally have similar meanings; therefore, words in those queries can be used to represent one salient sense, or meaning, of a concept that is uttered in the queries.\r• One could obtain major senses of a concept by collecting and analyzing its contexts in all query sessions over a large query log.\rWe also implement an early stage system to verify our approach. The query collection we chose is the AOL query log [5] because it is publicly available. Only queries with clicked URLs are considered and duplicated queries within the same session are condensed. The parsed information is saved in a relational database for later access. Each identified session will be assigned a unique id with information such as user, start time, and end time saved in the database. In addition, each valid click-through pair will be assigned a unique id with information such as query string, clicked URL, and time saved. The relationship between session and click-through is one-to-many. We have so far processed half million lines of user queries to for evaluation. Using the DNA example discussed at the beginning of the paper, we could extract its contexts from user session shown in table 1 (only a portion of the total record is displayed to save space).\rPage 120 of 122\r\nTable 1. Context of word DNA extracted from a query log\rFrom the table, we can conclude that context words in the first session demonstrate the health care topic of DNA whereas context words in the next two sessions probably establish the biology topic of DNA. Another finding of the experiment is that sessions are sparse, which means a large number of them only have one or two unique click-through pairs. In the future research, we will cluster sessions based on the click-through information to obtain distinct meanings. Finally, we could map induced word senses to categories of existing knowledge bases such as Wikipedia. The Wikipedia mapping will assign induced word senses with appropriate labels when, and will make the content of Wikipedia usable for query expansion.\r3. Query Disambiguation with Supervised\rLearning\rUser query disambiguation is difficult because the context in the query is inadequate to establish meanings. To overcome this problem, the general strategy of query disambiguation is to develop more effective disambiguation algorithm and adopt features that are more representative. Previous studies suggest that supervised learning approach produces the best disambiguation performance [1]. However, its effectiveness is restricted by the cost of building training data. Popular learning algorithms require hundreds of labeled instances for each sense to make reliable prediction on incoming queries. Given the number of words and the variety of word usage in reality, it is impossible to build the training set manually. Our query log mining approach provides a solution to this bottleneck. At one hand, it is automatic, which reduces the human involvement to minimum. At the other hand, our approach is more efficient and effective than clustering as it takes advantages of crowd intelligence contained in the query log.\rAnother issue of supervised learning is to extract the most representative features for the underlying decision. For query disambiguation, we propose syntactic features including the part of speech of context and the ambiguous word, identified phrase in the query, position and distance of surrounding context to the ambiguous word.\rIn the situation that the incoming query contains novel terms that the learning algorithm cannot make a confident decision, we will seek help from the richer context about the query topic contained inside the underlying collection. As this particular operation can be integrated with post-retrieval procedure, we will discuss what it means to the search interface together with query reformulation in the next section.\r4. Context-Aware Query Reformulation\rThe goal of query reformulation is modifying the initial user query to make it more descriptive and specific on user’s information need. Previous studies on query reformulation\rprimarily focused on synonym problem, or how to replace a word in the query with more descriptive one [3, 11]. In contrast, we attempt to address the problem of query disambiguation by emphasizing the importance of context in query reformulation. There are two types of contexts involved in query reformulation: the first type is referred as query context that includes query terms, user intention, and statistics of the query; the second type of the context is referred as document context, which includes surrounding words that are frequently co-occur with the target word in the top-ranked returns.\rXu and Croft [12] also conduct co-occurrence analysis on query terms in top-ranked documents. Our approach is different because it considers syntactic relationship (e.g., head-modifier, subject- verb, and verb-argument) in addition to statistical data as we believe that syntactic patterns are reliable evidence of meanings. As shown in figure 1, we demonstrate the process of using document context to disambiguate the meaning of marine in the query of marine vegetation.\rFigure 1. Disambiguate algorithm with document context\rSession_id\rContext1\rContext2\rContext3\r2849961\rmaternity\rpregnant\rtest\r2949075\renzymes\rrestriction\rphysical property\r2949076\rhydration\rconcentration\rspectrophoto meter\rPage 121 of 122\r\nTo implement our proposed approach, the developer of the search interface should answer the following two questions:\r1. How could a search interface take advantage from query disambiguation and reformulation for faceted and exploratory search?\r2. How could a search interface assist query disambiguation and reformulation?\rThe answer to the first question is obvious: the interface may benefit from the disambiguation process in tasks such as query recommendation (e.g., in sponsored search), query suggestion, and result summarization. In addition, our approach will also create a set of pseudo facets of the query topic from local documents that does not rely on any existing knowledge structures, which could be beneficial for rare query. However, the answer to the second question is still unsettled. Users are reluctant to get involved in an interactive IR process due to lack of motivation or frustration of the interface. An effective search interface should be explicit and engaging. For instance, it could highlight words that are salient to meanings in search snippets. In another example, when the system is unclear about user’s intention or the meaning of a polysemy, it could present the most representative result for each candidate to the user in a group with contexts highlighted in the snippet, and let user decide which result to select and learn from that decision as relevance feedback.\r5. Conclusions\rAccess to modern IR systems such as web search engine is not limited to professionals any more. In consequence, end users will expect assistance from the system to remedy their lack of knowledge. At the one hand, they prefer to view only a small number of relevant results; but at the other hand, they also anticipate that the system will provide them different perspectives on the topic to complement their initial search. To meet this requirement, we propose methods that will predict user intention and resolve query ambiguity for effective query reformulation. The proposed approach takes advantages of various resources such as query log Wikipedia. The preliminary result makes us believe that it is a promising direction.\r6. REFERENCES\r[1] Agirre, E. and Edmonds, P. Word Sense Disambiguation: Algorithms and Applications. Springer, 2007.\r[2] Belkin, N. J., Oddy, R. N. and Brooks, H. M. ASK for information retrieval: Part I. Background and theory. Journal of Documentation, 38, 2 (1982), 61-71.\r[3] Guo, J., Xu, G., Li, H. and Cheng, X. A unified and discriminative model for query refinement. In Proceedings of the Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (Singapore, 2008). ACM.\r[4] Liu, S., Yu, C. and Meng, W. Word sense disambiguation in queries. Proceedings of the 14th ACM international conference on Information and knowledge management (2005), 525-532.\r[5] Pass, G., Chowdhury, A. and Torgeson, C. A picture of search. ACM New York, NY, USA, City, 2006.\r[6] Sanderson, M. Word sense disambiguation and information retrieval. Springer-V erlag New Y ork, Inc. New Y ork, NY , USA, 1994.\r[7] Schutze, H. and Pedersen, J. Information retrieval based on word senses. Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval (1995), 161- 175.\r[8] Song, R., Luo, Z., Wen, J.-R., Yu, Y. and Hon, H.-W. Identifying ambiguous queries in web search. In Proceedings of the Proceedings of the 16th international conference on World Wide Web (Banff, Alberta, Canada, 2007). ACM.\r[9] Spink, A., Wolfram, D., Jansen, M. B. J. and Saracevic, T. Searching the web: The public and their queries. Journal of the American Society for Information Science and Technology, 52, 3 (2001), 226-234.\r[10] Voorhees, E. M. Using WordNet to disambiguate word senses for text retrieval. Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval (1993), 171-180.\r[11] Wang, X. and Zhai, C. Mining term association patterns from search logs for effective query reformulation. In Proceedings of the Proceeding of the 17th ACM conference on Information and knowledge management (Napa Valley, California, USA, 2008). ACM.\r[12] Xu, J. and Croft, W. B. Query expansion using local and global document analysis. Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (1996), 4-11.\rPage 122 of 122","title":"HCIR 2009\r","isStoredAs":"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo/#LocalFileDataObject","contentHash":"f655b68381c09877b3ffd266515161763bd2af98","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"mobil","time":1461915878168,"auto":true,"weight":1.0},{"@type":"Tag","text":"122","time":1461915878308,"auto":true,"weight":1.0},{"@type":"Tag","text":"search","time":1461915878285,"auto":true,"weight":1.0},{"@type":"Tag","text":"interfac","time":1461915878338,"auto":true,"weight":1.0},{"@type":"Tag","text":"task","time":1461915878362,"auto":true,"weight":1.0},{"@type":"Tag","text":"queri","time":1461915878206,"auto":true,"weight":1.0},{"@type":"Tag","text":"chunk","time":1461915878261,"auto":true,"weight":1.0},{"@type":"Tag","text":"poi","time":1461915878140,"auto":true,"weight":1.0},{"@type":"Tag","text":"tag","time":1461915878385,"auto":true,"weight":1.0},{"@type":"Tag","text":"user","time":1461915878229,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":6,"appId":"26bfa1c69e468830e78a0edb2e59e26849bf915c","timeCreated":1461915874775,"timeModified":1461915874865,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.026295073,"uri":"file:///Users/cheny13/Documents/2015vis/TVCG/papers/499-lee.pdf","plainTextContent":"499 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rHow do People Make Sense of Unfamiliar Visualizations?:\rA Grounded Model of Novice’s Information Visualization Sensemaking\r3/5/2015\rPCP\rCars From The 1970s And 1980s\r3/5/2015\rCD\rPrivate Driver Service in San Francisco\r(b) Chord diagram\r3/5/2015\rTM\rUnique Users Of The Internet\rM\ra\rr\ri\rn\ra\rFuel Economy (mpg) Cylinders\r5 3.5 0 3.0\rEngine Size (cc) 450\rPower (hp) 220\rWeight (lb) 5,000\r4,500 4,000 3,500 3,000 2,500 2,000\rAcceleration (s) 24\rYear 82\rSearch\rNews\rRetail\rt\rc\ri\rr\rt\rs\ri\rD\rN\rl\ro\ra\rb\ri\rH\rc\rn\ri\rl\r45\r40\r35\r30\r25\r20\r15\r10 4.0\r400 200 180\r81 22 80\r8.0 7.5 7.0 6.5 6.0 5.5 5.0 4.5\rl\ra\rn\ri\rF\rP\ra\rc\ri\rf\rn\ri\rc\rw\rH\rs\ro\rt\re\rn\ri\rg\rw\rh\ro\rt\rD\r350\r300\r250 20080\r60\r150\r40 100 20 0\rRefresh\rClose This Visualization\r20 18 16\r79 78 77 76 75\rSoftware\rReference\rVideo\rSocial Network\rCommerce Photo\rHealth\rR\ru\r160 140 120 100\rPortal\rFinancial\r(c) Treemap\rBlogging\rWeather Travel\rGaming\rs\rs\ri\ra\rn\rH\ri\rl\rl\rn\ro\ri\rt\ri\rd\rd\rA\rn\rr\re\rt\rs\re\rW\rC\ra\rs\rt\rr\ro\rH\ra\ri\rg\rh\rt\rM\ri\rs\rs\ri\ro\rn\rN\ro\re\rP\ro\rt\rr\re\rr\ro\rt\re\rk\rr\ra\rM\rf\rO\rh\rt\ru\ro\rS\r(a) Parallel-coordinates plot\rClose This Visualization\rClose This Visualization\rSukwon Lee, Sung-Hee Kim, Ya-Hsin Hung,\rHeidi Lam, Member, IEEE, Youn-ah Kang, and Ji Soo Yi, Member, IEEE\r1474\r12 73 72\rComputer\rTechnology Adult\rVoip\rRecruitment Online Sport Storage\r10\r8 70\r71\rDating\rFig. 1. The three unfamiliar information visualizations that were used in this study: (a) the parallel-coordinates plot (PCP), (b) the chord diagram (CD), and (c) the treemap (TM).\rhttp://web.ics.purdue.edu/~lee1499/vism/cd_v3.html 1/1\rAbstract— In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice’s information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: 1 encountering visualization, 2 constructing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we\rcompare with other existing models and share further research directions that arose from our observations.\rhttp://web.ics.purdue.edu/~lee1499/vism/pcp.html 1/1\rIndex Terms—Sensemaking model; information visualization; novice users; grounded theory; qualitative study\rhttp://web.ics.purdue.edu/~lee1499/vism/tm.html 1/1\r1 INTRODUCTION\rRecently, it has become more common to see various types of visu- alizations for the general public. Well-known newspaper companies, for example, The New York Times, are actively using visualizations in order to deliver news and opinions with data. Furthermore, new types of visualizations are getting the spotlight as online data storytelling is gaining interest and popularity as we can see through the Tapestry conference. In the past, most of the visualizations that were shown to the general public were primitive visualizations (e.g., line charts, bar charts, and pie charts), however, this is no longer the case.\rEven though various visualizations are exposed to the general pub- lic, it does not matter what visualizations are used and what the un-\r• Sukwon Lee, Ya-Hsin Hung, and Ji Soo Yi are with the School of Industrial Engineering at Purdue University in West Lafayette, IN, USA. E-mail: {sukwon, hung17, yij}@purdue.edu\r• Sung-Hee Kim is with the Department of Computer Science at the University of British Columbia in Vancouver, BC, Canada. E-mail: kim731@cs.ubc.ca\r• Heidi Lam is with Google Inc. in Mountain View, CA, USA. E-mail: heidi.lam@gmail.com\r• Youn-ahKangiswithInformationandInteractionDesign,Techno-Art Division at Yonsei University in Incheon, South Korea. E-mail: yakang@yonsei.ac.kr\rManuscriiptt receiived 31 Mar.. 2015;; acceptted 1 Aug.. 2200115;; dattee off publicationx2x0Aug..2015;dateofcurrenttversiion25Octt..2015. For information on obtaining reprints of this article,, please send e-mail to: tvcg@computer.org.\rDigital Object Identi er no. 10.1109/TVCG.2015.2467195\rderlying stories are if the visualizations do not make sense to the users and audience. As visualization researchers, we should design commu- nicable visualizations and promote the usage of visualizations for the general public. However, we have less understanding on how people make sense of information visualizations. Although some visualiza- tion researchers have recently begun to explore the assessment way of visualization literacy, a user’s ability to use and interpret a visualiza- tion, through a couple of workshops 1 and a paper [6], we have very few studies that carefully describe how a user makes sense of a visu- alization that they encounter for the first time with comprehensive and empirical evidence as Liu and Stasko [29] have pointed out.\rThus, the goal of this study is to investigate how people make sense of unfamiliar visualizations. In particular, we are interested in “novice users” that can be defined as users who have seen a partic- ular type of visualization for the first time. For example, if one sees a parallel-coordinates plot for the first time, the person is a novice user for the parallel-coordinate plot. By focusing on observing novice users when they endeavor to make sense of unfamiliar visualizations, we try to understand the information visualization sensemaking activ- ities. Sensemaking can be defined variously depending on the disci- pline [21, 22, 35, 36]. However, we refer to information visualization sensemaking as “conscious efforts to achieve understanding of how to interpret visual objects and underlying content in an information visu- alization” in this study.\rThe main contributions of this study are as follows:\r• We propose a grounded model of NOvice’s information VIsual-\r1IEEE VIS 2014 Workshop: Towards An Open Visualization Literacy Test- ing Platform and EuroVis 2014 Workshop: Towards Visualization Literacy\r1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\rFile Sharing\rChildren\rForum\r\n500 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rization Sensemaking (NOVIS model) to describe how a novice\ruser makes sense of an unfamiliar information visualization.\r• We share further research directions that arose from our observa-\rtions.\r2 BACKGROUND\r2.1 How Novice Users Making Sense of Visualizations\rCompared with copious literature regarding what kinds of tasks and activities conducted on visualization, there are only few of studies conducted on the topic of how a novice user makes sense of a visu- alization. We found two studies investigating how novice users inter- act with visualizations. Grammel et al. [18] observed novice users’ behaviors while constructing visualization with a given data set and found obstructions (“barriers” in their term) in the process. Likewise, Kwon et al. [26] explored challenges (“roadblocks” in their term) faced by novice users in investigative analysis while using a mature visualization tool, Jigsaw. However, the novice users in their studies more likely mean “users those who are not familiar with information visualization and data analysis” and “non-expert users in investigative analysis,” which differ from the “novice users” in this study, who have seen a particular type of visualization for the first time.\rThe most relevant work we found was a recent work by Peebles et al. [33], which investigated how people, who are not familiar with a parallel-coordinates plot, make sense of this visualization. They, even- tually, identified the following factors hindering proper interpretation: (1) prior knowledge of coordinate systems and (2) crossing lines and visual clutter. While their findings are insightful, they did not provide a comprehensive description of the sensemaking activities or processes of their participants.\r2.2 Graph Comprehension\rMore comprehensive work has been done in the field of graph compre- hension. Most of these studies were conducted with relatively primi- tive visualizations without interaction techniques, such as line charts, bar charts, or pie charts. According to Friel et al. [17], many re- searchers considered graph comprehension as reading and interpreting graph.\rOne interesting finding was that many researchers (e.g., [4, 9, 15]) in graph comprehension converged into three levels of graph compre- hension: the elementary level (can read a specific value in a graph), the intermediate level (can read relationships or trends in a graph), and the advanced level (can read beyond what is presented in a graph). However, we quickly found that not only the three levels of graph comprehension are rather simplistic to explain more comprehensive cognitive tasks with advanced visualizations, but also the studies did not clearly explain how a novice user achieves the elementary level with an unfamiliar visualization.\rMore interestingly, we elicited the following four factors influenc- ing graph comprehension: (1) graph formats, (2) visual characteristics, (3) knowledge about graphs, and (4) knowledge about content [16, 43]. We would like to provide a summary of the factors below.\rGraph Formats Different graphs have different effects on graph comprehension [16, 42]. The graph formats are highly related to char- acteristics of tasks or purposes [24, 43] with the support of empirical studies [45, 48]. For example, line graphs are better for retrieving trends and interactions; bar charts are better for identifying individual data points; and pie charts facilitate proportion judgements [45].\rVisual Characteristics Other visualization characteristics (e.g., color, shade, animation, and data density) affect graph comprehen- sion [43]. This factor arose from graphical perception, which is the de- coding process of information encoded in visual variables [4, 12, 17]. In addition, the graphical perceptual tasks were empirically verified in terms of accuracy [12, 30].\rKnowledge About Graphs Obviously, prior knowledge about graphs influences the level of graph comprehension [15, 16]. If a graph reader possesses prior knowledge about a particular graph, the graph reader would easily translate visually represented information\rin the graph into conceptual knowledge [42]. When a graph format supports expectations that are formed by the knowledge about the particular graph, it positively influences graph comprehension, how- ever, if the expectations are inconsistent with the graph format, it is likely that the graph reader makes errors in interpretations of the graph [43, 33]. Some researchers described the knowledge of graphs as a graph schema [34, 43].\rKnowledge About Content Lastly, knowledge about content af- fects graph comprehension [15, 16, 43]. A graph reader can identify the content of a graph from a title, labels, and other textual informa- tion in the graph and retrieve associated familiar knowledge about the content [15]. The retrieved familiar knowledge can be previous expe- rience or domain knowledge. The knowledge about content influences graph comprehension in three ways: trying to confirm the knowledge, keeping track of information in the graph, and finding errors [42]. Sim- ilar to knowledge about graphs, prior knowledge about content forms expectations or beliefs about data [16]. In particular, Kosslyn [25] pointed out that a graph reader’s previous knowledge about graphs and content is one of the major factors to interpreting graphs.\rIn a generic perspective, Shah and her colleagues proposed a model of graph comprehension based the four factors. According to the model, the first two factors influence bottom-up processes; the last two factors influence top-down processes; and, eventually, graph com- prehension involves the interaction between bottom-up and top-down processes [16, 41, 42]. However, the major assumption of the model is that a graph reader has knowledge about the graph format [41].\r2.3 Other Models of Sensemaking\rIn spite of copious work in the field of graph comprehension, we were not able to find a clear answer to our research question, how novice users make sense of unfamiliar visualizations. Thus, we expanded our literature review into sensemaking in the field of human-computer in- teraction and information visualization to collect relevant information.\rA well-known sensemaking model is a notional model of analysts sensemaking loop by Pirolli and Card [35]. The model describes the intelligence analysts’ cognitive processes from searching external data to presenting to audience. However, this model obviously does not focus on novice users and, furthermore, does not explain how the users make sense of unfamiliar visualizations.\rAnother model is the data/frame theory of sensemaking by Klein et al. [22, 23]. We found that this model might be quite relevant to our study because the model focuses on the seven sensemaking activities and describes how people construct and revise internal mental struc- tures (they call it “frame”) when they make sense of external events. They defined data as “the interpreted signals of events” and frame as “the explanatory structures that account for data” [23] (p. 120). De- pending on data from the events, people refine the existing frame (i.e., elaborating cycle) or put effort to construct a new and better frame (i.e., re-framing cycle) based on the seven sensemaking activities.\rThe notion of the internal mental structure was further refined and applied to the field of information visualization by Liu and Stasko [29]. They paid attention to the dynamic relationships between mental struc- ture (they call it “mental model”) and external visualization, and de- scribed visual reasoning and interaction in this perspective. Although they emphasized the interactive nature between the internal mental structure and visualization, it was not detailed enough how the internal mental structure is created from a visualization and how an individual make sense of a new visualization using the internal mental structure, which are referred to as internalization and processing in their paper.\r3 METHODS\rIn order to fill the identified research gap, we conducted a qualita- tive study by observing how novice users make sense of unfamiliar visualizations. The experiment mainly consisted of three observation sessions and a semi-structured interview. During an observation ses- sion, the experimenters observed how a participant makes sense of one of three visualizations while the participant thought aloud [2, 11]. The semi-structure interview was conducted to find any complemen- tary findings that might be missed in the observation sessions.\r\nLEE ET AL.: HOW DO PEOPLE MAKE SENSE OF UNFAMILIAR VISUALIZATION?... 501\r3.1 Visualizations\rIn order to conduct the experiment within a reasonable time frame, we selected three instances of information visualization for this study (i.e., the parallel-coordinate plot (PCP), the chord diagram (CD), and the treemap (TM) as shown in Figure 1) based on the following criteria. First, we wanted to expand the diversity of visualizations. Thus, we considered a diverse set of underlying data structures: multivariate data, network data, and hierarchical data [20, 32]. Second, we wanted to select visualizations with which most of our potential participants are likely not familiar with. Thus, we ruled out visualizations that K-12 curricula covers (e.g., line charts, bar charts, histograms, scatter plots, and pie charts) [17]. Finally, we chose the specific visualizations and data sets from visualization libraries 2 , 3 and a news article 4 , and modified the visualizations according to our needs.\rThe contexts of the data sets (i.e., car, driver service, and the In- ternet) did not require specific expertise. The data that we used in the PCP was a car data set, which describes car models released from 1970’s to 1980’s. It contained main specifications of the cars (e.g., fuel economy, cylinders, engine size, horse power, and weight). For the CD, we used a data set that describes the frequency of rides through a private driver service, Uber, between various neighborhoods in San Francisco. However, in our pilot study, we noticed that some partic- ipants did not know the company “Uber,” therefore we modified the title using general terms in the actual experiment. For the TM, we used a data set collected by the Nielsen Company, which describes the number of unique users of the top 100 websites within January, 2010.\rEach visualization had a title, describing what they were about. In addition, each visualization had a minimal set of interaction techniques that were essential to use it properly [47]. In the PCP (Figure 1(a)), a user could select a line of interest by moving a mouse cursor on the line (Select) and filter data points by brushing along any axes (Filter). In the CD (Figure 1(b)), a user could filter data points by moving a mouse cursor on an arc (Filter) and find more details on a specific data point from a tool tip (Elaborate). In the TM (Figure 1(c)), a user only had the same tool tip feature as in the CD (Elaborate).\rThe three visualizations did not have any instructions that directly explained how to interpret the visualizations because, during the pi- lot study, we noticed that think-aloud sessions were quite uneventful if a detailed instruction was given. It became more like observing a reading comprehension task, rather than a visualization sensemaking task. Furthermore, we believe that if a user learned how to interpret the visualization from an instruction, the user is no longer a novice user. Thus, we decided to hide detailed instructions on how to interpret vi- sualizations. We realize that this setting is somewhat artificial, but we believe that this setting provides a chance to see how our novice users attempt to make sense of the unfamiliar visualizations.\r3.2 Participants\rA total of 13 participants 5 (9 females, ages ranging from 20 to 60 years old (μ = 32.23, σ = 12.89)) were recruited from a university in the United States. All the participants had basic computer skills, such as using a standard keyboard and a mouse. Particularly, we recruited only native English speakers since the verbalization using a non-native language can be an obstacle during thinking aloud [11, 37]. In order to recruit participants with a diverse demographic background, the ex- periment was announced through a central announcement channel of the university (6 participants were undergraduate students and 7 were university staff members). All the participants were volunteers and they were compensated with $10 for one hour of participation.\r3.3 Apparatuses\rA desktop computer with Microsoft Windows 8 along with a 19-inch LCD monitor (1280 × 1024 resolution), a standard keyboard (how-\r2 http://bl.ocks.org/jasondavies/1341281\r3 http://bost.ocks.org/mike/uberdata/\r4 http://news.bbc.co.uk/2/hi/technology/8562801.stm\r5Originally, we recruited 14 participants, but the data from P01 were\rdropped as the participant’s data were corrupted due to technical issues.\rever, the participants did not need to use a keyboard during the exper- iment), a mouse, and a USB desktop microphone were used. In order to record the verbalization of the participants and the experimenter as well as the participants’ interactions with the visualizations through the mouse, a screen recording software, Camtasia Studio 6, was used.\r3.4 Procedure\rThe experiment was conducted in the following steps. After obtaining the signature on the authorized consent form from a participant, we provided the overall instruction about the experiment. Then, we in- troduced the think-aloud method with some demonstration of how to do it. The participant took two practice sessions to familiarize them- selves with the think-aloud method. In the demo and practice sessions, we intentionally did not use any information visualizations in order to minimize exposure to any visualizations. After the demonstration and practice sessions, we conducted three observation sessions. For each session, a randomly selected visualization out of the three visualiza- tions (Figure 1) were used. The experimenter began each session with the following question: “Please verbalize your thoughts and behavior while trying to make sense of the visualization.” There was flexibil- ity in how long the participant spent on each session and the session ceased when the participant expressed the end of his/her sensemaking (e.g., “I think that is it” or “Okay, that is pretty much I got”). We provided a break between the sessions to the participant in order to minimize the effect of fatigue from each session. After all three ses- sions were finished, we conducted a brief semi-structured interview with the participant to clarify what the experimenter observed. Both the observation sessions and the interview were recorded. Lastly, the participant was asked to fill out a demographic survey questionnaire. The entire procedure for a participant took approximately one hour.\r3.5 Analysis\r3.5.1 Data Prescreening\rOne challenge in recruiting our participants was that we needed to confirm that the participants had not seen the three visualizations be- fore. However, we were not able to show the visualizations to the par- ticipants before the sessions to check the eligibility because it would contaminate them. Thus, we asked “Have you ever seen this type of visualization before?” in the post semi-structured interview in order to know if the participants had prior knowledge of the three visu- alizations. Although we intentionally chose the not-commonly used visualizations (see Section 3.1), in 13 out of 39 sessions, the partici- pants reported that they had seen the similar visualizations previously. However, further investigation (e.g., “Oh really, where did you see? Could you describe the visualization?”) revealed that they were con- fused with irrelevant visualizations (e.g., confusing the PCP with a line chart). As a result, we removed the following five sessions from our data analysis: P02-PCP, P11-PCP, P04-TM, P09-TM, and P11-TM be- cause these participants showed that they were not novice users for the corresponding visualizations. We analyzed the remaining 34 sessions.\r3.5.2 Analysis Procedure\rAll the recordings from the observation sessions and semi-structured interviews were transcribed. Then, we analyzed the transcription from the observation sessions using the qualitative inquiry approach [14, 39] and used the semi-structured interview results for the data triangula- tion purposes. Particularly, in order to come up with an understand- ing that was grounded in data from the participants, we followed a structured manner and procedure, grounded theory [10, 13]. As Creswell emphasized, grounded theory “generates a general explana- tion (a theory) of a process and an action shaped by the views of par- ticipants” [14] (p. 83). By following the grounded theory method, researchers can develop a general explanation does not come from existing models or theories but generated in data from the partici- pants [14]. The general process of grounded theory consists of three coding stages: open coding, axial coding, and selective coding (theo- retical coding or conceptual coding). The open coding stage involves segmenting data by proper size of excerpts, identifying concepts, and\r\n502 final + major (#)  + hung17_v3  IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\r(7) \r(13) \r(19) \rFail to construct  a frame (8) \rNot assure  the frame (8) \rTurn to other   visual objects (15) \r(26) \rTurn to other  visual object (4) \r2 \r1      Encountering Visualiza)on \rConstruc)ng a Frame \rVisual Object \r#  Textual Object \r#  Non#textual Object \rFrame \r#  Frame of Content \r#  Frame of Visual Encoding \r3 \rSuccess in   construc=ng a frame  (25) \rExploring Visualiza)on \r#  Retrieving informa=on \r#  Recalling domain knowledge \rand personal experience \r#  Wandering around visualiza=on \r(2) \rNo=ce  abnormali=es  (2) \rKeep the frame  and discard  abnormali=es (4) \r(12) \r(5) \rEndeavor   to construct a frame (7) \r4 \r    Ques)oning the Frame \r5 \rFloundering on Visualiza)on \rFig. 2. A Grounded Model of Novice’s Information Visualization Sensemaking (NOVIS model). The arrows indicate the major transitions between the five activities and the numbers in the parentheses indicate the number of transitions between the activities what we observed from the data.\rReject   the frame \rThree of authors began open coding from the perspective of inter- pretivists [10, 13, 27]. To precisely characterize the participants’ pro- cesses of making sense of the visualizations and reflect on the contents and nuances of the data, we also used audio/video recordings while coding. In particular, we followed Allen’s key point coding method [1] to determine the efficient excerpts’ sizes. After the coding process, we drew an affinity diagram in order to categorize the codes and to see similarities and differences. In the axial coding process, we reassem- bled the open codes and the categories of open coding, and then identi- fied a core phenomenon while our participants made sense of the three visualizations through several iterations of this process [14, 39]. Dur- ing the iterations, the coder generated several (six) versions of poten- tial models and had intensive discussion sessions with all of authors. Eventually, we were in agreement on the last version of the potential models. In the final selective coding process, we crystallized the core phenomenon of novice users’ information visualization sensemaking and reviewed relevant quotes from the coded data.\r4 FINDINGS\rBy employing the grounded theory method, we attempted to identify cognitive activities of information visualization sensemaking that our participants demonstrated. As a result, we identified the five most salient cognitive activities and observed that the participants traversed the five activities. Based on the result, we developed a grounded model of NOvice’s information VIsualization Sensemaking (NO- VIS model) (Figure 2).\r4.1 Two Core Concepts\rBefore explaining the NOVIS model in detail, we would like to clarify the two terms in here:\rVisual Object A visual object is “any identifiable, separate, or distinct part” (p. 227) as defined by Ware [46]. Visual object is akin to “data”, which is defined as “the interpreted signals of events” (p. 120), in the data/frame theory of sensemaking by Klein et al. [23]. How- ever, we use visual object instead of “data” because we believe the term “data” could be confusing in the context of information visual- ization. Visual objects can be broadly categorized into two: textual objects and non-textual objects. Textual objects are objects that are represented in alphanumeric text format (e.g., a title, legends, axis la- bels, and data labels in a visualization). Non-textual objects are any objects that are represented in non-textual format (e.g., polylines and\rTextual Object: Title \rNon3textual Object: Box \rTextual Object: Label \rcategorizing excerpts; the axial coding stage involves identifying high- level phenomena and central ideas; and the selective coding stage in- volves taking and conceptualizing the central phenomenon and devel- oping a narrative [7].\rConfirm the frame \rFig. 3. Examples of textual objects and non-textual objects with the TM.\raxes of the PCP, chords and arcs of the CD, and boxes of the TM). Some non-textual objects have one or more visual encoding schemes (e.g., in the TM, number of unique users for a website is encoded in size of box and category of website is encoded in color of box). Thus, an information visualization can be considered as a composite set of textual and non-textual visual objects. We marked textual and non- textual objects using the TM in Figure 3.\rFrame A frame is defined as “an explanatory internal structure that accounts for visual objects” according to Klein’s definition [23]. Based on our study, we noticed two distinctive types of frames: frame of content and frame of visual encoding. A frame of content is an explanatory internal structure that explains underlying topic and data. For example, if one thinks, “Hmm, this visualization is about differ- ent websites” while interacting with the TM, the thought is a frame of content. Likewise, a frame of visual encoding is an explanatory inter- nal structure that explains how to interpret non-textual objects. If one thinks, “Ah, I think that the size of each box means a number of users for each website,” the thought is a frame of visual encoding.\r4.2 Five Activities in the NOVIS Model\rAs shown in Figure 2, the NOVIS model consists of the following five activities as well as miscellaneous activities:\r1 Encountering visualization\r2 Constructing a frame\r3 Exploring visualization\r4 Questioning the frame\r5 Floundering on visualization\rM Miscellaneous\rIn this subsection, we would like to introduce the activities in the NO- VIS model with representative quotes from our participants.\r1 Encountering Visualization\r\nLEE ET AL.: HOW DO PEOPLE MAKE SENSE OF UNFAMILIAR VISUALIZATION?... 503\rWe defined encountering visualization as the cognitive activity in which a user faces and looks at an information visualization as a whole image. Obviously, this activity usually happens at the very beginning of each session. However, in 13 out of 34 sessions, the participants did not clearly verbalize their encountering as shown in Figure 4(a).\rIn this activity, the participants did not actively try to make sense of the visualizations yet. However, we thought that this activity was dis- tinctive comparing to other activities because the participants appeared to build their first impression during this activity. In the think-aloud sessions, we observed that the participants expressed feelings or opin- ions about impressions (e.g., “a really crazy chart” (P02-CD)), over- all layout (e.g., “a lot of rectangles and sort of squares” (P06-TM)), colors (e.g., “a bunch of colors, bright colors” (P12-TM)), and com- plexity (e.g., “oh, my goodness, that’s impossible to follow because there are so many blue lines, and they overlap so much” (P07-PCP)) of the visualizations.\r2 Constructing a Frame\rWe defined constructing a frame as the cognitive activity in which a\ruser attempts to construct a frame to make sense of a given visualiza- tion. In our study, the participants often constructed their initial frames after the encountering activity ( 1 ), but we often observed that the participants revisited this activity after other activities.\rIt should be noted that there were different types of frames that our participants attempted to construct. As we discussed in Section 4.1, there are two major types of frames, which are frame of content and frame of visual encoding. When the participants attempted to con- struct a frame of content, they often relied on textual objects (e.g., the title, axis labels, or data labels) as shown in the following quote 6:\r“Let’s see. Financial district, downtown, western addition, south of market, [...] Okay. Private driver service in San Francisco [...] It sounded like neighborhoods. So private driver service in San Francisco. [...] if that means private driver picking you up where youareandtakingyousomewhereinacar.” (P13-CD)\rWhen they attempted to construct frame of visual encoding, we no- ticed that the participants attempted to build a potential frame of visual encoding by conjecturing what each visual encoding scheme meant. Interestingly, they also tried to confirm whether the potential frame was appropriate by comparing the interpretation of a visual object and a corresponding textual object or prior knowledge. For example, P09 successfully constructed a frame of visual encoding (i.e., meaning of each polyline) in the PCP by the aid of the title and labels (i.e., textual objects) and the prior knowledge (i.e., knowledge about cars):\r“Cars from the 1970s and 1980s [...] fuel economies, cylinders, engine size, cubic centimeters, power and horsepower, weight in pounds, acceleration seconds, [...] There’s blue lines connecting from each graph to the other [...] I think each line represents like asingularautomobile.” (P09-PCP)\rObviously, the participants sometimes constructed incorrect frames and such incorrect frames negatively influenced further sensemaking activities. Interestingly, regardless of whether the constructed frames were correct or not, the participants relied on the frames to explore the visualization ( 3 ). We also observed that some participants failed to construct a frame, which will be further discussed in Section 4.4.1.\r3 Exploring Visualization\rWe defined exploring visualization as the cognitive activity in\rwhich a user interacts with an information visualization to discover facts and insights from the visualization based on the constructed frames ( 2 ). The behaviors could be categorized into three sub activi- ties: retrieving information, recalling domain knowledge and personal experience, and wandering around visualization. We listed the three sub activities under the exploring visualization activity because the three sub-activities often happened together and it was challenging to separate them cleanly as the examples in this section show.\r6Embellishments on some visualizations [3, 5] actually may be helpful to construct frame of content, but such cases have not been observed in this study because the three visualizations did not have such embellishments.\rRetrieving Information We observed that the participants re- trieved various information while exploring the visualization. First, the participants retrieved specific data values by reading textual ob- jects (e.g., words and numbers) in the visualization. Particularly, since the participants already had a frame of content in their mind, they were able to interpret the values within the frame (e.g., “It says Burling Heights, 0.9 percent of origins so he really doesn’t start there very of- ten because it’s only less than 1 percent, but if I go up here that’s 4 percent so he starts off at Russian Hill a lot more.” (P03-CD)).\rLikewise, the participants were able to retrieve information by in- terpreting non-textual objects because they had frames of visual en- coding to account for the non-textual objects. When they retrieved information using non-textual objects, they showed the behaviors of comparing, ranking, grouping, and finding patterns in the visualiza- tion rather than reading specific data values. For example, P06, who constructed a frame of visual encoding (i.e., meaning of size of box), ranked the social network services in terms of the number of users by comparing the size of the boxes in the TM:\r“I see that Facebook has the most it looks like because it’s the biggest box in the Social Network’s square or rectangle. Then it looks like the second biggest is Twitter and then Orca, LinkedIn, Classmates, and Meebo look like they’re pretty similar in the numberofusers.” (P06-TM)\rIn addition, another participant, P09, discerned interesting patterns by interpreting lines between pairs of axes in the PCP as correlationships:\r”One is under 20 always seem like they go up to eight cylinders, and ones over 20 are migrating down to four. And then the one is with four cylinders have engine sizes that correlate with the fewer cylinders,andthehorsepowerislowerofcourse.” (P09-PCP)\rRecalling Domain Knowledge and Personal Experience\rDuring the exploring visualization, we observed that some participants recalled domain knowledge and/or personal experience. This is ex- actly what Liu and Heer describe as “Recall” [28]. These activities were often used to check whether they knew a specific textual object in the visualization or not, and they frequently mentioned “I know what it is” or “I’ve never heard of that.” Interestingly, P09 tried to explain why the specific pattern between the year and fuel economy or accel- eration emerged in the PCP using his/her specific domain knowledge (i.e., oil crises in 1973 and 1979):\r“It actually seems most fast cars were produced in 1970. [...] that’s also because with the oil crisis people are focusing more on the fuel economy over here rather than how fast it goes.” (P09- PCP)\rA number of participants recalled their personal experience while exploring the visualization. For example, P04 recalled an experience of visiting a friend who lived in San Francisco while exploring the CD:\r“San Francisco - I have a friend who lives in San Francisco. We tookthetrain,prettymuch,intotown.” (P04-CD)\rRecalling personal experience did not seem to contribute much to a deeper exploration of the visualization. However, it is worth noticing that people showed recalling in not only domain knowledge but also personal experience, which was not directly related to the content of the visualization.\rWandering Around Visualization As a sub activity of exploring visualization, we also observed some participants read text on a given visualization without any specific order. In this case, they successfully built frames of content and visual encoding already in the previous ac- tivity ( 2 ) thus, they knew what the visualization was about and how to interpret the visual objects. They often did the retrieving informa- tion activity based on the constructed frames. However,at some point, they just skimmed through textual objects, especially labels, until they found another interesting point or moved on to read a different visual object. This behavior would seem to be similar to retrieving informa- tion on the surface but the participants just read textual objects one by one meaninglessly. For example, the following quote represents this behavior with the TM:\r\n504 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\r4\r“Shopping dot com, ShopZilla, Target, Best Buy, Priceline, Free. [...]Sears,software,Microsoft.[...].” (P08-TM)\rQuestioning the Frame\rdown.” (P13-CD)), it includes interesting comments from the par- ticipants. They provided positive and negative comments, and even suggestions of a new representation.\rFor example, P07 appreciated the representation of the TM by com- paring it to a visualization of what he/she knew, a pie chart:\r“So I guess this works better if you have a lot of things that you would be trying to fit. [...] I guess if you’re trying to compare everything within each category to everything else, then it would be hard to have a pie graph because there’s so many categories.” (P07-TM)\rInterestingly, P04 suggested a new representation instead of the CD:\r“It would be better if you had each location on the top and on the bottom and figured out a way to lengthen that way [...] It shows you in each direction so I guess you could have one line say origination point and then the other destination and then the crossover points and have them all listed and the percentages. [...]Itwouldbemuchmoreappealing.” (P04-CD)\r4.3 Dynamics in the NOVIS model\rAfter identifying the five activities, we explored series of activities of the participants and transitions between the activities. Figure 4(a) rep- resents series of activities that was demonstrated by each participant while making sense of the visualizations. From the results, it is dif- ficult to say that every participant followed a common sequences of activities. Some participants showed a relatively longer series of di- verse activities (e.g., P03-TM and P04-CD) while other participants showed shorter activities (e.g., P10-PCP and P11-CD).\rHowever, different visualizations or underlying data appear to in- fluence the sensemaking activities. The participants floundered ( 5 ) more often with the PCP (17 times) and the CD (15 times) than with the TM (3 times). Furthermore, the most of the questioning the frame activity ( 4 ) were observed in the sessions with the CD (8 times) compared to those with the PCP (1 time) and TM (4 times). These patterns indicate that the participants with the PCP and the CD failed more times to construct proper frames to explore the visualizations than those with the TM. Due to the nature of the qualitative study and a lack of proper control, it is impossible to discern whether different visualizations, data sets, or something else influence these differences. However, we observed that the same participant showed drastically different sensemaking processes depending on the visualizations (e.g., P06 and P13), so it is worthwhile to know that future studies should be conducted to test effects of diverse visualizations and data sets on information visualization sensemaking.\rAnother pattern in the overall activity transitions of the participants (Figure 4(a)) is that 13 out of 34 sessions do not have the encountering visualization activity ( 1 ). However, we believe that the activity hap- pened and it was merely not verbalized by the participants because we are not able to start a visualization sensemaking without encountering the visualization.\rIn order to do a deeper exploration of the transitions between the activities in the NOVIS model, we calculated aggregate transitions be- tween the activities as shown in Figure 4(b). From the various transi- tions between the activities, we found some general patterns in it.\rFirst, from the encountering activity ( 1 ), the participants showed a pattern of transitions to the constructing a frame activity ( 2 ) or the floundering on visualization activity ( 5 ) but rarely to other activi- ties. This pattern would indicate that the participants were required to construct initial frames of content or visual encoding to move to other activities, especially exploring visualization ( 3 ); or the participants floundered on visualization ( 5 ) without attempting to construct a frame because the visualization was too confusing to the participants (e.g., “Alright. Well, this is really confusing. Orange kind of stuck out to me first. [...] Okay, that’s different. Alright, so what does this mean?” (P11-CD)).\rAnother notable pattern is that there are many mutual transitions between the constructing a frame activity ( 2 ) and the exploring vi- sualization activity ( 3 ). This pattern would indicate that the partic- ipants tried to construct additional frames when they turned to other\rWe defined questioning the frame as the cognitive activity in which a user begins to doubt the constructed frame or tries to verify the frame. This activity occurs when a user is uncertain about the frame after constructing a frame ( 2 ) when he/she finds abnormal visual objects that are not compatible with the frame while exploring visualization (\r3 ). Through this activity, the user tries to confirm that the constructed frame is reasonable to explain the visual object and he/she can adopt the frame to explore the visualization.\rIn the think-aloud sessions, we found that a number of participants showed clear evidence for the activity of questioning the frame. For example, P12 constructed a frame of visual encoding by conjecturing the meaning of colors of arcs in the CD:\r“That’s orange, and I’m wondering if there’s a reason why that’s orange [...] Thinking that maybe the colors could have to do with geographicallocationinSanFrancisco.” (P12-CD)\rHowever, P12 was not confident about the frame because of his/her lack of knowledge about San Francisco. Then, P12 tested and, finally, confirmed the frame with another part of the arcs in the CD:\r5\r“I don’t know really anything about San Francisco, so I don’t know where any of these places are located, [...] Noticing here though, there’s a green section [...]. When I hover over this [...], inner Richmond, outer Richmond, outer Sunset, and then, Golden Gate Park, so I’m going to guess these are still parts of San Fran- cisco.” (P12-CD)\rFloundering on Visualization\rWe defined floundering on visualization as the cognitive activity in which a user does not know what to do with an information visualiza- tion because the user failed to construct any reasonable frames. Since a user does not have proper frames to account for visual objects, vi- sual objects in the visualization do not make sense. Thus, it often leads him/her to be frustrated and confused.\rFor example, we observed P04 who fell into the floundering activity with the PCP. It seemed that P04 failed to construct not only a frame of content after reading the title and/or labels but also a frame of visual encoding to account for lines in the PCP. P04 expressed confusion and superficially described visual objects in the visualization as follows:\r“Cars from the 1970s and ‘80s and a topic I don’t like. [...] It’s kind of hard to tell what I’m actually following. [...] So there’s like, okay. This one I can highlight and a blue thing came over here, but that’s just highlighting those areas. So that’s not helpful. [...]Whatisthat?Oh,mygoodness.” (P04-PCP)\rBased on our observations, we found that the following quotes com- monly represented the floundering on visualization activity: “I don’t know,” “what does this mean?” and “I’m not really sure what this is.”\rWhile external user behaviors in this activity might seem to be simi- lar to those in wandering around visualization of the exploring activity ( 3 ), they are inherently different behaviors. Contrary to the floun- dering on visualization activity, a user has a constructed frame which he/she can base the exploration upon in the wandering around state.\rFrom our observation, we also found that there were mainly two ways to deal with the activity of floundering on visualization. The first was to give up making sense of the visualization. In this case, the par- ticipants exhibited confusion and superficially described objects that caught his/her eyes, and then finished the think-aloud session. In con- trast, some participants made an effort to seek and construct a frame so that they would make sense of the visualization (see Section 4.4.2).\rM Miscellaneous\rOn top of the five activities, the participants demonstrated other ac-\rtivities and we categorized the activities as miscellaneous. As well as minor quibbles that arose while interacting with the visualization (e.g., “The line’s kind of thin, so they’re kind of hard to hover over.” (P09-PCP) and “It’s sort of hard to read the ones that are upside\r\nName\rP03_PCP P04_PCP P05_PCP P06_PCP P07_PCP P08_PCP P09_PCP P10_PCP\rP12_PCP P13_PCP P14_PCP\rP02_cd P03_cd P04_cd P05_cd P06_cd P07_cd P08_cd P09_cd P10_cd P11_cd P12_cd P13_cd P14_cd\rP02_tm P03_tm\rP05_tm P06_tm P07_tm P08_tm\rP10_tm\rP12_tm P13_tm P14_tm\rActivity Dynamics P03    M23\rLEE ET AL.: HOW DO PEOPLE MAKE SENSE OF UNFAMILIAR VISUALIZATION?...\r505\rParticipant\rEncountering  Visualiza1on \rConstruc1ng  a Frame \rExploring  Visualiza1on \rQues1oning  the Frame \rFloundering  on Visualiza1on \rMISC \r1\r2\r3\r4\r5\rM\r1\r2\r3\r4\r3 \r5  26  2  2  21 \r5\rM\rP02\rP04 P05 P06 P07 P08 P09 P10 P11 P12 P13 P14\rP02 P03 P04 P05 P06 P07 P08 P09 P10 P11 P12 P13 P14\rP02\rP03\rP04\rP05     M P06\rP07\rP08           4 P09\rP10\rP11\rP12\rP13\rP14\r5 1 2\r2\r1\r1\r2\r1\r1\r1\r1\r1\r1\r2\r5 5 M\r2\r5\r2\r5 5 3\rM\rM\r3\rM\r2\r3\r5\r2\r2\r2\r5 5 M\rM\rM\r2\r2\r5\r4\r3\r3 5 3\r5\r5\r2\r2\r2\r3M5M3M5\r3\r3\rM\rM\r3\r5\r2\r4\rM\r3\r3\r5\r5\r2\r3\rM\r2\r1\r2\r3\r3\r3\rM\r3\r2\r4\r5\r4\r2\r3\r2\rM\r5\r2\r2\rM\r2\rM\r3\r2\r2\r3\r2\r3\r3\r2\rM\r5\r1\r5\r5\r2\r5\r3\r4\r5\r1\r2\rM\r1\r2\r2\r2\r3\r3\r4\r2\r3\r3\rM\r3\r2\r3\r5\rM\r5\r3\r5\rM\rM\rM\r2\r1\r1\r2\r5\r2\r2\r4\r2\r5\r5\rM\r1 1\r2 2\r3 2\r2 2\r3 2\r2 4\rM 3\r4 M\r3 3\r3 2\r5\rM\r3\rM\r3\rM\r3\r2\r3\r3\r2 1\r2 5\r3 5\r3 2\r3 4\r3 4\r3 3\r3 3\r2 1 1 2\r2 3 2 3\r3 3 2 3\r3 3 M 3\r3 3 3 3\r3 4 M 2\r2\r3\r2\r3\r2\r2\r3\r3\r1\r2\r2\r2\r2\r5\r3\r2\rM\rM\r3\r2\rM\r2\r3\r2\r3\rM\r3\r3\r5\r5\r3\r3\r3\rM\rM\r3\ractivity 5 -- > activity 4\rvisual objects, and then continued to explore the visualization with the constructed frame.\rIn addition, there are many internal transitions within the construct- ing a frame ( 2 ), exploring visualization ( 3 ), and floundering on visualization activities ( 5 ). This pattern would indicate the follow- ing three sensemaking patterns: (1) the participants tried to construct enough frames so that they could explore the visualization; (2) once the participants started to explore the visualization, they could con- tinue the exploring by relying on constructed frames; and (3) likewise, once the participants fell into the floundering on visualization, they kept doing the floundering until they constructed a proper frame.\rFinally, the miscellaneous activities ( M ) were taken back and forth with various activities, but mostly during the exploring visualization activities ( 3 ).\rOn top of these four patterns, we were able to find some interest- ing transitions in the sensemaking procedures from our participants. First, we found that there were direct transitions from the exploring visualization activity ( 3 ) to the floundering on visualization activity ( 5 ) and vice versa. Even though it does not seem to make sense, we found from the transcripts data that the participants suddenly changed his/her attention to other visual objects that he/she did not have asso- ciated frames and vice versa. For example, P12 was interpreting the TM based on a constructed frame of visual encoding regarding size of box but he/she turned to another visual object (i.e., color of box). Eventually, P12 floundered on the TM and tried to find or construct a frame since he/she did not have a proper frame of color in the TM:\r“It says Microsoft in the top one. [...] I’m wondering now why some of - how they chose the colors [...] how they color coordi- natedthese.” (P12-TM)\rSecond, we also found some intriguing results of the questioning theframeactivity( 4 → 2 and 4 → 3 ).Ifwethinkaboutit,itis possible to reject the current frame and construct an alternative frame after questioning the current frame. However, none of the participants demonstrated such cognitive activities. From the transcripts data, we found that the participants kept the current frame and explored the visualization; or just turned to other visual object and tried to construct another frame even though they began to doubt the constructed frame due to uncertainty or abnormalities in the visualization. This pattern indicates that novice users do not tend to revise their frame once it is constructed in their mind and it is worth remembering.\r(a)\r1 1  7 \r19  25  8  8  8  1\r4  4  1  1 \r7 5  12 6 \r2  17  2  6  6 \r(b)\rFig. 4. The dynamics in the NOVIS model: (a) Activities identified in the 34 observation sessions. The numbers (1 to 5) and the letter M in the boxes indicate different activities in the NOVIS model. The grey participant IDs indicate non-novice users; (b) Transitions between activities. The area and number in each circle represent the number of transitions between activities. The rows represent launching activities and the columns represent arrival activities.\rsu\r4.4 Challenges and Responses\rWe were also interested in what caused our participants to be in the floundering (challenges) as well as what kinds of effort the participants exerted to overcome the challenges (responses). First, we explored cases when the participants constructed incorrect frames or failed to construct frames. We believed that investigating such cases would help understand how people get to floundering and what causes the floundering. Second, we investigated what kinds of attempts our par- ticipants made to escape from the floundering. Understanding the at- tempts could be helpful to consider design features for helping users escape from floundering with visualization.\r4.4.1 Challenges: Incorrect and Failed Frames\rAs we have mentioned before, we observed that not all of the con- structed frames were correct. Particularly, incorrect frames of content and visual encoding cause confusion and frustration in exploring vi- sualization. For example, P13 constructed a wrong frame of visual encoding and tried to explore the PCP based on the frame. However, the retrieved information did not perfectly fit into the frame and, even- tually, the participant gave up further engagement in the visualization:\r“So a car in 1978, maybe it’s the average [...] The gold line must be maybe an average for that year. [...] So I don’t know. It seems like a complicated graph to me [...] that’s about all I know.” (P13-PCP)\rIn addition, P08 conjectured the meaning of color of arc in the CD was private driver service companies (i.e., “I guess I should read the heading: Private Driver Service in San Francisco. So, I guess this is all the different companies maybe [...]” (P08-CD)). This was probably because the participant rushed into constructing the frame by just re- lying on the title without considering relationship between non-textual objects and corresponding textual objects.\rIn order to better understand the challenges, we collected the num- ber of right (constructed a correct frame), wrong (constructed an incor- rect frame), and failed (failed to construct a frame) attempts for each visual encoding in the visualizations as Figure 5 shows. Even though the PCP looked complicated, it had the relatively simple encoding scheme only for its polylines. We observed five attempts to construct the frame of visual encoding and two attempts were wrong or failed among them. In contrast, the CD had six visual encoding schemes. Generally, the number of attempts to construct the frames associated with chords (18 attempts) was more than the number of attempts to\rTM CD PCP\rk\r\n3/31/2015 VISM\rAttempts to Construct a Frame of Visual Encoding\r506 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rAttempts 8\r6 4 2 0\rFailed Wrong Right\rVisual Encoding\rFig. 5. Observed the number of attempts to construct a frame of visual encoding.\rconstruct the frames associated with arcs (9 attempts). Interestingly, the attempts to construct a frame of visual encoding at color of chord were observed most frequently with the CD (8 attempts), but every at- tempt was wrong or failed. The similar behaviors were observed with the TM. Although the visual encoding schemes in the TM appeared to be relatively understandable than others, we found that there are wrong attempts only at color of box. It is interesting to see that participants tended to assign some meanings on colors even though these colors did not have any particular meaning as shown in the following quote:\r“I wonder if the reason why it’s that shade of blue is because Facebook uses blue [...] I’m surprised that financial would be blueandnotgreenbecauseIthinkgreenandmoney.” (P12-TM)\r4.4.2 Responses: Overcoming Floundering\rWe noticed that some participants, who were in the floundering on vi-\rsualization activity ( 5 ), endeavored to find and construct a frame\r5 DISCUSSION\r5.1 Exceptional Cases\rAlthough we believe the NOVIS model is a useful framework to un- derstand how novice users make sense of unfamiliar visualizations, it certainly cannot exhaustively explain all the information visualization sensemaking behaviors. We observed some exceptional sessions.\rFirst, there was a participant, P02, who quickly made sense of the TM despite the fact that he/she saw a treemap for the first time. As soon as P02 saw the TM, he/she immediately constructed frames of content and visual encoding:\r“Okay, this is a graph showing unique users of the Internet and each one of these is - seems like they’re organized in amount of [...]theyuseit.” (P02-TM)\rIt was not clearly captured by the think-aloud method that how quickly he/she understood the TM. We still do not know how P02 achieved such efficient sensemaking with the TM and the follow-up interview did not provide much clue.\rIn contrast, there was a participant who intentionally refused to en- gage in the visualization. For example, P14 refused to engage in the PCP because the content of the visualization was not in his/her inter- est. However, it was also interesting to see that P14 performed well in the sessions with the CD and the TM:\r“Cars from the ‘70s to ‘80s, so it looks like we’re gonna talk about - or it’s talking of how fuel economy and perhaps maybe getting better and worse or how many cylinders. [...] Not really a high interest of mine. So I think that that’s pretty much all that Iseeonjustthegraph.” (P14-PCP)\rEmotions aroused by the visualization, personal interest about the con- tent, and how they affect the visualization sensemaking process are not precisely explained in the model. However, it would be interesting to further investigate these exceptional cases (see Section 5.3).\r5.2 Comparison with Other Models\rSince we presented the NOVIS model that is grounded in the data from our participants, it is worthwhile to compare with other existing mod- els in the field of graph comprehension and information visualization.\rWe believe that the NOVIS model would clearly demonstrate novice users’ sensemaking activities that are not described in the exist- ing models. For instance, the cognitive operations of a graph reader are described in the Pinker’s graph theory of comprehension [34]. Even though some operations in the model (i.e., having a visual array and creating a visual description) are similar to some sensemaking activ- ities in the NOVIS model (i.e., encountering visualization ( 1 ) and constructing a frame ( 2 )), the Pinker’s model does not clearly cap- ture the cognitive activities when a user does not know how to read a visualization because the model assumes that “he or she knows how to read a graph” [34](p. 73). However, the NOVIS model distinctly de- scribe what frustration and confusion a novice user has in the flounder- ing on visualization activity ( 5 ) and we observed the user’s attempts to overcome the floundering (see Section 4.4.2).\rThe graph comprehension model by Carpenter and Shah [8] also has different focus since they did not target novice users as defined in this study. According to the model, interpretive conceptual processes occur after pattern recognition processes in order to transform the pat- tern to quantitative/qualitative information. This was supported by a series of two experiments that were conducted with college students using line graphs. However, for a novice user, the transition from the perceptual to interpretive conceptual processes would not be smooth with an unfamiliar visualization because they do not have proper ex- planatory internal structures to conceptualize the perceived visual pat- terns. We believe that constructing a frame ( 2 ) can bridge the gap between the two comprehension processes for a novice user.\rIn addition, the NOVIS model would enrich the notionally de- scribed existing models with the observed evidence. For example, Liu and Stasko [29] conceptually elaborated the dynamics of internal mental structure in the relationships with information visualizations. However, they did not describe how each level of mental structure is\rin order to account for visual objects that did not make sense. Un-\rhttp://localhost:8888/visual_encoding.html 1/1\rlike the participants who gave up making sense of the visualization in the floundering on visualization activity, they gleaned visual objects from the visualization and made efforts to construct a frame. From our observations, we found three strategies that were applied by the participants to overcome the floundering activity.\rFirst, they focused on a certain part of the visualization which they might know or might have been interested in after reading various tex- tual objects in the visualization. The part was usually related to their domain knowledge (e.g., a known website with the TM) or personal experience (e.g., a year of birth with the PCP). While staying with the part, they read the associated textual object carefully and tried to build a frame. For example, P03 mentioned:\r“Cars from ‘70s and ‘80s - I don’t know much about cars. I guess when I move the mouse it highlights different lines, [...] it’s just fun just to play with the stuff - But I was born in ‘84 so maybe I should go up here and see ‘84 [...] A 46, maybe, miles per gallon. It has four cylinders, which I thought maybe the more cylinders youhavethefasterthecargoes[...].” (P03-PCP)\rSecond, they focused on gleaning informative textual objects for them from the visualization. By making this effort, they were grad- ually informed about the content of the visualization and they finally constructed a frame. For example, P12 mentioned:\r“[...] Let me look here and see what the title says. Private Driver Service in San Francisco. [...] look to see some of the other information that pops up [...] it says 13.7 percent of origins and now I know that it says private driver. [...] it gives me even more information that pops up. It says from south of market to financial district and [...] I’m going to guess that this information isdrivingroutesforprivatedrivers.” (P12-CD)\rLast, they tentatively constructed alternative frames that would pro- vide possible explanation of visual objects and compared them. For example, a participant considered the two alternative frames about the size of boxes in the TM and he/she compared them based on the visual objects in order to find a more reasonable one:\r“So, I don’t know if the size of the box has anything to do with how big the company is, or how many users there are. 121 mil- lion, 102. No, not necessarily, I don’t think. YouTube has 203 mil- lion. Apple 119 users. Well, maybe it is by size. Social network, 220 million. [...] The size of the companies - they all companies? Twitter.Twitter’snotreallyacompany,orisit?” (P13-TM)\rPCP: Polyline\rCD: Chord\rCD: Color of Chord\rCD: Size of Chord\rCD: Arc\rCD: Color of Arc\rCD: Size of Arc\rTM: Box\rTM: Color of Box\rTM: Size of Box\r\nLEE ET AL.: HOW DO PEOPLE MAKE SENSE OF UNFAMILIAR VISUALIZATION?... 507\rdeveloped. We believe that the NOVIS model could provide a possi- ble description about the dynamics. In particular, the five sensemaking activities in the NOVIS model would describe how the initial mental structure is initially developed and how an individual make sense of a new visualization using the mental structure with the observed data.\rOn top of that, we indirectly confirmed and validated the inter- action of bottom-up and top-down processes in graph comprehen- sion [16, 41, 42]. We observed that the same participant showed differ- ent sensemaking processes depending on the visualizations (see Sec- tion 4.3) and this observation supports that visualization format would make the visualization easy or difficult to understand. In addition, as seen in the various quotes from the participants, we observed that the participants’ prior knowledge about content played a role in the con- structing a frame ( 2 ), exploring visualization ( 3 ), and questioning the frame ( 4 ) activities. Furthermore, the participants sometimes re- lied on prior knowledge about content when they endeavored to over- come the floundering (see Section 4.4.2). From these observations, we would infer the novice user’s information visualization sensemak- ing involves the interaction of bottom-up and top-down processes.\r5.3 Further Research Directions\rAlthough we came up with the NOVIS model that is grounded on our observations, a number of research questions arose. Due to the nature of qualitative study, we believe that it is worthwhile to share the ques- tions. The following questions should be investigated through further observation and empirical studies in order to deeply understand the novice users’ information visualization sensemaking.\rFirst, what other presentation strategies could be implemented to assist in constructing a correct frame? One of the surprising behav- iors we observed was that participants had difficulties in moving away from initial constructed frames that were incorrect. Out of eight cases when participants had incorrect frames (see Figure 5), there were no participants who realized that they had constructed an incorrect frame and corrected it later on. Thus, the first impression that users get dur- ing the encountering visualizations activities is quite important to help users construct a correct frame. Confusing them with too much in- formation might not be a good idea. In this sense, the first step in Shneiderman’s mantra, “overview first” [44] should be carefully re- considered when novice users are expected to encounter a visualiza- tion. Although “overview first” has been shown as an effective way to provide a big picture, showing too much information from the begin- ning might not be a good idea for novice users. Instead, we need to consider other presentation strategies and investigate how the strate- gies affect the novice users’ information visualization sensemaking. Gradually showing visualizations with some instructions, for example, the Martini Glass structure [40], can be a strategy that can be consid- ered for novice users. More directly, selected annotations for a couple of data points could be presented to demonstrate how to interpret them.\rSecond, how does personal interest on content affect the visual- ization sensemaking? One of the exceptional behavior we observed was that a participant refused to engage in the visualization because its underlying content was not of the participant’s interest (see Sec- tion 5.1). It might be an important phenomenon in both explanatory and exploratory visualizations. Even though there is a study showing the effect of emotional priming on visual perception performance [19], we need to investigate the influence of personal interest on the novice users’ information visualization sensemaking. On top of the personal interest, we need to explore individual factors, for example, mathe- matical and statistical skills, that could be affecting the sensemaking. Furthermore, we need to study how to promote the users’ engagement even if they do not have personal interest about content.\rThird, what are the uncovered sensemaking activities? For exam- ple, we noticed that the exploring visualization activity ( 3 ) could be expanded. In the NOVIS model, we mainly observed the three sub- activities within the exploring visualization. If users had more time and motivation and by extension, they were familiar with the visual- izations and its content, they could demonstrate other activities, such as formulating a hypothesis, testing the hypothesis, and formulating another hypothesis again, gaining insight and generating interesting\rquestions, iteratively. We believe that these additional sensemaking activities are relatively well captured in other models, such as the no- tional model of analysts sensemaking loop by Pirolli and Card [35] and the knowledge generation model by Sacha et al. [38]. By combin- ing these two models with the NOVIS model, a more comprehensive model could be developed with additional empirical studies.\rFinally, how do experts make sense of unfamiliar visualizations? A more fundamental understanding will come from investigating the thought processes of visualization experts. We believe that experts could make sense of unfamiliar visualizations more effectively with different sensemaking processes or activities. If this were to be true, such understanding would help us design effective strategies for edu- cating the general public, so that they can have the ability to properly and accurately interpret visualizations.\r5.4 Limitations\rIn this study, we must admit some limitations due to the nature of the think-aloud method. A prime limitation of the think-aloud method is that the participants might miss some of their thought processes dur- ing the think-aloud sessions. As Nielsen et al. [31] pointed out, par- ticipants’ thought processes might be faster and more complex than they can verbalize. Furthermore, thinking aloud is uncommon in a natural setting when they interact with an information visualization. Thus, it might be impossible for us to comprehensively observe the participants’ thought processes to make sense of the visualization.\rAnother concern is that we only used three types of information vi- sualization: the parallel-coordinates plot, the treemap, and the chord diagram. While we attempted to diversify visualization types that we used, it was impossible to comprehensively use all kinds of visualiza- tions. Given the influences of different visualizations we observed in our study, more studies with other visualizations are necessary.\rFinally, the experimental setting of having a standalone visualiza- tion could be unrealistic. In many cases visualizations are used during a presentation with additional explanations or in an article with textual contents. These narratives would have definitely helped participants to better understand the visualization. However, in order to focus more on the visual representations, we made a trade-off in the experiment.\r6 CONCLUSIONS\rIn this study, we presented a grounded model of NOvices informa- tion VIsualization Sensemaking (NOVIS model), which consists of five sensemaking activities: 1 encountering visualization, 2 con- structing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We believe that this is one of earliest qualitative studies that meticulously reports the partici- pants’ sensemaking activities and their behaviors when they encounter unfamiliar visualizations for the first time.\rThis study makes the following contributions to the field of Infor- mation Visualization. Our efforts draw attention to understanding of novice users’ cognitive activities while making sense of an informa- tion visualization. We believe that research about understanding of users’ cognitive activities should be actively conducted in order to de- velop communicable novel information visualization techniques. Fur- thermore, we also believe that the research will lay ground work to develop visualization literacy test items and to improve novice users’ visualization literacy.\rCertainly, the presented sensemaking model in this study is not a definitive model describing how a novice user makes sense of an in- formation visualization. The model should be updated and verified by further observation and a series of quantitative, controlled, and com- parative studies. However, we hope that the NOVIS model will be a first step to understand users’ sensemaking activities with an informa- tion visualization.\rACKNOWLEDGMENTS\rWe would like to thank all the participants who voluntarily participated in this study. In addition, we would like to thank Dr. Carly Roberts for constructive advice on the grounded theory study. Finally, Google Research Award (Winter, 2013) has made this study possible.\r\n508 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016\rREFERENCES\r[1] G.Allan.Acritiqueofusinggroundedtheoryasaresearchmethod.The Electronic Journal of Business Research Methods, 2(1):1–10, 2003.\r[2] E. Anders, K. and H. A. Simon. Verbal reports as data. Psychological\rReview, 87(3):215–251, 1980.\r[3] S. Bateman, R. L. Mandryk, C. Gutwin, A. Genest, D. McDine, and\rC. Brooks. Useful junk?: The effects of visual embellishment on com- prehension and memorability of charts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’10, pages 2573–2582. ACM, 2010.\r[4] J.Bertin.SemiologyofGraphics:Diagrams,Networks,Maps.EsriPress, 1st edition, 2010.\r[5] M.A.Borkin,A.A.Vo,Z.Bylinskii,P.Isola,S.Sunkavalli,A.Oliva,and H. Pfister. What makes a visualization memorable? IEEE Transactions on Visualization and Computer Graphics, 19(12):2306–2315, 2013.\r[6] J. Boy, R. Rensink, E. Bertini, and J.-D. Fekete. A principled way of assessing visualization literacy. IEEE Transactions on Visualization and Computer Graphics, 20(12):1963–1972, 2014.\r[7] P. Cairns and A. L. Cox. Research Methods for Human-Computer Inter- action. Cambridge University Press New York, 2008.\r[8] P. A. Carpenter and P. Shah. A model of the perceptual and conceptual processes in graph comprehension. Journal of Experimental Psychology: Applied, 2(4):75–100, 1998.\r[9] M. C. Carswell. Choosing specifiers: an evaluation of the basic tasks model of graphical perception. Human Factors, 34(5):535–554, 1992.\r[10] K.Charmaz.ConstructingGroundedTheory:APracticalGuidethrough Qualitative Analysis. SAGE Publications Ltd, 1st edition, 2006.\r[11] E. Charters. The use of think-aloud methods in qualitative research An introduction to think-aloud methods. Brock Education Journal, 12(2):68– 82, 2003.\r[12] W. S. Cleveland and R. McGill. Graphical perception: Theory, exper- imentation, and application to the development of graphical methods. Journal of the American Statistical Association, 79(387):531–554, 1984.\r[13] J.M.CorbinandA.Strauss.BasicsofQualitativeResearch:Techniques and Procedures for Developing Grounded Theory. SAGE Publications, 3rd edition, 2008.\r[14] J. W. Creswell. Qualitative Inquiry and Research Design: Choosing Among Five Approaches. SAGE Publications, 3rd edition, 2012.\r[15] F.R.Curcio.Comprehensionofmathematicalrelationshipsexpressedin graphs. Journal for Research in Mathematics Education, 18(5):382–393, 1987.\r[16] E.G.FreedmanandP.Shah.TowardaModelofKnowledge-BasedGraph Comprehension. In M. Hegarty, B. Meyer, and N. H. Narayanan, editors, Diagrammatic Representation and Inference, number 2317 in Lecture Notes in Computer Science, pages 18–30. Springer Berlin Heidelberg, 2002.\r[17] S. N. Friel, F. R. Curcio, and G. W. Bright. Making sense of graphs: Critical factors influencing comprehension and instructional implications. Journal for Research in Mathematics Education, 32(2):124–158, 2001.\r[18] L. Grammel, M. Tory, and M.-A. Storey. How information visualization novices construct visualizations. IEEE Transactions on Visualization and Computer Graphics, 16(6):943–952, 2010.\r[19] L. Harrison, D. Skau, S. Franconeri, A. Lu, and R. Chang. Influencing visual judgment through affective priming. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13, pages 2949–2958. ACM, 2013.\r[20] J. Heer, M. Bostock, and V. Ogievetsky. A tour through the visualization zoo. Communications of the ACM, 53(6):59–67, 2010.\r[21] G.Klein,B.Moon,andR.R.Hoffman.Makingsenseofsensemaking1: Alternative perspectives. IEEE Intelligent Systems, 21(4):70–73, 2006.\r[22] G.Klein,B.Moon,andR.R.Hoffman.Makingsenseofsensemaking2: A macrocognitive model. IEEE Intelligent Systems, 21(5):88–92, 2006.\r[23] G. Klein, J. K. Phillips, and D. A. Peluso. A data-frame theory of sense- making. Expertise Out of Context: Proceedings of the Sixth International Conference on Naturalistic Decision Making, 2007.\r[24] S.M.Kosslyn.Graphicsandhumaninformationprocessing:Areviewof five books. Journal of the American Statistical Association, 80(391):499– 512, 1985.\r[25] S. M. Kosslyn. Understanding charts and graphs. Applied Cognitive Psychology, 3(3):185–225, 1989.\r[26] B.C.Kwon,B.Fisher,andJ.S.Yi.Visualanalyticroadblocksfornovice investigators. In IEEE Conference on Visual Analytics Science and Tech-\rnology (VAST), pages 3–11, 2011.\r[27] D. Levy. Qualitative methodology and grounded theory in property re-\rsearch. Pacific Rim Property Research Journal, 12(4):369–388, 2006.\r[28] Z.LiuandJ.Heer.Theeffectsofinteractivelatencyonexploratoryvisual analysis. IEEE Transactions on Visualization and Computer Graphics,\r20(12):2122–2131, 2014.\r[29] Z. Liu and J. T. Stasko. Mental models, visual reasoning and interaction\rin information visualization: a top-down perspective. IEEE Transactions\ron Visualization and Computer Graphics, 16(6):999–1008, 2010.\r[30] J. Mackinlay. Automating the design of graphical presentations of rela- tional information. ACM Transactions on Graphics, 5(2):110–141, 1986.\r[31] J.Nielsen,T.Clemmensen,andC.Yssing.GettingAccesstoWhatGoes on in People’s Heads?: Reflections on the Think-aloud Technique. In Proceedings of the Second Nordic Conference on Human-computer In-\rteraction, NordiCHI ’02, pages 101–110. ACM, 2002.\r[32] P.ParsonsandK.Sedig.CommonVisualizations:TheirCognitiveUtility. In W. Huang, editor, Handbook of Human Centric Visualization, pages\r671–691. Springer New York, 2014.\r[33] D. Peebles, D. Ramduny-Ellis, G. Ellis, and J. V. H. Bonner. The in-\rfluence of graph schemas on the interpretation of unfamiliar diagrams. Technical report, School of Human and Health Sciences. University of Huddersfield, UK, 2013.\r[34] S. Pinker. A theory of graph comprehension. In R. Freedle, editor, Ar- tificial Intelligence and the Future of Testing, pages 73–126. Lawrence Erlbaum, 1990.\r[35] P. Pirolli and C. Card. The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis. In Proceedings of International Conference on Intelligence Analysis, pages 2–4, 2005.\r[36] P. Pirolli and D. M. Russell. Introduction to this special issue on sense- making. HumanComputer Interaction, 26(1-2):1–8, 2011.\r[37] D.Qi.Aninquiryintolanguage-switchinginsecondlanguagecomposing processes. Canadian Modern Language Review, 54(3):413–435, 1998.\r[38] D. Sacha, A. Stoffel, F. Stoffel, B. C. Kwon, G. Ellis, and D. A. Keim. Knowledge generation model for visual analytics. IEEE Transactions on Visualization and Computer Graphics, 20(12):1604 – 1613, 2014.\r[39] J. Saldaa. The Coding Manual for Qualitative Researchers. SAGE Pub- lications Ltd, 2nd edition, 2012.\r[40] E. Segel and J. Heer. Narrative visualization: Telling stories with data. IEEE Transactions on Visualization and Computer Graphics, 16(6):1139–1148, 2010.\r[41] P. Shah. A model of the cognitive and perceptual processes in graphi- cal display comprehension. In AAAIs Fall Symposium of Reasoning the Diagrammatic Representations, pages 94–101, 1997.\r[42] P. Shah and E. G. Freedman. Bar and line graph comprehension: An interaction of top-down and bottom-up processes. Topics in Cognitive Science, 3(3):560–578, 2011.\r[43] P. Shah and J. Hoeffner. Review of graph comprehension research: Im- plications for instruction. Educational Psychology Review, 14(1):47–69, 2002.\r[44] B. Shneiderman. The eyes have it: a task by data type taxonomy for information visualizations. In IEEE Symposium on Visual Languages, pages 336–343, 1996.\r[45] D. Simkin and R. Hastie. An information-processing analysis of graph perception. Journal of the American Statistical Association, 82(398):454–465, 1987.\r[46] C. Ware. Information Visualization: Perception for Design. Morgan Kaufmann, 2nd edition, 2004.\r[47] J.S.Yi,Y.-a.Kang,J.T.Stasko,andJ.A.Jacko.Towardadeeperunder- standing of the role of interaction in information visualization. IEEE Transactions on Visualization and Computer Graphics, 13(6):1224– 1231, 2007.\r[48] J.ZacksandB.Tversky.Barsandlines:Astudyofgraphiccommunica- tion. Memory & Cognition, 27(6):1073–1079, 2013.","mimeType":"application/pdf","authors":[],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"cd","time":1461915874822,"auto":true,"weight":1.0},{"@type":"Tag","text":"novi","time":1461915874835,"auto":true,"weight":1.0},{"@type":"Tag","text":"flounder","time":1461915874812,"auto":true,"weight":1.0},{"@type":"Tag","text":"tm","time":1461915874784,"auto":true,"weight":1.0},{"@type":"Tag","text":"visual","time":1461915874792,"auto":true,"weight":1.0},{"@type":"Tag","text":"activ","time":1461915874865,"auto":true,"weight":1.0},{"@type":"Tag","text":"pcp","time":1461915874804,"auto":true,"weight":1.0},{"@type":"Tag","text":"sensemak","time":1461915874856,"auto":true,"weight":1.0},{"@type":"Tag","text":"novic","time":1461915874845,"auto":true,"weight":1.0},{"@type":"Tag","text":"frame","time":1461915874775,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":10,"appId":"cc535375f23dd31cb289d988bf86ac34f5cbc679","timeCreated":1455803740811,"timeModified":1461915877286,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.018406551,"uri":"file:///Users/cheny13/Documents/References/From Keyword Search to Exploration WebSci-Wilson.pdf","plainTextContent":"Foundations and Trends⃝R in\rWeb Science\rVol. 2, No. 1 (2010) 1–97\r⃝c 2010 M. L. Wilson, B. Kules, m. c. schraefel and B. Shneiderman\rDOI: 10.1561/1800000003\rFrom Keyword Search to Exploration: Designing Future Search Interfaces for the Web\rBy Max L. Wilson, Bill Kules,\rm. c. schraefel and Ben Shneiderman\rContents\r1 Introduction 4\r1.1 A Challenge Faced by Designers of Future\rSearch Systems 9\r1.2 A Taxonomy to Overcome the Challenge 11\r1.3 The Scope of Our Approach 12\r2 A Model of Search 14\r2.1 A Context Model of Search 15\r2.2 Information Retrieval 16\r2.3 Information Seeking 17\r2.4 Work Context 19\r2.5 Summary 20\r3 Survey of Search Systems 21\r3.1 Adding Classifications 22\r3.2 Result Organization 41\r\n3.3 3.4\r4\r4.1 4.2 4.3 4.4\r5\r5.1 5.2\rAdditional Functions 57 Summary 64\rEvaluations of Search Systems 65\rInformation Retrieval Evaluations 65 Information-Seeking Evaluations 67 Work-Context Evaluations 71 Summary 74\rTaxonomy of Search Visualizations 75\rThe Taxonomy 75 Using the Taxonomy 79\r6 Conclusions 81\rAcknowledgments 83 References 84\r\nFoundations and Trends⃝R in\rWeb Science\rVol. 2, No. 1 (2010) 1–97\r⃝c 2010 M. L. Wilson, B. Kules, m. c. schraefel and B. Shneiderman\rDOI: 10.1561/1800000003\rFrom Keyword Search to Exploration: Designing Future Search Interfaces for the Web\rMax L. Wilson1, Bill Kules2,\rm. c. schraefel3 and Ben Shneiderman4\r1 Intelligence, Agents, Multimedia (IAM) Group, School of Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ, UK, mlw05r@ecs.soton.ac.uk\r2 School of Library and Information Science, The Catholic University of America, Washington, DC 20064, USA, kules@cua.edu\r3 Intelligence, Agents, Multimedia (IAM) Group, School of Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ, UK, mc+ft@ecs.soton.ac.uk\r4 Department of Computer Science and Human-Computer Interaction Laboratory, University of Maryland at College Park, College Park, MD 20742, ben@cs.umd.edu\rAbstract\rThis monograph is directed at researchers and developers who are designing the next generation of web search user interfaces, by focusing on the techniques and visualizations that allow users to interact with and have control over their findings. Search is one of the keys to the Web’s success. The elegant way in which search results are returned has been well researched and is usually remarkably effective. However,\r\nthe body of work produced by decades of research into information retrieval continues to grow rapidly and so it has become hard to syn- thesize the current state-of-the-art to produce a search interface that is both highly functional, but not cluttered and distracting. Further, recent work has shown that there is substantial room for improving the support provided to users who are exhibiting more exploratory forms of search, including when users may need to learn, discover, and under- stand novel or complex topics. Overall, there is a recognized need for search systems to provide effective user experiences that do more than simply return results.\rWith the aim of producing more effective search interfaces, human computer interaction researchers and web designers have been devel- oping novel interactions and features that enable users to conveniently visualize, parse, manipulate, and organize their Web search results. For instance, while a simple set of results may produce specific infor- mation (e.g., the capital of Peru), other methods may let users see and explore the contexts of their requests for information (more about the country, city, and nearby attractions), or the properties that asso- ciate groups of information assets (grouping hotels, restaurants, and attractions by their type, district, or price). Other techniques support information-seeking processes that may last weeks or months or may even require collaboration between multiple searchers. The choice of relevant result visualization strategies in new search systems should reflect the searchers and the higher-level information needs that moti- vate their searches. These examples provide further motivation for sup- porting designers, who are challenged to synthesize and understand the breadth of advances in search, so that they can determine the bene- fits of varied strategies and apply them appropriately to build better systems.\rTo support researchers and designers in synthesizing and under- standing the advances in search, this monograph offers a structured means to think about web search result visualization, based on an inclusive model of search that integrates information retrieval, infor- mation seeking and a higher-level context of tasks and goals. We exam- ine each of these levels of search in a survey of advances in browsers and related tools by defining search-related cognitive processes and\r\nanalyzing innovative design approaches. We then discuss evaluations at each of these levels of search, presenting significant results and iden- tifying both the traditional and novel means used to produce them. Based on this examination, we propose a taxonomy of search result visualization techniques that can be used to identify gaps for future research and as a reference for designers of next generation web search systems.\r\n1\rIntroduction\rThis monograph is for designers thinking about how they might enhance the experience of discovering, exploring and putting to work information they can access over the Web. This monograph is also for researchers who may be interested in how search interaction approaches have developed over the past decade. In both cases, a fundamental ques- tion is at play: what else users could possibly need besides Google to search the Web? That’s a fair question, and readers of this survey may have the same question. So to tackle that head on, let us agree: Google is really good. For what it does.\rOur monograph considers the approaches for exploring informa- tion spaces that Google’s elegant keyword search cannot do. Over the past decade, research on alternative search paradigms has emphasized Web front ends on single or unified databases. Such work is productive when the designer has the luxury of working with well-curated docu- ments from a single source, but the elegant visualization or well-tailored faceted browser may not scale to the size and diversity of Web-based information, links, scientific data sets, personal digital photos, creative videos, music, animations, and more. So why does keyword search scale? Because (a) huge resources have been thrown at the problem\r4\r\n5\rand (b) textual data have a satisfying and compliant order to it. We hope this monograph shows that the research and evaluations of alter- native approaches to data exploration for knowledge building are the best preparation we have for the next generation Web, the Web of Linked Data.\rIn a little more than a decade the Web has become the default global repository for information. Many factors have contributed to this remarkable result, from the success of the robust technologies that enable its networked connections, to the commercialization of the back- bone that enticed business to support and utilize it, to the ease with which ordinary citizens can publish information to it. But perhaps the key technology that took the Web from a useful supplement of current information practice to become the default communication medium is search. Web search, as provided by Google, Microsoft, Yahoo, etc., enables users to find the information they want via the simplest of interaction paradigms: type some keywords into a box and get back an informative result list that is ranked, as if by magic, so that the first results most likely match what we’re trying to find.\rSearch engines automated what was initially a community-based and commercially coordinated Easter egg hunt: category systems were proposed and documents as they were found either by humans recom- mending them to such sites, or discovered by human trawlers and some early web crawlers, were assigned to categories. The Web was set up like a giant Yellow Pages. Further, before these nascent directories, the Web was explored by the link. As recently as 2004, surfing the Web was still a common trope for browsing the Web, following from link to link, from one site to another. Not unlike blogs today, web sites might publish best new finds on a topic, and away one would go. Only five years later, who “surfs” or presumes to browse “the Web”? It has grown beyond that scale of the surfable, with its pockets of Dark Web and ice caps of Public Web, where so much more than is indexed by search engines is below the visible water line of documents. This growth is itself related to the existence of search: because it can be found by search, rather than relying on recommendations alone, it is worth publishing on the Web; indeed it is necessary to publish on the Web. Because conversely, if it cannot be found on the Web, does it exist?\r\n6 Introduction\rSearch as embodied by the text box and keyword has framed our understanding of what the Web is [145]. It has become so ubiquitously associated with the Web that it is difficult to find a web browser that does not have as a default tool, a keyword search box built into upper right of the browser window, right next to the box for the location. In some cases the URL address bar is already polymorphic: acting as either address area or search bar. The prominence of web search based on the fundamental efficacy of keyword search makes it difficult to imagine what an entire monograph on search interaction may be about. It turns out that this elegant paradigm is especially effective for the 1-min search — find the address for Chez Panisse, get a biography of Madame Curie, or locate the home page for a local car rental. But many users have come to the Web for substantive research that takes hours or weeks — find all the songs written about poverty in the 1930s, prove that there are no patents that cover my emerging innovation, or locate the best legal precedents for my brief.\rA second motivator for new search strategies is that the next gen- eration web will offer fresh possibilities that go well beyond finding documents by way of keyword search. Hall and O’Hara [69] stress that what we know as the Web today is the Document Web, and not the Web of Linked Data that is imminently upon us. The older Document Web is about the information, readable by us, written by us, and framed for our use. It is this very human-readable orientation of the Web and it is the presentation technologies in the browser that have enabled keyword search engines to become so very good: the words in the documents are all a search engine has to go on to find appropriate results. It is because the search engine is searching in documents that we get a list of docu- ments back: we may only want a sentence in the middle of a document, but we get the whole thing (the document) back.\rBy contrast, in the newer Web of Linked Data, often called the Semantic Web, the idea is to give the search engine designers more to work with than making best guesses about what results to return based on keyword frequency and number of pages linked to a document. Imagine if instead of a list of results, the machine simply returned “the answer”? Some queries have specific answers: “mc’s phone number at work” or “British obesity rate in 2009?” There may be several sources\r\n7\rfor this information, but if they concur, why not just present that result, and give the six places the result occurs? Right now, this kind of search result is not feasible because the Web for the most part does not have consistent tags to indicate what is a phone number or where to find obesity rates. Search engines check if a term requested maps to the term in a document, and does very effective ranking very fast. That is of course an oversimplification of all the sophistication to make that experience as fast and accurate as it appears to be.\rThe goal of the Web of Linked Data is to have information about information available so that not only can designers provide better, clearer answers to such simple queries about phone numbers and statis- tics, but also users can resolve questions that are difficult to handle without metadata. Some researchers are conducting intriguing research that attempts to create this metadata automatically to derive seman- tics from the documents themselves. In this monograph we are less concerned with how metadata becomes available. We are concerned with the question of what designers can do with it, once it exists.\rWhile the power of Semantic Web of Linked Data is that it can enhance large diverse, unorganized, and heterogeneous datasets, the unique affordances also challenge our assumptions about how we access information [176]. As the links between data can be numerous, endless, and of any granularity, the assumptions about carefully structured clas- sifications, for example, breakdown. Similarly, while web searches are typically for web pages, it is not clear whether searching at the data level should return any object [21], specific types of objects [146], object relationships [21, 76], portions of RDF [47], entire ontologies [2, 63], and so on.\rFurther, as the work on semantically linked data has separated the data from presentation, designers and users are able to represent the data however they like [21]. The flipside, however is that someone, either the interface designer or the end user, has to decide how to represent the data. In summary, the freedom enabled by semantically organized data sets has in turn broadened the options and increased the number of decisions that designers and end users have to make. Recent work has shown, however, that increasing numbers of options can make designers and users feel less confident in their decisions, and less happy\r\n8 Introduction\rwith their results [130, 149], rather than making them feel empowered. What effect, then, does this have on confidence during search interface design, given that designers and users now have more freedom.\rThese issues are becoming national policy issues, especially as the United States, United Kingdom, and other governments intend to release increasing amounts of information onto the Web as data with sufficient metadata to support more automatic analysis. Metadata tags will indicate that a number represents the reported cases of obesity at a given time and place. The number also has a source and date of creation associated with it so users can verify accuracy and timeliness. It is not simply a number in a document, instead it comes with well-associated meanings.\rAs soon as this information is available new opportunities for rep- resentation beyond document lists become possible. Users will be able to see quickly for themselves: are obesity levels rising at the rates pre- sented in the media, or, by mapping these data from several sources, are they too conservative or aggressive? Imagine being able to look at such sources to ask these kinds of questions with the same facility as we use keyword search now. Now that is an interaction challenge.\rDesigners already offer such representations on smaller than Web scale information sources; that is what most of the literature we will review considers. In that sense we have some preparation for what is to come. But there are also entirely new interaction challenges. There will be many data sources from many places that map to given meanings, like obesity statistics. How can these disparate sources be gathered, inspected, and explored?\rRight now, we are on the cusp of a transition from the Document Web to the Document/Data Web. It is an incredible time to be inter- ested in designing or researching how to engage with these immense volumes of data. Are designers and researchers ready for this transi- tion? The findings presented here may act as guideposts for this near future. We may also look back in ten years at these nascent efforts to imagine exploring data at scale and say either that was clever or that was na ̈ıve. It will be more than intriguing to see what principles remain, and what have yet to be imagined. In the meantime, we look back in order to leap ahead.\r\n1.1 A Challenge Faced by Designers of Future Search Systems 9\rIn the remainder of this monograph, Section 2 identifies and explains a model of search that is used to structure the discussion in the fol- lowing sections and forms the basis of the taxonomy of advances in interactive search. In short, the model describes search in increasing layers of context, from simple retrieval, to broader strategies, and to the wider tasks that motivate them. Section 3 identifies specific advances in interactive search, and prominent examples of their use, organized by the different layers of the model to which they apply. Section 4 explains how advances at each layer of the model have been evaluated. Section 5 then presents the final taxonomy, and identifies areas of rel- atively little research as potential focal points for future research. For search interface designers, the taxonomy provides a list of potential features to include in designs, describing (a) how they support users, (b) how their support has been evaluated, and (c) how prevalent they have become on the Web.\r1.1 A Challenge Faced by Designers of Future Search Systems\rUnderstanding how search interfaces and visualization affect searcher success is a hard challenge, and cannot be as easily measured as speed, document similarity, and result accuracy. In the early 1970s, Cooper [43] suggested that instead of speed metrics, search evaluations should be based on subjective user satisfaction with the results. Later, Robertson and Hancock-Beaulieu [143] noted that the recent, at the time, revolutions of IR research had begun to focus more on users and less on systems. Even more recently, though, researchers have identified just how inadequate the familiar keyword search paradigms, provided by environments such as Google and Bing1 (Microsoft’s search engine), might be for users who need to do more than just find a website that answers a factual question.\rThe recent focus on these more exploratory forms of search, known as Exploratory Search [172, 174], has identified some search scenarios that require much more diverse searching strategies, including when the\r1 http://www.bing.com\r\n10 Introduction\rusers are (a) unfamiliar with a domain and its terminology, (b) unfa- miliar with a system and it’s capabilities, or (c) unfamiliar with the full detail of their task or goal. Experts may also conduct demanding searches such as those needing to do: (a) comprehensive searches to find every relevant record (Legal, patent, medical), (b) negation searchers to prove the absence of relevant work (e.g., patent, pharmaceuticals), (c) exception searches to find outlier documents (that take a different or contradictory point of view than commonly held), or (d) bridging searches that connect two disparate fields of study [164]. Exploratory search scenarios are characterized by needs that are “open-ended, per- sistent, and multifaceted, and information-seeking processes that are opportunistic, iterative, and multitactical” [174]. In each case advances in search need to do more than simply improve the matching of results to terms entered into a single box. Even in the late 1980s, Motro [126] designed the VAGUE interface based on the notion that often, when limited to simple keyword interfaces, users submit numerous evolutions of an original vague query in order to figure out which terms are going to produce all, or even just part, of their desired information.\rIn many cases, searching involves a range of tactics and techniques, rather than simply submitting a query and seeing a list of matching results. As part of the special issue on Exploratory Search, Marchion- ini [116] identified, although not exhaustively, a series of strategies that users may often need to employ to achieve their goals, such as compar- ing, synthesizing, and evaluating. MacKay and Watters [114] present a diary study documenting examples of search tasks that span multiple search sessions, where users return days later to continue on tasks such as job seeking or house hunting. Similarly, Morris [125] has doc- umented the breadth of occasions where users clearly collaborate with family and colleagues on tasks such as holiday planning and team work projects. It is plain to see that a search interface needs to provide more than a simple keyword search form to support users in applying such strategies.\rThe recognition that there is more to search than basic Informa- tion Retrieval has led to many extensions and alternatives to the keyword search paradigm. An example extension is to cluster the results into groups that share attributes [190]. Alternatively, faceted browsing\r\n1.2 A Taxonomy to Overcome the Challenge 11\r[75, 168] provides meaningful or significant attributes of web pages to users at the beginning, so that they do not even have to think of words to put in the keyword search box. This can be especially useful in the occasions where people are unfamiliar with a domain and its terminol- ogy, for example. These, and many other advances, have much in com- mon and while they each have specific advantages, it is not clear that including them all would provide a stronger search interface. Instead, designers now have the challenge of deciding: (a) what types of search strategies should be supported, if not all of them, and (b) which new features to include in order to support them. This challenge is particu- larly difficult, when so many advances have been proposed, each with different benefits, and when the benefits of each advance have often been shown independent of others.\r1.2 A Taxonomy to Overcome the Challenge\rThe goal of this monograph is to support designers with this challenge by building a taxonomy of advances in the field of search that can be used as common ground when choosing which features to include within future search interfaces. To build the taxonomy we:\r(1) identify a model, produced by theory, which covers the full breadth of search from context and tasks, down to specific actions and results (Section 2);\r(2) summarize the specific advances in interactive search (Sec- tion 3);\r(3) discuss the way that these interactive search advances have been evaluated (Section 4), in accordance with the model of search presented in Section 2; and\r(4) present a taxonomy (in Section 5) of the search advances (from Section 3) that takes into account the type of search supported (from Section 2), how the advances have been eval- uated (Section 4) and how prevalent they are on the Web.\rProducing a structured and consistent taxonomy allows us to compare advances in interactive search, with two benefits. First, the taxonomy can be a reference for designers. The latter half of Section 5\r\n12 Introduction\rdescribes a detailed process that designers can apply to systematically decide which features to include in their search designs. As the taxon- omy includes advances in the field, their benefits, and how they have been evaluated, a designer can quickly compare options and choose appropriate features for their new interface. Second, the taxonomy can be used by academics to identify areas that require further study and contextualize future advances in the field.\r1.3 The Scope of Our Approach\rWhen defining a means of categorizing and communicating existing and on-going research, it is important to define the scope of our approach, so that it is correctly used. Here we specifically bound the content of this monograph in two areas: what we mean by search and what we mean by the Web.\r1.3.1 What We Mean by Search\rSo far, this monograph has used the terms: search, seeking, and Infor- mation Retrieval interchangeably. For the rest of the monograph, how- ever, we intend to follow a specific set of terminology, which is defined carefully in the model discussed in Section 2. Information Retrieval is perhaps the most well studied, and so most well-defined term used to describe searching. Typically, Information Retrieval refers to the paradigm where users enter a keyword into a system, which responds by returning the results that are most relevant to the keywords used. This monograph covers a much broader view of search than simply Information Retrieval. Information Seeking is another common term used to describe people’s searching behavior, including activities such as searching, browsing, and navigating. Again, however, in this mono- graph we use the word search in a broader sense than Information Seeking. The model described in Section 2 defines search as the set of activities that take users from identifying a problem all the way to achieving their goals, which will, at times, involve Information Seeking, which in turn, may include Information Retrieval.\r\n1.3 The Scope of Our Approach 13 1.3.2 What We Mean by the Web\rThe “indexable web”, that is, the portion of the web indexed by major search engines, was estimated at 11.5-billion pages in 2005 [67], with Google reporting that they surpassed the 1-trillion mark in July 2008.2 One major characteristic of the Web, therefore, is scale. In this mono- graph, however, search systems are discussed that search both the whole web, and certain domains within the Web and so scale is not always a primary concern for design. A more indicative and remarkable charac- teristic is the heterogeneity of the contents. The Web contains a variety of data and documents. Documents may be in plain text, HTML, XML, PDF, Rich Text Format (RTF), Microsoft Word (.DOC), spreadsheets, and a multitude of specialized or proprietary formats, including micro- formats [6]. Multiple forms of media, including still images, audio, and video, are widely available and indexed by general purpose as well as specialized search engines. These documents vary from highly struc- tured databases, to semi-structured web pages, to unstructured text. Again, however, while some web search engines focus on the entire het- erogeneous content of the Web, others focus on specific and bounded domains within the Web. In these known and bounded conditions, the format of documents is often known and fixed, and so is not always a concern for all web-based search systems.\rIn summary, as web search systems are discussed, some are limited by the diversity of online material, and the design of others is motivated by unique features of web collections. Both are important areas that sometimes share concerns but often differ significantly in the challenges they present during the design of search interfaces. An e-commerce site might, for example, try to support searchers with the categorization, price, and availability of their products. Such product-related attributes are not a concern for general Web search, which may includes product results, reviews, specifications, and informational documents. Finally, it is important to remember that as users engage in search, they may be moving between the entire web and known collections within it, in order to achieve their goals.\r2 http://googleblog.blogspot.com/2008/07/we-knew-web-was-big.html.\r\n2\rA Model of Search\rIn order to survey search interface advances, we need to establish a framework and a vocabulary that allows us to organize, describe, and analyze the value of their contributions. This section describes a model of search [84] which grounds the remainder of the monograph by con- veying both a structure that defines search and a set of relevant termi- nology. While many models of search exist, as discussed further below, the selected model was chosen because of its holistic perspective, con- sidering different granularities of context determined by “a person’s task, its phase, and situation”, while still including important finite concepts such as topical relevance. The key benefit of this holistic view is that it maintains an understanding of the situational nature of infor- mation and that search activities are performed in the context of a larger work task, and even cultural influences. Therefore, the discussion of advances in search interaction, visualization, and Human Computer Interaction, must be set in the broader context of a human’s task. The model described in this section will be referenced throughout Sections 3 and 4, and will be used to classify search interfaces within the taxonomy described in Section 5.\r14\r\n2.1 A Context Model of Search\rFigure 2.1 shows the model of searching contexts, produced by Jarvelin and Ingersen [84], that provides the framework we will be referring to for the remainder of this monograph. Search can be modeled as a hier- archy of goals and tasks. Each task provides the goal and serves as the context for its subsidiary task(s). Figure 2.1 makes multiple levels of context explicit: socio-organizational and cultural, work task, seek- ing, and retrieval. Information Retrieval, as the smallest granule in the model, represents the action most often embodied by Keyword Search, where users are trying to find an often known nugget of information, such as the price of a particular server. Searchers may often find them- selves performing a series of Information Retrieval actions as part of a broader Information-seeking task, such as trying to find the best possi- ble server given a specific budget. Any Information-seeking task, how- ever, is set within a larger Work Task, such as being asked to procure\rFig. 2.1 Searching behavior is made up of multiple layered contexts, where simple informa- tion retrieval is the narrowly focused; figure from J ̈arvelin and Ingwersen [84].\r2.1 A Context Model of Search 15\r\n16 A Model of Search\rresources for a forthcoming project. The result of a Work Task will contain the product of one or more Information-seeking tasks. Finally, every Work Task sits within a much broader Socio-Organizational and Cultural context. The requirements and the importance of meeting them, for example, will be different when buying a server for a large organization, a safety critical organization (such as a hospital) and a small start-up business. Clearly, a home/personal context will have dif- ferent demands than a corporate professional environment, and success in each will be under different criteria.\r2.2 Information Retrieval\rWithin the information retrieval (IR) context, the searcher’s goal is focused on finding documents, document sub-elements, summaries, or surrogates that are relevant to a query. This may be an iterative pro- cess, with human feedback, but it usually is limited to a single session. Typical IR tasks involve finding documents with terms that match terms presented by the searcher, or finding relevant facts or resources related to a query. Typically, within each IR task, the searcher for- mulates queries, examines results, and selects individual documents to view. As a result of examining search results and viewing docu- ments, searchers gather information to help satisfy their immediate information-seeking problem and eventually the higher-level informa- tion need. The common element of all IR tasks as defined in this web- focused monograph is the query-result-evaluation cycle conducted over a collection with the “unit” of information being the document, a sub- element of the document, a summary, or a document surrogate. In the server purchasing scenario used above, the document being sought may be the information page on an e-commerce website, but may also be a picture, or downloadable PDF of its specification.\rWeb search engines like Google support one of the more common IR tasks on the Web: searching for web pages that match a given set of query terms. Services such as Ask.com, however, also attempt to provide fact-oriented answers to natural language queries, while Google will also answer specific conversions of mathematical calculations if the queries are constructed in certain formats. Keyword search is not the\r\n2.3 Information Seeking 17\ronly example of Information Retrieval on the Web, however, with tools like Flamenco and Clusty, each discussed in more detail in Section 3, provide hyperlinked terms that allow searchers to narrow their results and browse relationships between documents.\rEvaluation measures traditionally include precision, recall, and their variations, which simply assess how relevant a document is to the given query. Consequently an IR system can be tested simply by whether it returns the most appropriate documents to a given query. Although the concept of relevance has multiple aspects, at the IR level, topical relevance is the typical aspect considered, that is, how relevant the documents are to the topic expressed by the query, and most systems focus on returning the most relevant documents rather than all of the relevant documents.\r2.3 Information Seeking\rThe objective of the information-seeking (IS) task is to satisfy a per- ceived information need or problem [115, 117, 151]. Often, searchers undertake one or more IR tasks as part of a larger information-seeking (IS) task, although it is possible for a simple need and IS task to be achieved with a single IR task. At the IS level, searchers make strate- gic decisions about where, how, and even whether to find informa- tion related to their information needs. They may adopt an analytical strategy to decide whether to use an IR system (a specific website or searching service). They may also adopt a browsing strategy, where, for example, they start from a known point (perhaps the results of an IR query) and follow successive links to locate other related documents. The documents found while browsing or returned by an IR task will, as part of the IS task, be examined to extract information and synthesize it into a solution to the information need.\rWhile the Web provides an environment for many IR tasks, and, consequently, many IS subtasks, searchers may also consult non-IR sys- tems as well as other resources such as printed material, colleagues or friends, in order to achieve their goal. In fact, people do not always choose to seek information. They may prefer to exhibit alternative Information Behaviors and avoid information that is discomforting or\r\n18 A Model of Search\rtroublesome or which is judged not worth the effort [124]. They may also be biased in what information they seek, looking for information that supports preconceived ideas or opinions [119].\rInformation-seeking tasks have previously been structured as, for example, linear sequence of stages [101] or hierarchical decompositions of tasks [23]. Each of these tasks requires selecting a source, and then engaging in one or more information retrieval tasks which satisfy a por- tion of the overall need. From the perspective of an organization, Choo et al., [39] developed a behavioral model of organizational information seeking on the Web by integrating Ellis’ [57] six stages of information seeking (starting, chaining, browsing, differentiating, monitoring, and extracting) with Aguilar’s [1] four modes of scanning (undirected view- ing, conditioned viewing, informal search, and formal search). Each of these tasks helps to satisfy part of an organization’s information needs.\rThe hyperlink capability of the Web provides support for a browsing strategy. When browsing, each new piece of information that is gathered can provide new ideas, suggest new directions, and change the nature of the information need [13]. This leads to behavior that Bates refers to as berrypicking, reflecting the incremental collection of pieces of information that, as a whole, help to satisfy the information need. The choices made at each step are guided by the seeker’s assessment of what is most likely to produce useful answers as they forage for information [134]. In an environment like the Web, this assessment is based on cues such as hyperlink text that provide an information scent which help users make cost/benefit trade-offs in their choices. In line with research suggesting that search is made up of sequences that are affected by the discovered information, Belkin et al. [18] created a set of “scripts” that describe the typical paths taken by 16 different types of users, including switch points between them for when discoveries change their user type.\rSystems that attempt to provide support of IS tasks typically pro- vide functionality beyond the query-result-evaluation cycle supported by IR systems. This may include search history mechanisms to sup- port information seeking over multiple sessions, or mechanisms such as tagging to collect a set of documents that are each relevant to the larger information need. They may also provide overviews of a collec- tion using textual or graphical methods. Individually, such UI features\r\n2.4 Work Context 19\rmay be tailored to support specific elements of the IS stages (e.g., topic exploration when writing a paper or monitoring of web resources).\rThe evaluation of interfaces that support information-seeking tasks typically involves assessing the quality of information acquired by users relative to the information need provided. IS tasks, such as purchasing a server, are usually assessed by the support the interface provided to users while carrying out the task, and the judged accuracy of the final decision, given the initial requirements. Hearst [72] provides a thorough review of information-seeking user interfaces and their evaluation.\r2.4 Work Context\rThe perceived information need that motivates an IS task is itself moti- vated and initiated by a higher-level work task or context [29, 30, 84] or personally motivated goal [92]. The process is initiated when a searcher recognizes an information need (or is instructed to investigate one) based on an organizational or personal need [29, 30, 115].\rWork tasks are situated in the context of work organization and reflect organizational culture and social norms, as well as organizational resources and constraints. As such, they constrain or guide the IS tasks. For example, the work context provides important information about the domain of information relevant to an information need. It defines and constrains the resources available to satisfy the need. Immediate access to reference books, libraries, electronic resources (e.g., the gen- eral web or specialized online databases), and human experts affect the strategic decisions searchers make at the work-context level.\rSystems that support work-context tasks may provide special- ized functions or support strategies for the work domain. Certain working domains, such as medicine, law, and even academia, have well- established work-context tasks and procedures for information seeking in support of those tasks. For example, in the medical field, studies have examined how physicians and healthcare searchers search for infor- mation [22, 61]. Consequently, services can provide mechanisms, for example, that identify prior art or key documents on a certain topic to support the work-context task of writing a paper. To support the learning of work-context processes, search systems may provide access\r\n20 A Model of Search\rto tutorials or communities of practice related to the work task. They may also provide domain-specific frameworks for making sense of, inte- grating, and synthesizing information. For example, a system to sup- port the development of teaching materials based on an oral history collection may provide a lesson plan tool that supports organizing and annotating retrieved information in a lesson plan format with links from items in the lesson plan back to retrieved elements of the collection.\rEvaluation of systems in a work context usually focuses on the achievement of users in realistic scenarios, such as the grading of school or college essays, or, in the extreme, the success of organizations at procuring academic funding.\r2.5 Summary\rIn the previous subsections, we have described the three key levels of searching context: information retrieval, information seeking, and work contexts. The last of these is also surrounded and influenced by the environment and social context that the work task is being carried out within. Each of these contexts provides a useful lens for understand- ing the benefits and contributions of novel user interfaces for search. When a new technique is designed, built, and tested, the work is usually motivated by a problematic scenario and a hypothesis that it can be overcome. This scenario might be carefully defined and IR focused, such that users cannot easily express a particular query or find the correct result. Similarly, the scenario and hypothesis may be much broader, as to support a work task like planning a vacation or researching for a report. In the following sections, we discuss the design and evaluation of recent interface contributions, by considering how they have been designed and tested to support searchers at different levels of context: IR, IS, and WC.\r\n3\rSurvey of Search Systems\rThe aim of this section is to review many of the search visualization and exploration techniques that have developed and studied. Further, the aim is to demonstrate the diversity and range of available techniques, rather than produce a complete catalog like some much larger sur- veys [72]. The techniques described in this section, which are classified in the taxonomy in Section 5, are presented according to the following structure. First, techniques that have made use of the advantages of enriched metadata are discussed in Section 3.1, which has afforded a change in the way that searchers can interact and control the presenta- tion of results. Second, Section 3.2 describes the varying approaches to directly organizing and presenting results. Finally, Section 3.3 addresses some alternative functionality that has enhanced visualizations, such as animation and use of alternative senses. Each technique described below is briefly discussed with the contexts of search described in Section 2. Further, while here many techniques are listed, the tax- onomy described in Section 5 provides the means to compare their aims, strengths, and weaknesses, in terms of the context of information seeking supported and the way they have been evaluated, according to Section 4.\r21\r\n22 Survey of Search Systems\r3.1 Adding Classifications\rOne of the main streams of research for enhancing search environments has been to use annotations, or classifications, to the documents or collections. Two challenges in adding classifications are the increasing scale of collections and the associated cost of annotating each docu- ment. We review four common approaches to adding classification to collections and overcoming the challenges: hierarchical classifications, faceted classifications, automated clustering, and social classifications.\r3.1.1 Hierarchical Classifications\rOne early research project, the SuperBook interface, showed the ben- efits of using categories in a document collection, by organizing search results within a book according to the text’s table of contents; here the book is the collection and the pages of the book are the documents. An evaluation found that it expedited simple IR tasks and improved accuracy, both by 25% [55]. This categorization approach has been used in many cases and has shown success in both fixed and uncontrolled collections, however, the usual approach for the latter is to model the first and allow document owners to assign their own documents into the hierarchy.\rAn example of a fixed and managed data set may be the genre- based classification of music in Amazon’s1 online music store, where every CD is assigned to one or more categories of music, including Pop, Rock, and Indie. Here there is incentive for Amazon, as the collection owner, to annotate the collection in this way to make it easier for their clients to find the music they want and, subsequently, encourage sales. Allen [5] investigated two such digital library interfaces, the Dewey Decimal System and the ACM Computer Reviews system, and showed that both used hierarchical classification effectively for organizing the collections.\rIn the same paper, Allen discusses the potential for use of Internet-wide information samples. Examples of hierarchically classi- fied structures for the Web, as an ever increasing and unmanaged set\r1 http://www.amazon.com.\r\n3.1 Adding Classifications 23\rof documents, are Google Directory2 and Yahoo Directory.3 In both of these directories, the individuals that own or manage documents on the Web can submit their websites for inclusion under different parts of the directory. This approach has been popular in the past but has suffered where document owners do not submit their websites for inclusion.\rIn an attempt to remove the annotation cost and limitation of clas- sification systems, Kules et al., [102] took a simple result set of federal agency/department reports, by mapping a set of URL prefixes to a known finite list. This approach took a known pattern from the data and used it to effectively categorize search results by government agency and department. In a more advanced approach, Chen and Dumais [36] showed that machine learning techniques can be applied to known cat- egorized data items to automatically assign new web documents into categories. Using the documents that already exist in the LookSmart4 directory as a training-set, the machine-learning algorithm success- fully and automatically categorized the results of a Yahoo search. The Support Vector Machine (SVM) algorithm [87] achieved 70% accu- racy with the categorization provided by human participants, where the remaining 30% included partially matching annotations. The user study showed a strong preference for the added categorization provided by the process. Other approaches to automation exist and are mainly described in the automatic clustering section below.\rThe benefits of applying hierarchical categorizations have been proven numerous times in research. For question answering tasks, Drori and Alon have shown that search results augmented with category labels produced faster performance and were preferred over results without category labels [49]. Dumais et al. [50] also studied the effect of grouping search results by a two-level category hierarchy and found that grouping by a well-defined classification speeds user retrieval of documents.\rIn a similar approach to the book-search mentioned above, the Cha-Cha system organizes intranet search results by an automati- cally generated website overview (Figure 3.1). It reflects the underlying\r2 http://directory.google.com. 3 http://dir.yahoo.com.\r4 http://www.looksmart.com.\r\n24 Survey of Search Systems\rFig. 3.1 The Cha-Cha system organized intranet search results by an automatically gener- ated website overview.\rstructure of the Website, using the shortest path from the root to each document to dynamically generate a hierarchy for search results. Pre- liminary evaluations were mixed, but promising, particularly for what users considered “hard-to-find information” [38]. The WebTOC system (Figure 3.2) provides a table of contents visualization that supports search within a website, although no evaluation of its search capability has been reported [128]. WebTOC displays an expandable/collapsible outliner (similar to a tree widget), with embedded colored histograms showing quantitative variables such as size or number of documents under the branch.\r\n3.1 Adding Classifications 25\rFig. 3.2 The WebTOC system provides a table of contents visualization that supports search within a website.\rHierarchical classifications, as demonstrated by the examples dis- cussed above, are largely designed to organize results into groups. While this allows users to perform some additional browsing tactics [12], such as going to a parent or child areas of the classification, the main ben- efits have been studied at the IR level. The studies performed by Allen [5] and Drori and Alon [49] show users performing faster in basic information retrieval tasks, without loss of accuracy. Faceted clas- sifications discussed below, however, extend the notion of hierarchi- cal classifications to support additional information-seeking behavior. Consequently, studies of faceted classification systems have more often focused on information-seeking behavior.\r3.1.2 Faceted Classifications\rAnother approach that helps users find documents based on multiple orthogonal categorizations, such as thematic, temporal, and\r\n26 Survey of Search Systems\rgeographic, builds on the traditional library science method called faceted classification. Restaurants, for example, can be classified by facets such as by location, price, food type, customer ratings, and size, with around 2–20 attribute values per facet (although some facets have many more values). Early designs, one of which was called query pre- views, also showed the number of documents having each attribute value. Query previews were updated with new values so as to pre- vent users from submitting searches that would produce zero results [48, 135, 165]. Faceted classification allows users to apply relevant con- straints on their search in the context of the current problem and exist- ing knowledge. For example, if users know their own budget and a minimum specification required for a computer, then they can apply constraints in these facets separately, and vary them individually (to check, for example, the effect of increasing the budget slightly). After applying their constraints they can then see all the relevant computers and possibly choose between them based upon the facets that remain unused.\rMany systems have applied this approach in some way, where a single classification system has not been expressive enough for the doc- uments in a collection. For example, before prototyping a faceted sys- tem,5 eBay already allowed users to apply constraints such as price, color, and size; the available constraints depended, of course, on the type of object being sought. Whereas eBay indexes its own auctions, websites such as shopping.com and Google Product Search6 provide a faceted search over many shopping sites from the whole web.\rFlamenco7 (Figure 3.3) is a clear example of the features provided by faceted search using multiple hierarchical facets. Providing interfaces to fixed collections, including art, architecture, and tobacco documents, Flamenco presents faceted hierarchies to produce menus of choices for navigational searching [188]. A selection made in any facet is added to a list of constraints that make it clear to users what is forming the list of results that are being shown.\r5 http://express.ebay.com.\r6 http://google.com/products.\r7 http://flamenco.berkeley.edu/.\r\n3.1 Adding Classifications 27\rFig. 3.3 The Flamenco interface permits users to navigate by selecting from multiple facets. In this example, the displayed images have been filtered by specifying values in two facets (Materials and Structure Types). The matching images are grouped by subcategories of the Materials facet’s selected Building Materials category.\rA usability study compared the Flamenco interface to a keyword search interface for an art and architecture collection for structured and open-ended exploratory tasks [188]. With Flamenco, users were more successful at finding relevant images (for the structured tasks) and reported higher subjective measures (for both the structured and exploratory tasks). The exploratory tasks were evaluated using subjec- tive measures, because there was no (single) correct answer and the goal was not necessarily to optimize a quantitative measure such as task duration.\rThe success seen by Flamenco, having provided faceted classifica- tions to assist search, has been used in many commercial and aca- demic projects. Endeca8 is a commercial company that provides faceted\r8 http://www.endeca.com.\r\n28 Survey of Search Systems\rFig. 3.4 epicurious is a commercial recipe website that uses Flamenco style faceted search in conjunction with keyword search.\rsearch, branded as Guided Navigation, to large businesses including Wal-Mart and IBM. epicurious, shown in Figure 3.4, provides both faceted and keyword search over food recipes in a style that is simi- lar to Flamenco’s experience. The lead researcher of Flamenco, Marti Hearst, refers to epicurious as a good example of faceted browsing in commercial conditions [73].\rHuynh et al. [81] has developed Exhibit9 (Figure 3.5), a faceted search system that is similar to flamenco in many ways, but has some significant developments. One key advance is that, where a selection in Flamenco filters all the facets, a selection in Exhibit filters all the other facets and leaves the facet with the selection unchanged. This provides two benefits: first, users can easily change their selection and second, users can make multiple selections in one facet. This allows users to see, for example, the union of all the red and blue clothes, rather than just red or just blue. This support for multiple selection within a single facet has been recently added to ebay.com, but remains unavailable in services such as Google Product Search.\r9 http://simile.mit.edu/exhibit/.\r\n3.1 Adding Classifications 29\rFig. 3.5 The Exhibit faceted search interface takes a slightly different approach, only filter- ing facets without a selection, so that previous selections can be see in the context of the options at the time.\rThe Relation Browser10 [192], shown in Figure 3.6, takes another approach to faceted search. One notable difference is that multiple selections lead to their intersection of results being displayed. Another feature that the Relation Browser provides is a preview of the affect of clicking has on other facets. Graphical representations behind each item in each facet show how many documents can be found by selecting it. When users hover over any item in any facet, the bar in the graphi- cal representations is shortened to indicate how many documents will remain under each annotation should the users make the selection. This technique revives the query preview strategy, which is a helpful alterna- tive to the simple numeric volume indicators [181] that are included in most classification-based systems. Aside from the graphical representa- tion, the preview of the affect by simply hovering (or “brushing”) over the item is a technique that is being included in many new projects. A new version of the Relation Browser is now in development [32].\r10 http://idl.ils.unc.edu/rave/.\r\n30 Survey of Search Systems\rFig. 3.6 The Relation Browser interface provides consistent facets, where the list of values is not filtered by selections. Instead, users can see (and preview by simply hovering) the reduction in files associated with each facet-value with the bar-chart style visualizations.\rmSpace11 [148], shown in Figure 3.7, represents yet another type of faceted search: a column-faceted interface. Like iTunes, mSpace presents facets in a left-to-right set of columns. Each column is fully populated so that users can still make a selection in any facet, but then only the columns to the right filter. This allows the facets to represent additional information that would otherwise be lost in faceted search. In their classical music example, where the facets are Era, Composer, Arrangement, and Piece (from left to right), if users select a Composer, they see a filtered list of the arrangements (s)he used and then a list of the pieces (s)he composed. When users make a second selection in the arrangement, other forms of faceted search would remove all the composers that did not use that arrangement. In mSpace, users are still\r11 http://mspace.fm.\r\n3.1 Adding Classifications 31\rFig. 3.7 The mSpace faceted column browser provides facets as columns that filter from left to right, as in iTunes. The facets, however, can be rearranged so that different parent-child relationships can be visualized spatially.\rable to see all the arrangements that the selected composer used, and all the pieces of the selected arrangement.\rThe functionality of mSpace means that there is a type of informa- tion that is not conveyed by mSpace but seen in other forms of faceted search: the Era of the selected composer. This missing information is not a problem in mSpace, unlike other column-faceted browsers like iTunes, because the related items in facets to the left of a selection are highlighted. This allows users to see which era the selected composer is in, whilst still allowing them to get the added facts provided. The effect of this leftward highlighting, named Backward Highlighting [179] is that users incidentally discover more facts about the structure of a collection, which can make search easier for future searches in the same domain [183].\rFinally, given the importance placed on the direction and order of facets in columns, mSpace allows users to reorder, remove, and\r\n32 Survey of Search Systems\rsupplement the facets shown, using any of the facets that are available from the collection. This allows users to say that they would rather know all the composers that used a given arrangement than all of the arrangements used by a given composer. Another unique aspect of mSpace is that it assumes that a selection that is different to a previous selection in a facet is a change of selection and not a multi- ple selection. Consequently, users can quickly compare the difference between two items in a facet. The default assumption that users are changing their selection supports the concept of answering subjunctive questions about the collection [113], which means simply to compare the outcomes of multiple actions.\rA recent longitudinal study of mSpace [182] has indicated that this more complex form of faceted search is easy to learn and is thereafter perceived as a powerful system, receiving positive subjective views. A log analysis showed that 50% of participants used facets in their first visit to the site and 90% in their second visit. Over the whole month- long period, there were more interactions with facets than individual keyword searches. Further, given that facets allow users to produce complicated queries than basic keyword searches, faceted searches rep- resented two times the number of Boolean searches and three times the number of advanced searches.\rEach of the faceted classification examples so far has been on fixed collections, but some research into faceted browsing has been looking at unknown and un-bounded document sets like the Web. The SERVICE12 (Search Result Visualization and Interactive Categorized Exploration) search system couples the typical ranked list of web search results list with automatically generated facets [106] (Figure 3.8). Clicking on a category filters (or narrows) the displayed results to just the pages within that category. Moving the pointer over a category highlights the visible search results in that category in yellow. Moving the pointer over a result highlights all the categories in the overview that contain the result.\rThe facets are automatically generated by applying fast-feature clas- sifiers [105] over the top 100 results of a Google query, and organized\r12 http://www.cs.umd.edu/hcil/categorizedovervew.\r\n3.1 Adding Classifications 33\rFig. 3.8 Unlike most faceted systems, which work on known collections of documents, the SERVICE web search interface classifies un-bounded web collections in multiple facets [106].\rthem into known possible categories drawn from the Open Directory Project (ODP) and a database of US Government websites13: Topic, Geography, and US Government. A similar project: Dyna-Cat [136], shown in Figure 3.9, also automatically produced facets for sets of search results, showed that not only was there improvements in objec- tive and subjective measures, that users were 50% faster in fact-finding tasks using Dyna-cat over typical ranked list keyword search interfaces.\rNorthern Light,14 a commercial search service, provides a simi- lar capability by grouping results in their Custom Search Folders.\r13 http://www.lib.lsu.edu/gov/tree. 14 http://www.northernlight.com.\r\n34 Survey of Search Systems\rFig. 3.9 The Dyna-Cat search interface automatically classifies search results within un-bounded document collations.\rExalead15 is another project that successfully organizes search results according to categories drawn the Open Directory Project, and presents them along side search results in a publicly available web search engine. PunchStock search, shown in Figure 3.10, presents a faceted view of personal image collections. The NCSU library, shown in Figure 3.11 also uses facets to enhance the search if their collection.\rThe faceted search interfaces described so far each have a space allo- cated to the presentation of facets, such as down one of the sides, across the top, or even along the bottom as in Google’s product search.16 A recently proposed faceted search interface, called FacetPatch, embeds faceted options into the existing layouts of existing website [120]. Hover- ing over an attribute of a camera, for example, converts the attribute’s value into a drop-down list from which users can select an alternative. Users can move, therefore, directly from result to result, by changing the brand or altering the pixel-count that they desire.\r15 http://exalead.com.\r16 http://www.google.com/products/.\r\n3.1 Adding Classifications 35\rFig. 3.10 The PunchStock photo search interface provides categorized overviews of photo search results.\rWhile some of the studies discussed above focus on typically IR level metrics, like speed and accuracy of simple search tasks, there are notable exceptions. Flamenco, the Relation Browser, and mSpace have each been studied in more exploratory contexts, where the task is often more than to simply find a search result. The study by Yee et al. [188], sets users exploratory tasks that were not measured by speed or accu- racy, as depth of exploration and amount of content covered could be considered more important than how fast and perhaps under-researched an answer is. This type of IS level analysis was also performed by Capra et al. [33]. Wilson et al [179] also studied the amount of incidental infor- mation (information not part of the core task) that could be remem- bered by participants in a study. The same team took this further by studying, over time, users of an mSpace interface providing access to a news footage archive [182]. Participants used mSpace, which was being logged, for their own work-goals over a four-week period, and engaged in periodic communication with the evaluating team. We can see from these studies that the types of exploring and browsing facilitated by faceted browsers have been studied for their support of IS and even WC levels of searching behavior.\r\n36 Survey of Search Systems\rFig. 3.11 The NCSU library catalog provides categorized overviews of search results using subject headings, format, and library location, provided by a commercial enterprise search vendor: Endeca.\r3.1.3 Automatic Clustering\rWhere collections have been large or unmanaged, faceted classifications have been less successfully applied. The examples where it has been\r\n3.1 Adding Classifications 37\rused above provide generic web-oriented facets, and often the facets that would be important depend on the query or subject of the users’ varying goals. Another approach is to automatically identify attributes of a collection or result set that are important, rather than explicitly producing labeled annotations. This approach is called clustering, and has shown success under popular information retrieval metrics such as precision and recall [74, 118, 189, 191] or task completion time [169]\rA one-level clustered overview was found helpful when the search engine failed to place desirable web pages high in the ranked results, possibly due to imprecise queries [89]. Clusty17 uses the cluster- ing technique to produce an expandable overview of labeled clusters (Figure 3.12). The benefits of clustering include domain independence, scalability, and the potential to capture meaningful themes within a set of documents, although results can be highly variable [71].\rGenerating meaningful groups and effective labels, however, is a recognized problem [139] and where possible (usually, but not exclu- sively in fixed or managed collections) having an information archi- tect design optimal annotations or labels, will provide a better search interface [162]. Stocia and Hearst extracted a category hierarchy from\rFig. 3.12 The Clusty metasearch engine uses automated clustering to produce an expand- able overview of labeled clusters.\r17 http://www.clusty.com.\r\n38 Survey of Search Systems\rWordNet [122] using keywords from the document collection. They propose that this can be manually adjusted by an information archi- tect. Similarly, Efron et al. [54] investigated the use of semi-automated methods, combining k-means clustering, and statistical classification techniques to generate a set of categories that span the concepts of the Bureau of Labor Statistics web pages and assign all to these categories. They found that concept learning based on human-supplied keywords performed better than methods using the title or full-text.\rAutomated classifications have, in large, been studied separated from the three levels of search, but have often compared the accuracy of the automatic classification with human classifiers. The studies by Stocia and Hearst [162] and Rivadeneria and Bederson [139] are exam- ples, however, which show that automatic classifications can still lead to improved user performance in IR level searching activities.\r3.1.4 Social Classifications\rRecent developments in web technologies, such as web2.0, have led to a rise in social classifications such as bookmarks, tagging, and collab- orative feedback ratings. For example, the Yoople search engine allows users to explicitly move the search results up or down to provide rel- evance feedback into the weightings used in future searches. Allowing users to reorder results has recently been supported for Google users with accounts, but is designed to improve personalization of search results. It remains unclear as to whether Google will use this rele- vance feedback to reorder results. Little research, however, has explic- itly proven the benefits of social classification schemes, even though the notion has become increasingly popular on the Web. Some research, however, is emerging: Millen et al. [121] have investigated the use of social bookmarking in enterprise search software and an eight-week field trial has shown positive results.\rIn Yoople18 (Figure 3.13) mentioned above, the classification produced is only numeric (document ranking order) and affects the background search algorithms. Google has recently introduced a similar\r18 http://www.yoople.net/.\r\n3.1 Adding Classifications 39\rFig. 3.13 Yoople Search, where users can drag items in the results list to indicate where they think a result should appear.\rpersonalization scheme called Search Wiki.19 While Google’s Search Wiki allows users to simply promote or remove results, Wikia Search20 (Figure 3.14) allows users to be more specific and rate results out of five. Both Google’s Search Wiki and Wikia Search allow users to directly influence search rankings in a similar way to Yoople. Further, however, they both also allow users to make annotations on results.\rThe notion of social tagging has allowed communities of people to develop flat classification schemes for collections. Flickr,21 a photo archiving website, depends almost entirely, although photo names and descriptions can be included, on user tagging to return images that relate to a keyword search. A negative side to such tagging classifica- tion schemes is that they are hard to present to users. Therefore, they are usually only used to aid keyword search rather than to help users interactively browse through documents, like in some of the faceted and category-based systems listed above. A popular example of presenting a flat set of tags to users has been to foreground popular tags in a tag cloud (Figure 3.15). Although little research has been performed on\r19 http://googleblog.blogspot.com/2008/11/searchwiki-make-search-your-own.html. 20 http://search.wikia.com/.\r21 http://www.flickr.com.\r\n40 Survey of Search Systems\rFig. 3.14 Wikia Search allows users to explicitly rate results out of five to directly influence search ranking.\rFig. 3.15 A tag cloud taken from Flickr website, included in [65].\r\n3.2 Result Organization 41\rFig. 3.16 The MrTaggy interface allows users to include or exclude tags from their search instead of using keywords.\rtag clouds, there is a growing consensus that they are more valuable for users who are making sense of information, rather than for finding specific information [157].\rSome systems are trying to take the benefits of tagging to produce what are known as folksonomies [80] or community-driven classifica- tion systems. Wu et al. [186] present some design prototypes that over- come some of the challenges in converting flat tags into a structure classification system. One approach, used by the MrTaggy interface shown in Figure 3.16, allows users to perform searches based entirely on community-generated tags by including or excluding tags from a list of related tags [90]. Searches are initiated with a pair of selections from two tag clouds: one containing adjectives and the other containing nouns and other objects.\r3.2 Result Organization\rAt the opposite end to supporting users in better defining their own search query, which has been discussed above, is the presentation of the result sets and items that will help users to identify the specific result item(s) that help them achieve their goals. There is ample research that specifically looks at the best way to present a single result in a linear set, which is perhaps best represented by the view currently provided by Google: name, text snippet, and web address.\r\n42 Survey of Search Systems\rIn the following subsections, however, we discuss the research that looks beyond a linear results list and visualizes a result set to help users find the specific results they are looking for.\r3.2.1 Result Lists\rThe most common method to show a result set, seen in most web search engines, is to provide a simple list of results. The order that the result items are listed is determined by some metric, usually based on how relevant the result is to the search terms used. The importance of constructing this algorithm well has been motivated many times in research where users regularly enter single-term ambiguous queries [171], and view only a few results [82] and rarely stray past the first page of results [17]. The accuracy and efficiency of such algorithms, however, has been research for many years in the information retrieval space, and is not covered in the scope of this report; Becks et al. [16] present a good discussion of result ordering, including occasions where ranked relevance may not be the most appropriate ordering.\rThe representation of each result has also received much research and is especially important when research has shown that the accep- tance of search systems has been significantly reduced by unclear or confusing representations.22 In Google, each search result has name (also a link to the finished document), a sample of text from the doc- ument, a URL for the document, and the size of the document. Addi- tional information, such as previous visit dates and counts, can be added if a user account is available. Work by Chen et al. [37] investi- gated the structure of result representations and provides a framework for considering the presentation of search results. In one of the more sig- nificant publications in this area, White et al. [175] showed that best objective results occurred when the text sample in a representation included the query terms used in the original search. This allows users to see the context of their query in each result item, so that they can best judge its value in viewing the whole document. This was backed up by Drori and Alon [49], who showed objective and subjective benefits\r22 http://www.antarctica.net.\r\n3.2 Result Organization 43\rfor both query-relevant snippets and associated categories. Microsoft’s Bing, however, includes an exception to this keyword-in-context rule for text snippets, providing the first whole paragraph of a Wikipedia page, which usually provides the best short overview for the topic being covered. This exception to the basic search result layout has been fur- thered by the now customizable search results provided by Yahoo’s Search Monkey,23 which allows users to apply templates for different popular result types, such as Wikipedia results, IMDB results, facebook results, and so on.\rOther search engines, including Ask Jeeves24 and Exalead25 (Fig- ure 3.17), also include thumbnails of the document represented by each result. Research into the benefits of page thumbnails [185] has shown most advantage when users are returning to a previous search to find a result from the previous session. With these conditional objective ben- efits, however, subjective results have been positive. Further, Teevan et al. [166] have shown that more abstract images, which integrate\rFig. 3.17 Exalead search provides images and categories with each search result, and facets to help users narrow the results.\r23 http://www.yahoo.com/searchmonkey. 24 http://www.ask.com.\r25 http://www.exalead.com.\r\n44 Survey of Search Systems\rcolor, titles, and key pictures from web pages, can support both search and re-visitation more evenly.\rThe advances made in the basic style of Results Lists have had lit- tle effect on the interaction that people have with search interfaces. Advances such as thumbnails have changed the representation, and have encouraged users to make better-educated judgments on results they see. Consequently, the contribution and subsequent study has mainly focused on IR level improvements of speed and accuracy.\r3.2.2 2D Result Representations\rWith the aim of providing useful overview visualizations of result sets to users, much research has aimed at taking information visualization techniques and applying them to search results. One example is self- organizing maps (SOMs), originally produced by Kohonen [97]. SOMs automatically produce a two-dimensional (2D) visualization of data items, according to the attributes that they share, using an unsuper- vised machine-learning algorithm. Consequently, the SOM approach applies well when visualizing automatically clustered documents.\rAu et al. [10] used SOMs that have been used to support exploration of a document space to search for patterns and gain overviews of avail- able documents and relationships between documents. Chen et al., [37] compared an SOM with the Yahoo! Entertainment category, for brows- ing and searching tasks (Figure 3.18). They found that recall improved when searchers were allowed to augment their queries with terms from a thesaurus generated via a clustering-based algorithm. Similar work [108, 110] has shown positive results in using SOMs to support search.\rAn alternative 2D organization on result sets is known as a treemap [60]. In a treemap, the clusters of documents are grouped using a space- filling algorithm. As shown in Figure 3.19, the 2D space is divided into the top-level items in the hierarchy produced by the clustering algorithm, where the number of search results found within the category dictates the size of each part. Each of these sections is divided into their children within the hierarchy, where the number of search results again dictates the size of the subparts. This process is repeated as necessary according to the hierarchy and space available. With this,\r\n3.2 Result Organization 45\rFig. 3.18 A SOM of search results, image taken from http://vizier.u-strasbg.fr. The results are shown as clusters of key topics relating to a query.\rusers can access categories at any level of the hierarchy and see the results associated with that cluster. Color coding, or use of increasing color density, is often used to indicate a secondary dimension over the data, so that popularity and price, for example, are both conveyed (Figure 3.20).\rUsing another information visualization alternative, Citiviz (Fig- ure 3.21) displays the clusters in search results using a hyperbolic tree [109] and a scatterplot [133]. The Technical Report Visualizer proto- type [64] allows users to browse a digital library by one of two user- selectable hierarchical classifications, also displayed as hyperbolic trees and coordinated with a detailed document list. Chen [35] implemented clustered and time sliced views of a research literature to display the evolving research fronts over time. Chen’s work concludes that there are a number of benefits in visualizing results like this, but that the metrics used to build and display the visualizations are important to the acceptance by users.\r\n46 Survey of Search Systems\rFig. 3.19 The HiveGroup built a demonstration shopping application that allows users to explore Amazon.com products. (image from http://www.cs.umd.edu/hcil/treemap- history/hive-birdwatching.jpg).\rSeveral web search (or metasearch) engines, including Grokker,26 Kartoo,27 and FirstStop WebSearch28 incorporate visualizations similar to treemaps and hyperbolic trees. Kartoo (Figure 3.23) produces a map of clusters that can be interactively explored using Flash animations. Grokker clusters documents into a hierarchy and produces an Euler diagram, a colored circle for each top-level cluster with sub-clusters nested recursively (Figure 3.22). Users explore the results by “drilling down” into clusters using a 2D zooming metaphor. It also provides several dynamic query controls for filtering results. Unlike treemaps, however, the circular form often leads to wasted space in the visualiza- tion. Further, this early version of the Grokker interface has been found\r26 http://www.grokker.com.\r27 http://www.kartoo.com.\r28 http://www.firststopwebsearch.com\r\n3.2 Result Organization 47\rFig.3.20 Thisoverviewofwebsearchresultsusesatreemap.Nestingisusedtoshowtopand second-level categories simultaneously. The top 200 results for the query “urban sprawl” have been categorized into a two-level government hierarchy, which is used to present a categorized overview on the left. The National Park Service has been selected to filter the results. The effect on the right side is to show just the three results from the Park Service [106].\rto compare poorly with textual alternatives [139]. The authors found that the textual interfaces were significantly preferred. The conclusions were that web search results lack “1). . . a natural spatial layout of the data; and 2). . . good small representations,” which makes designing effective visual representations of search results challenging. Grokker’s website is no longer available. Refined visual structures making better use of space and built around meaningful classifications and coloring may ameliorate this problem, as illustrated by promising interfaces like WebTOC, which uses a familiar hierarchy of classification and a coded– coded visualization of each site’s content. Many users appreciate the visual presentation and animated transitions, so designing them to be more effective could lead to increased user acceptance.\rIn support of the conclusions about cluster map visualizations from Rivadeneira and Bederson [139], early information visualization\r\n48 Survey of Search Systems\rFig. 3.21 The CitiViz search interface visualizes search results using scatterplots, hyperbolic trees, and stacked discs. The hyperbolic tree, stacked disks, and textual list on the left are all based on the ACM Computing Classification System. Although CitiViz is offline, its techniques have been replicated both online and offline many times.\rFig. 3.22 Grokker clusters documents into a hierarchy and produces an Euler diagram, a colored circle for each top-level cluster with sub-clusters nested recursively. Later versions removed the random coloring in favor of a more muted interface.\r\n3.2 Result Organization 49\rFig. 3.23 Kartoo generates a thematic map from the top dozen search results for a query, laying out small icons representing results onto the map.\rresearch by Tufte [167] states that graphical visualizations need to have clear and meaningful axes to be effective for users. Other approaches to 2D graphical representations have focused on representing results in meaningful and configurable grids, in the same vein as scatterplots, where each axis is a specific metric, such as time, location, theme, size, or format, etc. The GRiDL prototype (Figure 3.24) displays search result overviews in a matrix using two hierarchical categories [154]. The users can easily identify interesting results by cross-referencing the two dimensions. The List and Matrix Browsers provide similar function- ality [107]. The Scatterplot browser [53], also allows the users to see results in a visualization that is closer to a graph, where results are plotted on two configurable axes. Without specific regions plotted in the scatterplot visualization, it is harder for users to view subsets of results, but easier to identify single result items. Informal evaluations of these interfaces have been promising, although no extensive studies of the techniques have been published.\rAnother stream of research has focused on attributes of documents that can form dimensions that leverage common knowledge. For exam- ple, GeoVIBE (Figure 3.25) tightly coupled a geographic layout with an abstract layout of documents relative to defined points of interest\r\n50 Survey of Search Systems\rFig. 3.24 The GRiDL prototype displays search results within the ACM Digital Library along two axes. In this screenshot, documents are organized by ACM classification and publication year. Individual dots are colored by type of document.\r[31]. A similar, and familiar dimension is time, and research has also looked at visualizing documents and classifications on a timeline. An example timeline browser, Continuum [9], is shown in Figure 3.26.\rEach of the visualizations above has focused on visualizing a result set, so that users can identify areas of results that may be relevant to them. A different approach is to provide an overview visualization to help direct people to relevant items in linear result sets. Query term similarity allows searchers to explore the contribution that each query term makes to the relevance of each result by displaying the terms and results in a 2D or 3D space [27]. Similar work on Hotmaps (Figure 3.27), by Hoeber and Yang [77], provides an overview of 100 search results, displaying a grid, with query terms as the horizontal axis and the 100 results as the vertical axis. Users are able to click on parts of the grid with high color intensity to find more relevant documents, at the term level, that would not have been in the top 10 results returned by Google. The approach of query and result set visualizations has\r\n3.2 Result Organization 51\rFig. 3.25 GeoVIBE, a search interface using geography visualization for results.\rFig. 3.26 Continuum represents documents in nested categorizations on a timeline.\r\n52 Survey of Search Systems\rFig. 3.27 Hotmaps, a 2D visualization of how query terms relate to search results.\rshown significant search benefits for users, and augments, rather than visualizes, result sets.\rThe strength of the more successful 2D visualizations has been when the use of space is clear and meaningful to users. An alter- native approach, to computer-generated visualizations or clusters of documents, is to allow users to arrange documents under their own parameters. The TopicShop interface [8], shown in Figure 3.28, per- mits users to meaningfully arrange sites on a canvas, where the clusters are meaningful to the individual and, when returning to a document, spatial consistency makes it easier for users to remember where they placed the result. More recent research in this area has explored 3D spaces, and is described in the section below.\rSimilar to the ideas behind TopicShop, Furnas and Rauch [62] present a set of design principles for information spaces and instan- tiates them in the NaviQue workspace. In NaviQue, a single informa- tion surface is used to display queries, results, information structures, and ad hoc collections of documents. Users can initiate a query simply\r\n3.2 Result Organization 53\rFig. 3.28 The TopicShop Explorer interfaces combines a hierarchical set of topics with a user-controlled spatial layout of sites within each topic (shown here) or a detailed list of titles and attributes [8].\rby typing onto a blank section of the workspace. Results are clustered around the query text. Kerne et al. [95] furthered this work in software called combinFormation by considering the spatial layout of different multimedia types for constructing personal collections (Figure 3.29).\rThe contributions made in different 2D visualizations have largely supported users in IS level activities, allowing them to see and often manipulate clusters of documents by interacting with the two axes. In Hotmaps [77], for example, the 2D space allows users to choose alternative approaches to parsing the result lists, other than linearly processing every result. The matrix browser allows users to change the dimensions used for each axis, and alter their scale and granularity. These have been studied at the IR and IS levels, showing improved speed or accuracy. These methods have also been shown to help users find results that would not necessarily come up in the top 10 of a keyword search interface. Some work has focused on the users’ work context, for example. the TopicShop interface [8], has given control of\r\n54 Survey of Search Systems\rFig. 3.29 combinFormation automatically searches the Web for multimedia relating to a query term and displays them on a 2D plane. Users can drag and drop the items to make arrange their own subcollections.\rthe 2D space to users as an open canvas, and they are able to search and organize resources for their work tasks.\r3.2.3 3D Result Representations\rWhen presenting research into 3D visualizations, it is first important to consider research into the benefits and weakness of 3D over 2D. Although research has shown success for some situations, such as sim- ulations and 3D CAD/CAM design [152], research into the visualiza- tion of information has shown that a third dimension can inhibit users and make interfaces more confusing [138, 163]. Aspects of 3D repre- sentations, such as occlusion and cost of visualizing depth, must be considered [44]. Further, research by Modjeska [123] has shown that 25% of the population struggle with 3D visualizations displayed on a 2D device, such as a computer screen. Investigation by Sebrechts et al. [150] also showed that participants were significantly slower at using a 3D interface, unless they had significant computer skills. Considering these challenges, however, the research described below highlights some of the ideas that have been proposed for 3D visualizations.\r\n3.2 Result Organization 55\rFig. 3.30 Data mountain provides an inclined plane upon which web pages are organized (image from PDF).\rThe Data Mountain browser (Figure 3.30), like the TopicShop explorer described above, allows users to arrange documents on a plane, creating a 2 1/2 dimension view with perspective, reduced size for dis- tant items, and occlusion. In the Data Mountain, however, users have a 3D plane to arrange the documents [140]. Subsequent studies of the Data Mountain have shown to be unproductive [42]. Another approach that has been expanded to 3D environments is the hyperbolic tree [127].\rAs part of a discussion on applying 3D visualizations to web search results, Benford et al. [20] describe the VR-VIBE system (Figure 3.31). Each query is manually or automatically positioned in a 3D space. Doc- uments are positioned near the queries for which they are relevant. If a document is relevant to multiple queries, it is positioned between them. Each document’s overall relevance is shown by the size and shade of its representative icon.\r\n56 Survey of Search Systems\rFig. 3.31 VR-VIBE represents search results in a 3D environment, where results are dis- played in relative proximity to the keywords they match.\rCone Trees [142] were designed to display a hierarchy in a 3D view, using depth to increase the amount of the tree that is visible. Users are then able to rotate the tree using a smooth animation, although this was found confusing by a number of participants in an evaluation and user studies have not shown advantages [41]; supporting the concerns noted by Modjeska.\rFinally, to fully embrace 3D environments Bo ̈rner [27] used Latent Semantic Analysis and clustering to organize and display a set of doc- uments extracted from a digital library in a 3D space. A multi-modal, virtual reality interface, called the CAVE (Figure 3.32) enables users to explore the document collection; users control their movement through the CAVE with a special input device called a Wand. This require- ment for a special input device, however, makes it an unrealistic option for web environments where most users will have typical mouse and keyboard input.\rThe effect of adding a third dimension has not made any specific advances in the three levels of search compared to the 2D advances. The majority of the interfaces discussed above are, in fact, a 3D version\r\n3.3 Additional Functions 57\rFig. 3.32 Search results being visualized using the CAVE explorer (image from PDF).\rof a 2D visualization, including the hyperbolic tree and Data Moun- tain interfaces. The implementation and evaluation of 3D visualizations have been scarce due to the limited capability of most web browsers, and when tested at the three levels of search (IR, IS, and WC), 3D visualizations have often hindered, rather than supported, participants in their searching activities.\r3.3 Additional Functions\rThe previous two subsections have focused on: (1) coupling results with additional metadata and classifications, and (2) on providing alter- native or complementary representations of results. This sub-section, however, focuses on specific individual features that can work together with both classifications and alternative visualizations to enhance the usability of search interfaces.\r3.3.1 Previews\rOne of the challenges users face with classification systems is deciding which categories to select in order to find the documents they require.\r\n58 Survey of Search Systems\rThis heavily depends on the label chosen to represent a category, where designers have to find a careful balance between clarity and simplicity; where Nielsen’s heuristics [129] recommend clear layman language, it may not always be possible in some domains, at least with single terms or short phrases. If users do not recognize or understand an option, then they may not know how to proceed in their search, except through trial and error. Some users, however, simply abandon their search, at least with the search interface in question [147].\rThe Relation Browser previews the affect that a selection will have on the remaining items and facets, by temporarily reducing individual bar graph representations; selecting the item will make this change per- manent. This notion has been used in research into the mSpace browser [147] to help users choose between the terms presented in a facet. When users hover over an item in an mSpace column facet, they are presented with a multimedia preview cue, which provides an example of the docu- ments included within the category. A user evaluation showed that this preview cue significantly supported users in finding documents. Further research by Endeca [100] is investigating the ability to automatically summarize the result items and present users with a text description that is typical of the cluster.\rFor the three levels of search, presented in Section 2, previews contribute mainly toward IR and IS, by supporting users in making informed browsing and exploring decisions. This supports a number of additional search tactics [12], including the ability to weigh up a set of options, and tracing the metadata of the multimedia examples being previewed. schraefel et al. [147], however, evaluated mSpace’s multime- dia cues in a work-context scenario of purchasing music within a given budget.\r3.3.2 More Like This\rSearch engines typically provide users with a link by each result to see more results that are similar according to the server’s metrics. Although no specific research has been published, Endeca’s search interface (Fig- ure 3.33) allows users to express a similar desire to see related docu- ments, but allows them to express the dimension by which they want\r\n3.3 Additional Functions 59\rFig. 3.33 Basic Endeca search interface, a commercial enterprise search vendor, studied by Capra et al. [33].\rto see similarity. For example, they can choose to see more documents of a similar category, or size, or creation date.\rBy allowing users to follow un-anticipated paths during search, the simple notion of choosing to see more similar results to a particular result also supports a number of information tactics that are not sup- ported by standard keyword search interfaces [12], and so contributes mainly to the information-seeking level of Jarvelin and Ingwersen’s model of search. A study by Capra et al. [33] included the basic Endeca interface and, although this feature was not explicitly tested, the study involved both exploratory IS level and quick IR level tasks.\r3.3.3 Connections\rNetworks have been used for knowledge discovery tasks, displaying con- nections between related documents and between related literatures. Stepping Stones (Figure 3.34) visualizes search results for a pair of\r\n60 Survey of Search Systems\rFig. 3.34 The Stepping Stones interface visualizes the results of a pair of queries (1) as a graph (2) of topic nodes connected to the queries. Individual documents, shown in (3), cover portions of the path between the queries; image from [46].\rqueries, using a graph to show relationships between the two sets of results [46].\rSimilarly, Beale et al. [15] visualizes sequences of queries using a force-directed layout of node-link diagrams. Queries and their resulting documents are represented as nodes, with links between a query and its results. When a document is in the result set for multiple queries, it is linked to each query.\rIn some representations, single documents can appear in multiple collections or subcollections. When these different groups are arranged into regions that correspond to document attributes, often lines or col- ors are used to highlight related items. Such regions could be parts of a geographic map or, as in Figure 3.35 separates groups in a hierarchy of courts [91, 153]. Figure 3.35 shows a visualization where arcs that jump between the regions represent single items in different groups.\r\n3.3 Additional Functions 61\rFig. 3.35 This visualization arranges 287 court cases into three regions (Supreme Court, Circuit Court, and District Court). Within each region, cases are arranged chronologically. Of the 2032 citations, the selection box in the District region limits the display to just the citations from the three District court cases in 2001. This shows the greater importance of older Supreme Court opinions [153].\rOne challenge of representing links across multiple sets or dimen- sions is the limited number of methods to make connections. In Fig- ure 3.35, the arcs are in various colors. Such use of colors and arrows can be limited. For example, it may be hard to display how a single item in\r\n62 Survey of Search Systems\rthree different groups may be connected, or how an item relates to two parent levels of a hierarchy; such challenges become more important when hierarchical and faceted classification schemes are involved. In Continuum, relationships across multiple levels of hierarchy are shown by clustering and nesting results rather than keeping them in separate panes, see Figure 3.26.\rExpressing and exploring connections provides much higher-level search interaction than simple IR activities, as the users are going beyond parsing results to assess how results fit into the domain of information, and thus the other results around them.\r3.3.4 Animation\rAnimation on the Web is a debated area of research. Although lots of research has aimed at developing animation tools for the Web [59, 68], other research has investigated the limitations of technologies such as Flash [79]. Research by Robertson, Cameron, Czerwinski and Robbins, however, has shown that there are advantages of using animation to support users through transitions in an interface [141]. Further, recent research by Baudisch et al. [14] has presented a tool called Phosphor that highlights, with a fading period, any changes that occur in an interface. We can see examples of this sort of animated support on the Web through technologies such as AJAX [132]. Facebook, for example, allows users to post comments with AJAX, where the page does not refresh and both color and smooth spatial changes indicate to users that the interface is changing to accommodate their actions. Research into mSpace, which uses a lot of AJAX technology, also identified a similar emphasis on smooth transitions during discussions with participants [182].\rThe main emphasis on animation in web search visualization is on supporting users in comprehending changes that occur as a result of their actions. The Polyarchy browser [141], for example, helps users explore by animating the transition to alternative dimensions as the users browse across different attributes in the data set. Consequently, the advance provided by animation is mainly in the Information-seeking level of search, and has been mostly evaluated in kind.\r\n3.3.5 Semantic Zooming\rSemantic zooming is characterized by the progressive inclusion of addi- tional information, as opposed to simply enlarging information. In the design of NaviQue [62], users can zoom into clusters and into informa- tion structures. The semantic zooming replaces items with representa- tive icons as users zoom out, and replaces icons with the specific items when zooming in to show more detail. A history of all ad hoc collections is automatically maintained in one corner of the display. Users can drag and drop collections into a “pocket” in another corner so that they are immediately accessible, without need to pan or zoom the workspace.\rAs a form of animation, effective zooming on the Web is challenging and must be done to enhance a website and not affect its usability. In maintaining a typical search result list style, another option presented by research is a technique called WaveLens, which allows users to zoom in more on a single result, such that the information snippet expands to reveal more lines of text from the source document [131].\rThe contribution of semantic zooming supports users at IR and IS levels of search in that the technique allows users to investigate deeper into a topic as they “zoom” into it. The semantic zoom is considered a request for more detailed information on the item of focus, and is thus connected to more exploratory methods. Semantic zooming, however, can also simply reveal the answer to a quick keyword query, in which case it is related to selecting a web search result (as in the study of WaveLens) but without leaving the search engine.\r3.3.6 Alternative Inputs\rMultimedia can often be hard to describe with words, and querying over such collections requires that the documents be annotated. While some research is aimed at automatically increasing the amount of annotation of multimedia [25, 86], other approaches have examined the querying medium. Query-by-example [93, 144] is a strand of research, where users can submit audio to find related audio, images to find similar images, video to find similar video, etc. For example, Shazam29 is a\r29 http://www.shazam.com.\r3.3 Additional Functions 63\r\n64 Survey of Search Systems\rmobile phone service that users can ring while music is playing and responds by sending an SMS message with the name and artist of a song. Similarly, Retrievr30 is a service that allows users to construct a simple image with paint tools, and finds images from Flickr31 that match the approximate shapes and colors. Although some research has discussed practical applications for systems such as query-by-humming, to search for music [99], the challenge for the Web is providing a means of input for users. Where Google provides a keyword box and Retrievr provides a sketch box with paint tools, it can become difficult to allow audio input, without requiring users to make recordings and upload them. Similar problems arise for video-querying for video.\r3.4 Summary\rIn this Section, a range of diverse strategies to visualizing search results has been presented. Covering the use of enriched metadata (Section 3.1), multi-dimensional representations (Section 3.2) and visualization-enhancing techniques (e.g., animation in Section 3.3), a diverse range of approaches have been presented that try to support different contexts within the model of information-seeking described in Section 2. In the next section, we discuss the range of evaluation techniques that have been applied to these visualizations, in order to demonstrate their benefits for the different contexts of information seeking.\r30 http://labs.systemone.at/retrievr/. 31 http://www.flickr.com.\r\n4\rEvaluations of Search Systems\rThis section examines evaluation methods that have been used to assess different parts of information systems. Extensive reviews of information retrieval evaluations are provided by Hearst [72], which includes a dis- cussion of formative evaluation and usability testing, and Yang [187]. In this section, we provide a synthesized view of techniques as appropri- ate to the three-level model. We break this evaluation discussion down into the three levels included in the framework from Section 2. The different levels of information-seeking context each require multiple, different, and complementary modes of evaluation. A single study may incorporate multiple levels of evaluation. In Section 5, as we classify the search interfaces described above by visualization approach and context of information seeking support, we also include the styles of evaluation within the taxonomy.\r4.1 Information Retrieval Evaluations\rThe main concerns for much of the information retrieval community has been to assess the quality of indexing methods and the algorithms that match documents to the queries provided by users. The TREC conferences [70] have been arranged to evaluate document retrieval\r65\r\n66 Evaluations of Search Systems\rsystems for domains like the Web, spam, video, legal documents, and genomics; for collections with interesting or important characteristics such as very large collections; and for specialized tasks such as cross- language IR, filtering, finding novel documents, fact finding, and ques- tion answering.1 Important concepts for evaluations at this level include document relevance, precision/recall and related measures, and batch versus interactive evaluation.\rTo support the TREC evaluations, predefined collections of doc- uments, relevant queries and human relevance assessments, were pro- duced and used as benchmark standards across any studies. The shared platform of evaluation provided the opportunity to not only evalu- ate, but also compete for most improved retrieval times or retrieval accuracy within the community. The most commonly used measure- ments in TREC were precision and recall. Precision is concerned with returning only relevant results, whereas recall measures the number of relevant documents being returned. Typically returning more docu- ments means potentially sacrificing precision, and guaranteeing preci- sion means reducing recall. Common approaches examine precision at certain levels (precision @ N), average precision, or by using precision– recall (P–R) curves [85]. Yang [187] and Kobayashi and Takeda [96] provide extensive reviews of evaluations focusing on the techniques and evaluations used for information retrieval.\rWhen not testing the accuracy of results returned against a pre- defined corpus of documents, most IR studies, including many of the systems evaluated in the sections above, focus on simple measures of task performance such as speed for defining how well an interface sup- ports users in finding information. A simple task, of finding a specific document, or a fact within a document, is provided to users along with a basic scenario for context. If users are able to find that answer quickly with one system than another, then the novel system has pro- vided better support for search. This is a fairly accurate test for many basic search contexts. A basic fact-finding task represents many of the Web searches performed on search engines [156, 173], and so many studies are either based on this model or includes tasks of this nature.\r1 http://trec.nist.gov/tracks.html.\r\n4.2 Information-Seeking Evaluations 67 Table 4.1. A sample of measures used at the IR eval-\ruation level.\rMeasure\rPrecision, recall\rPrecision @ N\rMean average precision (MAP) Expected search length Average search length (ASL) Cumulated gain, discounted\rcumulated gain\rRelative Relevance, Ranked Half-Life Query length\rNumber of query refinements Number of unique queries\rNumber of operators in query Number of results viewed\rSearch time\rReference\r[24, 34, 40] [3]\r[3]\r[52]\r[112] [85]\r[26]\r[83, 156] [78, 83, 156] [156, 173] [156]\r[78, 83, 156] [173]\rA summary of commonly used IR-level measures, along with references to example evaluations that have used them, is shown in Table 4.1.\rIt is clear from the model presented in Section 2, however, that information retrieval tasks are part of a much larger view of search- ing behavior. While research, such as that provided by White and Drucker indicate that many web searches are simple fact-finding tasks, the natural conclusion is that there are some search sessions that are not simple lookup tasks [28, 173]. The TREC conferences have incor- porated the Interactive Track [51], the High Accuracy Retrieval of Documents (HARD) Track [3], and the Video Retrieval Evaluation (TRECVid) Track [158], which move beyond the batch-oriented evalua- tion of retrieval tasks by directly involving users conducting interactive searches in the evaluation process. However, the information needs in these tracks are still narrowly expressed in terms of documents to be retrieved, without reference to a higher-level information need. It is for this reason the methods of performing broader information-seeking evaluations is discussed in the next subsection.\r4.2 Information-Seeking Evaluations\rThe evaluations of information retrieval typically have different aims than the evaluations of information seeking [94]. Information retrieval\r\n68 Evaluations of Search Systems\revaluations have focused on system-oriented metrics, or the speed of simple, out of context, fact-finding tasks. Consequently, information retrieval studies have been criticized for a narrow conceptualization of the information need, relevance, and interaction [26]. Information- seeking research, therefore, focuses on evaluating systems for how they meet the needs of users. Important concepts at this level are the information need, the information-seeking context, information-seeking actions, tactics and strategies, and longitudinal changes, as are quan- titative and qualitative measures.\rTo understand how search interfaces support a wide range of information-seeking tactics, recent work by Wilson and schraefel [183] and Wilson et al. [180, 184] has proposed an evaluation framework to systematically assess support for a range of known search tactics and types of users, called the Search Interface Inspector.2 The framework first measures the functionality of the search system by the way it sup- ports known tactics and moves employed with information [11, 12]. The framework then uses a novel mapping to summarize the mea- sured support for the different user types [19], whose conditions vary on dimensions such as previous knowledge and intended use. This eval- uation framework was later refined and validated to show that it could accurately predict the results of user studies [178]. With the confidence provided by the validation, the framework can be used to identify weak- nesses in a system design or new function, so that it can be improved before user studies are carried out. Further, it can be used to inform the design of user studies, so that they accurately test the desire features.\rOne of the exploratory studies used to validate the framework produced by Wilson et al. was an information-seeking evaluation of faceted browsers [33]. The study used three types of tasks to evaluate the system, and performed a within participants study and a between participants study to get qualitative and quantitative results, respec- tively. The first type of task was a simple lookup task, which could be answered by using only one facet of the annotation. The second task type was a complex lookup, which involved multiple facets. The final task was exploratory, where users were asked to learn and produced a\r2 http://mspace.fm/sii/\r\n4.2 Information-Seeking Evaluations 69\rsummarized report about a certain topic. This third type of task is a good example of something that has not been included in Information Retrieval research, which, as mentioned above, has focused on match- ing documents to queries. Instead, by asking users to carry out learning tasks, we can assess the system for other types of information-seeking activities, such as comparison, synthesis, and summarization. As part of their discussion of higher-level problems encountered by users of infor- mation visualization systems, Amar and Stasko [7] discuss tasks that may be part of or require exploratory search.\rAnother contribution to information-seeking evaluations is on the discussion of time as a measurement by Capra et al. Although their tasks were timed, they suggest that time may not be a useful metric for exploratory tasks, as extended system use could mean that users have discovered increasing amounts of relevant information. In contrast to the information retrieval view that finding the answer quicker is more important, finishing an exploratory task early may indicate that a search system does not provide effective support for browsing. With this notion in mind, Kammerer et al. [90] conclude that the participants who used the MrTaggy interface (a) spent longer with their system, (b) had a higher cognitive load during search, and (c) produced better reports at the end of the tasks. In studies that aim to reduce cognitive load and reduce search time, however, there is usually only one correct answer. Conversely, during the MrTaggy experiment, the incentive was to produce a better report, and so a positive measure for the system was that it allows users to work harder.\rThe suitability of relevance in exploratory search conditions may also be in question for some information-seeking evaluations. In sys- tems that use faceted classifications, for example, each document with a particular annotation has an equal weighting and thus every docu- ment suggested as a result of selecting a particular part of the classi- fication will be equally relevant. Instead, Spink et al. [160, 161] have been designing a metric that tries to measure the progress of users in achieving their goal. Although designed for feedback to users, the rate of progress for similar tasks on different systems could be used to assess their support. Further, controlling for the amount of progress made by users in exploratory tasks would allow evaluators to once\r\n70 Evaluations of Search Systems\ragain consider reduced time and cognitive load as positive measures. For example, evaluators could measure the time spent, and cognitive load as they make a pre-determined amount of progress.\rKoshman [98] evaluated the VIBE (Visual Information Browsing Environment) prototype system, which graphically represents search results as geometric icons within one screen display. As part of under- standing approaches to information seeking, the researchers sought to differentiate expert and novice performance through the use of a quasi-experimental within-participants design (see Borlund [26] or Shneiderman and Plaisant [155] for discussions of information-seeking evaluation).\rHaving discussed search tasks and measurements that may be unique to or important for information seeking, carefully controlled user studies can still be performed to evaluate systems in terms of information seeking. Ka ̈ki [89] provides a good example of a study that employs a within-subjects design, balanced task sets, time limitations, pre-formulated queries, cached result pages and limiting access to result documents.\rEye-tracking techniques have been used to study information- seeking behaviors within web search interfaces and library catalogs [45, 66, 104, 111]. These studies examined specific elements that searchers look at, how often they looked at them, for how long, and in what order. They provide insight into the cognitive activities that searchers undertake and tactics used when examining search results. For example, Kules et al. [104] studied gaze behavior in a faceted library catalog and determined that for exploratory search tasks, participants spent about half as much time looking at facets as they spent looking at the individual search results. The study also suggested that partici- pants used the facets differently between the beginning and later stages of their searches. They conclude that the facets played an important role in the search process.\rFinally, the study approach that investigates user interaction with software over a long period of time, such as longitudinal studies or studies that are repeated periodically with the same participants, pro- vides a unique type of insight into realistic human behavior. One of the main arguments against the short task-oriented studies is the lack of\r\n4.3 Work-Context Evaluations 71\rTable 4.2. A sample of measures used at the IS evaluation level. Some measures may be similar to IR level measures, but are used here to evaluate the process of IS tasks.\rMeasure\rNumber of moves/actions to support a search tactic\rSubjective measures (e.g., confidence, usefulness, usability, satisfaction)\rTime on task\rTime to select document\rAccuracy (correctness) and/or errors in task\rresults\rNumber of results collected by user Number of search terms\rNumber of queries\rNumber of results viewed\rTime to learn\rSystem feature retention\rRank of selected document in results Usage counts of selected interface features Gaze-related measures, e.g., number of\rfixations, location of fixations, rank of results fixated on, fixation duration; scanpaths, scanpath length\rNumber of search result pages viewed\rReference [177, 184]\r[33, 98, 104, 106, 161]\r[33, 90, 98] [66, 89]\r[33, 98, 111]\r[89, 90, 111] [89, 159, 170] [106, 111, 160] [160, 66, 111] [98]\r[98]\r[89, 111, 106, 66] [89, 170]\r[45, 111, 104]\r[111]\rrealism. Further, the results of such user studies are often based on the participants’ first or early responses to new designs, compared to their familiarity with existing software. By studying interaction over time, we can begin to evaluate changes in search tactics and subjective views as users adopt and adapt to new interfaces [102, 170, 182]. A summary of commonly used IS-level measures, along with references to example evaluations that have used them, is shown in Table 4.2.\r4.3 Work-Context Evaluations\rEvaluating search systems and the work task level requires measuring the success of work-context style problems. For example, Allen [4] inves- tigated the interaction of spatial abilities with 2-D data representations. The work-context task given to participants was to read an article and then use the system to find a few good articles. At this level, many different information-seeking activities can be used, but ultimately the\r\n72 Evaluations of Search Systems\rsystem is assessed on its support for achieving the overall goal. As with evaluations at the information-seeking level, important concepts at this level include the information need, the information-seeking con- text, information-seeking actions, tactics, and strategies. At this level there is a stronger emphasis on domain-specific concepts. The quality of the work product also may be evaluated.\rOne of the challenges of evaluation, especially at the information seeking and work-context level, is that human participants interpret the tasks based on their own experiences and knowledge. There is a tension between the need to make results reliable and replicable and the need to make the task realistic for the participants. Borlund [26] advocated addressing this by incorporating participant-provided infor- mation needs into the experimental session along with a researcher- provided need. If the results for both tasks are consistent, the researcher can conclude that the researcher-provided task is realistic and thus reap the benefit of a realistic but tightly controlled task. Other research has used the previous actions of users to inform the realism of search tasks in system evaluations [56, 58]. Kules and Capra [103] propose a procedure for creating and validating tasks that exhibit exploratory characteristics, such as indicating uncertainty of the information need, the need for discovery, or being an unfamiliar domain for the searcher. The goal is to develop work tasks that are appropriate for the domain and system being evaluated and which are also comparable within and across studies.\rAside from the individuality of work task understanding, work- context tasks vary dramatically depending on the domain. Ja ̈rvelin and Ingwersen [84] argue that the main challenge ahead for evaluat- ing systems at the work-context level is that research needs to explic- itly integrate context in terms of the high-level work task, the specific information-seeking task and the systems context. In particular, they note that the Web “is not a single coherent unit but appears quite different for different actors, tasks, and domains.” Some research has addressed domain-specific work-context evaluations. One comparative study attempted to rigorously evaluate work tasks by defining domain- specific measures of work product quality (assessing motivation, com- pleteness, correctness, coherence, redundancy, and argument structure)\r\n4.3 Work-Context Evaluations 73\rin addition to measuring efficiency, effectiveness, and precision of the search [88]. Subjects used three information retrieval systems to develop lesson materials about gorillas using a biological database. The goal was highly structured to create a lesson using the provided lesson template that prescribed topics to cover. Results showed significant system dif- ferences in efficiency and effectiveness but results for quality measures were mixed. They were not significant overall; however, significant dif- ferences were noted between individual sections of the lesson. Kules and Shneiderman [106] similarly evaluated the quality of the work product (ideas for newspaper articles) but found no significant differences. These two studies highlight the challenges of evaluating the contribution of a system to a high-level work task. Evaluators face the dilemma of trying to assess a system under conditions that are more realistic than at the IR or even IS level, while still effectively understanding the contribu- tion that the system makes when it is only one of many factors. The use of longitudinal studies may help to overcome this challenge.\rQu and Furnas [137] used two variations of a sensemaking work task to motivate information seeking and topic organization tasks to study how people structure developing knowledge during an exploratory search. Study participants were given 50 minutes to collect and orga- nize information about an unfamiliar topic and produce an outline of an oral presentation. The researchers examined sequences of actions that participants took, including issuing queries, bookmarking pages, and creating folders. They interpreted different sequences as indications\rTable 4.3. A sample of measures used in stud- ies focused work-context evaluation level. Some measures may also be used in the IS but provide insight into the process of achieving work-con- text tasks.\rMeasure\rReference\rrecords printed [4] items viewed or printed [4] items selected [88] relevant items selected [88]\r[88, 106]\rNumber of\rNumber of\rNumber of\rNumber of\rQuality of results\rRank of relevant documents [56] Participant assessed relevance [56] Sequences of actions [137]\r\n74 Evaluations of Search Systems\rof different aspects of the sensemaking task. A summary of commonly used WC-level measures, along with references to example evaluations that have used them, is shown in Table 4.3.\r4.4 Summary\rEvaluations are most mature and rigorous at the IR level. At the IS level, evaluations are developing, with several good examples of stud- ies that balance rigor and realism. The work level is where the biggest challenges remain. It is also where improvements in evaluation can con- tribute the most. Certainly, evaluations at this level need to reflect the nature of the domain and the tasks and information needs characteristic of that domain. Methodologies that enhance rigor and comparability without sacrificing validity are necessary. Longitudinal methods are likely to be useful. More generally, studies are likely to benefit from an emphasis on fewer, but deeper, tasks, reflecting the more complex nature of work tasks.\r\n5\rTaxonomy of Search Visualizations\rSo far in the monograph, we have discussed (1) a model of search that captures multiple levels of context, (2) many novel advances in search result visualization, and (3) the way in which such advances have been evaluated. Below we present and discuss a taxonomy of these search advances, according to their support for search, amount of evaluation, and prevalence on the Web.\r5.1 The Taxonomy\rWe present a taxonomy of the search visualization advances dis- cussed in this monograph (Table 5.1). The purpose of the taxonomy is to capture (1) how these advances are designed to support search, (2) to what extent they have been evaluated, and (3) how preva- lent they are on the Web. The next subsection discusses the value of this information to academics and designers of future web search interfaces.\rThe first facet, shown in the first set of three columns, is the level of search context, as according to the model used throughout the mono- graph, that is being supported by the interface feature. Although each\r75\r\n76 Taxonomy of Search Visualizations\rTable 5.1. Interface advances discussed within the monograph above, categorized by three facets: Level of Search Support, Depth of Evaluation, and Prevalence on the Web.\rInformation Interface Advances Retrieval\rInformation Seeking\rWork Context\rInitial Testing\rMultiple User Studies\rDiverse Forms of Study\rDemonstrator Only\rInstances Available Used\r3.1.1 Hierarchical Classifications\r1. Standard Web Directories\rO O\rO\rO\r2. Cha-Cha 3. WebTOC\rO\rO O\r3.1.2 Faceted Classifications\rRB++\r8. SERVICE 9. Dyna-Cat 10. FacetPatch\rO O O\rO O\rO\rO O O\r(Contextual facets)\r3.1.3 Automatic Clustering\r11. Clusty\rO O\rO\rO O\r3.1.4 Social Classifications\r12. Social Rankings 13. Tag Clouds\r14. MrTaggy\rO O\rO O\rO\r3.2.1 Result Lists\r15. Page Thumbnails\r16. Keywords in Context 17. Larger Result\rO O O\rO O\rO\rSnippets\rLevel of Search Support\rDepth of Evaluation\rPrevalence Online Many\rO\rO\rO\r5.mSpace O OO\rO O\r4. Flamenco\rO\r6. Exhibit\r7. Relation Browser\rO O\rO\rO\rO\rO\rO\rO\rWidely\r(Continued)\rO\r\n5.1 The Taxonomy 77\rMultiple Interface Advances Retrieval Seeking Context Testing Studies\rDiverse Forms of Study\rDemonstrator Only\rInstances Available Used\r3.2.2 2D Result Representations\r18. SOMS O\r19. Treemaps O\r20. Hyperbolic Trees O O 21. Scatterplot/Matrix O\r22. Euler Circles O O 23. Geographical Plots O O 24. Timelines O O 25. Per-Query-Term O O\rO O\rO O\rRelevance\r26. User-Organized O O\rO O\rCanvases\r3.2.3 3D Result Representations\r27. User-Organized 3D O O Spaces\rO\r28. 3D Hyperbolic Trees O O\r29. Cone Trees O O 30. 3D VR Exploration O O\rO O O\r3.3 Additional Functions\r31. Multimedia Previews O O 32. Content O O\rO O\rSummarization\r33. Similar Results O O\r34. Results Connections O O 35. Animated O O\rO\rTransitions\r36. Semantic Zooming O O\r37. Query-by-Example O O\rO\rTable 5.1.\r(Continued)\rDepth of Evaluation\rLevel of Search Support\rInformation Information Work Initial User\rPrevalence Online Many\rO\rO\rO\rO O\rO\rO\rO\rO\rWidely\r\n78 Taxonomy of Search Visualizations\rinterface technique may support, to some extent, each of the three levels, they have been allocated to the level at which they are pri- marily designed to support. In fact, a lengthier alternative analysis of these interface advances could discuss the way in which the advances all contribute to each level of search context.\rThe second facet, shown in the second set of three columns, rep- resents the amount of study that the techniques have received. While these could be categorized into groups such as formative studies, empir- ical user studies, longitudinal log analyses, etc. we are more interested, here, to learn the diversity of evidence that has been produced. They could have also been categorized by whether they have been studied in the same dimensions as they have been designed to support. Many studies, however, include information retrieval and information-seeking tasks, and so they may have received relatively little study, but in both contexts. Some techniques have received little published study and eval- uation, like allowing users to view similar results to any one result returned [#33 in taxonomy], and so our understanding of their benefits is through experience and intuition. Other techniques, such as treemaps and SOMs [#18, #19], have received much evaluation and optimiza- tion since they were first proposed, including initial testing, carefully constructed lab studies, and analyses of working online deployments.\rThe third facet, contained in the final set of three columns, cap- tures how prevalent visualizations have become on the Web. The items familiar in most search engines, such as “similar results” and “keyword- in-context result snippets”, are dominant, regardless of how well they have been studied, or to what level of context they support searchers. Other techniques, however, have been well studied but are not yet in widespread use, such as faceted browsing on general Web search engines [#8, #9]. It might be noted that the most common model of faceted browsing, represented by Flamenco [#4], is more prevalent on the Web than some of the alternatives. Google product search,1 for example, provides this kind of typical faceted interaction to filter the results by price ranges and even specific vendors.\r1 http://www.google.com/products.\r\n5.2 Using the Taxonomy 79\rThese three facets, in combination, provide insights into the advances that have been made in web search visualizations. The diagram shows, for example, advances that are highly prevalent on the Web, but have received little published evaluation, and only support a low information retrieval context of search, such as web directories [#1]. Similarly, and perhaps more important for some readers, we can see heavily studied advances that are not yet prevalent on the Web, such as the faceted model of search provided by mSpace [#5].\r5.2 Using the Taxonomy\rThere are two main audiences for the taxonomy above: (1) search inter- face designers and (2) academics working on future advances in this area. Similarly, the content of the taxonomy and the gaps in the dia- gram provide useful information to designers and academics. These uses are discussed below.\rOne challenge in building web search interfaces is in choosing the best combination of tools and features that will best support the tar- get audience, if known. Clearly, there are many advances that can be used, but using lots, or even all of them, would provide a cluttered and perhaps unusable interface. For designers working with such a chal- lenge, the taxonomy acts as a guide to identifying potential appropriate options. Further, the gaps in the diagram highlight areas where new and novel ideas might make a distinct interface and provide a business edge.\rFor academics, the taxonomy provides two types of information. First, much prior art is discussed in this article and included in the diagram, which can be used to help identify related work. Second, and more importantly, however, the diagram quite clearly marks under- researched ideas and research gaps. One conclusion that can be drawn from the diagram is that there has been a lot of work on information- seeking level advances, but comparatively little work that has focused on work-contexts. Section 4.3 discusses some studies that have focused on work context scenarios, and some recent work has focused on tasks such as booking holidays, writing papers, and even collaborating with other people [125]. For the many existing advances that are captured by\r\n80 Taxonomy of Search Visualizations\rthe taxonomy, however, seeing how different techniques relate to each other across the captured dimensions raises some un-answered research questions. Academics may want to compare well studied, but less pop- ular advances, with those that are popular despite having received little published evaluation.\r\n6\rConclusions\rThis monograph presented four steps in discussing the advances in visualizing web search results. First, we presented previous and related work, which was followed by a model of search that has been used throughout the document. This model of search includes multiple levels of context from basic Information Retrieval needs, to more exploratory information-seeking behavior, and finally the context of the tasks being completed by users, known as the work context. Second, we offered a wide range of innovations in web search interfaces, while discussing how each makes contributions to the three levels of search context. Third, Section 4 discussed the techniques that have been used to evaluate these advances at the three different levels of search context. We discussed the increasing difficulty that evaluators experience while studying the contributions of new designs at higher-level work contexts, as regular simple measures such as time and accuracy of specific low-level tasks do not necessarily apply. Finally, we presented a taxonomy in Section 5 that captures the interface advances. The taxonomy captures (1) which level of search context the advance primarily support, (2) how much study the advances have received, and (3) how prevalent the advances are on the Web.\r81\r\n82 Conclusions\rAs well as supporting readers in finding much of the prior art that exists in web search result visualization, this monograph and taxon- omy helps the designers of future search systems to make informed choices about which advances may be appropriate for their system and audience. Further, the taxonomy helps identify under-researched ideas and research gaps in web search result visualization. Notably, there have been far fewer advances that consider the higher-level work con- texts of searchers. Consequently we advocate that, in order to better understand novel ideas, future evaluations focus on the work contexts of users, and in supporting them to achieve their higher-level goals.\rWe began this monograph by celebrating the success of keyword search, while clarifying when alternative modes of search are needed. First, as users’ demands continue to grow and their needs evolve, oppor- tunities are emerging for exploratory search strategies. Often users want to get answers to their questions, not just web pages. Second, their questions are increasingly complex and may takes hours or weeks to resolve. A third change is the move toward new Semantic Web tech- nologies and related strategies, which offer improved possibilities for machine-assisted resolution for complex queries, especially with key- words which have multiple meanings. We have shown that there are emerging innovative user interfaces and visual presentations that may help users achieve their goals more rapidly and with greater confidence in the validity of the results. Through empirical studies and log anal- ysis, researchers are coming to better understand the ways in which people search, the tasks they have, and how they collaborate with their colleagues. This survey monograph provides a resource to the design- ers and researchers who are developing search systems so that they can more make more informed and confident design decisions as they create novel and effective interfaces.\r\nAcknowledgments\rThe authors thank Marti Hearst, those who provided images, and sev- eral others for providing feedback and comments.\r83\r\nReferences\r[1] F. J. Aguilar, General Managers in Action. New York, NY: Oxford University Press, 1988.\r[2] H. Alani and C. Brewster, “Ontology ranking based on the analysis of concept structures,” in Proceedings of the 3rd International Conference on Knowledge Capture, pp. 51–58, New York, NY, USA: ACM Press, 2005.\r[3] J. Allan, “Hard track overview in trec 2003 high accuracy retrieval from doc- uments,” in Proceedings of the Text Retrieval Conference, pp. 24–37, 2003.\r[4] B. Allen, “Information space representation in interactive systems: Relation- ship to spatial abilities,” in Proceedings of the Third ACM Conference on Digital Libraries, pp. 1–10, Pittsburgh, Pennsylvania, United States: ACM Press, 1998.\r[5] R. Allen, “Two digital library interfaces that exploit hierarchical structure,” in Proceedings of Electronic Publishing and the Information Superhighway, pp. 134–141, Boston, MA, USA, 1995.\r[6] J. Allsop, Microformats: Empowering Your Markup for Web 2.0. friends of ED, 2007.\r[7] R. Amar and J. Stasko, “A knowledge task-based framework for design and evaluation of information visualizations,” in Proceedings of the IEEE Sympo- sium on Information Visualization, pp. 143–150, Austin, Texas, USA: IEEE Computer Society, 2004.\r[8] B. Amento, W. Hill, L. Terveen, D. Hix, and P. Ju, “An empirical evaluation of user interfaces for topic management of web sites,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 552–559, Pittsburgh, Pennsylvania, United States: ACM Press, 1999.\r84\r\nReferences 85\r[9] P.Andr ́e,M.L.Wilson,A.Russell,D.A.Smith,A.Owens,andm.c.schraefel, “Continuum: Designing timelines for hierarchies, relationships and scale,” in Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology, pp. 101–110, Newport, Rhode Island, USA: ACM Press, 2007.\r[10] P. Au, M. Carey, S. Sewraz, Y. Guo, and S. Ru ̈ger, “New paradigms in infor- mation visualization,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and Development in Information Retrieval, pp. 307–309, Athens, Greece: ACM Press, 2000.\r[11] M. J. Bates, “Idea tactics,” Journal of the American Society for Information Science, vol. 30, no. 5, pp. 280–289, 1979.\r[12] M. J. Bates, “Information search tactics,” Journal of the American Society for Information Science, vol. 30, no. 4, pp. 205–214, 1979.\r[13] M. J. Bates, “The design of browsing and berrypicking techniques for the online search interface,” Online Review, vol. 13, no. 5, pp. 407–424, 1989.\r[14] P. Baudisch, D. Tan, M. Collomb, D. Robbins, K. Hinckley, M. Agrawala, S. Zhao, and G. Ramos, “Phosphor: Explaining transitions in the user interface using afterglow effects,” in Proceedings of the 19th annual ACM symposium on User Interface Software and Technology, pp. 169–178, Montreux, Switzerland: ACM Press, 2006.\r[15] R. Beale, R. J. Mcnab, and I. H. Witten, “Visualising sequences of queries: A new tool for information retrieval,” in Proceedings of the IEEE Conference on Information Visualisation, pp. 57–63, Phoenix, AZ, USA: IEEE Computer Society, 1997.\r[16] A.Becks,C.Seeling,andR.Minkenberg,“Benefitsofdocumentmapsfortext access in knowledge management: A comparative study,” in Proceedings of the ACM Symposium on Applied Computing, pp. 621–626, Madrid, Spain: ACM Press, 2002.\r[17] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder, “Hourly analysis of a very large topically categorized web query log,” in Pro- ceedings of the 27th annual international ACM SIGIR conference on Research and Development in Information Retrieval, pp. 321–328, Sheffield, UK: ACM Press, 2004.\r[18] N.J.Belkin,C.Cool,A.Stein,andU.Thiel,“Cases,scripts,andinformation- seeking strategies: On the design of interactive information retrieval systems,” Expert Systems with Applications, vol. 9, no. 3, pp. 379–395, 1995.\r[19] N. J. Belkin, P. G. Marchetti, and C. Cool, “Braque: Design of an interface to support user interaction in information retrieval,” Information Processing & Management, vol. 29, no. 3, pp. 325–344, 1993.\r[20] S. Benford, I. Taylor, D. Brailsford, B. Koleva, M. Craven, M. Fraser, G. Rey- nard, and C. Greenhalgh, “Three dimensional visualization of the world wide web,” ACM Computing Surveys, vol. 31, no. 4, p. 25, 1999.\r[21] T.Berners-Lee,Y.Chen,L.Chilton,D.Connolly,R.Dhanaraj,J.Hollenbach, A. Lerer, and D. Sheets, “Tabulator: Exploring and analyzing linked data on the semantic web,” in Proceedings of the 3rd International Semantic Web User Interaction Workshop, 2006.\r\n86 References\r[22] S. K. Bhavnani, “Important cognitive components of domain-specific search knowledge,” in NIST Special Publication 500-250: The Tenth Text Retrieval Conference, (E. M. Vorhees and D. K. Harman, eds.), Washington, DC: NIST, 2001.\r[23] S. K. Bhavnani and M. J. Bates, “Separating the knowledge layers: Cognitive analysis of search knowledge through hierarchical goal decompositions,” in Proceedings of the American Society for Information Science and Technology Annual Meeting, pp. 204–213, Medford, NJ: Information Today, 2002.\r[24] D. C. Blair and M. E. Maron, “An evaluation of retrieval effectiveness for a full-text document retrieval system,” Communications of the ACM, vol. 28, no. 3, pp. 289–299, 1985.\r[25] S. Bloehdorn, K. Petridis, C. Saathoff, N. Simou, V. Tzouvaras, Y. Avrithis, S. Handschuh, I. Kompatsiaris, S. Staab, and M. G. Strintzis, “Semantic anno- tation of images and videos for multimedia analysis,” in Proceedings of the 2nd European Semantic Web Conference, pp. 592–607, Heraklion, Crete, Greece: Springer, 2005.\r[26] P. Borlund, “The IIR evaluation model: A framework for evaluation of inter- active information retrieval systems,” Information Research, vol. 8, no. 3, p. Paper 152, 2003.\r[27] K. B ̈orner, “Visible threads: A smart vr interface to digital libraries,” in Pro- ceedings of IST/SPIE’s 12th Annual International Symposium on Visual Data Exploration and Analysis, pp. 228–237, San Jose, CA, USA: SPIE, 2000.\r[28] A. Broder, “A taxonomy of web search,” ACM SIGIR Forum, vol. 36, no. 2, pp. 3–10, 2002.\r[29] K. Bystr ̈om and P. Hansen, “Work tasks as units for analysis in informa- tion seeking and retrieval studies,” in Emerging Frameworks and Methods, (H. Bruce, R. Fidel, P. Ingwersen, and P. Vakkari, eds.), Greenwood Village, CO: Libraries Unlimited, 2002.\r[30] K. Bystr ̈om and P. Hansen, “Conceptual framework for tasks in information studies: Book reviews,” Journal American Society Information Science and Technology, vol. 56, no. 10, pp. 1050–1061, 2005.\r[31] G. Cai, “Geovibe: A visual interface for geographic digital libraries,” in Proceedings of the First Visual Interfaces to Digital Libraries Workshop, (K. Borner and C. Chen, eds.), pp. 171–187, Roanoke, VA, USA: Springer- Verlag, 2002.\r[32] R. Capra and G. Marchionini, “The Relation Browser tool for faceted exploratory search,” in Proceedings of the 2008 Conference on Digital Libraries (JCDL ’08), Pittsburgh, Pennsylvania, June 2008.\r[33] R. Capra, G. Marchionini, J. S. Oh, F. Stutzman, and Y. Zhang, “Effects of structure and interaction style on distinct search tasks,” in Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 442–451, Vancouver, British Columbia, Canada: ACM Press, 2007.\r[34] S. Chakrabarti, B. Dom, D. Gibson, S. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins, “Experiments in topic distillation,” in ACM SIGIR work- shop on Hypertext Information Retrieval on the Web, pp. 13–21, Melbourne, Australia, 1998.\r\nReferences 87\r[35] C. Chen, “Citespace II: Detecting and visualizing emerging trends and tran- sient patterns in scientific literature,” Journal of the American Society for Information Science and Technology, vol. 57, no. 3, pp. 359–377, 2006.\r[36] H. Chen and S. Dumais, “Bringing order to the web: Automatically categoriz- ing search results,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 145–152, The Hague, The Netherlands: ACM Press, 2000.\r[37] H. Chen, A. L. Houston, R. R. Sewell, and B. R. Schatz, “Internet brows- ing and searching: User evaluations of category map and concept space techniques,” Journal of the American Society for Information Science, vol. 49, no. 7, pp. 582–608, 1998.\r[38] M. Chen, M. Hearst, J. Hong, and J. Lin, “Cha-cha: A system for organizing intranet search results,” in Proceedings of the 2nd USENIX Symposium on Internet Technologies and Systems, Boulder, CO, 1999.\r[39] C. W. Choo, B. Detlor, and D. Turnbull, Web Work: Information Seeking and Knowledge Work on the World Wide Web. Dordrecht, The Netherlands: Kluwer Academic Publishers, 2000.\r[40] C.W.Cleverdon,J.Mills,andM.Keen,“Factorsdeterminingtheperformance of indexing systems,” in ASLIB Cranfield Project, Cranfield, 1966.\r[41] A. Cockburn and B. Mckenzie, “An evaluation of cone trees,” in Proceedings of the British Computer Society Conference on Human-Computer Interaction, Sheffield, UK: Springer-Verlag, 2000.\r[42] A. Cockburn and B. Mckenzie, “Evaluating the effectiveness of spatial mem- ory in 2D and 3D physical and virtual environments,” in Proceedings of the SIGCHI conference on Human factors in computing systems: Changing our world, changing ourselves, pp. 203–210, Minneapolis, Minnisota, USA, 2002.\r[43] W. S. Cooper, “On selecting a measure of retrieval effectiveness,” Journal of the American Society for Information Science, vol. 24, no. 2, pp. 87–100, 1973.\r[44] J. Cugini, C. Piatko, and S. Laskowski, “Interactive 3D visualization for doc- ument retrieval,” in Proceedings of the Workshop on New Paradigms in Infor- mation Visualization and Manipulation, ACM Conference on Information and\rKnowledge Management, Rockville, Maryland, USA, 1996.\r[45] E. Cutrell and Z. Guan, “What are you looking for?: An eye-tracking study of information usage in web search,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 407–416, San Jose, California,\rUSA: ACM press, 2007.\r[46] F. Das-Neves, E. A. Fox, and X. Yu, “Connecting topics in document col-\rlections with stepping stones and pathways,” in Proceedings of the 14th ACM International Conference on Information and Knowledge Management, pp. 91–98, Bremen, Germany: ACM Press, 2005.\r[47] L. Ding, A. J. T. Finin, R. Pan, R. Cost, Y. Peng, P. Reddivari, V. Doshi, and J. Sachs, “Swoogle: A search and metadata engine for the semantic web,” in Proceedings of the thirteenth ACM international conference on Information and knowledge management, pp. 652–659, Washington DC, USA, ACM New York, NY, USA, 2004.\r\n88 References\r[48] K. Doan, C. Plaisant, and B. Shneiderman, “Query previews in networked information systems,” in Proceedings of the Third Forum on Research and Technology Advances in Digital Libraries, pp. 120–129, Washington, DC, USA: IEEE Computer Society, 1996.\r[49] O. Drori and N. Alon, “Using documents classification for displaying search results list,” Journal of Information Science, vol. 29, no. 2, pp. 97–106, 2003.\r[50] S. Dumais, E. Cutrell, and H. Chen, “Optimizing search by showing results in context,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Seattle, WA, New York: ACM Press, 2001.\r[51] S.T.DumaisandN.J.Belkin,“TheTRECinteractivetracks:Puttingtheuser into search,” in TREC: Experiment and Evaluation in Information Retrieval, (E. Voorhees and D. Harman, eds.), MIT Press, 2005.\r[52] M. D. Dunlop, “Time, relevance and interaction modelling for information retrieval,” SIGIR Forum, vol. 31, no. SI, pp. 206–213, 1997.\r[53] C. Eaton and H. Zhao, Visualizing Web Search Results. 2001.\r[54] M. Efron, J. Elsas, G. Marchionini, and J. Zhang, “Machine learning for infor- mation architecture in a large governmental website,” in Proceedings of the 4th ACM/IEEE-CS Joint Conference on Digital libraries, pp. 151–159, Tuscon,\rAZ, USA: ACM Press, 2004.\r[55] D. E. Egan, J. R. Remde, L. M. Gomez, T. K. Landauer, J. Eberhardt, and\rC. C. Lochbaum, “Formative design evaluation of superbook,” ACM Trans-\ractions on Information Systems, vol. 7, no. 1, pp. 30–57, 1989.\r[56] S. Elbassuoni, J. Luxenburger, and G. Weikum, “Adaptive personalization of web search,” in Workshop on Web Information Seeking and Interaction at\rSIGIR, pp. 23–30, Amsterdam, Netherlands, 2007.\r[57] D.Ellis,“Abehavioralmodelforinformationretrievalsystemdesign,”Journal\rof Information Science, vol. 15, no. 4–5, pp. 237–247, 1989.\r[58] D. Elsweiler and I. Ruthven, “Towards task-based personal information man- agement evaluations,” in Proceedings of the 30th annual international ACM SIGIR conference on Research and Development in Information Retrieval,\rpp. 23–30, Amsterdam, The Netherlands, 2007.\r[59] P. Faraday and A. Sutcliffe, “Authoring animated web pages using “contact\rpoints”,” in Proceedings of the SIGCHI conference on Human factors in com- puting systems: The CHI is the limit, pp. 458–465, Pittsburgh, PA, USA: ACM Press, 1999.\r[60] Y. Feng and K. Borner, “Using semantic treemaps to categorize and visualize bookmark files,” in Proceedings of SPIE — Volume 4665, Visualization and Data Analysis 2002, (R. F. Erbacher, P. C. Chen, M. Groehn, J. C. Roberts, and C. M. Wittenbrink, eds.), Bellingham, WA, USA: SPIE–The International Society for Optical Engineering, 2002.\r[61] V. Florance and G. Marchionini, “Information processing in the context of medical care,” in Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 158– 163, Seattle, WA, USA: ACM Press, 1995.\r\nReferences 89\r[62] G. W. Furnas and S. J. Rauch, “Considerations for information environments and the navique workspace,” in Proceedings of the Third ACM Conference on Digital libraries, pp. 79–88, Pittsburgh, PA, USA: ACM Press, 1998.\r[63] M. Gao, C. Liu, and F. Chen, “An ontology search engine based on seman- tic analysis,” in ICITA’05. Third International Conference on Information Technology and Applications, 2005, pp. 256–259, IEEE, 2005.\r[64] M. Ginsburg, “Visualizing digital libraries with open standards,” Commu- nications of the Association for Information Systems, vol. 13, pp. 336–356, 2004.\r[65] R.Godwin-Jones,“Emergingtechnologiestagcloudsintheblogosphere:Elec- tronic literacy and social networking,” Language Learning and Technology, vol. 10, no. 2, pp. 8–15, 2006.\r[66] L. A. Granka, T. Joachims, and G. Gay, “Eye-tracking analysis of user behavior in WWW search,” in Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, Sheffield, United Kingdom: ACM Press, 2004.\r[67] A.GulliandA.Signorini,“Theindexablewebismorethan11.5billionpages,” in Proceedings of the 14th International Conference on the World Wide Web, pp. 902–903, Chiba, Japan: ACM Press, 2005.\r[68] J. Haajanen, M. Pesonius, E. Sutinen, J. Tarhio, T. Terasvirta, and P. Vanni- nen, “Animation of user algorithms on the web,” in Proceedings of IEEE Sym- posium on Visual Languages, pp. 356–363, Isle of Capri, Italy: IEEE Computer Society, 1997.\r[69] W. Hall and K. O’Hara, “Semantic web,” in Encyclopedia of Complexity and Systems Science, (R. A. Meyers, ed.), Springer, 2009.\r[70] D. K. Harman, “The TREC conferences,” in Readings on information retrieval, San Francisco, CA, USA: Morgan Kaufmann Publishers Inc, 1997.\r[71] M.Hearst,“Theuseofcategoriesandclustersfororganizingretrievalresults,” in Natural Language Information Retrieval, (T. Strzalkowski, ed.), Boston: Kluwer Academic Publishers, 1999.\r[72] M. Hearst, Search User Interfaces. Cambridge University Press, 2009.\r[73] M.Hearst,A.Elliot,J.English,R.Sinha,K.Swearingen,andP.Yee,“Finding the flow in web site search,” Communications of the ACM, vol. 45, no. 9,\rpp. 42–49, 2002.\r[74] M. Hearst and J. Pedersen, “Reexamining the cluster hypothesis: Scatter/\rgather on retrieval results,” in Proceedings of the 19th Annual Interna- tional ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich, Switzerland, New York: ACM Press, 1996.\r[75] M. A. Hearst, “Next generation web search: Setting our sites,” IEEE Data Engineering Bulletin: Special Issue on Next Generation Web Search, vol. 23, no. 3, pp. 38–48, 2000.\r[76] M. Hildebrand, J. V. Ossenbruggen, and L. Hardman, “/facet: A browser for heterogeneous semantic web repositories,” in Proceedings of the 5th Interna- tional Conference on the Semantic Web (ISWC’06), pp. 272–285, Athens, GA, USA, 2006.\r\n90 References\r[77] O. Hoeber and X. D. Yang, “The visual exploration of web search results using hotmap,” in Proceedings of the International Conference on Information Visualization, pp. 272–285, London, UK: IEEE Computer Society, 2006.\r[78] C.HolscherandG.Strube,“Websearchbehaviorofinternetexpertsandnew- bies,” in Proceedings of the 9th International WWW Conference, pp. 157–165, 2000.\r[79] A. Holzinger and M. Ebner, “Interaction and usability of simulations and ani- mations: A case study of the flash technology,” in Proceedings of IFIP TC13 International Conference on Human Computer Interaction, pp. 777–780, Zu ̈rich, Switzerland: IOS Press, 2003.\r[80] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, “Information retrieval in folksonomies: Search and ranking,” The Semantic Web: Research and Appli- cations, vol. 4011, pp. 411–426, 2006.\r[81] D. F. Huynh, R. Miller, and D. Karger, “Exhibit: Lightweight structured data publishing,” in Proceedings of the World Wide Web Conference, pp. 737–746, Banff, Alberta, Canada: ACM Press, 2007.\r[82] B. Jansen and A. Spink, “An analysis of web information seeking and use: Documents retrieved versus documents viewed,” in Proceedings of the 4th international conference on Internet computing, pp. 65–69, Las Vagas, NV, USA, 2003.\r[83] B. Jansen, A. Spink, J. Bateman, and T. Saracevic, “Real life information retrieval: A study of user queries on the web,” ACM SIGIR Forum, vol. 32, no. 1, pp. 5–17, 1998.\r[84] K. J ̈arvelin and P. Ingwersen, “Information seeking research needs extension towards tasks and technology,” Information Research, vol. 10, no. 1, p. paper 212, 2004.\r[85] K. Jarvelin and J. Kekalainen, “IR evaluation methods for retrieving highly relevant documents,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 41–48, Athens, Greece, ACM, 2000.\r[86] J. Jeon, V. Lavrenko, and R. Manmatha, “Automatic image annotation and retrieval using cross-media relevance models,” in Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pp. 119–126, Toronto, Canada: ACM Press, 2003.\r[87] T.Joachims,TextCategorizationwithSupportVectorMachines:Learningwith Many Relevant Features. Springer, 1997.\r[88] S. Kabel, R. De Hoog, B. J. Wielinga, and A. Anjewierden, “The added value of task and ontology-based markup for information retrieval,” Jour- nal of the American Society for Information Science and Technology, vol. 55, no. 4, pp. 348–362, 2004.\r[89] M. K ̈aki, “Findex: Search result categories help users when document ranking fails,” in Proceeding of the SIGCHI Conference on Human Factors in Com- puting Systems, pp. 131–140, Portland, OR, USA: ACM Press, 2005.\r[90] Y. Kammerer, R. Narin, P. Pirolli, and E. Chi, “Signpost from the masses: Learning effecs in an exploratory social tag search browser,” in Proceedings\r\nReferences 91\rof the 27th international conference on Human factors in computing systems,\rpp. 625–634, Boston, MA: ACM Press, 2009.\r[91] H. Kang and B. Shneiderman, “Exploring personal media: A spatial interface\rsupporting user-defined semantic regions,” Journal of Visual Languages &\rComputing, vol. 17, no. 3, pp. 254–283, 2006.\r[92] J. Kari, “Evolutionary information seeking: A case study of personal develop-\rment and internet searching,” First Monday, vol. 11, no. 1, 2006.\r[93] T.Kato,T.Kurita,N.Otsu,andK.Hirata,“Asketchretrievalmethodforfull color image database-query byvisual example,” in Proceedings of 11th IAPR International Conference on Pattern Recognition, pp. 530–533, The Hague,\rThe Netherlands: IEEE Computer Society, 1992.\r[94] D. Kelly, S. Dumais, and J. Pedersen, “Evaluation challenges and directions\rfor information-seeking support systems,” Computer, vol. 42, no. 3, pp. 60–66,\r2009.\r[95] A.Kerne,E.Koh,B.Dworaczyk,J.M.Mistrot,H.Choi,M.S.Smith,R.Grae-\rber, D. Caruso, A. Webb, R. Hill, and J. Albea, “Combinformation: A mixed- initiative system for representing collections as compositions of image and text surrogates,” in Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, pp. 11–20, Chapel Hill, NC, USA: ACM Press, 2006.\r[96] M.KobayashiandK.Takeda,“Informationretrievalontheweb,”ACMCom- puting Surveys, vol. 32, no. 2, pp. 144–173, 2000.\r[97] T. Kohonen, Self-Organizing Maps. Springer, 2001.\r[98] S. Koshman, “Testing user interaction with a prototype visualization-based\rinformation retrieval system,” Journal of the American Society for Informa-\rtion Science and Technology, vol. 56, no. 8, pp. 824–833, 2005.\r[99] N.Kosugi,Y.Nishihara,T.Sakata,M.Yamamuro,andK.Kushima,“Aprac- tical query-by-humming system for a large music database,” in Proceedings of the eighth ACM international conference on Multimedia, pp. 333–342, New\rYork, NY, USA: ACM Press, 2000.\r[100] B. Kotelly, “Resonance: Introducing the concept of penalty-free deep look\rahead with dynamic summarization of arbitrary results sets,” in Work- shop on Human-Computer Interaction and Information Retrieval, Cambridge Mssachusetts, USA: MIT CSAIL, 2007.\r[101] C. C. Kuhlthau, “Inside the search process: Information seeking from the user’s perspective,” Journal of the American Society for Information Science, vol. 42, no. 5, pp. 361–371, 1991.\r[102] B. Kules, “Methods for evaluating changes in search tactics induced by exploratory search systems,” in ACM SIGIR 2006 Workshop on Evaluating Exploratory Search Systems, Seattle, WA, 2006.\r[103] B. Kules and R. Capra, “Creating exploratory tasks for a faceted search inter- face,” in Second Workshop on Human-Computer Interaction and Information Retrieval (HCIR’08), Seattle, WA, USA, 2008.\r[104] B. Kules, R. Capra, M. Banta, and T. Sierra, “What do exploratory searchers look at in a faceted search interface?,” in Proceedings of the 9th ACM/IEEE- CS joint conference on Digital libraries, pp. 313–322, Austin, TX, USA: ACM Press, 2009.\r\n92 References\r[105]\r[106]\r[107] [108]\r[109]\r[110]\r[111]\r[112] [113]\r[114]\r[115] [116] [117] [118]\r[119]\r[120]\rB. Kules, J. Kustanowitz, and B. Shneiderman, “Categorizing web search results into meaningful and stable categories using fast-feature techniques,” in Proceedings of the Sixth ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 210–219, Chapel Hill, NC: ACM Press, 2006.\rB. Kules and B. Shneiderman, “Users can change their web search tactics: Design guidelines for categorized overviews,” Information Processing & Man- agement, vol. 44, no. 2, pp. 463–484, 2008.\rC. Kunz, “Sergio — an interface for context driven knowledge retrieval,” in Proceedings of eChallenges, Bologna, Italy, 2003.\rK. Lagus, S. Kaski, and T. Kohonen, “Mining massive document collections by the WEBSOM method,” Information Sciences: An International Journal, vol. 163, no. 1–3, pp. 135–156, 2004. J.LampingandR.Rao,“Thehyperbolicbrowser:Afocus+contexttechnique for visualizing large hierarchies,” Journal of Visual Languages and Computing, vol. 7, no. 1, pp. 33–55, 1996.\rX. Lin, D. Soergel, and G. Marchionini, “A self-organizing semantic map for information retrieval,” in Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval, Chicago, Illinois, United States, New York, NY, USA: ACM Press, 1991. L.Lorigo,H.H.B.Pan,T.Joachims,L.Granka,andG.Gay,“Theinfluenceof task and gender on search and evaluation behavior using google,” Information Processing and Management, vol. 42, no. 4, pp. 1123–1131, 2006.\rR. Losee, Text Retrieval and Filtering: Analytic Models of Performance. Kluwer Acadedmic Publishers, 1998.\rA. Lunzer and K. Hornbæk, “Side-by-side display and control of multiple scenarios: Subjunctive interfaces for exploring multi-attribute data,” in Pro- ceedings of Australian Special Interest Group Conference on Computer-Human Interaction, Brisbane, Australia, 2003.\rB. Mackay and C. Watters, “Exploring multi-session web tasks,” in CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, Florence, Italy: ACM Press, 2008.\rG. Marchionini, Information Seeking in Electronic Environments. Cambridge University Press, 1995.\rG. Marchionini, “Exploratory search: From finding to understanding,” Com- munications of the ACM, vol. 49, no. 4, pp. 41–46, 2006.\rG. Marchionini and B. Shneiderman, “Finding facts vs. browsing knowledge in hypertext systems,” Computer, vol. 21, no. 1, pp. 70–80, 1988. B.Marshall,D.Mcdonald,H.Chen,andW.Chung,“Ebizport:Collectingand analyzing business intelligence information,” Journal of the American Society for Information Science and Technology, no. 10, pp. 873–891, 2004. K.Mcpherson,“Opinion-relatedinformationseeking:Personalandsituational variables,” Personality and Social Psychology Bulletin, vol. 9, no. 1, p. 116, 1983. Y.Medynskiy,M.Dontcheva,andS.M.Drucker,“Exploringwebsitesthrough contextual facets,” in Proceedings of the 27th International Conference on Human Factors in Computing Systems, pp. 2013–2022, Boston, MA, USA: ACM Press, 2009.\r\n[121]\r[122] [123] [124] [125]\r[126]\r[127]\r[128]\r[129]\r[130]\r[131]\r[132] [133]\r[134]\r[135]\rReferences 93\rD. R. Millen, J. Feinberg, and B. Kerr, “Dogear: Social bookmarking in the enterprise,” in Proceedings of the SIGCHI conference on Human Factors in computing systems, pp. 111–120, Montr ́eal, Qu ́ebec Canada: ACM Press, 2006. G. A. Miller, “Wordnet: A lexical database for english,” Communications of the ACM, vol. 38, no. 11, p. 39, 1995.\rD. K. Modjeska, Hierarchical Data Visualization In Desktop Virtual Reality. University of Toronto, 2000. C.N.Mooers,“‘mooers’laworwhysomeretrievalsystemsareusedandothers are not,” American Documentation, vol. 11, no. 3, 1960.\rM. R. Morris, “A survey of collaborative web search practices,” in Proceed- ings of the SIGCHI Conference on Human Factors in Computing Systems, Florence, Italy: ACM Press, 2008.\rA. Motro, “Vague: A user interface to relational databases that permits vague queries,” ACM Transactions on Information Systems, vol. 6, no. 3, pp. 187– 214, 1988.\rT. Munzner, “H3: Laying out large directed graphs in 3D hyperbolic space,” in Proceedings of IEEE Symposium on Information Visualization, pp. 2–10, Phoenix, AZ, USA: IEEE Computer Society, 1997.\rD. A. Nation, C. Plaisant, G. Marchionini, and A. Komlodi, “Visualizing web- sites using a hierarchical table of contents browser: WebTOC,” in Proceedings of the Third Conference on Human Factors and the Web, Denver, CO, USA, 1997.\rJ. Nielsen and R. Molich, “Heuristic evaluation of user interfaces,” in CHI’90: Proceedings of the SIGCHI conference on Human factors in computing sys- tems, Seattle, WA, USA: ACM Press, 1990.\rA. Oulasvirta, J. P. Hukkinen, and B. Schwartz, “When more is less: The paradox of choice in search engine use,” in Proceedings of the 32nd interna- tional ACM SIGIR conference on Research and development in information retrieval, Boston, MA, USA: ACM Press, 2009.\rT. Paek, S. Dumais, and R. Logan, “Wavelens: A new view onto internet search results,” in Proceedings of the 2004 conference on Human factors in computing systems, pp. 727–734, Vienna, Austria: ACM Press, 2004.\rL. D. Paulson, “Building rich web applications with AJAX,” Computer, vol. 38, no. 10, pp. 14–17, 2005.\rS. Perugini, K. Mcdevitt, R. Richardson, M. Perez-Quiones, R. Shen, N. Ramakrishnan, C. Williams, and E. A. Fox, “Enhancing usability in citidel: Multimodal, multilingual, and interactive visualization interfaces,” in Proceed- ings of the 4th ACM/IEEE-CS joint conference on Digital libraries, pp. 315– 324, Tuscon, AZ, USA: ACM Press, 2004.\rP. Pirolli and S. Card, “Information foraging in information access environ- ments,” in Proceedings of the SIGCHI conference on Human factors in com- puting systems, pp. 51–58, Denver, CO, USA: ACM Press, 1995.\rC. Plaisant, B. Shneiderman, K. Doan, and T. Bruns, “Interface and data architecture for query preview in networked information systems,” ACM Transactions on Information Systems, vol. 17, no. 3, pp. 320–341, 1999.\r\n94 References\r[136] [137]\r[138]\r[139]\r[140]\r[141]\r[142]\r[143]\r[144]\r[145] [146]\r[147]\r[148]\r[149] [150]\rW. Pratt, “Dynamic organization of search results using the umls,” American Medical Informatics Association Fall Symposium, vol. 480, no. 4, 1997.\rY. Qu and G. Furnas, “Model-driven formative evaluation of exploratory search: A study under a sensemaking framework,” Information Processing and Management, vol. 44, no. 2, pp. 534–555, 2008.\rK. Risden, M. P. Czerwinski, T. Munzner, and D. B. Cook, “Initial examina- tion of ease of use for 2D and 3D information visualizations of web content,” International Journal of Human-Computers Studies, vol. 53, no. 5, pp. 695– 714, 2000.\rW. Rivadeneira and B. B. Bederson, A Study of Search Result Clustering Interfaces: Comparing Textual and Zoomable User Interfaces. University of Maryland: HCIL, 2003.\rG. Robertson, M. Czerwinski, K. Larson, D. C. Robbins, D. Thiel, and M. V. Dantzich, “Data mountain: Using spatial memory for document manage- ment,” in Proceedings of the 11th annual ACM symposium on User interface software and technology, pp. 53–162, San Francisco, CA, USA: ACM Press, 1998.\rG. Robertson, M. C. K. Cameron, and D. Robbins, “Polyarchy visualization: Visualizing multiple intersecting hierarchies,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 423–430, Minneapolis, Minnesota, USA: ACM Press, 2002.\rG. G. Robertson, J. D. Mackinlay, and S. K. Card, “Cone trees: Animated 3D visualizations of hierarchical information,” in Proceedings of the SIGCHI con- ference on Human factors in computing systems: Reaching through technology, pp. 189–194, New Orleans, Louisiana, USA: ACM Press, 1991.\rS. E. Robertson and M. M. Hancock-Beaulieu, “On the evaluation of IR sys- tems,” Information Processing and Management, vol. 28, no. 4, pp. 457–466, 1992.\rS. Santini and R. Jain, “Beyond query by example,” in Proceedings of the sixth ACM international conference on Multimedia, pp. 345–350, Bristol, UK, 1998.\rm. c. schraefel, “Building knowledge: What’s beyond keyword search?,” Com- puter, vol. 42, no. 3, pp. 52–59, 2009.\rm. c. schraefel, D. A. Smith, A. Owens, A. Russell, C. Harris, and M. Wilson, “The evolving mSpace platform: Leveraging the semantic web on the trail of the memex,” in Proceedings of the Sixteenth ACM Conference on Hypertext and Hypermedia, pp. 174–183, Salzburg, Austria: ACM Press, 2005. m.c.schraefel,M.L.Wilson,andM.Karam,PreviewCues:EnhancingAccess to Multimedia Content. School of Electronics and Computer Science, Univer- sity of Southampton, 2004.\rm. c. schraefel, M. L. Wilson, A. Russell, and D. A. Smith, “mSpace: Improv- ing information access to multimedia domains with multimodal exploratory search,” Communications of the ACM, vol. 49, no. 4, pp. 47–49, 2006.\rB. Schwartz, The Paradox of Choice: Why More Is Less. Harper Perennial, 2005.\rM. M. Sebrechts, J. V. Cugini, S. J. Laskowski, J. Vasilakis, and M. S. Miller, “Visualization of search results: A comparative evaluation of text, 2D, and\r\n[151]\r[152] [153]\r[154]\r[155]\r[156]\r[157] [158]\r[159]\r[160]\r[161]\r[162]\r[163]\r[164]\rReferences 95\r3D interfaces,” in Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 3–10, Philadelphia, PA, USA: ACM Press, 1999.\rB. Shneiderman, “Designing information-abundant web sites: Issues and rec- ommendations,” International Journal of Human-Computer Studies, vol. 47, no. 1, pp. 5–29, 1997.\rB. Shneiderman, “Why not make interfaces better than 3D reality?,” Com- puter Graphics and Applications, IEEE, vol. 23, no. 6, pp. 12–15, 2003.\rB. Shneiderman and A. Aris, “Network visualization by semantic substrates,” IEEE Transactions on Visualization and Computer Graphics, vol. 12, no. 5, pp. 733–740, 2006.\rB. Shneiderman, D. Feldman, A. Rose, and X. F. Grau, “Visualizing digital library search results with categorical and hierarchial axes,” in Proceedings of the Fifth ACM International Conference on Digital Libraries, pp. 57–66, San Antonio, TX: ACM Press, 2000.\rB. Shneiderman and C. Plaisant, Designing the User Interface: Strategies for Effective Human-Computer Interaction. Boston: Pearson/Addison-Wesley, 2004.\rC. Silverstein, H. Marais, M. Henzinger, and M. Moricz, “Analysis of a very large web search engine query log,” ACM SIGIR Forum, vol. 33, no. 1, pp. 6– 12, 1999.\rJ. Sinclair and M. Cardew-Hall, “The folksonomy tag cloud: When is it use- ful?,” Journal of Information Science, vol. 34, no. 1, pp. 15–29, 2008.\rA. F. Smeaton, P. Over, and W. Kraaij, “Evaluation campaigns and TRECVid,” in Proceedings of the 8th ACM international workshop on Mul- timedia information retrieval, pp. 321–330, Santa Barbara, California, USA: ACM Press, 2006.\rA. Spink, “Study of interactive feedback during mediated information retrieval,” Journal of the American Society for Information Science, vol. 48, pp. 382–394, 1997.\rA. Spink, “A user-centered approach to evaluating human interaction with web search engines: An exploratory study,” Information Processing & Man- agement, vol. 38, no. 3, pp. 401–426, 2002.\rA. Spink and T. D. Wilson, “Toward a theoretical framework for information retrieval (IR) evaluation in an information seeking context,” in Proceedings of Multimedia Information Retrieval Applications, p. paper 9, Glasgow, UK: BCS, 1999. E.StoicaandM.A.Hearst,“Nearly-automatedmetadatahierarchycreation,” in Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, MA USA, 2004.\rA. G. Sutcliffe and U. Patel, “3D or not 3D: Is it nobler in the mind?,” in\rProceedings of the BCS Human Computer Interaction Conference on People and Computers XI, pp. 79–94, London, UK: Springer-Verlag, 1996.\rD. Swanson, N. Smalheiser, and A. Bookstein, “Information discovery from complementary literatures: Categorizing viruses as potential weapons,”\r\n96 References\r[165]\r[166]\r[167] [168] [169]\r[170]\r[171]\r[172] [173]\r[174] [175]\r[176] [177]\r[178]\r[179]\rJournal American Society Information Science and Technology, vol. 52, no. 10, pp. 797–812, 2001.\rE. Tanin, C. Plaisant, and B. Shneiderman, “Browsing large online data with query previews,” in Proceedings of the Symposium on New Paradigms in Infor- mation Visualization and Manipulation, Washington D.C., USA: ACM Press, 2000.\rJ. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, P. A. G. Ramos, and C. Hu, “Visual snippets: Summarizing web pages for search and revisitation,” in Pro- ceedings of the 27th international conference on Human factors in computing systems, pp. 2023–2032, Boston, MA, USA: ACM Press, 2009.\rE. R. Tufte, Envisioning Information. Graphics Press Cheshire, 1990.\rD. Tunkelang, Faceted Search. Morgan and Claypool, 2009.\rO. Turetken and R. Sharda, “Clustering-based visual interfaces for presenta- tion of web search results: An empirical investigation,” Information Systems Frontiers, vol. 7, no. 3, pp. 273–297, 2005.\rP. Vakkari, “Cognition and changes of search terms and tactics during task performance: A longditudinal case study,” in Proceedings of the Interna- tional Conference on Computer Assisted Information Retrieval (RAIO), Paris, France, 2000.\rP. Wang, M. Berry, and Y. Yang, “Mining londitudinal web queries: Trends and patterns,” Journal of the American Society for Information Science and Technology, vol. 54, no. 8, pp. 742–758, 2003.\rR. W. White, S. M. D. B. Kules, and m. c. schraefel, “Introduction,” Com- munications of the ACM, vol. 49, no. 8, pp. 36–39, 2006.\rR. W. White and S. M. Drucker, “Investigating behavioral variability in web search,” in Proceedings of the 16th International Conference on World Wide Web, Banff, Canada: ACM Press, 2007.\rR. W. White and R. Roth, Exploratory Search: Beyond the Query-Response Paradigm. Morgan & Claypool, 2009.\rR. W. White, I. Ruthven, and J. M. Jose, “Finding relevant documents using top ranking sentences: An evaluation of two alternative schemes,” in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 57–64, Tempere, Fin- land: ACM Press, 2002.\rM. Wilson, “mspace: What do numbers and totals mean in a flexible semantic browser,” SWUI’06, 2006.\rM. Wilson and m. c. schraefel, “Bridging the gap: Using ir models for evaluat- ing exploratory search interfaces,” in SIGCHI 2007 Workshop on Exploratory Search and HCI, San Jose, CA, USA, 2007.\rM. L. Wilson, “An analytical inspection framework for evaluating the search tactics and user profiles supported by information seeking interfaces,” PhD Thesis, University of Southampton, 2009.\rM. L. Wilson, P. Andr ́e, and m. c. schraefel, “Backward highlighting: Enhanc- ing faceted search,” in Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology, Monterey, CA, USA: ACM Press, 2008.\r\n[180]\r[181]\r[182]\r[183]\r[184]\r[185]\r[186]\r[187] [188]\r[189] [190]\r[191]\r[192]\rReferences 97\rM. L. Wilson, P. Andr ́e, D. A. Smith, and m. c. schraefel, Spatial Consistency and Contextual Cues for Incidental Learning in Browser Design. School of Electronics and Computer Science, University of Southampton, 2007.\rM. L. Wilson and m. c. schraefel, “mSpace: What do numbers and totals mean in a flexible semantic browser,” in Proceedings of the 3rd International Semantic Web User Interaction Workshop, Athens, GA, USA, 2006.\rM. L. Wilson and m. c. schraefel, “A longitudinal study of both faceted and keyword search,” in Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries, Pittsburgh, PA, USA: IEEE Computer Society, 2008.\rM. L. Wilson and m. c. schraefel, “The importance of conveying inter-facet relationships for making sense of unfamiliar domains,” in CHI’09 Workshop on Sensemaking, Boston, MA, USA, 2009.\rM. L. Wilson, m. c. schraefel, and R. W. White, “Evaluating advanced search interfaces using established information-seeking models,” Journal of the Amer- ican Society for Information Science and Technology, vol. 60, no. 7, pp. 1407– 1422, 2009.\rA. Woodruff, R. R. A. Faulring, J. Morrsion, and P. Pirolli, “Using thumb- nails to search the web,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 198–205, Seattle, Washington, USA: ACM Press, 2001.\rH. Wu, M. Zubair, and K. Maly, “Harvesting social knowledge from folk- sonomies,” in Proceedings of the seventeenth conference on Hypertext and hypermedia, pp. 111–114, Odense, Denmark: ACM Press, 2006.\rK. Yang, “Information retrieval on the web,” Annual Review of Information Science and Technology, vol. 39, pp. 33–80, 2005.\rK.-P. Yee, K. Swearingen, K. Li, and M. Hearst, “Faceted metadata for image search and browsing,” in Proceedings of the ACM Conference on Human fac- tors in Computing Systems, pp. 401–408, Fort Lauderdale, FL, USA: ACM Press, 2003.\rO. Zamir and O. Etzioni, “Grouper: A dynamic clustering interface to web search results,” Computer Networks, vol. 31, pp. 1361–1374, 1999.\rO. Zamir, O. Etzioni, O. Madani, and R. M. Karp, “Fast and intuitive clus- tering of web documents,” in Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining (KKD’97), pp. 287–290, Newport, CA, USA, 1997.\rH.-J. Zeng, Q.-C. HE, Z. Chen, W.-Y. Ma, and J. Ma, “Learning to cluster web search results,” in Proceedings of the 27th Annual International Confer- ence on Research and Dvelopment in Information Retrieval, Sheffield, United Kingdom, New York: ACM Press, 2004.\rJ. Zhang and G. Marchionini, “Evaluation and evolution of a browse and search interface: Relation browser,” in Proceedings of the National Conference on Digital Government Research, pp. 179–188, Atlanta, GA, USA: Digital Government Research Center, 2005.","title":"web-003.dvi","mimeType":"application/pdf","authors":[{"id":22,"firstName":"Yoshio","lastName":"19:56:25","middleNames":["Nitta","(TMD","Univ.","JP)","5051","2004","Feb","18"]}],"keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"pp","time":1461915877130,"auto":true,"weight":1.0},{"@type":"Tag","text":"3d","time":1461915877251,"auto":true,"weight":1.0},{"@type":"Tag","text":"search","time":1461915877233,"auto":true,"weight":1.0},{"@type":"Tag","text":"proceed","time":1461915877218,"auto":true,"weight":1.0},{"@type":"Tag","text":"interfac","time":1461915877271,"auto":true,"weight":1.0},{"@type":"Tag","text":"web","time":1461915877285,"auto":true,"weight":1.0},{"@type":"Tag","text":"acm","time":1461915877191,"auto":true,"weight":1.0},{"@type":"Tag","text":"facet","time":1461915877177,"auto":true,"weight":1.0},{"@type":"Tag","text":"confer","time":1461915877204,"auto":true,"weight":1.0},{"@type":"Tag","text":"o","time":1461915877163,"auto":true,"weight":1.0}]},{"@type":"ScientificDocument","id":32,"appId":"PeyeDF_cc535375f23dd31cb289d988bf86ac34f5cbc679","timeCreated":1456740240839,"timeModified":1461915877571,"user":{"id":2,"username":"yi","role":"USER"},"type":"http://www.hiit.fi/ontologies/dime/#ScientificDocument","score":0.018406551,"uri":"file:///Users/cheny13/Documents/References/From Keyword Search to Exploration WebSci-Wilson.pdf","plainTextContent":"Foundations and Trends⃝R in\rWeb Science\rVol. 2, No. 1 (2010) 1–97\r⃝c 2010 M. L. Wilson, B. Kules, m. c. schraefel and B. Shneiderman\rDOI: 10.1561/1800000003\rFrom Keyword Search to Exploration: Designing Future Search Interfaces for the Web\rBy Max L. Wilson, Bill Kules,\rm. c. schraefel and Ben Shneiderman\rContents\r1 Introduction 4\r1.1 A Challenge Faced by Designers of Future\rSearch Systems 9\r1.2 A Taxonomy to Overcome the Challenge 11\r1.3 The Scope of Our Approach 12\r2 A Model of Search 14\r2.1 A Context Model of Search 15\r2.2 Information Retrieval 16\r2.3 Information Seeking 17\r2.4 Work Context 19\r2.5 Summary 20\r3 Survey of Search Systems 21\r3.1 Adding Classifications 22\r3.2 Result Organization 41\r\n3.3 3.4\r4\r4.1 4.2 4.3 4.4\r5\r5.1 5.2\rAdditional Functions 57 Summary 64\rEvaluations of Search Systems 65\rInformation Retrieval Evaluations 65 Information-Seeking Evaluations 67 Work-Context Evaluations 71 Summary 74\rTaxonomy of Search Visualizations 75\rThe Taxonomy 75 Using the Taxonomy 79\r6 Conclusions 81\rAcknowledgments 83 References 84\r\nFoundations and Trends⃝R in\rWeb Science\rVol. 2, No. 1 (2010) 1–97\r⃝c 2010 M. L. Wilson, B. Kules, m. c. schraefel and B. Shneiderman\rDOI: 10.1561/1800000003\rFrom Keyword Search to Exploration: Designing Future Search Interfaces for the Web\rMax L. Wilson1, Bill Kules2,\rm. c. schraefel3 and Ben Shneiderman4\r1 Intelligence, Agents, Multimedia (IAM) Group, School of Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ, UK, mlw05r@ecs.soton.ac.uk\r2 School of Library and Information Science, The Catholic University of America, Washington, DC 20064, USA, kules@cua.edu\r3 Intelligence, Agents, Multimedia (IAM) Group, School of Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ, UK, mc+ft@ecs.soton.ac.uk\r4 Department of Computer Science and Human-Computer Interaction Laboratory, University of Maryland at College Park, College Park, MD 20742, ben@cs.umd.edu\rAbstract\rThis monograph is directed at researchers and developers who are designing the next generation of web search user interfaces, by focusing on the techniques and visualizations that allow users to interact with and have control over their findings. Search is one of the keys to the Web’s success. The elegant way in which search results are returned has been well researched and is usually remarkably effective. However,\r\nthe body of work produced by decades of research into information retrieval continues to grow rapidly and so it has become hard to syn- thesize the current state-of-the-art to produce a search interface that is both highly functional, but not cluttered and distracting. Further, recent work has shown that there is substantial room for improving the support provided to users who are exhibiting more exploratory forms of search, including when users may need to learn, discover, and under- stand novel or complex topics. Overall, there is a recognized need for search systems to provide effective user experiences that do more than simply return results.\rWith the aim of producing more effective search interfaces, human computer interaction researchers and web designers have been devel- oping novel interactions and features that enable users to conveniently visualize, parse, manipulate, and organize their Web search results. For instance, while a simple set of results may produce specific infor- mation (e.g., the capital of Peru), other methods may let users see and explore the contexts of their requests for information (more about the country, city, and nearby attractions), or the properties that asso- ciate groups of information assets (grouping hotels, restaurants, and attractions by their type, district, or price). Other techniques support information-seeking processes that may last weeks or months or may even require collaboration between multiple searchers. The choice of relevant result visualization strategies in new search systems should reflect the searchers and the higher-level information needs that moti- vate their searches. These examples provide further motivation for sup- porting designers, who are challenged to synthesize and understand the breadth of advances in search, so that they can determine the bene- fits of varied strategies and apply them appropriately to build better systems.\rTo support researchers and designers in synthesizing and under- standing the advances in search, this monograph offers a structured means to think about web search result visualization, based on an inclusive model of search that integrates information retrieval, infor- mation seeking and a higher-level context of tasks and goals. We exam- ine each of these levels of search in a survey of advances in browsers and related tools by defining search-related cognitive processes and\r\nanalyzing innovative design approaches. We then discuss evaluations at each of these levels of search, presenting significant results and iden- tifying both the traditional and novel means used to produce them. Based on this examination, we propose a taxonomy of search result visualization techniques that can be used to identify gaps for future research and as a reference for designers of next generation web search systems.\r\n1\rIntroduction\rThis monograph is for designers thinking about how they might enhance the experience of discovering, exploring and putting to work information they can access over the Web. This monograph is also for researchers who may be interested in how search interaction approaches have developed over the past decade. In both cases, a fundamental ques- tion is at play: what else users could possibly need besides Google to search the Web? That’s a fair question, and readers of this survey may have the same question. So to tackle that head on, let us agree: Google is really good. For what it does.\rOur monograph considers the approaches for exploring informa- tion spaces that Google’s elegant keyword search cannot do. Over the past decade, research on alternative search paradigms has emphasized Web front ends on single or unified databases. Such work is productive when the designer has the luxury of working with well-curated docu- ments from a single source, but the elegant visualization or well-tailored faceted browser may not scale to the size and diversity of Web-based information, links, scientific data sets, personal digital photos, creative videos, music, animations, and more. So why does keyword search scale? Because (a) huge resources have been thrown at the problem\r4\r\n5\rand (b) textual data have a satisfying and compliant order to it. We hope this monograph shows that the research and evaluations of alter- native approaches to data exploration for knowledge building are the best preparation we have for the next generation Web, the Web of Linked Data.\rIn a little more than a decade the Web has become the default global repository for information. Many factors have contributed to this remarkable result, from the success of the robust technologies that enable its networked connections, to the commercialization of the back- bone that enticed business to support and utilize it, to the ease with which ordinary citizens can publish information to it. But perhaps the key technology that took the Web from a useful supplement of current information practice to become the default communication medium is search. Web search, as provided by Google, Microsoft, Yahoo, etc., enables users to find the information they want via the simplest of interaction paradigms: type some keywords into a box and get back an informative result list that is ranked, as if by magic, so that the first results most likely match what we’re trying to find.\rSearch engines automated what was initially a community-based and commercially coordinated Easter egg hunt: category systems were proposed and documents as they were found either by humans recom- mending them to such sites, or discovered by human trawlers and some early web crawlers, were assigned to categories. The Web was set up like a giant Yellow Pages. Further, before these nascent directories, the Web was explored by the link. As recently as 2004, surfing the Web was still a common trope for browsing the Web, following from link to link, from one site to another. Not unlike blogs today, web sites might publish best new finds on a topic, and away one would go. Only five years later, who “surfs” or presumes to browse “the Web”? It has grown beyond that scale of the surfable, with its pockets of Dark Web and ice caps of Public Web, where so much more than is indexed by search engines is below the visible water line of documents. This growth is itself related to the existence of search: because it can be found by search, rather than relying on recommendations alone, it is worth publishing on the Web; indeed it is necessary to publish on the Web. Because conversely, if it cannot be found on the Web, does it exist?\r\n6 Introduction\rSearch as embodied by the text box and keyword has framed our understanding of what the Web is [145]. It has become so ubiquitously associated with the Web that it is difficult to find a web browser that does not have as a default tool, a keyword search box built into upper right of the browser window, right next to the box for the location. In some cases the URL address bar is already polymorphic: acting as either address area or search bar. The prominence of web search based on the fundamental efficacy of keyword search makes it difficult to imagine what an entire monograph on search interaction may be about. It turns out that this elegant paradigm is especially effective for the 1-min search — find the address for Chez Panisse, get a biography of Madame Curie, or locate the home page for a local car rental. But many users have come to the Web for substantive research that takes hours or weeks — find all the songs written about poverty in the 1930s, prove that there are no patents that cover my emerging innovation, or locate the best legal precedents for my brief.\rA second motivator for new search strategies is that the next gen- eration web will offer fresh possibilities that go well beyond finding documents by way of keyword search. Hall and O’Hara [69] stress that what we know as the Web today is the Document Web, and not the Web of Linked Data that is imminently upon us. The older Document Web is about the information, readable by us, written by us, and framed for our use. It is this very human-readable orientation of the Web and it is the presentation technologies in the browser that have enabled keyword search engines to become so very good: the words in the documents are all a search engine has to go on to find appropriate results. It is because the search engine is searching in documents that we get a list of docu- ments back: we may only want a sentence in the middle of a document, but we get the whole thing (the document) back.\rBy contrast, in the newer Web of Linked Data, often called the Semantic Web, the idea is to give the search engine designers more to work with than making best guesses about what results to return based on keyword frequency and number of pages linked to a document. Imagine if instead of a list of results, the machine simply returned “the answer”? Some queries have specific answers: “mc’s phone number at work” or “British obesity rate in 2009?” There may be several sources\r\n7\rfor this information, but if they concur, why not just present that result, and give the six places the result occurs? Right now, this kind of search result is not feasible because the Web for the most part does not have consistent tags to indicate what is a phone number or where to find obesity rates. Search engines check if a term requested maps to the term in a document, and does very effective ranking very fast. That is of course an oversimplification of all the sophistication to make that experience as fast and accurate as it appears to be.\rThe goal of the Web of Linked Data is to have information about information available so that not only can designers provide better, clearer answers to such simple queries about phone numbers and statis- tics, but also users can resolve questions that are difficult to handle without metadata. Some researchers are conducting intriguing research that attempts to create this metadata automatically to derive seman- tics from the documents themselves. In this monograph we are less concerned with how metadata becomes available. We are concerned with the question of what designers can do with it, once it exists.\rWhile the power of Semantic Web of Linked Data is that it can enhance large diverse, unorganized, and heterogeneous datasets, the unique affordances also challenge our assumptions about how we access information [176]. As the links between data can be numerous, endless, and of any granularity, the assumptions about carefully structured clas- sifications, for example, breakdown. Similarly, while web searches are typically for web pages, it is not clear whether searching at the data level should return any object [21], specific types of objects [146], object relationships [21, 76], portions of RDF [47], entire ontologies [2, 63], and so on.\rFurther, as the work on semantically linked data has separated the data from presentation, designers and users are able to represent the data however they like [21]. The flipside, however is that someone, either the interface designer or the end user, has to decide how to represent the data. In summary, the freedom enabled by semantically organized data sets has in turn broadened the options and increased the number of decisions that designers and end users have to make. Recent work has shown, however, that increasing numbers of options can make designers and users feel less confident in their decisions, and less happy\r\n8 Introduction\rwith their results [130, 149], rather than making them feel empowered. What effect, then, does this have on confidence during search interface design, given that designers and users now have more freedom.\rThese issues are becoming national policy issues, especially as the United States, United Kingdom, and other governments intend to release increasing amounts of information onto the Web as data with sufficient metadata to support more automatic analysis. Metadata tags will indicate that a number represents the reported cases of obesity at a given time and place. The number also has a source and date of creation associated with it so users can verify accuracy and timeliness. It is not simply a number in a document, instead it comes with well-associated meanings.\rAs soon as this information is available new opportunities for rep- resentation beyond document lists become possible. Users will be able to see quickly for themselves: are obesity levels rising at the rates pre- sented in the media, or, by mapping these data from several sources, are they too conservative or aggressive? Imagine being able to look at such sources to ask these kinds of questions with the same facility as we use keyword search now. Now that is an interaction challenge.\rDesigners already offer such representations on smaller than Web scale information sources; that is what most of the literature we will review considers. In that sense we have some preparation for what is to come. But there are also entirely new interaction challenges. There will be many data sources from many places that map to given meanings, like obesity statistics. How can these disparate sources be gathered, inspected, and explored?\rRight now, we are on the cusp of a transition from the Document Web to the Document/Data Web. It is an incredible time to be inter- ested in designing or researching how to engage with these immense volumes of data. Are designers and researchers ready for this transi- tion? The findings presented here may act as guideposts for this near future. We may also look back in ten years at these nascent efforts to imagine exploring data at scale and say either that was clever or that was na ̈ıve. It will be more than intriguing to see what principles remain, and what have yet to be imagined. In the meantime, we look back in order to leap ahead.\r\n1.1 A Challenge Faced by Designers of Future Search Systems 9\rIn the remainder of this monograph, Section 2 identifies and explains a model of search that is used to structure the discussion in the fol- lowing sections and forms the basis of the taxonomy of advances in interactive search. In short, the model describes search in increasing layers of context, from simple retrieval, to broader strategies, and to the wider tasks that motivate them. Section 3 identifies specific advances in interactive search, and prominent examples of their use, organized by the different layers of the model to which they apply. Section 4 explains how advances at each layer of the model have been evaluated. Section 5 then presents the final taxonomy, and identifies areas of rel- atively little research as potential focal points for future research. For search interface designers, the taxonomy provides a list of potential features to include in designs, describing (a) how they support users, (b) how their support has been evaluated, and (c) how prevalent they have become on the Web.\r1.1 A Challenge Faced by Designers of Future Search Systems\rUnderstanding how search interfaces and visualization affect searcher success is a hard challenge, and cannot be as easily measured as speed, document similarity, and result accuracy. In the early 1970s, Cooper [43] suggested that instead of speed metrics, search evaluations should be based on subjective user satisfaction with the results. Later, Robertson and Hancock-Beaulieu [143] noted that the recent, at the time, revolutions of IR research had begun to focus more on users and less on systems. Even more recently, though, researchers have identified just how inadequate the familiar keyword search paradigms, provided by environments such as Google and Bing1 (Microsoft’s search engine), might be for users who need to do more than just find a website that answers a factual question.\rThe recent focus on these more exploratory forms of search, known as Exploratory Search [172, 174], has identified some search scenarios that require much more diverse searching strategies, including when the\r1 http://www.bing.com\r\n10 Introduction\rusers are (a) unfamiliar with a domain and its terminology, (b) unfa- miliar with a system and it’s capabilities, or (c) unfamiliar with the full detail of their task or goal. Experts may also conduct demanding searches such as those needing to do: (a) comprehensive searches to find every relevant record (Legal, patent, medical), (b) negation searchers to prove the absence of relevant work (e.g., patent, pharmaceuticals), (c) exception searches to find outlier documents (that take a different or contradictory point of view than commonly held), or (d) bridging searches that connect two disparate fields of study [164]. Exploratory search scenarios are characterized by needs that are “open-ended, per- sistent, and multifaceted, and information-seeking processes that are opportunistic, iterative, and multitactical” [174]. In each case advances in search need to do more than simply improve the matching of results to terms entered into a single box. Even in the late 1980s, Motro [126] designed the VAGUE interface based on the notion that often, when limited to simple keyword interfaces, users submit numerous evolutions of an original vague query in order to figure out which terms are going to produce all, or even just part, of their desired information.\rIn many cases, searching involves a range of tactics and techniques, rather than simply submitting a query and seeing a list of matching results. As part of the special issue on Exploratory Search, Marchion- ini [116] identified, although not exhaustively, a series of strategies that users may often need to employ to achieve their goals, such as compar- ing, synthesizing, and evaluating. MacKay and Watters [114] present a diary study documenting examples of search tasks that span multiple search sessions, where users return days later to continue on tasks such as job seeking or house hunting. Similarly, Morris [125] has doc- umented the breadth of occasions where users clearly collaborate with family and colleagues on tasks such as holiday planning and team work projects. It is plain to see that a search interface needs to provide more than a simple keyword search form to support users in applying such strategies.\rThe recognition that there is more to search than basic Informa- tion Retrieval has led to many extensions and alternatives to the keyword search paradigm. An example extension is to cluster the results into groups that share attributes [190]. Alternatively, faceted browsing\r\n1.2 A Taxonomy to Overcome the Challenge 11\r[75, 168] provides meaningful or significant attributes of web pages to users at the beginning, so that they do not even have to think of words to put in the keyword search box. This can be especially useful in the occasions where people are unfamiliar with a domain and its terminol- ogy, for example. These, and many other advances, have much in com- mon and while they each have specific advantages, it is not clear that including them all would provide a stronger search interface. Instead, designers now have the challenge of deciding: (a) what types of search strategies should be supported, if not all of them, and (b) which new features to include in order to support them. This challenge is particu- larly difficult, when so many advances have been proposed, each with different benefits, and when the benefits of each advance have often been shown independent of others.\r1.2 A Taxonomy to Overcome the Challenge\rThe goal of this monograph is to support designers with this challenge by building a taxonomy of advances in the field of search that can be used as common ground when choosing which features to include within future search interfaces. To build the taxonomy we:\r(1) identify a model, produced by theory, which covers the full breadth of search from context and tasks, down to specific actions and results (Section 2);\r(2) summarize the specific advances in interactive search (Sec- tion 3);\r(3) discuss the way that these interactive search advances have been evaluated (Section 4), in accordance with the model of search presented in Section 2; and\r(4) present a taxonomy (in Section 5) of the search advances (from Section 3) that takes into account the type of search supported (from Section 2), how the advances have been eval- uated (Section 4) and how prevalent they are on the Web.\rProducing a structured and consistent taxonomy allows us to compare advances in interactive search, with two benefits. First, the taxonomy can be a reference for designers. The latter half of Section 5\r\n12 Introduction\rdescribes a detailed process that designers can apply to systematically decide which features to include in their search designs. As the taxon- omy includes advances in the field, their benefits, and how they have been evaluated, a designer can quickly compare options and choose appropriate features for their new interface. Second, the taxonomy can be used by academics to identify areas that require further study and contextualize future advances in the field.\r1.3 The Scope of Our Approach\rWhen defining a means of categorizing and communicating existing and on-going research, it is important to define the scope of our approach, so that it is correctly used. Here we specifically bound the content of this monograph in two areas: what we mean by search and what we mean by the Web.\r1.3.1 What We Mean by Search\rSo far, this monograph has used the terms: search, seeking, and Infor- mation Retrieval interchangeably. For the rest of the monograph, how- ever, we intend to follow a specific set of terminology, which is defined carefully in the model discussed in Section 2. Information Retrieval is perhaps the most well studied, and so most well-defined term used to describe searching. Typically, Information Retrieval refers to the paradigm where users enter a keyword into a system, which responds by returning the results that are most relevant to the keywords used. This monograph covers a much broader view of search than simply Information Retrieval. Information Seeking is another common term used to describe people’s searching behavior, including activities such as searching, browsing, and navigating. Again, however, in this mono- graph we use the word search in a broader sense than Information Seeking. The model described in Section 2 defines search as the set of activities that take users from identifying a problem all the way to achieving their goals, which will, at times, involve Information Seeking, which in turn, may include Information Retrieval.\r\n1.3 The Scope of Our Approach 13 1.3.2 What We Mean by the Web\rThe “indexable web”, that is, the portion of the web indexed by major search engines, was estimated at 11.5-billion pages in 2005 [67], with Google reporting that they surpassed the 1-trillion mark in July 2008.2 One major characteristic of the Web, therefore, is scale. In this mono- graph, however, search systems are discussed that search both the whole web, and certain domains within the Web and so scale is not always a primary concern for design. A more indicative and remarkable charac- teristic is the heterogeneity of the contents. The Web contains a variety of data and documents. Documents may be in plain text, HTML, XML, PDF, Rich Text Format (RTF), Microsoft Word (.DOC), spreadsheets, and a multitude of specialized or proprietary formats, including micro- formats [6]. Multiple forms of media, including still images, audio, and video, are widely available and indexed by general purpose as well as specialized search engines. These documents vary from highly struc- tured databases, to semi-structured web pages, to unstructured text. Again, however, while some web search engines focus on the entire het- erogeneous content of the Web, others focus on specific and bounded domains within the Web. In these known and bounded conditions, the format of documents is often known and fixed, and so is not always a concern for all web-based search systems.\rIn summary, as web search systems are discussed, some are limited by the diversity of online material, and the design of others is motivated by unique features of web collections. Both are important areas that sometimes share concerns but often differ significantly in the challenges they present during the design of search interfaces. An e-commerce site might, for example, try to support searchers with the categorization, price, and availability of their products. Such product-related attributes are not a concern for general Web search, which may includes product results, reviews, specifications, and informational documents. Finally, it is important to remember that as users engage in search, they may be moving between the entire web and known collections within it, in order to achieve their goals.\r2 http://googleblog.blogspot.com/2008/07/we-knew-web-was-big.html.\r\n2\rA Model of Search\rIn order to survey search interface advances, we need to establish a framework and a vocabulary that allows us to organize, describe, and analyze the value of their contributions. This section describes a model of search [84] which grounds the remainder of the monograph by con- veying both a structure that defines search and a set of relevant termi- nology. While many models of search exist, as discussed further below, the selected model was chosen because of its holistic perspective, con- sidering different granularities of context determined by “a person’s task, its phase, and situation”, while still including important finite concepts such as topical relevance. The key benefit of this holistic view is that it maintains an understanding of the situational nature of infor- mation and that search activities are performed in the context of a larger work task, and even cultural influences. Therefore, the discussion of advances in search interaction, visualization, and Human Computer Interaction, must be set in the broader context of a human’s task. The model described in this section will be referenced throughout Sections 3 and 4, and will be used to classify search interfaces within the taxonomy described in Section 5.\r14\r\n2.1 A Context Model of Search\rFigure 2.1 shows the model of searching contexts, produced by Jarvelin and Ingersen [84], that provides the framework we will be referring to for the remainder of this monograph. Search can be modeled as a hier- archy of goals and tasks. Each task provides the goal and serves as the context for its subsidiary task(s). Figure 2.1 makes multiple levels of context explicit: socio-organizational and cultural, work task, seek- ing, and retrieval. Information Retrieval, as the smallest granule in the model, represents the action most often embodied by Keyword Search, where users are trying to find an often known nugget of information, such as the price of a particular server. Searchers may often find them- selves performing a series of Information Retrieval actions as part of a broader Information-seeking task, such as trying to find the best possi- ble server given a specific budget. Any Information-seeking task, how- ever, is set within a larger Work Task, such as being asked to procure\rFig. 2.1 Searching behavior is made up of multiple layered contexts, where simple informa- tion retrieval is the narrowly focused; figure from J ̈arvelin and Ingwersen [84].\r2.1 A Context Model of Search 15\r\n16 A Model of Search\rresources for a forthcoming project. The result of a Work Task will contain the product of one or more Information-seeking tasks. Finally, every Work Task sits within a much broader Socio-Organizational and Cultural context. The requirements and the importance of meeting them, for example, will be different when buying a server for a large organization, a safety critical organization (such as a hospital) and a small start-up business. Clearly, a home/personal context will have dif- ferent demands than a corporate professional environment, and success in each will be under different criteria.\r2.2 Information Retrieval\rWithin the information retrieval (IR) context, the searcher’s goal is focused on finding documents, document sub-elements, summaries, or surrogates that are relevant to a query. This may be an iterative pro- cess, with human feedback, but it usually is limited to a single session. Typical IR tasks involve finding documents with terms that match terms presented by the searcher, or finding relevant facts or resources related to a query. Typically, within each IR task, the searcher for- mulates queries, examines results, and selects individual documents to view. As a result of examining search results and viewing docu- ments, searchers gather information to help satisfy their immediate information-seeking problem and eventually the higher-level informa- tion need. The common element of all IR tasks as defined in this web- focused monograph is the query-result-evaluation cycle conducted over a collection with the “unit” of information being the document, a sub- element of the document, a summary, or a document surrogate. In the server purchasing scenario used above, the document being sought may be the information page on an e-commerce website, but may also be a picture, or downloadable PDF of its specification.\rWeb search engines like Google support one of the more common IR tasks on the Web: searching for web pages that match a given set of query terms. Services such as Ask.com, however, also attempt to provide fact-oriented answers to natural language queries, while Google will also answer specific conversions of mathematical calculations if the queries are constructed in certain formats. Keyword search is not the\r\n2.3 Information Seeking 17\ronly example of Information Retrieval on the Web, however, with tools like Flamenco and Clusty, each discussed in more detail in Section 3, provide hyperlinked terms that allow searchers to narrow their results and browse relationships between documents.\rEvaluation measures traditionally include precision, recall, and their variations, which simply assess how relevant a document is to the given query. Consequently an IR system can be tested simply by whether it returns the most appropriate documents to a given query. Although the concept of relevance has multiple aspects, at the IR level, topical relevance is the typical aspect considered, that is, how relevant the documents are to the topic expressed by the query, and most systems focus on returning the most relevant documents rather than all of the relevant documents.\r2.3 Information Seeking\rThe objective of the information-seeking (IS) task is to satisfy a per- ceived information need or problem [115, 117, 151]. Often, searchers undertake one or more IR tasks as part of a larger information-seeking (IS) task, although it is possible for a simple need and IS task to be achieved with a single IR task. At the IS level, searchers make strate- gic decisions about where, how, and even whether to find informa- tion related to their information needs. They may adopt an analytical strategy to decide whether to use an IR system (a specific website or searching service). They may also adopt a browsing strategy, where, for example, they start from a known point (perhaps the results of an IR query) and follow successive links to locate other related documents. The documents found while browsing or returned by an IR task will, as part of the IS task, be examined to extract information and synthesize it into a solution to the information need.\rWhile the Web provides an environment for many IR tasks, and, consequently, many IS subtasks, searchers may also consult non-IR sys- tems as well as other resources such as printed material, colleagues or friends, in order to achieve their goal. In fact, people do not always choose to seek information. They may prefer to exhibit alternative Information Behaviors and avoid information that is discomforting or\r\n18 A Model of Search\rtroublesome or which is judged not worth the effort [124]. They may also be biased in what information they seek, looking for information that supports preconceived ideas or opinions [119].\rInformation-seeking tasks have previously been structured as, for example, linear sequence of stages [101] or hierarchical decompositions of tasks [23]. Each of these tasks requires selecting a source, and then engaging in one or more information retrieval tasks which satisfy a por- tion of the overall need. From the perspective of an organization, Choo et al., [39] developed a behavioral model of organizational information seeking on the Web by integrating Ellis’ [57] six stages of information seeking (starting, chaining, browsing, differentiating, monitoring, and extracting) with Aguilar’s [1] four modes of scanning (undirected view- ing, conditioned viewing, informal search, and formal search). Each of these tasks helps to satisfy part of an organization’s information needs.\rThe hyperlink capability of the Web provides support for a browsing strategy. When browsing, each new piece of information that is gathered can provide new ideas, suggest new directions, and change the nature of the information need [13]. This leads to behavior that Bates refers to as berrypicking, reflecting the incremental collection of pieces of information that, as a whole, help to satisfy the information need. The choices made at each step are guided by the seeker’s assessment of what is most likely to produce useful answers as they forage for information [134]. In an environment like the Web, this assessment is based on cues such as hyperlink text that provide an information scent which help users make cost/benefit trade-offs in their choices. In line with research suggesting that search is made up of sequences that are affected by the discovered information, Belkin et al. [18] created a set of “scripts” that describe the typical paths taken by 16 different types of users, including switch points between them for when discoveries change their user type.\rSystems that attempt to provide support of IS tasks typically pro- vide functionality beyond the query-result-evaluation cycle supported by IR systems. This may include search history mechanisms to sup- port information seeking over multiple sessions, or mechanisms such as tagging to collect a set of documents that are each relevant to the larger information need. They may also provide overviews of a collec- tion using textual or graphical methods. Individually, such UI features\r\n2.4 Work Context 19\rmay be tailored to support specific elements of the IS stages (e.g., topic exploration when writing a paper or monitoring of web resources).\rThe evaluation of interfaces that support information-seeking tasks typically involves assessing the quality of information acquired by users relative to the information need provided. IS tasks, such as purchasing a server, are usually assessed by the support the interface provided to users while carrying out the task, and the judged accuracy of the final decision, given the initial requirements. Hearst [72] provides a thorough review of information-seeking user interfaces and their evaluation.\r2.4 Work Context\rThe perceived information need that motivates an IS task is itself moti- vated and initiated by a higher-level work task or context [29, 30, 84] or personally motivated goal [92]. The process is initiated when a searcher recognizes an information need (or is instructed to investigate one) based on an organizational or personal need [29, 30, 115].\rWork tasks are situated in the context of work organization and reflect organizational culture and social norms, as well as organizational resources and constraints. As such, they constrain or guide the IS tasks. For example, the work context provides important information about the domain of information relevant to an information need. It defines and constrains the resources available to satisfy the need. Immediate access to reference books, libraries, electronic resources (e.g., the gen- eral web or specialized online databases), and human experts affect the strategic decisions searchers make at the work-context level.\rSystems that support work-context tasks may provide special- ized functions or support strategies for the work domain. Certain working domains, such as medicine, law, and even academia, have well- established work-context tasks and procedures for information seeking in support of those tasks. For example, in the medical field, studies have examined how physicians and healthcare searchers search for infor- mation [22, 61]. Consequently, services can provide mechanisms, for example, that identify prior art or key documents on a certain topic to support the work-context task of writing a paper. To support the learning of work-context processes, search systems may provide access\r\n20 A Model of Search\rto tutorials or communities of practice related to the work task. They may also provide domain-specific frameworks for making sense of, inte- grating, and synthesizing information. For example, a system to sup- port the development of teaching materials based on an oral history collection may provide a lesson plan tool that supports organizing and annotating retrieved information in a lesson plan format with links from items in the lesson plan back to retrieved elements of the collection.\rEvaluation of systems in a work context usually focuses on the achievement of users in realistic scenarios, such as the grading of school or college essays, or, in the extreme, the success of organizations at procuring academic funding.\r2.5 Summary\rIn the previous subsections, we have described the three key levels of searching context: information retrieval, information seeking, and work contexts. The last of these is also surrounded and influenced by the environment and social context that the work task is being carried out within. Each of these contexts provides a useful lens for understand- ing the benefits and contributions of novel user interfaces for search. When a new technique is designed, built, and tested, the work is usually motivated by a problematic scenario and a hypothesis that it can be overcome. This scenario might be carefully defined and IR focused, such that users cannot easily express a particular query or find the correct result. Similarly, the scenario and hypothesis may be much broader, as to support a work task like planning a vacation or researching for a report. In the following sections, we discuss the design and evaluation of recent interface contributions, by considering how they have been designed and tested to support searchers at different levels of context: IR, IS, and WC.\r\n3\rSurvey of Search Systems\rThe aim of this section is to review many of the search visualization and exploration techniques that have developed and studied. Further, the aim is to demonstrate the diversity and range of available techniques, rather than produce a complete catalog like some much larger sur- veys [72]. The techniques described in this section, which are classified in the taxonomy in Section 5, are presented according to the following structure. First, techniques that have made use of the advantages of enriched metadata are discussed in Section 3.1, which has afforded a change in the way that searchers can interact and control the presenta- tion of results. Second, Section 3.2 describes the varying approaches to directly organizing and presenting results. Finally, Section 3.3 addresses some alternative functionality that has enhanced visualizations, such as animation and use of alternative senses. Each technique described below is briefly discussed with the contexts of search described in Section 2. Further, while here many techniques are listed, the tax- onomy described in Section 5 provides the means to compare their aims, strengths, and weaknesses, in terms of the context of information seeking supported and the way they have been evaluated, according to Section 4.\r21\r\n22 Survey of Search Systems\r3.1 Adding Classifications\rOne of the main streams of research for enhancing search environments has been to use annotations, or classifications, to the documents or collections. Two challenges in adding classifications are the increasing scale of collections and the associated cost of annotating each docu- ment. We review four common approaches to adding classification to collections and overcoming the challenges: hierarchical classifications, faceted classifications, automated clustering, and social classifications.\r3.1.1 Hierarchical Classifications\rOne early research project, the SuperBook interface, showed the ben- efits of using categories in a document collection, by organizing search results within a book according to the text’s table of contents; here the book is the collection and the pages of the book are the documents. An evaluation found that it expedited simple IR tasks and improved accuracy, both by 25% [55]. This categorization approach has been used in many cases and has shown success in both fixed and uncontrolled collections, however, the usual approach for the latter is to model the first and allow document owners to assign their own documents into the hierarchy.\rAn example of a fixed and managed data set may be the genre- based classification of music in Amazon’s1 online music store, where every CD is assigned to one or more categories of music, including Pop, Rock, and Indie. Here there is incentive for Amazon, as the collection owner, to annotate the collection in this way to make it easier for their clients to find the music they want and, subsequently, encourage sales. Allen [5] investigated two such digital library interfaces, the Dewey Decimal System and the ACM Computer Reviews system, and showed that both used hierarchical classification effectively for organizing the collections.\rIn the same paper, Allen discusses the potential for use of Internet-wide information samples. Examples of hierarchically classi- fied structures for the Web, as an ever increasing and unmanaged set\r1 http://www.amazon.com.\r\n3.1 Adding Classifications 23\rof documents, are Google Directory2 and Yahoo Directory.3 In both of these directories, the individuals that own or manage documents on the Web can submit their websites for inclusion under different parts of the directory. This approach has been popular in the past but has suffered where document owners do not submit their websites for inclusion.\rIn an attempt to remove the annotation cost and limitation of clas- sification systems, Kules et al., [102] took a simple result set of federal agency/department reports, by mapping a set of URL prefixes to a known finite list. This approach took a known pattern from the data and used it to effectively categorize search results by government agency and department. In a more advanced approach, Chen and Dumais [36] showed that machine learning techniques can be applied to known cat- egorized data items to automatically assign new web documents into categories. Using the documents that already exist in the LookSmart4 directory as a training-set, the machine-learning algorithm success- fully and automatically categorized the results of a Yahoo search. The Support Vector Machine (SVM) algorithm [87] achieved 70% accu- racy with the categorization provided by human participants, where the remaining 30% included partially matching annotations. The user study showed a strong preference for the added categorization provided by the process. Other approaches to automation exist and are mainly described in the automatic clustering section below.\rThe benefits of applying hierarchical categorizations have been proven numerous times in research. For question answering tasks, Drori and Alon have shown that search results augmented with category labels produced faster performance and were preferred over results without category labels [49]. Dumais et al. [50] also studied the effect of grouping search results by a two-level category hierarchy and found that grouping by a well-defined classification speeds user retrieval of documents.\rIn a similar approach to the book-search mentioned above, the Cha-Cha system organizes intranet search results by an automati- cally generated website overview (Figure 3.1). It reflects the underlying\r2 http://directory.google.com. 3 http://dir.yahoo.com.\r4 http://www.looksmart.com.\r\n24 Survey of Search Systems\rFig. 3.1 The Cha-Cha system organized intranet search results by an automatically gener- ated website overview.\rstructure of the Website, using the shortest path from the root to each document to dynamically generate a hierarchy for search results. Pre- liminary evaluations were mixed, but promising, particularly for what users considered “hard-to-find information” [38]. The WebTOC system (Figure 3.2) provides a table of contents visualization that supports search within a website, although no evaluation of its search capability has been reported [128]. WebTOC displays an expandable/collapsible outliner (similar to a tree widget), with embedded colored histograms showing quantitative variables such as size or number of documents under the branch.\r\n3.1 Adding Classifications 25\rFig. 3.2 The WebTOC system provides a table of contents visualization that supports search within a website.\rHierarchical classifications, as demonstrated by the examples dis- cussed above, are largely designed to organize results into groups. While this allows users to perform some additional browsing tactics [12], such as going to a parent or child areas of the classification, the main ben- efits have been studied at the IR level. The studies performed by Allen [5] and Drori and Alon [49] show users performing faster in basic information retrieval tasks, without loss of accuracy. Faceted clas- sifications discussed below, however, extend the notion of hierarchi- cal classifications to support additional information-seeking behavior. Consequently, studies of faceted classification systems have more often focused on information-seeking behavior.\r3.1.2 Faceted Classifications\rAnother approach that helps users find documents based on multiple orthogonal categorizations, such as thematic, temporal, and\r\n26 Survey of Search Systems\rgeographic, builds on the traditional library science method called faceted classification. Restaurants, for example, can be classified by facets such as by location, price, food type, customer ratings, and size, with around 2–20 attribute values per facet (although some facets have many more values). Early designs, one of which was called query pre- views, also showed the number of documents having each attribute value. Query previews were updated with new values so as to pre- vent users from submitting searches that would produce zero results [48, 135, 165]. Faceted classification allows users to apply relevant con- straints on their search in the context of the current problem and exist- ing knowledge. For example, if users know their own budget and a minimum specification required for a computer, then they can apply constraints in these facets separately, and vary them individually (to check, for example, the effect of increasing the budget slightly). After applying their constraints they can then see all the relevant computers and possibly choose between them based upon the facets that remain unused.\rMany systems have applied this approach in some way, where a single classification system has not been expressive enough for the doc- uments in a collection. For example, before prototyping a faceted sys- tem,5 eBay already allowed users to apply constraints such as price, color, and size; the available constraints depended, of course, on the type of object being sought. Whereas eBay indexes its own auctions, websites such as shopping.com and Google Product Search6 provide a faceted search over many shopping sites from the whole web.\rFlamenco7 (Figure 3.3) is a clear example of the features provided by faceted search using multiple hierarchical facets. Providing interfaces to fixed collections, including art, architecture, and tobacco documents, Flamenco presents faceted hierarchies to produce menus of choices for navigational searching [188]. A selection made in any facet is added to a list of constraints that make it clear to users what is forming the list of results that are being shown.\r5 http://express.ebay.com.\r6 http://google.com/products.\r7 http://flamenco.berkeley.edu/.\r\n3.1 Adding Classifications 27\rFig. 3.3 The Flamenco interface permits users to navigate by selecting from multiple facets. In this example, the displayed images have been filtered by specifying values in two facets (Materials and Structure Types). The matching images are grouped by subcategories of the Materials facet’s selected Building Materials category.\rA usability study compared the Flamenco interface to a keyword search interface for an art and architecture collection for structured and open-ended exploratory tasks [188]. With Flamenco, users were more successful at finding relevant images (for the structured tasks) and reported higher subjective measures (for both the structured and exploratory tasks). The exploratory tasks were evaluated using subjec- tive measures, because there was no (single) correct answer and the goal was not necessarily to optimize a quantitative measure such as task duration.\rThe success seen by Flamenco, having provided faceted classifica- tions to assist search, has been used in many commercial and aca- demic projects. Endeca8 is a commercial company that provides faceted\r8 http://www.endeca.com.\r\n28 Survey of Search Systems\rFig. 3.4 epicurious is a commercial recipe website that uses Flamenco style faceted search in conjunction with keyword search.\rsearch, branded as Guided Navigation, to large businesses including Wal-Mart and IBM. epicurious, shown in Figure 3.4, provides both faceted and keyword search over food recipes in a style that is simi- lar to Flamenco’s experience. The lead researcher of Flamenco, Marti Hearst, refers to epicurious as a good example of faceted browsing in commercial conditions [73].\rHuynh et al. [81] has developed Exhibit9 (Figure 3.5), a faceted search system that is similar to flamenco in many ways, but has some significant developments. One key advance is that, where a selection in Flamenco filters all the facets, a selection in Exhibit filters all the other facets and leaves the facet with the selection unchanged. This provides two benefits: first, users can easily change their selection and second, users can make multiple selections in one facet. This allows users to see, for example, the union of all the red and blue clothes, rather than just red or just blue. This support for multiple selection within a single facet has been recently added to ebay.com, but remains unavailable in services such as Google Product Search.\r9 http://simile.mit.edu/exhibit/.\r\n3.1 Adding Classifications 29\rFig. 3.5 The Exhibit faceted search interface takes a slightly different approach, only filter- ing facets without a selection, so that previous selections can be see in the context of the options at the time.\rThe Relation Browser10 [192], shown in Figure 3.6, takes another approach to faceted search. One notable difference is that multiple selections lead to their intersection of results being displayed. Another feature that the Relation Browser provides is a preview of the affect of clicking has on other facets. Graphical representations behind each item in each facet show how many documents can be found by selecting it. When users hover over any item in any facet, the bar in the graphi- cal representations is shortened to indicate how many documents will remain under each annotation should the users make the selection. This technique revives the query preview strategy, which is a helpful alterna- tive to the simple numeric volume indicators [181] that are included in most classification-based systems. Aside from the graphical representa- tion, the preview of the affect by simply hovering (or “brushing”) over the item is a technique that is being included in many new projects. A new version of the Relation Browser is now in development [32].\r10 http://idl.ils.unc.edu/rave/.\r\n30 Survey of Search Systems\rFig. 3.6 The Relation Browser interface provides consistent facets, where the list of values is not filtered by selections. Instead, users can see (and preview by simply hovering) the reduction in files associated with each facet-value with the bar-chart style visualizations.\rmSpace11 [148], shown in Figure 3.7, represents yet another type of faceted search: a column-faceted interface. Like iTunes, mSpace presents facets in a left-to-right set of columns. Each column is fully populated so that users can still make a selection in any facet, but then only the columns to the right filter. This allows the facets to represent additional information that would otherwise be lost in faceted search. In their classical music example, where the facets are Era, Composer, Arrangement, and Piece (from left to right), if users select a Composer, they see a filtered list of the arrangements (s)he used and then a list of the pieces (s)he composed. When users make a second selection in the arrangement, other forms of faceted search would remove all the composers that did not use that arrangement. In mSpace, users are still\r11 http://mspace.fm.\r\n3.1 Adding Classifications 31\rFig. 3.7 The mSpace faceted column browser provides facets as columns that filter from left to right, as in iTunes. The facets, however, can be rearranged so that different parent-child relationships can be visualized spatially.\rable to see all the arrangements that the selected composer used, and all the pieces of the selected arrangement.\rThe functionality of mSpace means that there is a type of informa- tion that is not conveyed by mSpace but seen in other forms of faceted search: the Era of the selected composer. This missing information is not a problem in mSpace, unlike other column-faceted browsers like iTunes, because the related items in facets to the left of a selection are highlighted. This allows users to see which era the selected composer is in, whilst still allowing them to get the added facts provided. The effect of this leftward highlighting, named Backward Highlighting [179] is that users incidentally discover more facts about the structure of a collection, which can make search easier for future searches in the same domain [183].\rFinally, given the importance placed on the direction and order of facets in columns, mSpace allows users to reorder, remove, and\r\n32 Survey of Search Systems\rsupplement the facets shown, using any of the facets that are available from the collection. This allows users to say that they would rather know all the composers that used a given arrangement than all of the arrangements used by a given composer. Another unique aspect of mSpace is that it assumes that a selection that is different to a previous selection in a facet is a change of selection and not a multi- ple selection. Consequently, users can quickly compare the difference between two items in a facet. The default assumption that users are changing their selection supports the concept of answering subjunctive questions about the collection [113], which means simply to compare the outcomes of multiple actions.\rA recent longitudinal study of mSpace [182] has indicated that this more complex form of faceted search is easy to learn and is thereafter perceived as a powerful system, receiving positive subjective views. A log analysis showed that 50% of participants used facets in their first visit to the site and 90% in their second visit. Over the whole month- long period, there were more interactions with facets than individual keyword searches. Further, given that facets allow users to produce complicated queries than basic keyword searches, faceted searches rep- resented two times the number of Boolean searches and three times the number of advanced searches.\rEach of the faceted classification examples so far has been on fixed collections, but some research into faceted browsing has been looking at unknown and un-bounded document sets like the Web. The SERVICE12 (Search Result Visualization and Interactive Categorized Exploration) search system couples the typical ranked list of web search results list with automatically generated facets [106] (Figure 3.8). Clicking on a category filters (or narrows) the displayed results to just the pages within that category. Moving the pointer over a category highlights the visible search results in that category in yellow. Moving the pointer over a result highlights all the categories in the overview that contain the result.\rThe facets are automatically generated by applying fast-feature clas- sifiers [105] over the top 100 results of a Google query, and organized\r12 http://www.cs.umd.edu/hcil/categorizedovervew.\r\n3.1 Adding Classifications 33\rFig. 3.8 Unlike most faceted systems, which work on known collections of documents, the SERVICE web search interface classifies un-bounded web collections in multiple facets [106].\rthem into known possible categories drawn from the Open Directory Project (ODP) and a database of US Government websites13: Topic, Geography, and US Government. A similar project: Dyna-Cat [136], shown in Figure 3.9, also automatically produced facets for sets of search results, showed that not only was there improvements in objec- tive and subjective measures, that users were 50% faster in fact-finding tasks using Dyna-cat over typical ranked list keyword search interfaces.\rNorthern Light,14 a commercial search service, provides a simi- lar capability by grouping results in their Custom Search Folders.\r13 http://www.lib.lsu.edu/gov/tree. 14 http://www.northernlight.com.\r\n34 Survey of Search Systems\rFig. 3.9 The Dyna-Cat search interface automatically classifies search results within un-bounded document collations.\rExalead15 is another project that successfully organizes search results according to categories drawn the Open Directory Project, and presents them along side search results in a publicly available web search engine. PunchStock search, shown in Figure 3.10, presents a faceted view of personal image collections. The NCSU library, shown in Figure 3.11 also uses facets to enhance the search if their collection.\rThe faceted search interfaces described so far each have a space allo- cated to the presentation of facets, such as down one of the sides, across the top, or even along the bottom as in Google’s product search.16 A recently proposed faceted search interface, called FacetPatch, embeds faceted options into the existing layouts of existing website [120]. Hover- ing over an attribute of a camera, for example, converts the attribute’s value into a drop-down list from which users can select an alternative. Users can move, therefore, directly from result to result, by changing the brand or altering the pixel-count that they desire.\r15 http://exalead.com.\r16 http://www.google.com/products/.\r\n3.1 Adding Classifications 35\rFig. 3.10 The PunchStock photo search interface provides categorized overviews of photo search results.\rWhile some of the studies discussed above focus on typically IR level metrics, like speed and accuracy of simple search tasks, there are notable exceptions. Flamenco, the Relation Browser, and mSpace have each been studied in more exploratory contexts, where the task is often more than to simply find a search result. The study by Yee et al. [188], sets users exploratory tasks that were not measured by speed or accu- racy, as depth of exploration and amount of content covered could be considered more important than how fast and perhaps under-researched an answer is. This type of IS level analysis was also performed by Capra et al. [33]. Wilson et al [179] also studied the amount of incidental infor- mation (information not part of the core task) that could be remem- bered by participants in a study. The same team took this further by studying, over time, users of an mSpace interface providing access to a news footage archive [182]. Participants used mSpace, which was being logged, for their own work-goals over a four-week period, and engaged in periodic communication with the evaluating team. We can see from these studies that the types of exploring and browsing facilitated by faceted browsers have been studied for their support of IS and even WC levels of searching behavior.\r\n36 Survey of Search Systems\rFig. 3.11 The NCSU library catalog provides categorized overviews of search results using subject headings, format, and library location, provided by a commercial enterprise search vendor: Endeca.\r3.1.3 Automatic Clustering\rWhere collections have been large or unmanaged, faceted classifications have been less successfully applied. The examples where it has been\r\n3.1 Adding Classifications 37\rused above provide generic web-oriented facets, and often the facets that would be important depend on the query or subject of the users’ varying goals. Another approach is to automatically identify attributes of a collection or result set that are important, rather than explicitly producing labeled annotations. This approach is called clustering, and has shown success under popular information retrieval metrics such as precision and recall [74, 118, 189, 191] or task completion time [169]\rA one-level clustered overview was found helpful when the search engine failed to place desirable web pages high in the ranked results, possibly due to imprecise queries [89]. Clusty17 uses the cluster- ing technique to produce an expandable overview of labeled clusters (Figure 3.12). The benefits of clustering include domain independence, scalability, and the potential to capture meaningful themes within a set of documents, although results can be highly variable [71].\rGenerating meaningful groups and effective labels, however, is a recognized problem [139] and where possible (usually, but not exclu- sively in fixed or managed collections) having an information archi- tect design optimal annotations or labels, will provide a better search interface [162]. Stocia and Hearst extracted a category hierarchy from\rFig. 3.12 The Clusty metasearch engine uses automated clustering to produce an expand- able overview of labeled clusters.\r17 http://www.clusty.com.\r\n38 Survey of Search Systems\rWordNet [122] using keywords from the document collection. They propose that this can be manually adjusted by an information archi- tect. Similarly, Efron et al. [54] investigated the use of semi-automated methods, combining k-means clustering, and statistical classification techniques to generate a set of categories that span the concepts of the Bureau of Labor Statistics web pages and assign all to these categories. They found that concept learning based on human-supplied keywords performed better than methods using the title or full-text.\rAutomated classifications have, in large, been studied separated from the three levels of search, but have often compared the accuracy of the automatic classification with human classifiers. The studies by Stocia and Hearst [162] and Rivadeneria and Bederson [139] are exam- ples, however, which show that automatic classifications can still lead to improved user performance in IR level searching activities.\r3.1.4 Social Classifications\rRecent developments in web technologies, such as web2.0, have led to a rise in social classifications such as bookmarks, tagging, and collab- orative feedback ratings. For example, the Yoople search engine allows users to explicitly move the search results up or down to provide rel- evance feedback into the weightings used in future searches. Allowing users to reorder results has recently been supported for Google users with accounts, but is designed to improve personalization of search results. It remains unclear as to whether Google will use this rele- vance feedback to reorder results. Little research, however, has explic- itly proven the benefits of social classification schemes, even though the notion has become increasingly popular on the Web. Some research, however, is emerging: Millen et al. [121] have investigated the use of social bookmarking in enterprise search software and an eight-week field trial has shown positive results.\rIn Yoople18 (Figure 3.13) mentioned above, the classification produced is only numeric (document ranking order) and affects the background search algorithms. Google has recently introduced a similar\r18 http://www.yoople.net/.\r\n3.1 Adding Classifications 39\rFig. 3.13 Yoople Search, where users can drag items in the results list to indicate where they think a result should appear.\rpersonalization scheme called Search Wiki.19 While Google’s Search Wiki allows users to simply promote or remove results, Wikia Search20 (Figure 3.14) allows users to be more specific and rate results out of five. Both Google’s Search Wiki and Wikia Search allow users to directly influence search rankings in a similar way to Yoople. Further, however, they both also allow users to make annotations on results.\rThe notion of social tagging has allowed communities of people to develop flat classification schemes for collections. Flickr,21 a photo archiving website, depends almost entirely, although photo names and descriptions can be included, on user tagging to return images that relate to a keyword search. A negative side to such tagging classifica- tion schemes is that they are hard to present to users. Therefore, they are usually only used to aid keyword search rather than to help users interactively browse through documents, like in some of the faceted and category-based systems listed above. A popular example of presenting a flat set of tags to users has been to foreground popular tags in a tag cloud (Figure 3.15). Although little research has been performed on\r19 http://googleblog.blogspot.com/2008/11/searchwiki-make-search-your-own.html. 20 http://search.wikia.com/.\r21 http://www.flickr.com.\r\n40 Survey of Search Systems\rFig. 3.14 Wikia Search allows users to explicitly rate results out of five to directly influence search ranking.\rFig. 3.15 A tag cloud taken from Flickr website, included in [65].\r\n3.2 Result Organization 41\rFig. 3.16 The MrTaggy interface allows users to include or exclude tags from their search instead of using keywords.\rtag clouds, there is a growing consensus that they are more valuable for users who are making sense of information, rather than for finding specific information [157].\rSome systems are trying to take the benefits of tagging to produce what are known as folksonomies [80] or community-driven classifica- tion systems. Wu et al. [186] present some design prototypes that over- come some of the challenges in converting flat tags into a structure classification system. One approach, used by the MrTaggy interface shown in Figure 3.16, allows users to perform searches based entirely on community-generated tags by including or excluding tags from a list of related tags [90]. Searches are initiated with a pair of selections from two tag clouds: one containing adjectives and the other containing nouns and other objects.\r3.2 Result Organization\rAt the opposite end to supporting users in better defining their own search query, which has been discussed above, is the presentation of the result sets and items that will help users to identify the specific result item(s) that help them achieve their goals. There is ample research that specifically looks at the best way to present a single result in a linear set, which is perhaps best represented by the view currently provided by Google: name, text snippet, and web address.\r\n42 Survey of Search Systems\rIn the following subsections, however, we discuss the research that looks beyond a linear results list and visualizes a result set to help users find the specific results they are looking for.\r3.2.1 Result Lists\rThe most common method to show a result set, seen in most web search engines, is to provide a simple list of results. The order that the result items are listed is determined by some metric, usually based on how relevant the result is to the search terms used. The importance of constructing this algorithm well has been motivated many times in research where users regularly enter single-term ambiguous queries [171], and view only a few results [82] and rarely stray past the first page of results [17]. The accuracy and efficiency of such algorithms, however, has been research for many years in the information retrieval space, and is not covered in the scope of this report; Becks et al. [16] present a good discussion of result ordering, including occasions where ranked relevance may not be the most appropriate ordering.\rThe representation of each result has also received much research and is especially important when research has shown that the accep- tance of search systems has been significantly reduced by unclear or confusing representations.22 In Google, each search result has name (also a link to the finished document), a sample of text from the doc- ument, a URL for the document, and the size of the document. Addi- tional information, such as previous visit dates and counts, can be added if a user account is available. Work by Chen et al. [37] investi- gated the structure of result representations and provides a framework for considering the presentation of search results. In one of the more sig- nificant publications in this area, White et al. [175] showed that best objective results occurred when the text sample in a representation included the query terms used in the original search. This allows users to see the context of their query in each result item, so that they can best judge its value in viewing the whole document. This was backed up by Drori and Alon [49], who showed objective and subjective benefits\r22 http://www.antarctica.net.\r\n3.2 Result Organization 43\rfor both query-relevant snippets and associated categories. Microsoft’s Bing, however, includes an exception to this keyword-in-context rule for text snippets, providing the first whole paragraph of a Wikipedia page, which usually provides the best short overview for the topic being covered. This exception to the basic search result layout has been fur- thered by the now customizable search results provided by Yahoo’s Search Monkey,23 which allows users to apply templates for different popular result types, such as Wikipedia results, IMDB results, facebook results, and so on.\rOther search engines, including Ask Jeeves24 and Exalead25 (Fig- ure 3.17), also include thumbnails of the document represented by each result. Research into the benefits of page thumbnails [185] has shown most advantage when users are returning to a previous search to find a result from the previous session. With these conditional objective ben- efits, however, subjective results have been positive. Further, Teevan et al. [166] have shown that more abstract images, which integrate\rFig. 3.17 Exalead search provides images and categories with each search result, and facets to help users narrow the results.\r23 http://www.yahoo.com/searchmonkey. 24 http://www.ask.com.\r25 http://www.exalead.com.\r\n44 Survey of Search Systems\rcolor, titles, and key pictures from web pages, can support both search and re-visitation more evenly.\rThe advances made in the basic style of Results Lists have had lit- tle effect on the interaction that people have with search interfaces. Advances such as thumbnails have changed the representation, and have encouraged users to make better-educated judgments on results they see. Consequently, the contribution and subsequent study has mainly focused on IR level improvements of speed and accuracy.\r3.2.2 2D Result Representations\rWith the aim of providing useful overview visualizations of result sets to users, much research has aimed at taking information visualization techniques and applying them to search results. One example is self- organizing maps (SOMs), originally produced by Kohonen [97]. SOMs automatically produce a two-dimensional (2D) visualization of data items, according to the attributes that they share, using an unsuper- vised machine-learning algorithm. Consequently, the SOM approach applies well when visualizing automatically clustered documents.\rAu et al. [10] used SOMs that have been used to support exploration of a document space to search for patterns and gain overviews of avail- able documents and relationships between documents. Chen et al., [37] compared an SOM with the Yahoo! Entertainment category, for brows- ing and searching tasks (Figure 3.18). They found that recall improved when searchers were allowed to augment their queries with terms from a thesaurus generated via a clustering-based algorithm. Similar work [108, 110] has shown positive results in using SOMs to support search.\rAn alternative 2D organization on result sets is known as a treemap [60]. In a treemap, the clusters of documents are grouped using a space- filling algorithm. As shown in Figure 3.19, the 2D space is divided into the top-level items in the hierarchy produced by the clustering algorithm, where the number of search results found within the category dictates the size of each part. Each of these sections is divided into their children within the hierarchy, where the number of search results again dictates the size of the subparts. This process is repeated as necessary according to the hierarchy and space available. With this,\r\n3.2 Result Organization 45\rFig. 3.18 A SOM of search results, image taken from http://vizier.u-strasbg.fr. The results are shown as clusters of key topics relating to a query.\rusers can access categories at any level of the hierarchy and see the results associated with that cluster. Color coding, or use of increasing color density, is often used to indicate a secondary dimension over the data, so that popularity and price, for example, are both conveyed (Figure 3.20).\rUsing another information visualization alternative, Citiviz (Fig- ure 3.21) displays the clusters in search results using a hyperbolic tree [109] and a scatterplot [133]. The Technical Report Visualizer proto- type [64] allows users to browse a digital library by one of two user- selectable hierarchical classifications, also displayed as hyperbolic trees and coordinated with a detailed document list. Chen [35] implemented clustered and time sliced views of a research literature to display the evolving research fronts over time. Chen’s work concludes that there are a number of benefits in visualizing results like this, but that the metrics used to build and display the visualizations are important to the acceptance by users.\r\n46 Survey of Search Systems\rFig. 3.19 The HiveGroup built a demonstration shopping application that allows users to explore Amazon.com products. (image from http://www.cs.umd.edu/hcil/treemap- history/hive-birdwatching.jpg).\rSeveral web search (or metasearch) engines, including Grokker,26 Kartoo,27 and FirstStop WebSearch28 incorporate visualizations similar to treemaps and hyperbolic trees. Kartoo (Figure 3.23) produces a map of clusters that can be interactively explored using Flash animations. Grokker clusters documents into a hierarchy and produces an Euler diagram, a colored circle for each top-level cluster with sub-clusters nested recursively (Figure 3.22). Users explore the results by “drilling down” into clusters using a 2D zooming metaphor. It also provides several dynamic query controls for filtering results. Unlike treemaps, however, the circular form often leads to wasted space in the visualiza- tion. Further, this early version of the Grokker interface has been found\r26 http://www.grokker.com.\r27 http://www.kartoo.com.\r28 http://www.firststopwebsearch.com\r\n3.2 Result Organization 47\rFig.3.20 Thisoverviewofwebsearchresultsusesatreemap.Nestingisusedtoshowtopand second-level categories simultaneously. The top 200 results for the query “urban sprawl” have been categorized into a two-level government hierarchy, which is used to present a categorized overview on the left. The National Park Service has been selected to filter the results. The effect on the right side is to show just the three results from the Park Service [106].\rto compare poorly with textual alternatives [139]. The authors found that the textual interfaces were significantly preferred. The conclusions were that web search results lack “1). . . a natural spatial layout of the data; and 2). . . good small representations,” which makes designing effective visual representations of search results challenging. Grokker’s website is no longer available. Refined visual structures making better use of space and built around meaningful classifications and coloring may ameliorate this problem, as illustrated by promising interfaces like WebTOC, which uses a familiar hierarchy of classification and a coded– coded visualization of each site’s content. Many users appreciate the visual presentation and animated transitions, so designing them to be more effective could lead to increased user acceptance.\rIn support of the conclusions about cluster map visualizations from Rivadeneira and Bederson [139], early information visualization\r\n48 Survey of Search Systems\rFig. 3.21 The CitiViz search interface visualizes search results using scatterplots, hyperbolic trees, and stacked discs. The hyperbolic tree, stacked disks, and textual list on the left are all based on the ACM Computing Classification System. Although CitiViz is offline, its techniques have been replicated both online and offline many times.\rFig. 3.22 Grokker clusters documents into a hierarchy and produces an Euler diagram, a colored circle for each top-level cluster with sub-clusters nested recursively. Later versions removed the random coloring in favor of a more muted interface.\r\n3.2 Result Organization 49\rFig. 3.23 Kartoo generates a thematic map from the top dozen search results for a query, laying out small icons representing results onto the map.\rresearch by Tufte [167] states that graphical visualizations need to have clear and meaningful axes to be effective for users. Other approaches to 2D graphical representations have focused on representing results in meaningful and configurable grids, in the same vein as scatterplots, where each axis is a specific metric, such as time, location, theme, size, or format, etc. The GRiDL prototype (Figure 3.24) displays search result overviews in a matrix using two hierarchical categories [154]. The users can easily identify interesting results by cross-referencing the two dimensions. The List and Matrix Browsers provide similar function- ality [107]. The Scatterplot browser [53], also allows the users to see results in a visualization that is closer to a graph, where results are plotted on two configurable axes. Without specific regions plotted in the scatterplot visualization, it is harder for users to view subsets of results, but easier to identify single result items. Informal evaluations of these interfaces have been promising, although no extensive studies of the techniques have been published.\rAnother stream of research has focused on attributes of documents that can form dimensions that leverage common knowledge. For exam- ple, GeoVIBE (Figure 3.25) tightly coupled a geographic layout with an abstract layout of documents relative to defined points of interest\r\n50 Survey of Search Systems\rFig. 3.24 The GRiDL prototype displays search results within the ACM Digital Library along two axes. In this screenshot, documents are organized by ACM classification and publication year. Individual dots are colored by type of document.\r[31]. A similar, and familiar dimension is time, and research has also looked at visualizing documents and classifications on a timeline. An example timeline browser, Continuum [9], is shown in Figure 3.26.\rEach of the visualizations above has focused on visualizing a result set, so that users can identify areas of results that may be relevant to them. A different approach is to provide an overview visualization to help direct people to relevant items in linear result sets. Query term similarity allows searchers to explore the contribution that each query term makes to the relevance of each result by displaying the terms and results in a 2D or 3D space [27]. Similar work on Hotmaps (Figure 3.27), by Hoeber and Yang [77], provides an overview of 100 search results, displaying a grid, with query terms as the horizontal axis and the 100 results as the vertical axis. Users are able to click on parts of the grid with high color intensity to find more relevant documents, at the term level, that would not have been in the top 10 results returned by Google. The approach of query and result set visualizations has\r\n3.2 Result Organization 51\rFig. 3.25 GeoVIBE, a search interface using geography visualization for results.\rFig. 3.26 Continuum represents documents in nested categorizations on a timeline.\r\n52 Survey of Search Systems\rFig. 3.27 Hotmaps, a 2D visualization of how query terms relate to search results.\rshown significant search benefits for users, and augments, rather than visualizes, result sets.\rThe strength of the more successful 2D visualizations has been when the use of space is clear and meaningful to users. An alter- native approach, to computer-generated visualizations or clusters of documents, is to allow users to arrange documents under their own parameters. The TopicShop interface [8], shown in Figure 3.28, per- mits users to meaningfully arrange sites on a canvas, where the clusters are meaningful to the individual and, when returning to a document, spatial consistency makes it easier for users to remember where they placed the result. More recent research in this area has explored 3D spaces, and is described in the section below.\rSimilar to the ideas behind TopicShop, Furnas and Rauch [62] present a set of design principles for information spaces and instan- tiates them in the NaviQue workspace. In NaviQue, a single informa- tion surface is used to display queries, results, information structures, and ad hoc collections of documents. Users can initiate a query simply\r\n3.2 Result Organization 53\rFig. 3.28 The TopicShop Explorer interfaces combines a hierarchical set of topics with a user-controlled spatial layout of sites within each topic (shown here) or a detailed list of titles and attributes [8].\rby typing onto a blank section of the workspace. Results are clustered around the query text. Kerne et al. [95] furthered this work in software called combinFormation by considering the spatial layout of different multimedia types for constructing personal collections (Figure 3.29).\rThe contributions made in different 2D visualizations have largely supported users in IS level activities, allowing them to see and often manipulate clusters of documents by interacting with the two axes. In Hotmaps [77], for example, the 2D space allows users to choose alternative approaches to parsing the result lists, other than linearly processing every result. The matrix browser allows users to change the dimensions used for each axis, and alter their scale and granularity. These have been studied at the IR and IS levels, showing improved speed or accuracy. These methods have also been shown to help users find results that would not necessarily come up in the top 10 of a keyword search interface. Some work has focused on the users’ work context, for example. the TopicShop interface [8], has given control of\r\n54 Survey of Search Systems\rFig. 3.29 combinFormation automatically searches the Web for multimedia relating to a query term and displays them on a 2D plane. Users can drag and drop the items to make arrange their own subcollections.\rthe 2D space to users as an open canvas, and they are able to search and organize resources for their work tasks.\r3.2.3 3D Result Representations\rWhen presenting research into 3D visualizations, it is first important to consider research into the benefits and weakness of 3D over 2D. Although research has shown success for some situations, such as sim- ulations and 3D CAD/CAM design [152], research into the visualiza- tion of information has shown that a third dimension can inhibit users and make interfaces more confusing [138, 163]. Aspects of 3D repre- sentations, such as occlusion and cost of visualizing depth, must be considered [44]. Further, research by Modjeska [123] has shown that 25% of the population struggle with 3D visualizations displayed on a 2D device, such as a computer screen. Investigation by Sebrechts et al. [150] also showed that participants were significantly slower at using a 3D interface, unless they had significant computer skills. Considering these challenges, however, the research described below highlights some of the ideas that have been proposed for 3D visualizations.\r\n3.2 Result Organization 55\rFig. 3.30 Data mountain provides an inclined plane upon which web pages are organized (image from PDF).\rThe Data Mountain browser (Figure 3.30), like the TopicShop explorer described above, allows users to arrange documents on a plane, creating a 2 1/2 dimension view with perspective, reduced size for dis- tant items, and occlusion. In the Data Mountain, however, users have a 3D plane to arrange the documents [140]. Subsequent studies of the Data Mountain have shown to be unproductive [42]. Another approach that has been expanded to 3D environments is the hyperbolic tree [127].\rAs part of a discussion on applying 3D visualizations to web search results, Benford et al. [20] describe the VR-VIBE system (Figure 3.31). Each query is manually or automatically positioned in a 3D space. Doc- uments are positioned near the queries for which they are relevant. If a document is relevant to multiple queries, it is positioned between them. Each document’s overall relevance is shown by the size and shade of its representative icon.\r\n56 Survey of Search Systems\rFig. 3.31 VR-VIBE represents search results in a 3D environment, where results are dis- played in relative proximity to the keywords they match.\rCone Trees [142] were designed to display a hierarchy in a 3D view, using depth to increase the amount of the tree that is visible. Users are then able to rotate the tree using a smooth animation, although this was found confusing by a number of participants in an evaluation and user studies have not shown advantages [41]; supporting the concerns noted by Modjeska.\rFinally, to fully embrace 3D environments Bo ̈rner [27] used Latent Semantic Analysis and clustering to organize and display a set of doc- uments extracted from a digital library in a 3D space. A multi-modal, virtual reality interface, called the CAVE (Figure 3.32) enables users to explore the document collection; users control their movement through the CAVE with a special input device called a Wand. This require- ment for a special input device, however, makes it an unrealistic option for web environments where most users will have typical mouse and keyboard input.\rThe effect of adding a third dimension has not made any specific advances in the three levels of search compared to the 2D advances. The majority of the interfaces discussed above are, in fact, a 3D version\r\n3.3 Additional Functions 57\rFig. 3.32 Search results being visualized using the CAVE explorer (image from PDF).\rof a 2D visualization, including the hyperbolic tree and Data Moun- tain interfaces. The implementation and evaluation of 3D visualizations have been scarce due to the limited capability of most web browsers, and when tested at the three levels of search (IR, IS, and WC), 3D visualizations have often hindered, rather than supported, participants in their searching activities.\r3.3 Additional Functions\rThe previous two subsections have focused on: (1) coupling results with additional metadata and classifications, and (2) on providing alter- native or complementary representations of results. This sub-section, however, focuses on specific individual features that can work together with both classifications and alternative visualizations to enhance the usability of search interfaces.\r3.3.1 Previews\rOne of the challenges users face with classification systems is deciding which categories to select in order to find the documents they require.\r\n58 Survey of Search Systems\rThis heavily depends on the label chosen to represent a category, where designers have to find a careful balance between clarity and simplicity; where Nielsen’s heuristics [129] recommend clear layman language, it may not always be possible in some domains, at least with single terms or short phrases. If users do not recognize or understand an option, then they may not know how to proceed in their search, except through trial and error. Some users, however, simply abandon their search, at least with the search interface in question [147].\rThe Relation Browser previews the affect that a selection will have on the remaining items and facets, by temporarily reducing individual bar graph representations; selecting the item will make this change per- manent. This notion has been used in research into the mSpace browser [147] to help users choose between the terms presented in a facet. When users hover over an item in an mSpace column facet, they are presented with a multimedia preview cue, which provides an example of the docu- ments included within the category. A user evaluation showed that this preview cue significantly supported users in finding documents. Further research by Endeca [100] is investigating the ability to automatically summarize the result items and present users with a text description that is typical of the cluster.\rFor the three levels of search, presented in Section 2, previews contribute mainly toward IR and IS, by supporting users in making informed browsing and exploring decisions. This supports a number of additional search tactics [12], including the ability to weigh up a set of options, and tracing the metadata of the multimedia examples being previewed. schraefel et al. [147], however, evaluated mSpace’s multime- dia cues in a work-context scenario of purchasing music within a given budget.\r3.3.2 More Like This\rSearch engines typically provide users with a link by each result to see more results that are similar according to the server’s metrics. Although no specific research has been published, Endeca’s search interface (Fig- ure 3.33) allows users to express a similar desire to see related docu- ments, but allows them to express the dimension by which they want\r\n3.3 Additional Functions 59\rFig. 3.33 Basic Endeca search interface, a commercial enterprise search vendor, studied by Capra et al. [33].\rto see similarity. For example, they can choose to see more documents of a similar category, or size, or creation date.\rBy allowing users to follow un-anticipated paths during search, the simple notion of choosing to see more similar results to a particular result also supports a number of information tactics that are not sup- ported by standard keyword search interfaces [12], and so contributes mainly to the information-seeking level of Jarvelin and Ingwersen’s model of search. A study by Capra et al. [33] included the basic Endeca interface and, although this feature was not explicitly tested, the study involved both exploratory IS level and quick IR level tasks.\r3.3.3 Connections\rNetworks have been used for knowledge discovery tasks, displaying con- nections between related documents and between related literatures. Stepping Stones (Figure 3.34) visualizes search results for a pair of\r\n60 Survey of Search Systems\rFig. 3.34 The Stepping Stones interface visualizes the results of a pair of queries (1) as a graph (2) of topic nodes connected to the queries. Individual documents, shown in (3), cover portions of the path between the queries; image from [46].\rqueries, using a graph to show relationships between the two sets of results [46].\rSimilarly, Beale et al. [15] visualizes sequences of queries using a force-directed layout of node-link diagrams. Queries and their resulting documents are represented as nodes, with links between a query and its results. When a document is in the result set for multiple queries, it is linked to each query.\rIn some representations, single documents can appear in multiple collections or subcollections. When these different groups are arranged into regions that correspond to document attributes, often lines or col- ors are used to highlight related items. Such regions could be parts of a geographic map or, as in Figure 3.35 separates groups in a hierarchy of courts [91, 153]. Figure 3.35 shows a visualization where arcs that jump between the regions represent single items in different groups.\r\n3.3 Additional Functions 61\rFig. 3.35 This visualization arranges 287 court cases into three regions (Supreme Court, Circuit Court, and District Court). Within each region, cases are arranged chronologically. Of the 2032 citations, the selection box in the District region limits the display to just the citations from the three District court cases in 2001. This shows the greater importance of older Supreme Court opinions [153].\rOne challenge of representing links across multiple sets or dimen- sions is the limited number of methods to make connections. In Fig- ure 3.35, the arcs are in various colors. Such use of colors and arrows can be limited. For example, it may be hard to display how a single item in\r\n62 Survey of Search Systems\rthree different groups may be connected, or how an item relates to two parent levels of a hierarchy; such challenges become more important when hierarchical and faceted classification schemes are involved. In Continuum, relationships across multiple levels of hierarchy are shown by clustering and nesting results rather than keeping them in separate panes, see Figure 3.26.\rExpressing and exploring connections provides much higher-level search interaction than simple IR activities, as the users are going beyond parsing results to assess how results fit into the domain of information, and thus the other results around them.\r3.3.4 Animation\rAnimation on the Web is a debated area of research. Although lots of research has aimed at developing animation tools for the Web [59, 68], other research has investigated the limitations of technologies such as Flash [79]. Research by Robertson, Cameron, Czerwinski and Robbins, however, has shown that there are advantages of using animation to support users through transitions in an interface [141]. Further, recent research by Baudisch et al. [14] has presented a tool called Phosphor that highlights, with a fading period, any changes that occur in an interface. We can see examples of this sort of animated support on the Web through technologies such as AJAX [132]. Facebook, for example, allows users to post comments with AJAX, where the page does not refresh and both color and smooth spatial changes indicate to users that the interface is changing to accommodate their actions. Research into mSpace, which uses a lot of AJAX technology, also identified a similar emphasis on smooth transitions during discussions with participants [182].\rThe main emphasis on animation in web search visualization is on supporting users in comprehending changes that occur as a result of their actions. The Polyarchy browser [141], for example, helps users explore by animating the transition to alternative dimensions as the users browse across different attributes in the data set. Consequently, the advance provided by animation is mainly in the Information-seeking level of search, and has been mostly evaluated in kind.\r\n3.3.5 Semantic Zooming\rSemantic zooming is characterized by the progressive inclusion of addi- tional information, as opposed to simply enlarging information. In the design of NaviQue [62], users can zoom into clusters and into informa- tion structures. The semantic zooming replaces items with representa- tive icons as users zoom out, and replaces icons with the specific items when zooming in to show more detail. A history of all ad hoc collections is automatically maintained in one corner of the display. Users can drag and drop collections into a “pocket” in another corner so that they are immediately accessible, without need to pan or zoom the workspace.\rAs a form of animation, effective zooming on the Web is challenging and must be done to enhance a website and not affect its usability. In maintaining a typical search result list style, another option presented by research is a technique called WaveLens, which allows users to zoom in more on a single result, such that the information snippet expands to reveal more lines of text from the source document [131].\rThe contribution of semantic zooming supports users at IR and IS levels of search in that the technique allows users to investigate deeper into a topic as they “zoom” into it. The semantic zoom is considered a request for more detailed information on the item of focus, and is thus connected to more exploratory methods. Semantic zooming, however, can also simply reveal the answer to a quick keyword query, in which case it is related to selecting a web search result (as in the study of WaveLens) but without leaving the search engine.\r3.3.6 Alternative Inputs\rMultimedia can often be hard to describe with words, and querying over such collections requires that the documents be annotated. While some research is aimed at automatically increasing the amount of annotation of multimedia [25, 86], other approaches have examined the querying medium. Query-by-example [93, 144] is a strand of research, where users can submit audio to find related audio, images to find similar images, video to find similar video, etc. For example, Shazam29 is a\r29 http://www.shazam.com.\r3.3 Additional Functions 63\r\n64 Survey of Search Systems\rmobile phone service that users can ring while music is playing and responds by sending an SMS message with the name and artist of a song. Similarly, Retrievr30 is a service that allows users to construct a simple image with paint tools, and finds images from Flickr31 that match the approximate shapes and colors. Although some research has discussed practical applications for systems such as query-by-humming, to search for music [99], the challenge for the Web is providing a means of input for users. Where Google provides a keyword box and Retrievr provides a sketch box with paint tools, it can become difficult to allow audio input, without requiring users to make recordings and upload them. Similar problems arise for video-querying for video.\r3.4 Summary\rIn this Section, a range of diverse strategies to visualizing search results has been presented. Covering the use of enriched metadata (Section 3.1), multi-dimensional representations (Section 3.2) and visualization-enhancing techniques (e.g., animation in Section 3.3), a diverse range of approaches have been presented that try to support different contexts within the model of information-seeking described in Section 2. In the next section, we discuss the range of evaluation techniques that have been applied to these visualizations, in order to demonstrate their benefits for the different contexts of information seeking.\r30 http://labs.systemone.at/retrievr/. 31 http://www.flickr.com.\r\n4\rEvaluations of Search Systems\rThis section examines evaluation methods that have been used to assess different parts of information systems. Extensive reviews of information retrieval evaluations are provided by Hearst [72], which includes a dis- cussion of formative evaluation and usability testing, and Yang [187]. In this section, we provide a synthesized view of techniques as appropri- ate to the three-level model. We break this evaluation discussion down into the three levels included in the framework from Section 2. The different levels of information-seeking context each require multiple, different, and complementary modes of evaluation. A single study may incorporate multiple levels of evaluation. In Section 5, as we classify the search interfaces described above by visualization approach and context of information seeking support, we also include the styles of evaluation within the taxonomy.\r4.1 Information Retrieval Evaluations\rThe main concerns for much of the information retrieval community has been to assess the quality of indexing methods and the algorithms that match documents to the queries provided by users. The TREC conferences [70] have been arranged to evaluate document retrieval\r65\r\n66 Evaluations of Search Systems\rsystems for domains like the Web, spam, video, legal documents, and genomics; for collections with interesting or important characteristics such as very large collections; and for specialized tasks such as cross- language IR, filtering, finding novel documents, fact finding, and ques- tion answering.1 Important concepts for evaluations at this level include document relevance, precision/recall and related measures, and batch versus interactive evaluation.\rTo support the TREC evaluations, predefined collections of doc- uments, relevant queries and human relevance assessments, were pro- duced and used as benchmark standards across any studies. The shared platform of evaluation provided the opportunity to not only evalu- ate, but also compete for most improved retrieval times or retrieval accuracy within the community. The most commonly used measure- ments in TREC were precision and recall. Precision is concerned with returning only relevant results, whereas recall measures the number of relevant documents being returned. Typically returning more docu- ments means potentially sacrificing precision, and guaranteeing preci- sion means reducing recall. Common approaches examine precision at certain levels (precision @ N), average precision, or by using precision– recall (P–R) curves [85]. Yang [187] and Kobayashi and Takeda [96] provide extensive reviews of evaluations focusing on the techniques and evaluations used for information retrieval.\rWhen not testing the accuracy of results returned against a pre- defined corpus of documents, most IR studies, including many of the systems evaluated in the sections above, focus on simple measures of task performance such as speed for defining how well an interface sup- ports users in finding information. A simple task, of finding a specific document, or a fact within a document, is provided to users along with a basic scenario for context. If users are able to find that answer quickly with one system than another, then the novel system has pro- vided better support for search. This is a fairly accurate test for many basic search contexts. A basic fact-finding task represents many of the Web searches performed on search engines [156, 173], and so many studies are either based on this model or includes tasks of this nature.\r1 http://trec.nist.gov/tracks.html.\r\n4.2 Information-Seeking Evaluations 67 Table 4.1. A sample of measures used at the IR eval-\ruation level.\rMeasure\rPrecision, recall\rPrecision @ N\rMean average precision (MAP) Expected search length Average search length (ASL) Cumulated gain, discounted\rcumulated gain\rRelative Relevance, Ranked Half-Life Query length\rNumber of query refinements Number of unique queries\rNumber of operators in query Number of results viewed\rSearch time\rReference\r[24, 34, 40] [3]\r[3]\r[52]\r[112] [85]\r[26]\r[83, 156] [78, 83, 156] [156, 173] [156]\r[78, 83, 156] [173]\rA summary of commonly used IR-level measures, along with references to example evaluations that have used them, is shown in Table 4.1.\rIt is clear from the model presented in Section 2, however, that information retrieval tasks are part of a much larger view of search- ing behavior. While research, such as that provided by White and Drucker indicate that many web searches are simple fact-finding tasks, the natural conclusion is that there are some search sessions that are not simple lookup tasks [28, 173]. The TREC conferences have incor- porated the Interactive Track [51], the High Accuracy Retrieval of Documents (HARD) Track [3], and the Video Retrieval Evaluation (TRECVid) Track [158], which move beyond the batch-oriented evalua- tion of retrieval tasks by directly involving users conducting interactive searches in the evaluation process. However, the information needs in these tracks are still narrowly expressed in terms of documents to be retrieved, without reference to a higher-level information need. It is for this reason the methods of performing broader information-seeking evaluations is discussed in the next subsection.\r4.2 Information-Seeking Evaluations\rThe evaluations of information retrieval typically have different aims than the evaluations of information seeking [94]. Information retrieval\r\n68 Evaluations of Search Systems\revaluations have focused on system-oriented metrics, or the speed of simple, out of context, fact-finding tasks. Consequently, information retrieval studies have been criticized for a narrow conceptualization of the information need, relevance, and interaction [26]. Information- seeking research, therefore, focuses on evaluating systems for how they meet the needs of users. Important concepts at this level are the information need, the information-seeking context, information-seeking actions, tactics and strategies, and longitudinal changes, as are quan- titative and qualitative measures.\rTo understand how search interfaces support a wide range of information-seeking tactics, recent work by Wilson and schraefel [183] and Wilson et al. [180, 184] has proposed an evaluation framework to systematically assess support for a range of known search tactics and types of users, called the Search Interface Inspector.2 The framework first measures the functionality of the search system by the way it sup- ports known tactics and moves employed with information [11, 12]. The framework then uses a novel mapping to summarize the mea- sured support for the different user types [19], whose conditions vary on dimensions such as previous knowledge and intended use. This eval- uation framework was later refined and validated to show that it could accurately predict the results of user studies [178]. With the confidence provided by the validation, the framework can be used to identify weak- nesses in a system design or new function, so that it can be improved before user studies are carried out. Further, it can be used to inform the design of user studies, so that they accurately test the desire features.\rOne of the exploratory studies used to validate the framework produced by Wilson et al. was an information-seeking evaluation of faceted browsers [33]. The study used three types of tasks to evaluate the system, and performed a within participants study and a between participants study to get qualitative and quantitative results, respec- tively. The first type of task was a simple lookup task, which could be answered by using only one facet of the annotation. The second task type was a complex lookup, which involved multiple facets. The final task was exploratory, where users were asked to learn and produced a\r2 http://mspace.fm/sii/\r\n4.2 Information-Seeking Evaluations 69\rsummarized report about a certain topic. This third type of task is a good example of something that has not been included in Information Retrieval research, which, as mentioned above, has focused on match- ing documents to queries. Instead, by asking users to carry out learning tasks, we can assess the system for other types of information-seeking activities, such as comparison, synthesis, and summarization. As part of their discussion of higher-level problems encountered by users of infor- mation visualization systems, Amar and Stasko [7] discuss tasks that may be part of or require exploratory search.\rAnother contribution to information-seeking evaluations is on the discussion of time as a measurement by Capra et al. Although their tasks were timed, they suggest that time may not be a useful metric for exploratory tasks, as extended system use could mean that users have discovered increasing amounts of relevant information. In contrast to the information retrieval view that finding the answer quicker is more important, finishing an exploratory task early may indicate that a search system does not provide effective support for browsing. With this notion in mind, Kammerer et al. [90] conclude that the participants who used the MrTaggy interface (a) spent longer with their system, (b) had a higher cognitive load during search, and (c) produced better reports at the end of the tasks. In studies that aim to reduce cognitive load and reduce search time, however, there is usually only one correct answer. Conversely, during the MrTaggy experiment, the incentive was to produce a better report, and so a positive measure for the system was that it allows users to work harder.\rThe suitability of relevance in exploratory search conditions may also be in question for some information-seeking evaluations. In sys- tems that use faceted classifications, for example, each document with a particular annotation has an equal weighting and thus every docu- ment suggested as a result of selecting a particular part of the classi- fication will be equally relevant. Instead, Spink et al. [160, 161] have been designing a metric that tries to measure the progress of users in achieving their goal. Although designed for feedback to users, the rate of progress for similar tasks on different systems could be used to assess their support. Further, controlling for the amount of progress made by users in exploratory tasks would allow evaluators to once\r\n70 Evaluations of Search Systems\ragain consider reduced time and cognitive load as positive measures. For example, evaluators could measure the time spent, and cognitive load as they make a pre-determined amount of progress.\rKoshman [98] evaluated the VIBE (Visual Information Browsing Environment) prototype system, which graphically represents search results as geometric icons within one screen display. As part of under- standing approaches to information seeking, the researchers sought to differentiate expert and novice performance through the use of a quasi-experimental within-participants design (see Borlund [26] or Shneiderman and Plaisant [155] for discussions of information-seeking evaluation).\rHaving discussed search tasks and measurements that may be unique to or important for information seeking, carefully controlled user studies can still be performed to evaluate systems in terms of information seeking. Ka ̈ki [89] provides a good example of a study that employs a within-subjects design, balanced task sets, time limitations, pre-formulated queries, cached result pages and limiting access to result documents.\rEye-tracking techniques have been used to study information- seeking behaviors within web search interfaces and library catalogs [45, 66, 104, 111]. These studies examined specific elements that searchers look at, how often they looked at them, for how long, and in what order. They provide insight into the cognitive activities that searchers undertake and tactics used when examining search results. For example, Kules et al. [104] studied gaze behavior in a faceted library catalog and determined that for exploratory search tasks, participants spent about half as much time looking at facets as they spent looking at the individual search results. The study also suggested that partici- pants used the facets differently between the beginning and later stages of their searches. They conclude that the facets played an important role in the search process.\rFinally, the study approach that investigates user interaction with software over a long period of time, such as longitudinal studies or studies that are repeated periodically with the same participants, pro- vides a unique type of insight into realistic human behavior. One of the main arguments against the short task-oriented studies is the lack of\r\n4.3 Work-Context Evaluations 71\rTable 4.2. A sample of measures used at the IS evaluation level. Some measures may be similar to IR level measures, but are used here to evaluate the process of IS tasks.\rMeasure\rNumber of moves/actions to support a search tactic\rSubjective measures (e.g., confidence, usefulness, usability, satisfaction)\rTime on task\rTime to select document\rAccuracy (correctness) and/or errors in task\rresults\rNumber of results collected by user Number of search terms\rNumber of queries\rNumber of results viewed\rTime to learn\rSystem feature retention\rRank of selected document in results Usage counts of selected interface features Gaze-related measures, e.g., number of\rfixations, location of fixations, rank of results fixated on, fixation duration; scanpaths, scanpath length\rNumber of search result pages viewed\rReference [177, 184]\r[33, 98, 104, 106, 161]\r[33, 90, 98] [66, 89]\r[33, 98, 111]\r[89, 90, 111] [89, 159, 170] [106, 111, 160] [160, 66, 111] [98]\r[98]\r[89, 111, 106, 66] [89, 170]\r[45, 111, 104]\r[111]\rrealism. Further, the results of such user studies are often based on the participants’ first or early responses to new designs, compared to their familiarity with existing software. By studying interaction over time, we can begin to evaluate changes in search tactics and subjective views as users adopt and adapt to new interfaces [102, 170, 182]. A summary of commonly used IS-level measures, along with references to example evaluations that have used them, is shown in Table 4.2.\r4.3 Work-Context Evaluations\rEvaluating search systems and the work task level requires measuring the success of work-context style problems. For example, Allen [4] inves- tigated the interaction of spatial abilities with 2-D data representations. The work-context task given to participants was to read an article and then use the system to find a few good articles. At this level, many different information-seeking activities can be used, but ultimately the\r\n72 Evaluations of Search Systems\rsystem is assessed on its support for achieving the overall goal. As with evaluations at the information-seeking level, important concepts at this level include the information need, the information-seeking con- text, information-seeking actions, tactics, and strategies. At this level there is a stronger emphasis on domain-specific concepts. The quality of the work product also may be evaluated.\rOne of the challenges of evaluation, especially at the information seeking and work-context level, is that human participants interpret the tasks based on their own experiences and knowledge. There is a tension between the need to make results reliable and replicable and the need to make the task realistic for the participants. Borlund [26] advocated addressing this by incorporating participant-provided infor- mation needs into the experimental session along with a researcher- provided need. If the results for both tasks are consistent, the researcher can conclude that the researcher-provided task is realistic and thus reap the benefit of a realistic but tightly controlled task. Other research has used the previous actions of users to inform the realism of search tasks in system evaluations [56, 58]. Kules and Capra [103] propose a procedure for creating and validating tasks that exhibit exploratory characteristics, such as indicating uncertainty of the information need, the need for discovery, or being an unfamiliar domain for the searcher. The goal is to develop work tasks that are appropriate for the domain and system being evaluated and which are also comparable within and across studies.\rAside from the individuality of work task understanding, work- context tasks vary dramatically depending on the domain. Ja ̈rvelin and Ingwersen [84] argue that the main challenge ahead for evaluat- ing systems at the work-context level is that research needs to explic- itly integrate context in terms of the high-level work task, the specific information-seeking task and the systems context. In particular, they note that the Web “is not a single coherent unit but appears quite different for different actors, tasks, and domains.” Some research has addressed domain-specific work-context evaluations. One comparative study attempted to rigorously evaluate work tasks by defining domain- specific measures of work product quality (assessing motivation, com- pleteness, correctness, coherence, redundancy, and argument structure)\r\n4.3 Work-Context Evaluations 73\rin addition to measuring efficiency, effectiveness, and precision of the search [88]. Subjects used three information retrieval systems to develop lesson materials about gorillas using a biological database. The goal was highly structured to create a lesson using the provided lesson template that prescribed topics to cover. Results showed significant system dif- ferences in efficiency and effectiveness but results for quality measures were mixed. They were not significant overall; however, significant dif- ferences were noted between individual sections of the lesson. Kules and Shneiderman [106] similarly evaluated the quality of the work product (ideas for newspaper articles) but found no significant differences. These two studies highlight the challenges of evaluating the contribution of a system to a high-level work task. Evaluators face the dilemma of trying to assess a system under conditions that are more realistic than at the IR or even IS level, while still effectively understanding the contribu- tion that the system makes when it is only one of many factors. The use of longitudinal studies may help to overcome this challenge.\rQu and Furnas [137] used two variations of a sensemaking work task to motivate information seeking and topic organization tasks to study how people structure developing knowledge during an exploratory search. Study participants were given 50 minutes to collect and orga- nize information about an unfamiliar topic and produce an outline of an oral presentation. The researchers examined sequences of actions that participants took, including issuing queries, bookmarking pages, and creating folders. They interpreted different sequences as indications\rTable 4.3. A sample of measures used in stud- ies focused work-context evaluation level. Some measures may also be used in the IS but provide insight into the process of achieving work-con- text tasks.\rMeasure\rReference\rrecords printed [4] items viewed or printed [4] items selected [88] relevant items selected [88]\r[88, 106]\rNumber of\rNumber of\rNumber of\rNumber of\rQuality of results\rRank of relevant documents [56] Participant assessed relevance [56] Sequences of actions [137]\r\n74 Evaluations of Search Systems\rof different aspects of the sensemaking task. A summary of commonly used WC-level measures, along with references to example evaluations that have used them, is shown in Table 4.3.\r4.4 Summary\rEvaluations are most mature and rigorous at the IR level. At the IS level, evaluations are developing, with several good examples of stud- ies that balance rigor and realism. The work level is where the biggest challenges remain. It is also where improvements in evaluation can con- tribute the most. Certainly, evaluations at this level need to reflect the nature of the domain and the tasks and information needs characteristic of that domain. Methodologies that enhance rigor and comparability without sacrificing validity are necessary. Longitudinal methods are likely to be useful. More generally, studies are likely to benefit from an emphasis on fewer, but deeper, tasks, reflecting the more complex nature of work tasks.\r\n5\rTaxonomy of Search Visualizations\rSo far in the monograph, we have discussed (1) a model of search that captures multiple levels of context, (2) many novel advances in search result visualization, and (3) the way in which such advances have been evaluated. Below we present and discuss a taxonomy of these search advances, according to their support for search, amount of evaluation, and prevalence on the Web.\r5.1 The Taxonomy\rWe present a taxonomy of the search visualization advances dis- cussed in this monograph (Table 5.1). The purpose of the taxonomy is to capture (1) how these advances are designed to support search, (2) to what extent they have been evaluated, and (3) how preva- lent they are on the Web. The next subsection discusses the value of this information to academics and designers of future web search interfaces.\rThe first facet, shown in the first set of three columns, is the level of search context, as according to the model used throughout the mono- graph, that is being supported by the interface feature. Although each\r75\r\n76 Taxonomy of Search Visualizations\rTable 5.1. Interface advances discussed within the monograph above, categorized by three facets: Level of Search Support, Depth of Evaluation, and Prevalence on the Web.\rInformation Interface Advances Retrieval\rInformation Seeking\rWork Context\rInitial Testing\rMultiple User Studies\rDiverse Forms of Study\rDemonstrator Only\rInstances Available Used\r3.1.1 Hierarchical Classifications\r1. Standard Web Directories\rO O\rO\rO\r2. Cha-Cha 3. WebTOC\rO\rO O\r3.1.2 Faceted Classifications\rRB++\r8. SERVICE 9. Dyna-Cat 10. FacetPatch\rO O O\rO O\rO\rO O O\r(Contextual facets)\r3.1.3 Automatic Clustering\r11. Clusty\rO O\rO\rO O\r3.1.4 Social Classifications\r12. Social Rankings 13. Tag Clouds\r14. MrTaggy\rO O\rO O\rO\r3.2.1 Result Lists\r15. Page Thumbnails\r16. Keywords in Context 17. Larger Result\rO O O\rO O\rO\rSnippets\rLevel of Search Support\rDepth of Evaluation\rPrevalence Online Many\rO\rO\rO\r5.mSpace O OO\rO O\r4. Flamenco\rO\r6. Exhibit\r7. Relation Browser\rO O\rO\rO\rO\rO\rO\rO\rWidely\r(Continued)\rO\r\n5.1 The Taxonomy 77\rMultiple Interface Advances Retrieval Seeking Context Testing Studies\rDiverse Forms of Study\rDemonstrator Only\rInstances Available Used\r3.2.2 2D Result Representations\r18. SOMS O\r19. Treemaps O\r20. Hyperbolic Trees O O 21. Scatterplot/Matrix O\r22. Euler Circles O O 23. Geographical Plots O O 24. Timelines O O 25. Per-Query-Term O O\rO O\rO O\rRelevance\r26. User-Organized O O\rO O\rCanvases\r3.2.3 3D Result Representations\r27. User-Organized 3D O O Spaces\rO\r28. 3D Hyperbolic Trees O O\r29. Cone Trees O O 30. 3D VR Exploration O O\rO O O\r3.3 Additional Functions\r31. Multimedia Previews O O 32. Content O O\rO O\rSummarization\r33. Similar Results O O\r34. Results Connections O O 35. Animated O O\rO\rTransitions\r36. Semantic Zooming O O\r37. Query-by-Example O O\rO\rTable 5.1.\r(Continued)\rDepth of Evaluation\rLevel of Search Support\rInformation Information Work Initial User\rPrevalence Online Many\rO\rO\rO\rO O\rO\rO\rO\rO\rWidely\r\n78 Taxonomy of Search Visualizations\rinterface technique may support, to some extent, each of the three levels, they have been allocated to the level at which they are pri- marily designed to support. In fact, a lengthier alternative analysis of these interface advances could discuss the way in which the advances all contribute to each level of search context.\rThe second facet, shown in the second set of three columns, rep- resents the amount of study that the techniques have received. While these could be categorized into groups such as formative studies, empir- ical user studies, longitudinal log analyses, etc. we are more interested, here, to learn the diversity of evidence that has been produced. They could have also been categorized by whether they have been studied in the same dimensions as they have been designed to support. Many studies, however, include information retrieval and information-seeking tasks, and so they may have received relatively little study, but in both contexts. Some techniques have received little published study and eval- uation, like allowing users to view similar results to any one result returned [#33 in taxonomy], and so our understanding of their benefits is through experience and intuition. Other techniques, such as treemaps and SOMs [#18, #19], have received much evaluation and optimiza- tion since they were first proposed, including initial testing, carefully constructed lab studies, and analyses of working online deployments.\rThe third facet, contained in the final set of three columns, cap- tures how prevalent visualizations have become on the Web. The items familiar in most search engines, such as “similar results” and “keyword- in-context result snippets”, are dominant, regardless of how well they have been studied, or to what level of context they support searchers. Other techniques, however, have been well studied but are not yet in widespread use, such as faceted browsing on general Web search engines [#8, #9]. It might be noted that the most common model of faceted browsing, represented by Flamenco [#4], is more prevalent on the Web than some of the alternatives. Google product search,1 for example, provides this kind of typical faceted interaction to filter the results by price ranges and even specific vendors.\r1 http://www.google.com/products.\r\n5.2 Using the Taxonomy 79\rThese three facets, in combination, provide insights into the advances that have been made in web search visualizations. The diagram shows, for example, advances that are highly prevalent on the Web, but have received little published evaluation, and only support a low information retrieval context of search, such as web directories [#1]. Similarly, and perhaps more important for some readers, we can see heavily studied advances that are not yet prevalent on the Web, such as the faceted model of search provided by mSpace [#5].\r5.2 Using the Taxonomy\rThere are two main audiences for the taxonomy above: (1) search inter- face designers and (2) academics working on future advances in this area. Similarly, the content of the taxonomy and the gaps in the dia- gram provide useful information to designers and academics. These uses are discussed below.\rOne challenge in building web search interfaces is in choosing the best combination of tools and features that will best support the tar- get audience, if known. Clearly, there are many advances that can be used, but using lots, or even all of them, would provide a cluttered and perhaps unusable interface. For designers working with such a chal- lenge, the taxonomy acts as a guide to identifying potential appropriate options. Further, the gaps in the diagram highlight areas where new and novel ideas might make a distinct interface and provide a business edge.\rFor academics, the taxonomy provides two types of information. First, much prior art is discussed in this article and included in the diagram, which can be used to help identify related work. Second, and more importantly, however, the diagram quite clearly marks under- researched ideas and research gaps. One conclusion that can be drawn from the diagram is that there has been a lot of work on information- seeking level advances, but comparatively little work that has focused on work-contexts. Section 4.3 discusses some studies that have focused on work context scenarios, and some recent work has focused on tasks such as booking holidays, writing papers, and even collaborating with other people [125]. For the many existing advances that are captured by\r\n80 Taxonomy of Search Visualizations\rthe taxonomy, however, seeing how different techniques relate to each other across the captured dimensions raises some un-answered research questions. Academics may want to compare well studied, but less pop- ular advances, with those that are popular despite having received little published evaluation.\r\n6\rConclusions\rThis monograph presented four steps in discussing the advances in visualizing web search results. First, we presented previous and related work, which was followed by a model of search that has been used throughout the document. This model of search includes multiple levels of context from basic Information Retrieval needs, to more exploratory information-seeking behavior, and finally the context of the tasks being completed by users, known as the work context. Second, we offered a wide range of innovations in web search interfaces, while discussing how each makes contributions to the three levels of search context. Third, Section 4 discussed the techniques that have been used to evaluate these advances at the three different levels of search context. We discussed the increasing difficulty that evaluators experience while studying the contributions of new designs at higher-level work contexts, as regular simple measures such as time and accuracy of specific low-level tasks do not necessarily apply. Finally, we presented a taxonomy in Section 5 that captures the interface advances. The taxonomy captures (1) which level of search context the advance primarily support, (2) how much study the advances have received, and (3) how prevalent the advances are on the Web.\r81\r\n82 Conclusions\rAs well as supporting readers in finding much of the prior art that exists in web search result visualization, this monograph and taxon- omy helps the designers of future search systems to make informed choices about which advances may be appropriate for their system and audience. Further, the taxonomy helps identify under-researched ideas and research gaps in web search result visualization. Notably, there have been far fewer advances that consider the higher-level work con- texts of searchers. Consequently we advocate that, in order to better understand novel ideas, future evaluations focus on the work contexts of users, and in supporting them to achieve their higher-level goals.\rWe began this monograph by celebrating the success of keyword search, while clarifying when alternative modes of search are needed. First, as users’ demands continue to grow and their needs evolve, oppor- tunities are emerging for exploratory search strategies. Often users want to get answers to their questions, not just web pages. Second, their questions are increasingly complex and may takes hours or weeks to resolve. A third change is the move toward new Semantic Web tech- nologies and related strategies, which offer improved possibilities for machine-assisted resolution for complex queries, especially with key- words which have multiple meanings. We have shown that there are emerging innovative user interfaces and visual presentations that may help users achieve their goals more rapidly and with greater confidence in the validity of the results. Through empirical studies and log anal- ysis, researchers are coming to better understand the ways in which people search, the tasks they have, and how they collaborate with their colleagues. This survey monograph provides a resource to the design- ers and researchers who are developing search systems so that they can more make more informed and confident design decisions as they create novel and effective interfaces.\r\nAcknowledgments\rThe authors thank Marti Hearst, those who provided images, and sev- eral others for providing feedback and comments.\r83\r\nReferences\r[1] F. J. Aguilar, General Managers in Action. New York, NY: Oxford University Press, 1988.\r[2] H. Alani and C. Brewster, “Ontology ranking based on the analysis of concept structures,” in Proceedings of the 3rd International Conference on Knowledge Capture, pp. 51–58, New York, NY, USA: ACM Press, 2005.\r[3] J. Allan, “Hard track overview in trec 2003 high accuracy retrieval from doc- uments,” in Proceedings of the Text Retrieval Conference, pp. 24–37, 2003.\r[4] B. Allen, “Information space representation in interactive systems: Relation- ship to spatial abilities,” in Proceedings of the Third ACM Conference on Digital Libraries, pp. 1–10, Pittsburgh, Pennsylvania, United States: ACM Press, 1998.\r[5] R. Allen, “Two digital library interfaces that exploit hierarchical structure,” in Proceedings of Electronic Publishing and the Information Superhighway, pp. 134–141, Boston, MA, USA, 1995.\r[6] J. Allsop, Microformats: Empowering Your Markup for Web 2.0. friends of ED, 2007.\r[7] R. Amar and J. Stasko, “A knowledge task-based framework for design and evaluation of information visualizations,” in Proceedings of the IEEE Sympo- sium on Information Visualization, pp. 143–150, Austin, Texas, USA: IEEE Computer Society, 2004.\r[8] B. Amento, W. Hill, L. Terveen, D. Hix, and P. Ju, “An empirical evaluation of user interfaces for topic management of web sites,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 552–559, Pittsburgh, Pennsylvania, United States: ACM Press, 1999.\r84\r\nReferences 85\r[9] P.Andr ́e,M.L.Wilson,A.Russell,D.A.Smith,A.Owens,andm.c.schraefel, “Continuum: Designing timelines for hierarchies, relationships and scale,” in Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology, pp. 101–110, Newport, Rhode Island, USA: ACM Press, 2007.\r[10] P. Au, M. Carey, S. Sewraz, Y. Guo, and S. Ru ̈ger, “New paradigms in infor- mation visualization,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and Development in Information Retrieval, pp. 307–309, Athens, Greece: ACM Press, 2000.\r[11] M. J. Bates, “Idea tactics,” Journal of the American Society for Information Science, vol. 30, no. 5, pp. 280–289, 1979.\r[12] M. J. Bates, “Information search tactics,” Journal of the American Society for Information Science, vol. 30, no. 4, pp. 205–214, 1979.\r[13] M. J. Bates, “The design of browsing and berrypicking techniques for the online search interface,” Online Review, vol. 13, no. 5, pp. 407–424, 1989.\r[14] P. Baudisch, D. Tan, M. Collomb, D. Robbins, K. Hinckley, M. Agrawala, S. Zhao, and G. Ramos, “Phosphor: Explaining transitions in the user interface using afterglow effects,” in Proceedings of the 19th annual ACM symposium on User Interface Software and Technology, pp. 169–178, Montreux, Switzerland: ACM Press, 2006.\r[15] R. Beale, R. J. Mcnab, and I. H. Witten, “Visualising sequences of queries: A new tool for information retrieval,” in Proceedings of the IEEE Conference on Information Visualisation, pp. 57–63, Phoenix, AZ, USA: IEEE Computer Society, 1997.\r[16] A.Becks,C.Seeling,andR.Minkenberg,“Benefitsofdocumentmapsfortext access in knowledge management: A comparative study,” in Proceedings of the ACM Symposium on Applied Computing, pp. 621–626, Madrid, Spain: ACM Press, 2002.\r[17] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder, “Hourly analysis of a very large topically categorized web query log,” in Pro- ceedings of the 27th annual international ACM SIGIR conference on Research and Development in Information Retrieval, pp. 321–328, Sheffield, UK: ACM Press, 2004.\r[18] N.J.Belkin,C.Cool,A.Stein,andU.Thiel,“Cases,scripts,andinformation- seeking strategies: On the design of interactive information retrieval systems,” Expert Systems with Applications, vol. 9, no. 3, pp. 379–395, 1995.\r[19] N. J. Belkin, P. G. Marchetti, and C. Cool, “Braque: Design of an interface to support user interaction in information retrieval,” Information Processing & Management, vol. 29, no. 3, pp. 325–344, 1993.\r[20] S. Benford, I. Taylor, D. Brailsford, B. Koleva, M. Craven, M. Fraser, G. Rey- nard, and C. Greenhalgh, “Three dimensional visualization of the world wide web,” ACM Computing Surveys, vol. 31, no. 4, p. 25, 1999.\r[21] T.Berners-Lee,Y.Chen,L.Chilton,D.Connolly,R.Dhanaraj,J.Hollenbach, A. Lerer, and D. Sheets, “Tabulator: Exploring and analyzing linked data on the semantic web,” in Proceedings of the 3rd International Semantic Web User Interaction Workshop, 2006.\r\n86 References\r[22] S. K. Bhavnani, “Important cognitive components of domain-specific search knowledge,” in NIST Special Publication 500-250: The Tenth Text Retrieval Conference, (E. M. Vorhees and D. K. Harman, eds.), Washington, DC: NIST, 2001.\r[23] S. K. Bhavnani and M. J. Bates, “Separating the knowledge layers: Cognitive analysis of search knowledge through hierarchical goal decompositions,” in Proceedings of the American Society for Information Science and Technology Annual Meeting, pp. 204–213, Medford, NJ: Information Today, 2002.\r[24] D. C. Blair and M. E. Maron, “An evaluation of retrieval effectiveness for a full-text document retrieval system,” Communications of the ACM, vol. 28, no. 3, pp. 289–299, 1985.\r[25] S. Bloehdorn, K. Petridis, C. Saathoff, N. Simou, V. Tzouvaras, Y. Avrithis, S. Handschuh, I. Kompatsiaris, S. Staab, and M. G. Strintzis, “Semantic anno- tation of images and videos for multimedia analysis,” in Proceedings of the 2nd European Semantic Web Conference, pp. 592–607, Heraklion, Crete, Greece: Springer, 2005.\r[26] P. Borlund, “The IIR evaluation model: A framework for evaluation of inter- active information retrieval systems,” Information Research, vol. 8, no. 3, p. Paper 152, 2003.\r[27] K. B ̈orner, “Visible threads: A smart vr interface to digital libraries,” in Pro- ceedings of IST/SPIE’s 12th Annual International Symposium on Visual Data Exploration and Analysis, pp. 228–237, San Jose, CA, USA: SPIE, 2000.\r[28] A. Broder, “A taxonomy of web search,” ACM SIGIR Forum, vol. 36, no. 2, pp. 3–10, 2002.\r[29] K. Bystr ̈om and P. Hansen, “Work tasks as units for analysis in informa- tion seeking and retrieval studies,” in Emerging Frameworks and Methods, (H. Bruce, R. Fidel, P. Ingwersen, and P. Vakkari, eds.), Greenwood Village, CO: Libraries Unlimited, 2002.\r[30] K. Bystr ̈om and P. Hansen, “Conceptual framework for tasks in information studies: Book reviews,” Journal American Society Information Science and Technology, vol. 56, no. 10, pp. 1050–1061, 2005.\r[31] G. Cai, “Geovibe: A visual interface for geographic digital libraries,” in Proceedings of the First Visual Interfaces to Digital Libraries Workshop, (K. Borner and C. Chen, eds.), pp. 171–187, Roanoke, VA, USA: Springer- Verlag, 2002.\r[32] R. Capra and G. Marchionini, “The Relation Browser tool for faceted exploratory search,” in Proceedings of the 2008 Conference on Digital Libraries (JCDL ’08), Pittsburgh, Pennsylvania, June 2008.\r[33] R. Capra, G. Marchionini, J. S. Oh, F. Stutzman, and Y. Zhang, “Effects of structure and interaction style on distinct search tasks,” in Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 442–451, Vancouver, British Columbia, Canada: ACM Press, 2007.\r[34] S. Chakrabarti, B. Dom, D. Gibson, S. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins, “Experiments in topic distillation,” in ACM SIGIR work- shop on Hypertext Information Retrieval on the Web, pp. 13–21, Melbourne, Australia, 1998.\r\nReferences 87\r[35] C. Chen, “Citespace II: Detecting and visualizing emerging trends and tran- sient patterns in scientific literature,” Journal of the American Society for Information Science and Technology, vol. 57, no. 3, pp. 359–377, 2006.\r[36] H. Chen and S. Dumais, “Bringing order to the web: Automatically categoriz- ing search results,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 145–152, The Hague, The Netherlands: ACM Press, 2000.\r[37] H. Chen, A. L. Houston, R. R. Sewell, and B. R. Schatz, “Internet brows- ing and searching: User evaluations of category map and concept space techniques,” Journal of the American Society for Information Science, vol. 49, no. 7, pp. 582–608, 1998.\r[38] M. Chen, M. Hearst, J. Hong, and J. Lin, “Cha-cha: A system for organizing intranet search results,” in Proceedings of the 2nd USENIX Symposium on Internet Technologies and Systems, Boulder, CO, 1999.\r[39] C. W. Choo, B. Detlor, and D. Turnbull, Web Work: Information Seeking and Knowledge Work on the World Wide Web. Dordrecht, The Netherlands: Kluwer Academic Publishers, 2000.\r[40] C.W.Cleverdon,J.Mills,andM.Keen,“Factorsdeterminingtheperformance of indexing systems,” in ASLIB Cranfield Project, Cranfield, 1966.\r[41] A. Cockburn and B. Mckenzie, “An evaluation of cone trees,” in Proceedings of the British Computer Society Conference on Human-Computer Interaction, Sheffield, UK: Springer-Verlag, 2000.\r[42] A. Cockburn and B. Mckenzie, “Evaluating the effectiveness of spatial mem- ory in 2D and 3D physical and virtual environments,” in Proceedings of the SIGCHI conference on Human factors in computing systems: Changing our world, changing ourselves, pp. 203–210, Minneapolis, Minnisota, USA, 2002.\r[43] W. S. Cooper, “On selecting a measure of retrieval effectiveness,” Journal of the American Society for Information Science, vol. 24, no. 2, pp. 87–100, 1973.\r[44] J. Cugini, C. Piatko, and S. Laskowski, “Interactive 3D visualization for doc- ument retrieval,” in Proceedings of the Workshop on New Paradigms in Infor- mation Visualization and Manipulation, ACM Conference on Information and\rKnowledge Management, Rockville, Maryland, USA, 1996.\r[45] E. Cutrell and Z. Guan, “What are you looking for?: An eye-tracking study of information usage in web search,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 407–416, San Jose, California,\rUSA: ACM press, 2007.\r[46] F. Das-Neves, E. A. Fox, and X. Yu, “Connecting topics in document col-\rlections with stepping stones and pathways,” in Proceedings of the 14th ACM International Conference on Information and Knowledge Management, pp. 91–98, Bremen, Germany: ACM Press, 2005.\r[47] L. Ding, A. J. T. Finin, R. Pan, R. Cost, Y. Peng, P. Reddivari, V. Doshi, and J. Sachs, “Swoogle: A search and metadata engine for the semantic web,” in Proceedings of the thirteenth ACM international conference on Information and knowledge management, pp. 652–659, Washington DC, USA, ACM New York, NY, USA, 2004.\r\n88 References\r[48] K. Doan, C. Plaisant, and B. Shneiderman, “Query previews in networked information systems,” in Proceedings of the Third Forum on Research and Technology Advances in Digital Libraries, pp. 120–129, Washington, DC, USA: IEEE Computer Society, 1996.\r[49] O. Drori and N. Alon, “Using documents classification for displaying search results list,” Journal of Information Science, vol. 29, no. 2, pp. 97–106, 2003.\r[50] S. Dumais, E. Cutrell, and H. Chen, “Optimizing search by showing results in context,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Seattle, WA, New York: ACM Press, 2001.\r[51] S.T.DumaisandN.J.Belkin,“TheTRECinteractivetracks:Puttingtheuser into search,” in TREC: Experiment and Evaluation in Information Retrieval, (E. Voorhees and D. Harman, eds.), MIT Press, 2005.\r[52] M. D. Dunlop, “Time, relevance and interaction modelling for information retrieval,” SIGIR Forum, vol. 31, no. SI, pp. 206–213, 1997.\r[53] C. Eaton and H. Zhao, Visualizing Web Search Results. 2001.\r[54] M. Efron, J. Elsas, G. Marchionini, and J. Zhang, “Machine learning for infor- mation architecture in a large governmental website,” in Proceedings of the 4th ACM/IEEE-CS Joint Conference on Digital libraries, pp. 151–159, Tuscon,\rAZ, USA: ACM Press, 2004.\r[55] D. E. Egan, J. R. Remde, L. M. Gomez, T. K. Landauer, J. Eberhardt, and\rC. C. Lochbaum, “Formative design evaluation of superbook,” ACM Trans-\ractions on Information Systems, vol. 7, no. 1, pp. 30–57, 1989.\r[56] S. Elbassuoni, J. Luxenburger, and G. Weikum, “Adaptive personalization of web search,” in Workshop on Web Information Seeking and Interaction at\rSIGIR, pp. 23–30, Amsterdam, Netherlands, 2007.\r[57] D.Ellis,“Abehavioralmodelforinformationretrievalsystemdesign,”Journal\rof Information Science, vol. 15, no. 4–5, pp. 237–247, 1989.\r[58] D. Elsweiler and I. Ruthven, “Towards task-based personal information man- agement evaluations,” in Proceedings of the 30th annual international ACM SIGIR conference on Research and Development in Information Retrieval,\rpp. 23–30, Amsterdam, The Netherlands, 2007.\r[59] P. Faraday and A. Sutcliffe, “Authoring animated web pages using “contact\rpoints”,” in Proceedings of the SIGCHI conference on Human factors in com- puting systems: The CHI is the limit, pp. 458–465, Pittsburgh, PA, USA: ACM Press, 1999.\r[60] Y. Feng and K. Borner, “Using semantic treemaps to categorize and visualize bookmark files,” in Proceedings of SPIE — Volume 4665, Visualization and Data Analysis 2002, (R. F. Erbacher, P. C. Chen, M. Groehn, J. C. Roberts, and C. M. Wittenbrink, eds.), Bellingham, WA, USA: SPIE–The International Society for Optical Engineering, 2002.\r[61] V. Florance and G. Marchionini, “Information processing in the context of medical care,” in Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 158– 163, Seattle, WA, USA: ACM Press, 1995.\r\nReferences 89\r[62] G. W. Furnas and S. J. Rauch, “Considerations for information environments and the navique workspace,” in Proceedings of the Third ACM Conference on Digital libraries, pp. 79–88, Pittsburgh, PA, USA: ACM Press, 1998.\r[63] M. Gao, C. Liu, and F. Chen, “An ontology search engine based on seman- tic analysis,” in ICITA’05. Third International Conference on Information Technology and Applications, 2005, pp. 256–259, IEEE, 2005.\r[64] M. Ginsburg, “Visualizing digital libraries with open standards,” Commu- nications of the Association for Information Systems, vol. 13, pp. 336–356, 2004.\r[65] R.Godwin-Jones,“Emergingtechnologiestagcloudsintheblogosphere:Elec- tronic literacy and social networking,” Language Learning and Technology, vol. 10, no. 2, pp. 8–15, 2006.\r[66] L. A. Granka, T. Joachims, and G. Gay, “Eye-tracking analysis of user behavior in WWW search,” in Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, Sheffield, United Kingdom: ACM Press, 2004.\r[67] A.GulliandA.Signorini,“Theindexablewebismorethan11.5billionpages,” in Proceedings of the 14th International Conference on the World Wide Web, pp. 902–903, Chiba, Japan: ACM Press, 2005.\r[68] J. Haajanen, M. Pesonius, E. Sutinen, J. Tarhio, T. Terasvirta, and P. Vanni- nen, “Animation of user algorithms on the web,” in Proceedings of IEEE Sym- posium on Visual Languages, pp. 356–363, Isle of Capri, Italy: IEEE Computer Society, 1997.\r[69] W. Hall and K. O’Hara, “Semantic web,” in Encyclopedia of Complexity and Systems Science, (R. A. Meyers, ed.), Springer, 2009.\r[70] D. K. Harman, “The TREC conferences,” in Readings on information retrieval, San Francisco, CA, USA: Morgan Kaufmann Publishers Inc, 1997.\r[71] M.Hearst,“Theuseofcategoriesandclustersfororganizingretrievalresults,” in Natural Language Information Retrieval, (T. Strzalkowski, ed.), Boston: Kluwer Academic Publishers, 1999.\r[72] M. Hearst, Search User Interfaces. Cambridge University Press, 2009.\r[73] M.Hearst,A.Elliot,J.English,R.Sinha,K.Swearingen,andP.Yee,“Finding the flow in web site search,” Communications of the ACM, vol. 45, no. 9,\rpp. 42–49, 2002.\r[74] M. Hearst and J. Pedersen, “Reexamining the cluster hypothesis: Scatter/\rgather on retrieval results,” in Proceedings of the 19th Annual Interna- tional ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich, Switzerland, New York: ACM Press, 1996.\r[75] M. A. Hearst, “Next generation web search: Setting our sites,” IEEE Data Engineering Bulletin: Special Issue on Next Generation Web Search, vol. 23, no. 3, pp. 38–48, 2000.\r[76] M. Hildebrand, J. V. Ossenbruggen, and L. Hardman, “/facet: A browser for heterogeneous semantic web repositories,” in Proceedings of the 5th Interna- tional Conference on the Semantic Web (ISWC’06), pp. 272–285, Athens, GA, USA, 2006.\r\n90 References\r[77] O. Hoeber and X. D. Yang, “The visual exploration of web search results using hotmap,” in Proceedings of the International Conference on Information Visualization, pp. 272–285, London, UK: IEEE Computer Society, 2006.\r[78] C.HolscherandG.Strube,“Websearchbehaviorofinternetexpertsandnew- bies,” in Proceedings of the 9th International WWW Conference, pp. 157–165, 2000.\r[79] A. Holzinger and M. Ebner, “Interaction and usability of simulations and ani- mations: A case study of the flash technology,” in Proceedings of IFIP TC13 International Conference on Human Computer Interaction, pp. 777–780, Zu ̈rich, Switzerland: IOS Press, 2003.\r[80] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, “Information retrieval in folksonomies: Search and ranking,” The Semantic Web: Research and Appli- cations, vol. 4011, pp. 411–426, 2006.\r[81] D. F. Huynh, R. Miller, and D. Karger, “Exhibit: Lightweight structured data publishing,” in Proceedings of the World Wide Web Conference, pp. 737–746, Banff, Alberta, Canada: ACM Press, 2007.\r[82] B. Jansen and A. Spink, “An analysis of web information seeking and use: Documents retrieved versus documents viewed,” in Proceedings of the 4th international conference on Internet computing, pp. 65–69, Las Vagas, NV, USA, 2003.\r[83] B. Jansen, A. Spink, J. Bateman, and T. Saracevic, “Real life information retrieval: A study of user queries on the web,” ACM SIGIR Forum, vol. 32, no. 1, pp. 5–17, 1998.\r[84] K. J ̈arvelin and P. Ingwersen, “Information seeking research needs extension towards tasks and technology,” Information Research, vol. 10, no. 1, p. paper 212, 2004.\r[85] K. Jarvelin and J. Kekalainen, “IR evaluation methods for retrieving highly relevant documents,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 41–48, Athens, Greece, ACM, 2000.\r[86] J. Jeon, V. Lavrenko, and R. Manmatha, “Automatic image annotation and retrieval using cross-media relevance models,” in Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pp. 119–126, Toronto, Canada: ACM Press, 2003.\r[87] T.Joachims,TextCategorizationwithSupportVectorMachines:Learningwith Many Relevant Features. Springer, 1997.\r[88] S. Kabel, R. De Hoog, B. J. Wielinga, and A. Anjewierden, “The added value of task and ontology-based markup for information retrieval,” Jour- nal of the American Society for Information Science and Technology, vol. 55, no. 4, pp. 348–362, 2004.\r[89] M. K ̈aki, “Findex: Search result categories help users when document ranking fails,” in Proceeding of the SIGCHI Conference on Human Factors in Com- puting Systems, pp. 131–140, Portland, OR, USA: ACM Press, 2005.\r[90] Y. Kammerer, R. Narin, P. Pirolli, and E. Chi, “Signpost from the masses: Learning effecs in an exploratory social tag search browser,” in Proceedings\r\nReferences 91\rof the 27th international conference on Human factors in computing systems,\rpp. 625–634, Boston, MA: ACM Press, 2009.\r[91] H. Kang and B. Shneiderman, “Exploring personal media: A spatial interface\rsupporting user-defined semantic regions,” Journal of Visual Languages &\rComputing, vol. 17, no. 3, pp. 254–283, 2006.\r[92] J. Kari, “Evolutionary information seeking: A case study of personal develop-\rment and internet searching,” First Monday, vol. 11, no. 1, 2006.\r[93] T.Kato,T.Kurita,N.Otsu,andK.Hirata,“Asketchretrievalmethodforfull color image database-query byvisual example,” in Proceedings of 11th IAPR International Conference on Pattern Recognition, pp. 530–533, The Hague,\rThe Netherlands: IEEE Computer Society, 1992.\r[94] D. Kelly, S. Dumais, and J. Pedersen, “Evaluation challenges and directions\rfor information-seeking support systems,” Computer, vol. 42, no. 3, pp. 60–66,\r2009.\r[95] A.Kerne,E.Koh,B.Dworaczyk,J.M.Mistrot,H.Choi,M.S.Smith,R.Grae-\rber, D. Caruso, A. Webb, R. Hill, and J. Albea, “Combinformation: A mixed- initiative system for representing collections as compositions of image and text surrogates,” in Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, pp. 11–20, Chapel Hill, NC, USA: ACM Press, 2006.\r[96] M.KobayashiandK.Takeda,“Informationretrievalontheweb,”ACMCom- puting Surveys, vol. 32, no. 2, pp. 144–173, 2000.\r[97] T. Kohonen, Self-Organizing Maps. Springer, 2001.\r[98] S. Koshman, “Testing user interaction with a prototype visualization-based\rinformation retrieval system,” Journal of the American Society for Informa-\rtion Science and Technology, vol. 56, no. 8, pp. 824–833, 2005.\r[99] N.Kosugi,Y.Nishihara,T.Sakata,M.Yamamuro,andK.Kushima,“Aprac- tical query-by-humming system for a large music database,” in Proceedings of the eighth ACM international conference on Multimedia, pp. 333–342, New\rYork, NY, USA: ACM Press, 2000.\r[100] B. Kotelly, “Resonance: Introducing the concept of penalty-free deep look\rahead with dynamic summarization of arbitrary results sets,” in Work- shop on Human-Computer Interaction and Information Retrieval, Cambridge Mssachusetts, USA: MIT CSAIL, 2007.\r[101] C. C. Kuhlthau, “Inside the search process: Information seeking from the user’s perspective,” Journal of the American Society for Information Science, vol. 42, no. 5, pp. 361–371, 1991.\r[102] B. Kules, “Methods for evaluating changes in search tactics induced by exploratory search systems,” in ACM SIGIR 2006 Workshop on Evaluating Exploratory Search Systems, Seattle, WA, 2006.\r[103] B. Kules and R. Capra, “Creating exploratory tasks for a faceted search inter- face,” in Second Workshop on Human-Computer Interaction and Information Retrieval (HCIR’08), Seattle, WA, USA, 2008.\r[104] B. Kules, R. Capra, M. Banta, and T. Sierra, “What do exploratory searchers look at in a faceted search interface?,” in Proceedings of the 9th ACM/IEEE- CS joint conference on Digital libraries, pp. 313–322, Austin, TX, USA: ACM Press, 2009.\r\n92 References\r[105]\r[106]\r[107] [108]\r[109]\r[110]\r[111]\r[112] [113]\r[114]\r[115] [116] [117] [118]\r[119]\r[120]\rB. Kules, J. Kustanowitz, and B. Shneiderman, “Categorizing web search results into meaningful and stable categories using fast-feature techniques,” in Proceedings of the Sixth ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 210–219, Chapel Hill, NC: ACM Press, 2006.\rB. Kules and B. Shneiderman, “Users can change their web search tactics: Design guidelines for categorized overviews,” Information Processing & Man- agement, vol. 44, no. 2, pp. 463–484, 2008.\rC. Kunz, “Sergio — an interface for context driven knowledge retrieval,” in Proceedings of eChallenges, Bologna, Italy, 2003.\rK. Lagus, S. Kaski, and T. Kohonen, “Mining massive document collections by the WEBSOM method,” Information Sciences: An International Journal, vol. 163, no. 1–3, pp. 135–156, 2004. J.LampingandR.Rao,“Thehyperbolicbrowser:Afocus+contexttechnique for visualizing large hierarchies,” Journal of Visual Languages and Computing, vol. 7, no. 1, pp. 33–55, 1996.\rX. Lin, D. Soergel, and G. Marchionini, “A self-organizing semantic map for information retrieval,” in Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval, Chicago, Illinois, United States, New York, NY, USA: ACM Press, 1991. L.Lorigo,H.H.B.Pan,T.Joachims,L.Granka,andG.Gay,“Theinfluenceof task and gender on search and evaluation behavior using google,” Information Processing and Management, vol. 42, no. 4, pp. 1123–1131, 2006.\rR. Losee, Text Retrieval and Filtering: Analytic Models of Performance. Kluwer Acadedmic Publishers, 1998.\rA. Lunzer and K. Hornbæk, “Side-by-side display and control of multiple scenarios: Subjunctive interfaces for exploring multi-attribute data,” in Pro- ceedings of Australian Special Interest Group Conference on Computer-Human Interaction, Brisbane, Australia, 2003.\rB. Mackay and C. Watters, “Exploring multi-session web tasks,” in CHI ’08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, Florence, Italy: ACM Press, 2008.\rG. Marchionini, Information Seeking in Electronic Environments. Cambridge University Press, 1995.\rG. Marchionini, “Exploratory search: From finding to understanding,” Com- munications of the ACM, vol. 49, no. 4, pp. 41–46, 2006.\rG. Marchionini and B. Shneiderman, “Finding facts vs. browsing knowledge in hypertext systems,” Computer, vol. 21, no. 1, pp. 70–80, 1988. B.Marshall,D.Mcdonald,H.Chen,andW.Chung,“Ebizport:Collectingand analyzing business intelligence information,” Journal of the American Society for Information Science and Technology, no. 10, pp. 873–891, 2004. K.Mcpherson,“Opinion-relatedinformationseeking:Personalandsituational variables,” Personality and Social Psychology Bulletin, vol. 9, no. 1, p. 116, 1983. Y.Medynskiy,M.Dontcheva,andS.M.Drucker,“Exploringwebsitesthrough contextual facets,” in Proceedings of the 27th International Conference on Human Factors in Computing Systems, pp. 2013–2022, Boston, MA, USA: ACM Press, 2009.\r\n[121]\r[122] [123] [124] [125]\r[126]\r[127]\r[128]\r[129]\r[130]\r[131]\r[132] [133]\r[134]\r[135]\rReferences 93\rD. R. Millen, J. Feinberg, and B. Kerr, “Dogear: Social bookmarking in the enterprise,” in Proceedings of the SIGCHI conference on Human Factors in computing systems, pp. 111–120, Montr ́eal, Qu ́ebec Canada: ACM Press, 2006. G. A. Miller, “Wordnet: A lexical database for english,” Communications of the ACM, vol. 38, no. 11, p. 39, 1995.\rD. K. Modjeska, Hierarchical Data Visualization In Desktop Virtual Reality. University of Toronto, 2000. C.N.Mooers,“‘mooers’laworwhysomeretrievalsystemsareusedandothers are not,” American Documentation, vol. 11, no. 3, 1960.\rM. R. Morris, “A survey of collaborative web search practices,” in Proceed- ings of the SIGCHI Conference on Human Factors in Computing Systems, Florence, Italy: ACM Press, 2008.\rA. Motro, “Vague: A user interface to relational databases that permits vague queries,” ACM Transactions on Information Systems, vol. 6, no. 3, pp. 187– 214, 1988.\rT. Munzner, “H3: Laying out large directed graphs in 3D hyperbolic space,” in Proceedings of IEEE Symposium on Information Visualization, pp. 2–10, Phoenix, AZ, USA: IEEE Computer Society, 1997.\rD. A. Nation, C. Plaisant, G. Marchionini, and A. Komlodi, “Visualizing web- sites using a hierarchical table of contents browser: WebTOC,” in Proceedings of the Third Conference on Human Factors and the Web, Denver, CO, USA, 1997.\rJ. Nielsen and R. Molich, “Heuristic evaluation of user interfaces,” in CHI’90: Proceedings of the SIGCHI conference on Human factors in computing sys- tems, Seattle, WA, USA: ACM Press, 1990.\rA. Oulasvirta, J. P. Hukkinen, and B. Schwartz, “When more is less: The paradox of choice in search engine use,” in Proceedings of the 32nd interna- tional ACM SIGIR conference on Research and development in information retrieval, Boston, MA, USA: ACM Press, 2009.\rT. Paek, S. Dumais, and R. Logan, “Wavelens: A new view onto internet search results,” in Proceedings of the 2004 conference on Human factors in computing systems, pp. 727–734, Vienna, Austria: ACM Press, 2004.\rL. D. Paulson, “Building rich web applications with AJAX,” Computer, vol. 38, no. 10, pp. 14–17, 2005.\rS. Perugini, K. Mcdevitt, R. Richardson, M. Perez-Quiones, R. Shen, N. Ramakrishnan, C. Williams, and E. A. Fox, “Enhancing usability in citidel: Multimodal, multilingual, and interactive visualization interfaces,” in Proceed- ings of the 4th ACM/IEEE-CS joint conference on Digital libraries, pp. 315– 324, Tuscon, AZ, USA: ACM Press, 2004.\rP. Pirolli and S. Card, “Information foraging in information access environ- ments,” in Proceedings of the SIGCHI conference on Human factors in com- puting systems, pp. 51–58, Denver, CO, USA: ACM Press, 1995.\rC. Plaisant, B. Shneiderman, K. Doan, and T. Bruns, “Interface and data architecture for query preview in networked information systems,” ACM Transactions on Information Systems, vol. 17, no. 3, pp. 320–341, 1999.\r\n94 References\r[136] [137]\r[138]\r[139]\r[140]\r[141]\r[142]\r[143]\r[144]\r[145] [146]\r[147]\r[148]\r[149] [150]\rW. Pratt, “Dynamic organization of search results using the umls,” American Medical Informatics Association Fall Symposium, vol. 480, no. 4, 1997.\rY. Qu and G. Furnas, “Model-driven formative evaluation of exploratory search: A study under a sensemaking framework,” Information Processing and Management, vol. 44, no. 2, pp. 534–555, 2008.\rK. Risden, M. P. Czerwinski, T. Munzner, and D. B. Cook, “Initial examina- tion of ease of use for 2D and 3D information visualizations of web content,” International Journal of Human-Computers Studies, vol. 53, no. 5, pp. 695– 714, 2000.\rW. Rivadeneira and B. B. Bederson, A Study of Search Result Clustering Interfaces: Comparing Textual and Zoomable User Interfaces. University of Maryland: HCIL, 2003.\rG. Robertson, M. Czerwinski, K. Larson, D. C. Robbins, D. Thiel, and M. V. Dantzich, “Data mountain: Using spatial memory for document manage- ment,” in Proceedings of the 11th annual ACM symposium on User interface software and technology, pp. 53–162, San Francisco, CA, USA: ACM Press, 1998.\rG. Robertson, M. C. K. Cameron, and D. Robbins, “Polyarchy visualization: Visualizing multiple intersecting hierarchies,” in Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 423–430, Minneapolis, Minnesota, USA: ACM Press, 2002.\rG. G. Robertson, J. D. Mackinlay, and S. K. Card, “Cone trees: Animated 3D visualizations of hierarchical information,” in Proceedings of the SIGCHI con- ference on Human factors in computing systems: Reaching through technology, pp. 189–194, New Orleans, Louisiana, USA: ACM Press, 1991.\rS. E. Robertson and M. M. Hancock-Beaulieu, “On the evaluation of IR sys- tems,” Information Processing and Management, vol. 28, no. 4, pp. 457–466, 1992.\rS. Santini and R. Jain, “Beyond query by example,” in Proceedings of the sixth ACM international conference on Multimedia, pp. 345–350, Bristol, UK, 1998.\rm. c. schraefel, “Building knowledge: What’s beyond keyword search?,” Com- puter, vol. 42, no. 3, pp. 52–59, 2009.\rm. c. schraefel, D. A. Smith, A. Owens, A. Russell, C. Harris, and M. Wilson, “The evolving mSpace platform: Leveraging the semantic web on the trail of the memex,” in Proceedings of the Sixteenth ACM Conference on Hypertext and Hypermedia, pp. 174–183, Salzburg, Austria: ACM Press, 2005. m.c.schraefel,M.L.Wilson,andM.Karam,PreviewCues:EnhancingAccess to Multimedia Content. School of Electronics and Computer Science, Univer- sity of Southampton, 2004.\rm. c. schraefel, M. L. Wilson, A. Russell, and D. A. Smith, “mSpace: Improv- ing information access to multimedia domains with multimodal exploratory search,” Communications of the ACM, vol. 49, no. 4, pp. 47–49, 2006.\rB. Schwartz, The Paradox of Choice: Why More Is Less. Harper Perennial, 2005.\rM. M. Sebrechts, J. V. Cugini, S. J. Laskowski, J. Vasilakis, and M. S. Miller, “Visualization of search results: A comparative evaluation of text, 2D, and\r\n[151]\r[152] [153]\r[154]\r[155]\r[156]\r[157] [158]\r[159]\r[160]\r[161]\r[162]\r[163]\r[164]\rReferences 95\r3D interfaces,” in Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 3–10, Philadelphia, PA, USA: ACM Press, 1999.\rB. Shneiderman, “Designing information-abundant web sites: Issues and rec- ommendations,” International Journal of Human-Computer Studies, vol. 47, no. 1, pp. 5–29, 1997.\rB. Shneiderman, “Why not make interfaces better than 3D reality?,” Com- puter Graphics and Applications, IEEE, vol. 23, no. 6, pp. 12–15, 2003.\rB. Shneiderman and A. Aris, “Network visualization by semantic substrates,” IEEE Transactions on Visualization and Computer Graphics, vol. 12, no. 5, pp. 733–740, 2006.\rB. Shneiderman, D. Feldman, A. Rose, and X. F. Grau, “Visualizing digital library search results with categorical and hierarchial axes,” in Proceedings of the Fifth ACM International Conference on Digital Libraries, pp. 57–66, San Antonio, TX: ACM Press, 2000.\rB. Shneiderman and C. Plaisant, Designing the User Interface: Strategies for Effective Human-Computer Interaction. Boston: Pearson/Addison-Wesley, 2004.\rC. Silverstein, H. Marais, M. Henzinger, and M. Moricz, “Analysis of a very large web search engine query log,” ACM SIGIR Forum, vol. 33, no. 1, pp. 6– 12, 1999.\rJ. Sinclair and M. Cardew-Hall, “The folksonomy tag cloud: When is it use- ful?,” Journal of Information Science, vol. 34, no. 1, pp. 15–29, 2008.\rA. F. Smeaton, P. Over, and W. Kraaij, “Evaluation campaigns and TRECVid,” in Proceedings of the 8th ACM international workshop on Mul- timedia information retrieval, pp. 321–330, Santa Barbara, California, USA: ACM Press, 2006.\rA. Spink, “Study of interactive feedback during mediated information retrieval,” Journal of the American Society for Information Science, vol. 48, pp. 382–394, 1997.\rA. Spink, “A user-centered approach to evaluating human interaction with web search engines: An exploratory study,” Information Processing & Man- agement, vol. 38, no. 3, pp. 401–426, 2002.\rA. Spink and T. D. Wilson, “Toward a theoretical framework for information retrieval (IR) evaluation in an information seeking context,” in Proceedings of Multimedia Information Retrieval Applications, p. paper 9, Glasgow, UK: BCS, 1999. E.StoicaandM.A.Hearst,“Nearly-automatedmetadatahierarchycreation,” in Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, MA USA, 2004.\rA. G. Sutcliffe and U. Patel, “3D or not 3D: Is it nobler in the mind?,” in\rProceedings of the BCS Human Computer Interaction Conference on People and Computers XI, pp. 79–94, London, UK: Springer-Verlag, 1996.\rD. Swanson, N. Smalheiser, and A. Bookstein, “Information discovery from complementary literatures: Categorizing viruses as potential weapons,”\r\n96 References\r[165]\r[166]\r[167] [168] [169]\r[170]\r[171]\r[172] [173]\r[174] [175]\r[176] [177]\r[178]\r[179]\rJournal American Society Information Science and Technology, vol. 52, no. 10, pp. 797–812, 2001.\rE. Tanin, C. Plaisant, and B. Shneiderman, “Browsing large online data with query previews,” in Proceedings of the Symposium on New Paradigms in Infor- mation Visualization and Manipulation, Washington D.C., USA: ACM Press, 2000.\rJ. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, P. A. G. Ramos, and C. Hu, “Visual snippets: Summarizing web pages for search and revisitation,” in Pro- ceedings of the 27th international conference on Human factors in computing systems, pp. 2023–2032, Boston, MA, USA: ACM Press, 2009.\rE. R. Tufte, Envisioning Information. Graphics Press Cheshire, 1990.\rD. Tunkelang, Faceted Search. Morgan and Claypool, 2009.\rO. Turetken and R. Sharda, “Clustering-based visual interfaces for presenta- tion of web search results: An empirical investigation,” Information Systems Frontiers, vol. 7, no. 3, pp. 273–297, 2005.\rP. Vakkari, “Cognition and changes of search terms and tactics during task performance: A longditudinal case study,” in Proceedings of the Interna- tional Conference on Computer Assisted Information Retrieval (RAIO), Paris, France, 2000.\rP. Wang, M. Berry, and Y. Yang, “Mining londitudinal web queries: Trends and patterns,” Journal of the American Society for Information Science and Technology, vol. 54, no. 8, pp. 742–758, 2003.\rR. W. White, S. M. D. B. Kules, and m. c. schraefel, “Introduction,” Com- munications of the ACM, vol. 49, no. 8, pp. 36–39, 2006.\rR. W. White and S. M. Drucker, “Investigating behavioral variability in web search,” in Proceedings of the 16th International Conference on World Wide Web, Banff, Canada: ACM Press, 2007.\rR. W. White and R. Roth, Exploratory Search: Beyond the Query-Response Paradigm. Morgan & Claypool, 2009.\rR. W. White, I. Ruthven, and J. M. Jose, “Finding relevant documents using top ranking sentences: An evaluation of two alternative schemes,” in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 57–64, Tempere, Fin- land: ACM Press, 2002.\rM. Wilson, “mspace: What do numbers and totals mean in a flexible semantic browser,” SWUI’06, 2006.\rM. Wilson and m. c. schraefel, “Bridging the gap: Using ir models for evaluat- ing exploratory search interfaces,” in SIGCHI 2007 Workshop on Exploratory Search and HCI, San Jose, CA, USA, 2007.\rM. L. Wilson, “An analytical inspection framework for evaluating the search tactics and user profiles supported by information seeking interfaces,” PhD Thesis, University of Southampton, 2009.\rM. L. Wilson, P. Andr ́e, and m. c. schraefel, “Backward highlighting: Enhanc- ing faceted search,” in Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology, Monterey, CA, USA: ACM Press, 2008.\r\n[180]\r[181]\r[182]\r[183]\r[184]\r[185]\r[186]\r[187] [188]\r[189] [190]\r[191]\r[192]\rReferences 97\rM. L. Wilson, P. Andr ́e, D. A. Smith, and m. c. schraefel, Spatial Consistency and Contextual Cues for Incidental Learning in Browser Design. School of Electronics and Computer Science, University of Southampton, 2007.\rM. L. Wilson and m. c. schraefel, “mSpace: What do numbers and totals mean in a flexible semantic browser,” in Proceedings of the 3rd International Semantic Web User Interaction Workshop, Athens, GA, USA, 2006.\rM. L. Wilson and m. c. schraefel, “A longitudinal study of both faceted and keyword search,” in Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries, Pittsburgh, PA, USA: IEEE Computer Society, 2008.\rM. L. Wilson and m. c. schraefel, “The importance of conveying inter-facet relationships for making sense of unfamiliar domains,” in CHI’09 Workshop on Sensemaking, Boston, MA, USA, 2009.\rM. L. Wilson, m. c. schraefel, and R. W. White, “Evaluating advanced search interfaces using established information-seeking models,” Journal of the Amer- ican Society for Information Science and Technology, vol. 60, no. 7, pp. 1407– 1422, 2009.\rA. Woodruff, R. R. A. Faulring, J. Morrsion, and P. Pirolli, “Using thumb- nails to search the web,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 198–205, Seattle, Washington, USA: ACM Press, 2001.\rH. Wu, M. Zubair, and K. Maly, “Harvesting social knowledge from folk- sonomies,” in Proceedings of the seventeenth conference on Hypertext and hypermedia, pp. 111–114, Odense, Denmark: ACM Press, 2006.\rK. Yang, “Information retrieval on the web,” Annual Review of Information Science and Technology, vol. 39, pp. 33–80, 2005.\rK.-P. Yee, K. Swearingen, K. Li, and M. Hearst, “Faceted metadata for image search and browsing,” in Proceedings of the ACM Conference on Human fac- tors in Computing Systems, pp. 401–408, Fort Lauderdale, FL, USA: ACM Press, 2003.\rO. Zamir and O. Etzioni, “Grouper: A dynamic clustering interface to web search results,” Computer Networks, vol. 31, pp. 1361–1374, 1999.\rO. Zamir, O. Etzioni, O. Madani, and R. M. Karp, “Fast and intuitive clus- tering of web documents,” in Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining (KKD’97), pp. 287–290, Newport, CA, USA, 1997.\rH.-J. Zeng, Q.-C. HE, Z. Chen, W.-Y. Ma, and J. Ma, “Learning to cluster web search results,” in Proceedings of the 27th Annual International Confer- ence on Research and Dvelopment in Information Retrieval, Sheffield, United Kingdom, New York: ACM Press, 2004.\rJ. Zhang and G. Marchionini, “Evaluation and evolution of a browse and search interface: Relation browser,” in Proceedings of the National Conference on Digital Government Research, pp. 179–188, Atlanta, GA, USA: Digital Government Research Center, 2005.","title":"web-003.dvi","isStoredAs":"http://www.semanticdesktop.org/ontologies/2007/03/22/nfo/#LocalFileDataObject","contentHash":"cc535375f23dd31cb289d988bf86ac34f5cbc679","mimeType":"application/pdf","authors":[{"id":25,"firstName":"Yoshio","lastName":"19:56:25","middleNames":["Nitta","(TMD","Univ.","JP)","5051","2004","Feb","18"]}],"booktitle":"TeX output 2010.01.21:1649","keywords":[],"firstPage":0,"lastPage":0,"year":0,"volume":0,"tags":[{"@type":"Tag","text":"pp","time":1461915877430,"auto":true,"weight":1.0},{"@type":"Tag","text":"3d","time":1461915877533,"auto":true,"weight":1.0},{"@type":"Tag","text":"search","time":1461915877519,"auto":true,"weight":1.0},{"@type":"Tag","text":"proceed","time":1461915877504,"auto":true,"weight":1.0},{"@type":"Tag","text":"interfac","time":1461915877551,"auto":true,"weight":1.0},{"@type":"Tag","text":"web","time":1461915877568,"auto":true,"weight":1.0},{"@type":"Tag","text":"acm","time":1461915877473,"auto":true,"weight":1.0},{"@type":"Tag","text":"facet","time":1461915877459,"auto":true,"weight":1.0},{"@type":"Tag","text":"confer","time":1461915877489,"auto":true,"weight":1.0},{"@type":"Tag","text":"o","time":1461915877445,"auto":true,"weight":1.0}]}]}
